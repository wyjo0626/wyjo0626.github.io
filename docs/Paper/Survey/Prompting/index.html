<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Survey/2021-07-Prompting">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Survey/Prompting"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing | My Site"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Survey/Prompting"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Survey/Prompting" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Survey/Prompting" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.d2ad26d0.css">
<link rel="preload" href="/assets/js/runtime~main.947e1592.js" as="script">
<link rel="preload" href="/assets/js/main.d6888401.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Survey/Prompting">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Survey/PEFT for PVMs">Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Survey</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</h1></header><p>논문 및 이미지 출처 : <a href="https://dl.acm.org/doi/pdf/10.1145/3560815" target="_blank" rel="noopener noreferrer">https://dl.acm.org/doi/pdf/10.1145/3560815</a></p><h1>Abstract</h1><p>NLP 의 새로운 패러다임으로 <strong>prompt-based learning</strong> 이 등장</p><ul><li>기존 방식 supervised learning<ul><li>input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 를 사용하여 output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 을 예측하도록 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(y|x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> 를 훈련</li></ul></li><li><strong>prompt-based learning</strong><ul><li>텍스트의 확률 직접적으로 모델링</li><li>기존의 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 를 채워지지 않은 공백(unfilled slot)을 가진 텍스트 문자 prompt <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">x&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> 로 수정하여 사용</li><li>그 후, 미입력된 정보를 확률적으로 채워 최종 문자열 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">x</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.2222em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span> 를 얻음. 이를 최종 output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 으로 유도함</li></ul></li></ul><p>이 방식은 다음과 같은 이유로 강력함</p><ul><li>LM 이 대량의 raw text 를 pre-trained 할 수 있게 함</li><li>새로운 prompting 함수를 정의하여, few/no labeled data 로 few/zero-shot learning 가능</li></ul><p>본 논문은 prompting 패러다임을 소개하며 pretrained LM, prompt 및 튜닝 전략의 선택과 같은 다양한 측면의 리뷰를 소개함.</p><p>이 외에도 <a href="http://pretrain.nlpedia.ai/" target="_blank" rel="noopener noreferrer">NLPedia-Pretrain</a> 사이트를 제공하며, 지속적으로 업데이트되는 조사 및 논문 목록을 포함한다.</p><h1>1. Two Sea Changes in Natural Language Processing</h1><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/ae9330e9-0307-44a1-824d-3c74591821a0/image.png" class="img_ev3q"></p><ul><li><strong>Fully Supervised Learning</strong> : task-specific model 이며, target task 에 대한 데이터셋에 의존<ul><li>feature engineering : 연구자 또는 엔지니어가 raw data 에서 feature 를 추출하고 모델에 제공하는 방식<ul><li>제한된 데이터를 학습하여 bias 가 내재</li></ul></li><li>architecture engineering : NLP 의 neural network model 이 진보하며, 주요 feature 를 잘 학습하는 network architecture 구축에 초점을 둠<ul><li>적절한 아키텍처 설계로 inductive bias 를 제공</li></ul></li></ul></li></ul><p>2017~2019 에 NLP 의 큰 변화로 fully superviesd 패러다임은 축소됨</p><ul><li><strong>pre-train and fine-tune paradigm</strong> : language model (LM) 을 대규모 데이터셋에서 <strong>pre-train</strong>을 진행한 task-specific objective function 으로 <strong>fine-tune</strong> 을 진행한다.<ul><li>objective engineering : pre-training 및 fine-tuning 과정에 objective 맞게 훈련을 설계<ul><li>대규모 문장 예측에 loss function 을 도입하여 pre-trained LM 이 text summarization 에 좋은 성능을 보여주며, pre-trained LM 의 몸체가 일반적으로 downstream task 에 대한 해결책으로 적절하게 fine-tuning 된다.</li></ul></li></ul></li></ul><p>2021 에 다시 큰 변화가 일어나 &quot;pre-train and fine-tune&quot; 에서 &quot;pre-train, prompt, and predict&quot; 로 대체됨</p><ul><li><strong>pre-train, prompt, and predict</strong> : LM 을 downstream 에 object engineering 으로 적응시키는 대신 downstream task 와 유사하게 textual prompt 를 재구성<ul><li>예시로 다음과 같다.<ul><li>&quot;나 오늘 버스 놓쳤어,&quot; 그리고 prompt &quot;내 기분은 _&quot; 으로 계속하여 LM 에게 공백을 채우도록 요청</li><li>&quot;English:I missed the bus today. French: _&quot; 으로 번역을 예측하도록 공백을 채우게 한다</li></ul></li><li>이처럼 적절한 prompt 를 선택하여 task-specific training 없이 원하는 출력을 예측할 수 있음</li><li>장점으로 다양한 prompt 를 unsupervised 으로 LM 을 통해 많은 task 해결 가능</li><li>적절한 prompt 를 찾는 <strong>prompt engineering</strong> 이 필수</li></ul></li></ul><h1>2 A Formal Description of Prompting</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-supervised-learning-in-nlp">2.1 Supervised Learning in NLP<a href="#21-supervised-learning-in-nlp" class="hash-link" aria-label="Direct link to 2.1 Supervised Learning in NLP" title="Direct link to 2.1 Supervised Learning in NLP">​</a></h2><p>NLP 의 기존 supervised learning 은 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> (보통 text) 으로 output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 를 예측하는, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(y|x;\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span></span></span></span> 모델에 기반한다.</p><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 는 label, text 또는 다른 형태의 output 일 수 있다. </p><p>parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span></span> 를 학습시키기 위해, input 과 output 쌍의 데이터셋을 사용하며, 이의 확률 값을 예측하는 모델로 훈련시킨다.</p><p>먼저 <em>text classification</em> 에선 text <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 로 label set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7805em;vertical-align:-0.0972em"></span><span class="mord mathcal" style="margin-right:0.08222em">Y</span></span></span></span></span> 로부터 label <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 를 예측하게 한다.</p><ul><li><p><em>sentiment analysis</em> 에서 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> = &quot;I love this movie&quot; 로 label set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7805em;vertical-align:-0.0972em"></span><span class="mord mathcal" style="margin-right:0.08222em">Y</span></span></span></span></span> = {++, +, ~, -, --} (positive, negative 정도) 중 label <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> = ++ 을 예측하도록 한다.</p></li><li><p><em>conditional text generation</em> 에서 필란드어 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> = &quot;Hyvää huomenta&quot; 로 영어 output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> = &quot;Good morning&quot; 을 생성한다.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-prompting-basics">2.2 Prompting Basics<a href="#22-prompting-basics" class="hash-link" aria-label="Direct link to 2.2 Prompting Basics" title="Direct link to 2.2 Prompting Basics">​</a></h2><p>supervised learning 은 대규모의 annotated data 가 필수이다.</p><p>prompt-based learning 은 이 문제를 해결하기 위해 3 단계의 prompting 으로 가장 높은 점수의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span></span></span></span></span> 를 예측한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="221-prompt-addition">2.2.1 Prompt Addition<a href="#221-prompt-addition" class="hash-link" aria-label="Direct link to 2.2.1 Prompt Addition" title="Direct link to 2.2.1 Prompt Addition">​</a></h3><ul><li>input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> → <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>t</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{prompt}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> → prompt <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">x&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><ol><li>prompting function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>t</mi></mrow></msub><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{prompt}(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> 은 <em>template</em> 를 적용하는 것으로, 두 가지 slot 이 있는 textual string<ul><li>input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 를 위한 <strong>input slot <!-- -->[X]</strong></li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 로 매핑될 text <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span> 가 있는 <strong>answer slot <!-- -->[Z]</strong></li></ul></li><li>input text <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 으로 <strong>slot <!-- -->[X]</strong> 를 채움</li></ol></li></ul><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/775a6821-37bb-48ad-a974-044401e7bd63/image.png" class="img_ev3q"></p><p>이전의 sentiment analysis 의 예시로 들면 다음과 같다.</p><ol><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> = &quot;I love this movie,&quot;</li><li>template = &quot;<!-- -->[X]<!-- --> Overall, it was a <!-- -->[Z]<!-- --> movie&quot;</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">x&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> = &quot;I love this movie. Overall, it was a <!-- -->[Z]<!-- --> movie,&quot;</li></ol><p>위와 같은 과정으로 <!-- -->[Z]<!-- --> 를 예측하도록 한다. translation 의 예시로 한다면 &quot;Finnish: <!-- -->[X]<!-- --> English: <!-- -->[Z]<!-- -->&quot; 가 될 수 있겠다.</p><p>더 많은 예시는 Table 3 에서 볼 수 있다.</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/6d1ba202-7b39-4053-9e04-a39d7f68e336/image.png" class="img_ev3q"></p><p>세 가지의 주목할 점이 있다.</p><ul><li>위와 같은 prompt 는 z 를 채우기 위한 빈 슬롯을 middle, end 에 가진다.<ul><li><strong><em>close</em></strong> prompt : middle</li><li><strong><em>prefix</em></strong> prompt : end</li></ul></li><li>이러한 template 은 natural language token 가 아닌 다음과 같을 수 있다.<ul><li>continuous space 상의 embedding 될 수 있는 가상 단어</li><li>continuous vectors</li></ul></li><li><em>[X]</em> 및 <em>[Z]</em> 슬롯은 task 에 따라 유연하게 변경할 수 있음</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="222-answer-search">2.2.2 Answer Search<a href="#222-answer-search" class="hash-link" aria-label="Direct link to 2.2.2 Answer Search" title="Direct link to 2.2.2 Answer Search">​</a></h3><p>LM 의 score 를 최대화하는, 가장 높은 score 의 text <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>z</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span> 를 찾아야 한다.</p><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span> 는 언어 전체 범위나 작은 부분 집합일 수 있다.</p><p>예로, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Z</mi></mrow><annotation encoding="application/x-tex">\mathcal{Z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.07944em">Z</span></span></span></span></span> = {&quot;excellent&quot;, &quot;good&quot;, &quot;OK&quot;, &quot;bad&quot;, &quot;horrible&quot;} 괴 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7805em;vertical-align:-0.0972em"></span><span class="mord mathcal" style="margin-right:0.08222em">Y</span></span></span></span></span> = {++, +, ~, -, --} 처럼 나타낼 수 있다.</p><ul><li>prompt <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">x&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> 의 <!-- -->[Z]<!-- --> 의 answer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span> 를 채울 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>f</mi><mi>i</mi><mi>l</mi><mi>l</mi></mrow></msub><mo stretchy="false">(</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{fill}(x&#x27;, z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.038em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em">f</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">ll</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose">)</span></span></span></span></span> 함수 정의<ul><li><strong><em>filled prompt</em></strong> : 위 함수로 채워진 prompt</li><li><strong><em>answered prompt</em></strong> : true answer 로 채워진 prompt (예; Table 2)</li></ul></li><li>pretrained LM <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(\cdot ;\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span></span></span></span> 으로 <em>filled prompt</em> 의 확률을 계산하여 answer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span> 를 탐색</li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mover accent="true"><mi>z</mi><mo>^</mo></mover><mo>=</mo><mi><munder><mo><mtext>search</mtext></mo><mrow><mi>z</mi><mo>∈</mo><mi mathvariant="script">Z</mi></mrow></munder></mi><mtext> </mtext><mi>P</mi><mo stretchy="false">(</mo><msub><mi>f</mi><mrow><mi>f</mi><mi>i</mi><mi>l</mi><mi>l</mi></mrow></msub><mo stretchy="false">(</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\hat{z} = \underset{z \in \mathcal{Z}}{\textup{search}} \ P(f_{fill}(x&#x27;, z); \theta). \tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.5736em;vertical-align:-0.7717em"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-2.3557em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em">z</span><span class="mrel mtight">∈</span><span class="mord mathcal mtight" style="margin-right:0.07944em">Z</span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mord text"><span class="mord textup">search</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7717em"><span></span></span></span></span></span></span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em">f</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">ll</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose">)</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span><span class="mord">.</span></span><span class="tag"><span class="strut" style="height:1.5736em;vertical-align:-0.7717em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></div><p>위 search 함수는 다음이 가능</p><ul><li><em>argmax search</em> :  가장 높은 score 의 output 을 찾는 함수</li><li><em>sampling</em> : LM 확률 분포에 따른 무작위 output 생성</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="223-answer-mapping">2.2.3 Answer Mapping<a href="#223-answer-mapping" class="hash-link" aria-label="Direct link to 2.2.3 Answer Mapping" title="Direct link to 2.2.3 Answer Mapping">​</a></h3><p>마지막 단계로, 가장 score 가 높은 answer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>z</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span> 으로 output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span></span></span></span></span> 을 낸다.</p><p>번역 같은 생성 작업에선 answer 에 대한 output 생성은 쉽지만, multiple answer 에 대해 동일한 output 을 내는 경우가 있다.</p><p>예로, single class (예; ++)를 나타내는 여러 감정 단어 (예; &quot;excellent&quot;,  &quot;fabulous&quot;, &quot;wonderful&quot;)를 사용할 수 있다. </p><p>이 경우, <strong>searched answer 와 output 간의 매핑</strong>이 필요하다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-design-considerations-for-prompting">2.3 Design Considerations for Prompting<a href="#23-design-considerations-for-prompting" class="hash-link" aria-label="Direct link to 2.3 Design Considerations for Prompting" title="Direct link to 2.3 Design Considerations for Prompting">​</a></h2><p>수학적 공식을 알았으니, prompting 방법에 대한 기본적인 설계 고려사항을 보자.</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/f50d3289-95a5-4e61-ac81-3925c0c17216/image.png" class="img_ev3q"></p><ul><li>Pre-trained LM Choice<ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(x;\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span></span></span></span> 를 계산할 다양한 pretrained LM</li></ul></li><li>Prompt Template Engineering<ul><li>task 에 따른 proper prompt 선택</li><li>정확도뿐만 아니라 model 수행에도 영향 끼침</li><li><a href="#3">Section 3</a> 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>t</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{prompt}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> 로 template 선택법 설명</li></ul></li><li>Prompt Answer Engineering<ul><li>task 에 따른 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Z</mi></mrow><annotation encoding="application/x-tex">\mathcal{Z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.07944em">Z</span></span></span></span></span> 설계 가능</li><li>경우에 따라선 매핑 함수와 함께 설계도 할 수 있음</li><li><a href="#4">Section 4</a> 다양한 방법을 다룸</li></ul></li><li>Expanding the Paradigm<ul><li><a href="#5">Section 5</a> 에서 기본 패러다임을 확장하여 더욱 개선하고 적용 가능성을 높이는 방법을 다룸</li></ul></li><li>Prompt-based Training Strategies<ul><li>prompt, LM 둘의 parameter 를 훈련시키는 방법도 있음</li><li><a href="#6">Section 6</a> 에서 다양한 전략을 요약하고 이점을 설명</li></ul></li></ul><h1>3 Prompt Template Engineering</h1><p><strong><em>Prompt template engineering</em></strong> : downstream task 에 효과적인 성능을 만다는 prompting function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>t</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{prompt}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">pt</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> 를 생성하는 과정</p><p>task 에 따른 최상의 template 을 위해서, Figure 1 과 같이 두 가지 접근법으로 원하는 shape 의 prompt 를 만들 수 있다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-prompt-shape">3.1 Prompt Shape<a href="#31-prompt-shape" class="hash-link" aria-label="Direct link to 3.1 Prompt Shape" title="Direct link to 3.1 Prompt Shape">​</a></h2><p>앞서 <em>close prompt</em> 와 <em>prefix prompt</em> 를 언급했다. 이는 task 및 model 에 따라 선택이 갈린다.</p><ul><li>생성 관련 task / 표준 auto-regressive LM 의 경우 : <em>prefix prompt</em> 가 더 유리<ul><li>left-to-right 특성의 모델과 어울림</li></ul></li><li>masked LM 의 경우 : <em>close prompt</em> 가 적합<ul><li>pre-training task 형식과 유사</li></ul></li><li>텍스트 재구성 모델의 경우 : <em>close prompt</em> 및 <em>prefix prompt</em> 함께 사용 가능</li><li>text pair classification 같은 multiple input 의 경우 : prompt template 은 <!-- -->[X1]<!-- -->, <!-- -->[X2]<!-- --> 두 개의 input 또는 그 이상을 포함해야함</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-manual-template-engineering">3.2 Manual Template Engineering<a href="#32-manual-template-engineering" class="hash-link" aria-label="Direct link to 3.2 Manual Template Engineering" title="Direct link to 3.2 Manual Template Engineering">​</a></h2><p>가장 자연스러운 prompt 는 사람의 심리를 기반한 직관적인 template 을 수동으로 생성하는 것이다.</p><p>예로, LAMA 데이터셋은 LM 의 knowledge 를 조사하기 위해 수동으로 생성된 close template 을 제공한다.</p><ul><li><a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" target="_blank" rel="noopener noreferrer">Language Models are Few-Shot Learners</a> 논문에선 question answering, translation 및 probing task 와 같은 일반적인 추론 task 를 포함한 다양한 영역을 처리하기 위해 prefix prompt 를 수동으로 만든다</li><li><a href="https://aclanthology.org/2021.eacl-main.20.pdf" target="_blank" rel="noopener noreferrer">Exploiting Cloze Questions for Few Shot Text Classification and Natural
Language Inference</a>, <a href="https://aclanthology.org/2021.emnlp-main.32.pdf" target="_blank" rel="noopener noreferrer">Few-Shot Text Generation with Natural Language Instructions</a>, <a href="https://aclanthology.org/2021.naacl-main.185.pdf" target="_blank" rel="noopener noreferrer">It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</a> 논문들은 text classification 및 text generation task 에서 few-shot learning setting 에서 미리 정의한 template 을 사용</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-automated-template-learning">3.3 Automated Template Learning<a href="#33-automated-template-learning" class="hash-link" aria-label="Direct link to 3.3 Automated Template Learning" title="Direct link to 3.3 Automated Template Learning">​</a></h2><p>수동 제작된 template 은 직관적이고 다양한 task 를 어느 정도 해결할 수 있지만 몇 가지 이슈가 있다.</p><ul><li>prompt 를 생성하고 실험하는 시간 및 비용이 든다; 특히 의미 이해같은 복잡한 task 는 더 힒듬</li><li>경험이 많은 prompt 디자이너도 최적의 prompt 를 발견하는데 실패할 수 있음</li></ul><p>이 문제를 해결하기 위해 template 을 자동으로 설계하는 프로세스가 제안됨</p><ul><li><strong><em>discrete prompt</em></strong> : 실제 텍스트 문자열</li><li><strong><em>continuous prompt</em></strong> : LM 의 임베딩 공간에 직접 설명되는 prompt</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="331-discrete-prompt">3.3.1 Discrete Prompt<a href="#331-discrete-prompt" class="hash-link" aria-label="Direct link to 3.3.1 Discrete Prompt" title="Direct link to 3.3.1 Discrete Prompt">​</a></h3><p><em>discrete prompt</em> 방법은 다음과 같다.</p><ul><li><strong>D1: Prompt Mining</strong><ul><li>training input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 및 output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 에서 자동으로 template 를 찾는 마이닝 기법</li><li>large text corpus (예; 위키피디아) 에서, &quot;<!-- -->[X]<!-- --> middle words <!-- -->[Z]<!-- -->&quot; 와 같은 template 으로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 간의 <em>middle words</em> 또는 <em>dependency path</em> 를 찾음</li></ul></li><li><strong>D2: Prompt Paraphrasing</strong><ul><li>seed prompt 로 다른 후보 prompt 간에 paraphrase 하여 가장 높은 정확도를 선택하는 기법<ul><li>prompt 를 다른 언어로 번역한 후 재번역하는 방법</li><li>시사어에 구절을 바꾸는 방법</li><li>정확도 향상에 특화된 neural prompt rewriter 사용하는 방법<ul><li>특히 이 방법은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 가 template 에 입력된 후 paraphrase 수행 후 각각의 input 에 다른 paraphrase 생성 가능</li></ul></li></ul></li></ul></li><li><strong>D3: Gradient-based Search</strong><ul><li>실제 토큰에 대한 gradient-based search 로 생성가능한 짧은 시퀀스를 찾는 기법</li><li>반복 수행 및 prompt 토큰에 순차 탐색</li><li>downstream 에 대한 training sample 로 탐색하여 강력한 성능을 보여줌</li></ul></li><li><strong>D4: Prompt Generation</strong><ul><li>텍스트 생성 모델로 prompt 를 생성</li><li>누락된 범위를 채우는데 특화된 T5 를 사용한 <a href="https://arxiv.org/pdf/2012.15723.pdf" target="_blank" rel="noopener noreferrer">32</a> 이 있음<ol><li>template 내에 template token 을 삽입할 위치 지정</li><li>T5 가 template token 을 디코딩 하도록 training sample 제공</li></ol></li><li><a href="https://arxiv.org/pdf/2106.07704.pdf" target="_blank" rel="noopener noreferrer">36</a> : prompt 생성 제어를 위해 강화 학습을 사용하기도 함</li><li><a href="https://doi.org/10.1162/tacl_a_00468" target="_blank" rel="noopener noreferrer">5</a> : T5 를 각 input 에 domain relevant feature (DRFs) 을 생성하도록 훈련하는 도메인 적응 알고리즘 제안<ul><li>DRF (도메인 정보를 특징화하는 키워드 집합) 는 input 과 연결하여 template 을 형성하고 downstream task 에 의해 추가 사용</li></ul></li></ul></li><li><strong>D5: Prompt Scoring</strong><ul><li><a href="https://doi.org/10.18653/v1/D19-1109" target="_blank" rel="noopener noreferrer">19</a> : knowledge base 의 task 를 조사 후 LM 을 이용하여 input(head-relation-tail)에 대한 template 설계<ul><li>후보군 template 셋을 수동으로 작성하고 input 과 output slot 을 채워 prompt 를 형성</li><li>그 후, 단방향 LM 으로 filled prompt 를 평가하여 가장 높은 확률을 선택</li></ul></li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="332-continuous-prompt">3.3.2 Continuous Prompt<a href="#332-continuous-prompt" class="hash-link" aria-label="Direct link to 3.3.2 Continuous Prompt" title="Direct link to 3.3.2 Continuous Prompt">​</a></h3><p><strong><em>continuous prompt</em></strong> : LM 가 효과적으로 task 를 수행하도록 인간이 이해 가능한 자연어가 아닌, LM 의 임베딩 공간에 직접 prompting 을 수행하도록 하는 방법이다.</p><p>이 방법은 두 가지 제약을 없앤다.</p><ul><li>template 단어의 임베딩과 자연어 단어의 임베딩이 일치해야 한다는 제약 완화</li><li>template 가 pretrained LM 의 파라미터화 되야 한다는 제약 제거<ul><li>대신, downstream task 의 데이터에 따라 조정할 수 있는 자체 파라미터가 있음</li></ul></li></ul><p>아래 대표적인 방법들이 있다.</p><ul><li><strong>C1: Prefix Tuning</strong><ul><li>Prefix Tuning <a href="https://doi.org/10.18653/v1/2021.acl-long.353" target="_blank" rel="noopener noreferrer">17</a> : input 에 continuous task-specific vector 의 시퀀스를 접두어로 추가하는 기법. 동시에 frozen LM 파라미터를 유지<div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi><munder><mo><mi>max</mi><mo>⁡</mo></mo><mi>ϕ</mi></munder></mi><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo separator="true">;</mo><mi>θ</mi><mo separator="true">;</mo><mi>ϕ</mi><mo stretchy="false">)</mo><mo>=</mo><mi><munder><mo><mi>max</mi><mo>⁡</mo></mo><mi>ϕ</mi></munder></mi><munder><mo>∑</mo><msub><mi>y</mi><mi>i</mi></msub></munder><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo separator="true">;</mo><mi>θ</mi><mo separator="true">;</mo><mi>ϕ</mi><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\underset{\phi}{\max} \log P(y|x;\theta;\phi) = \underset{\phi}{\max}\sum_{y_i}\log P(y_i, h_{&lt;i};\theta;\phi)\tag{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.6382em;vertical-align:-0.8882em"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.3479em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mop">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8882em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">ϕ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4361em;vertical-align:-1.3861em"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.3479em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mop">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8882em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.9em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3861em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">ϕ</span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:2.4361em;vertical-align:-1.3861em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></div>수학적으로, matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mi>ϕ</mi></msub></mrow><annotation encoding="application/x-tex">M_{\phi}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 와 pre-trained LM 의 파라미터 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span></span> 가 주어였을 때 log-likelihood 목표에 따라 최적화한다.
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>=</mo><mo stretchy="false">[</mo><msubsup><mi>h</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">;</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">;</mo><msubsup><mi>h</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">h_{&lt;i} = [h^{(1)}_{&lt;i}; \cdots;h^{(n)}_{&lt;i}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8718em;vertical-align:-0.1774em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.349em;vertical-align:-0.3042em"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3042em"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">n</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3042em"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span> 는 timestep <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span> 에 따른 모든 neural network 층의 연결이다.
해당 timestep 이 prefix (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mi>ϕ</mi></msub><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">M_{\phi}[i])</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal">i</span><span class="mclose">])</span></span></span></span></span> 내에 있는 경우, 직접 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mi>ϕ</mi></msub></mrow><annotation encoding="application/x-tex">M_{\phi}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 에서 복하된다. 그렇지 않으면 pre-trained LM 으로 계산한다.</li><li><a href="https://doi.org/10.18653/v1/2021.acl-long.353" target="_blank" rel="noopener noreferrer">71</a> : continuous prefix-based learning 이 실제 단어로된 discrete prompt 보다 저데이터 설정에서 초기화에 더 민감하단 것을 관찰  </li><li><a href="https://doi.org/10.18653/v1/2020.acl-main.703" target="_blank" rel="noopener noreferrer">67</a> : input 시퀀스에 특수 토큰을 추가하여 template 를 형성 하고 토큰의 임베딩을 직접 tuning<ul><li><a href="https://doi.org/10.18653/v1/2021.acl-long.353" target="_blank" rel="noopener noreferrer">71</a> 와 비교하여, 이 방법이 추가 매개변수를 도입하지 않아, 더 적은 매개변수를 사용</li></ul></li><li><a href="https://proceedings.neurips.cc/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf" target="_blank" rel="noopener noreferrer">135</a> : 캡션 생성을 위해, frozen LM 을 사용하여 이미지를 임베딩 시퀀스로 인코딩하는 visual encoder 를 학습<ul><li>visual-language task 에 대한 few-shot learning 이 가능한 것을 보여줌</li><li>위 두 논문과 달리, prefix 는 샘플에 따라 다르며, task embedding 이 아닌 input image 의 representation 이다.</li></ul></li></ul></li><li><strong>C2: Tuning Initialized with Discrete Prompt</strong><ul><li>discrete prompt 탐색 기법으로 생성된 prompt 로 continuous prompt 를 초기화하는 기법</li><li><a href="https://doi.org/10.18653/v1/2021.naacl-main.398" target="_blank" rel="noopener noreferrer">152</a> 는 <a href="https://arxiv.org/pdf/2010.15980.pdf" target="_blank" rel="noopener noreferrer">AutoPrompt</a> 와 같은 discrete 탐색 기법으로 template 를 정의
이 prompt 를 기반으로 가상 토큰을 초기화 후 정확도 상승을 위해 임베딩을 finetuning<ul><li>수동 template 로 초기화하면 탐색 기법보다 더 나은 starting point 를 제공한 다는 점 발견</li></ul></li><li><a href="https://arxiv.org/pdf/2104.06599.pdf" target="_blank" rel="noopener noreferrer">103</a> : 각 input 에 대한 soft template 의 혼합을 학습한다.<ul><li>각 template 의 가중치와 파라미터를 training sample 로 공동 학습한다.</li><li>초기 template 셋은 수동으로 만들거나 <em>prompt mining</em> 기법으로 얻는다.</li></ul></li><li><a href="https://doi.org/10.18653/v1/2021.acl-long.381" target="_blank" rel="noopener noreferrer">40</a> : 수동 prompt template 의 shape 를 따르는 continuous template 을 사용한다.</li></ul></li><li><strong>C3: Hard-Soft Prompt Hybrid Tuning</strong><ul><li>단순히 learnable prompt template 사용 대신, hard prompt template 를 tunable 임베딩에 삽입하는 기법</li><li><a href="https://arxiv.org/abs/2103.10385" target="_blank" rel="noopener noreferrer">77</a> : <strong>P-tuning</strong> 을 제안<ul><li>continuous prompt 를 학습 가능한 변수를 embedded input 에 삽입하여 학습됨</li><li>prompt token 간의 상호작용을 위해, prompt embedding 을 <a href="https://doi.org/10.1109/ICASSP.2013.6638947" target="_blank" rel="noopener noreferrer">BiLSTM</a> 의 출력으로 나타냄</li><li>성능 향상을 위해 template 에 고정된 anchor token 을 사용</li></ul></li><li><a href="https://arxiv.org/abs/2105.11259" target="_blank" rel="noopener noreferrer">41</a> : <strong>prompt tuning with rules (PTR)</strong> 제안<ul><li>규칙 기반으로 완전한 template 을 만들기 위해 수동 제작된 sub-templates 를 사용</li><li>template ability 향상을 위해 training sample 을 통해, pretrained LM parameter 와 함께 tunable 가상 토큰을 삽입</li><li>PTR 의 template token 은 actual token 및 virtual token 포함</li><li>relation classification task 에 효과적</li></ul></li></ul></li></ul><h1>4 Prompt Answer Engineering</h1><p><strong><em>promt answer engineering</em></strong> : 예측 모델에서 answer space <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Z</mi></mrow><annotation encoding="application/x-tex">\mathcal{Z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.07944em">Z</span></span></span></span></span> 를 탐색하고 output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7805em;vertical-align:-0.0972em"></span><span class="mord mathcal" style="margin-right:0.08222em">Y</span></span></span></span></span> 와 매핑하는 것이 목표</p><p><em>answer shape</em> 및 <em>answer design</em> 기법에 대해 고려해야함</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-answer-shape">4.1 Answer Shape<a href="#41-answer-shape" class="hash-link" aria-label="Direct link to 4.1 Answer Shape" title="Direct link to 4.1 Answer Shape">​</a></h2><p>answer 의 모양은 세부도를 특징 짓는다. 일반적인 선택은 다음과 같다.</p><ul><li><strong>Token</strong> : pre-trained LM 의 vocabulary 에 있는 token 중 하나 또는 vocabulary 의 하위집합</li><li><strong>Span</strong> : 짧은 multi-token span. 보통 close prompt 와 사용됨</li><li><strong>Sentence</strong> : 문장 또는 문서. 보통 prefix prompt 와 사용</li></ul><p>answer shape 는 task 에 따라 다르다.</p><ul><li><strong>token/text-span</strong> 의 answer space 는 classification (감정 분류; <a href="https://doi.org/10.18653/v1/D19-1404" target="_blank" rel="noopener noreferrer">144</a>), relation extraction <a href="https://doi.org/10.18653/v1/D19-1250" target="_blank" rel="noopener noreferrer">100</a> 또는 entity recognition <a href="https://arxiv.org/pdf/2106.01760.pdf" target="_blank" rel="noopener noreferrer">17</a> 등 널리 사용된다.</li><li><strong>longer phrasal/sentential</strong> 의 answer space 는  언어 생성 <a href="https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf" target="_blank" rel="noopener noreferrer">105</a> 이나 mutliple-choice question answering <a href="https://doi.org/10.18653/v1/2020.findings-emnlp.171" target="_blank" rel="noopener noreferrer">55</a> task 등에 자주 사용된다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-answer-space-design-method">4.2 Answer Space Design Method<a href="#42-answer-space-design-method" class="hash-link" aria-label="Direct link to 4.2 Answer Space Design Method" title="Direct link to 4.2 Answer Space Design Method">​</a></h2><p>적절한 answer space <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Z</mi></mrow><annotation encoding="application/x-tex">\mathcal{Z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.07944em">Z</span></span></span></span></span> 을 설계하는 방법과 answer 이 최종 output 으로 사용되지 않을 경우 output space <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7805em;vertical-align:-0.0972em"></span><span class="mord mathcal" style="margin-right:0.08222em">Y</span></span></span></span></span> 를 매핑하는 방법이다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="421-manual-design">4.2.1 Manual Design<a href="#421-manual-design" class="hash-link" aria-label="Direct link to 4.2.1 Manual Design" title="Direct link to 4.2.1 Manual Design">​</a></h3><p><strong><em>manutal design</em></strong> : answer space <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Z</mi></mrow><annotation encoding="application/x-tex">\mathcal{Z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.07944em">Z</span></span></span></span></span> 와 매핑할 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7805em;vertical-align:-0.0972em"></span><span class="mord mathcal" style="margin-right:0.08222em">Y</span></span></span></span></span> 에 대해 관심사로 수동으로 만드는 기법</p><ul><li><strong>Unconstrained Spaces</strong> : answer space <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Z</mi></mrow><annotation encoding="application/x-tex">\mathcal{Z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.07944em">Z</span></span></span></span></span> 는 모든 토큰 공간 <a href="https://doi.org/10.18653/v1/D19-1250" target="_blank" rel="noopener noreferrer">100</a> , fixed-length spans <a href="https://doi.org/10.18653/v1/2020.emnlp-main.479" target="_blank" rel="noopener noreferrer">50</a> , 또는 token sequence <a href="https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf" target="_blank" rel="noopener noreferrer">105</a> 이다.<ul><li>identity mapping 으로 answer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span> 을 output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 와 직접 매핑하는 것이 일반적</li></ul></li><li><strong>Constrained Spaces</strong> : text classification / entitiy recognition / multiple-choice question answering 와 같은 제한된 라벨 공간에 대한 task 를 수행할 때 사용하는 기법<ul><li><a href="https://doi.org/10.18653/v1/2021.naacl-main.208" target="_blank" rel="noopener noreferrer">144</a> : input text 와 관련한 단어의 목록 (예; 감정 <!-- -->[&quot;anger&quot;, &quot;joy&quot;, &quot;sadness&quot;, &quot;fear&quot;]<!-- -->, topics <!-- -->[&quot;health&quot;, &quot;finance&quot;, &quot;politics&quot;]<!-- -->) 를 수동으로 설계</li><li><a href="https://arxiv.org/pdf/2106.01760.pdf" target="_blank" rel="noopener noreferrer">17</a> : <strong>named entity recognition (NER)</strong> task 에 대해 &quot;person&quot;, &quot;location&quot; 같은 목록을 수동으로 설계<ul><li>위 두 논문 같은 경우, answer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Z</mi></mrow><annotation encoding="application/x-tex">\mathcal{Z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.07944em">Z</span></span></span></span></span> 와 class <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Y</mi></mrow><annotation encoding="application/x-tex">\mathcal{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7805em;vertical-align:-0.0972em"></span><span class="mord mathcal" style="margin-right:0.08222em">Y</span></span></span></span></span> 간의 매핑이 필수</li></ul></li><li><a href="https://doi.org/10.18653/v1/2021.naacl-main.58" target="_blank" rel="noopener noreferrer">155</a> : multiple-choice question answering task 에 대해선 LM 을 사용하여 여러 선택 중 하나의 출력 확률 계산이 일반적</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="422-discrete-answer-search">4.2.2 Discrete Answer Search<a href="#422-discrete-answer-search" class="hash-link" aria-label="Direct link to 4.2.2 Discrete Answer Search" title="Direct link to 4.2.2 Discrete Answer Search">​</a></h3><p>수동 생성된 answer 으로 이상적인 예측 성능을 얻는 다는 것은 sub-optimal 이다.</p><p><strong><em>discrete answer search</em></strong> : 자동으로 answer search</p><ul><li><strong>Answer Paraphrasing</strong><ul><li>초기 answer space <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="script">Z</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">\mathcal{Z}&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.07944em">Z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> 을 시작으로, paraphrasing 을 사용하여 answer space 를 확장<ul><li>answer 및 output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo><msup><mi>z</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>y</mi><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&lt;z&#x27;, y&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.9463em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span></span></span></span></span> 주어지면, answer para <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>z</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(z&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 인 paraphased set 을 생성하는 함수 정의</li><li>최종 output 을 모든 answer 에 대한 마진 확률을 정의. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mrow><mi>z</mi><mo>∈</mo><mtext>para</mtext><mo stretchy="false">(</mo><msup><mi>z</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow></msub><mi>P</mi><mo stretchy="false">(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(y|x) = \sum_{z \in \textup{para}(z&#x27;)}P(z|x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2247em;vertical-align:-0.4747em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2253em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em">z</span><span class="mrel mtight">∈</span><span class="mord text mtight"><span class="mord textup mtight">para</span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em"><span style="top:-2.786em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4747em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></li></ul></li><li><a href="https://doi.org/10.1162/tacl_a_00407" target="_blank" rel="noopener noreferrer">51</a> : back-translation 방법을 사용<ul><li>다른 언어로 번역한 다음, 다시 되돌려 여러 paraphrased answer 을 생성한다</li></ul></li></ul></li><li><strong>Prune-then-Search</strong> : 그럴듯한 answers <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="script">Z</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">\mathcal{Z}&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.07944em">Z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> 를 초기 pruned answer space 에 생성한 후, 알고리즘으로 최종 answer 셋을 선택하는 기법<ul><li><a href="https://doi.org/10.18653/v1/2021.eacl-main.20" target="_blank" rel="noopener noreferrer">117</a>, <a href="https://doi.org/10.18653/v1/2020.coling-main.488" target="_blank" rel="noopener noreferrer">115</a> : label <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 에서 single answer toek <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span> 로 매핑하는 함수를 정의. <em>verbalizer</em> 라고 함<ul><li>최소 두 개의 알파벳 문자를 포함한 토큰 탐색</li><li>탐색 단계에서, 데이터의 likelihood 를 극대화하여 label <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 에 대한 answer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span> 로의 적합도 계산 </li></ul></li><li><a href="https://arxiv.org/pdf/2010.15980.pdf" target="_blank" rel="noopener noreferrer">AutoPrompt</a> : <!-- -->[Z]<!-- --> token 의 contextualized representation 을 입력으로 사용해 logistic classifier 를 학습<ul><li>탐색 단계에서, 학습된 logistic classifier 로 top-<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> token 을 선택</li><li>선택된 토큰들로 answer 형성</li></ul></li><li><a href="https://arxiv.org/pdf/2012.15723.pdf" target="_blank" rel="noopener noreferrer">32</a> : 훈련 샘플로 결정된 <!-- -->[Z]<!-- --> 위치의 확률값을 기반으로 top-<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> vocabulary 단어를 선택하여, pruned search space <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="script">Z</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">\mathcal{Z}&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.07944em">Z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> 를 구성<ul><li>훈련 샘플에 zero-shot 정확도를 기반으로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="script">Z</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">\mathcal{Z}&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.07944em">Z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> 의 하위 집합을 선택하여 search space 를 더욱 pruning 함</li><li>탐색 단계에서, 고정된 template 와 모든 answer 매핑을 사용하여 LM 을 finetuning 한 후, 이의 정확도를 기반으로 가장 좋은 label word 를 선택</li></ul></li></ul></li><li><strong>Label Decomposition</strong><ul><li><a href="https://doi.org/10.1145/3485447.3511998" target="_blank" rel="noopener noreferrer">13</a> : 관계 추출 시, 관계 라벨을 구성 요소 단어로 자동으로 분해하고 answer 로 사용<ul><li>예; <code>per:city_of_death</code> 의 경우, <code>{person,city,death}</code> 로 분해한다.</li><li>answer span 의 확률은 각 토큰의 확률의 합으로 계산</li></ul></li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="423-continuous-answer-search">4.2.3 Continuous Answer Search<a href="#423-continuous-answer-search" class="hash-link" aria-label="Direct link to 4.2.3 Continuous Answer Search" title="Direct link to 4.2.3 Continuous Answer Search">​</a></h3><p>몇몇 연구는 경사하강법으로 최적화할 수 있는 soft answer token 을 사용하는 가능성을 연구한다.</p><p><a href="https://doi.org/10.18653/v1/2021.acl-long.381" target="_blank" rel="noopener noreferrer">40</a> 에서는 각 class label 에 대한 가상 토큰을 할당하고 prompt token embedding 과 함께 각 클래스에 대한 token embedding 을 최적화한다.</p><p>answer token 은 임베딩 공간에 직접 최적화할 수 있어, LM 으로 학습된 임베딩을 사용하는 대신 각 라벨을 처음부터 학습한다.</p><h1>5 Multi-Prompt Learning</h1><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/bf22da8f-56a5-49e4-b26a-d775c95ce26e/image.png" class="img_ev3q"></p><p><strong><em>multi-prompt learning</em></strong> : single prompt learning 을 확장하여 효율성을 향상한 방법</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-prompt-ensembling">5.1 Prompt Ensembling<a href="#51-prompt-ensembling" class="hash-link" aria-label="Direct link to 5.1 Prompt Ensembling" title="Direct link to 5.1 Prompt Ensembling">​</a></h2><p><strong><em>prompt ensembling</em></strong> : 추론 시, 여러 <em>unanswered prompt</em> 를 이용한 프로세스</p><p>multiple prompt 는 <em>discrete prompt</em> 또는 <em>continuous prompt</em> 일 수 있다.</p><ul><li>서로 보완적인 prompt 의 이점을 활용</li><li>prompt engineering 비용 완화</li><li>downstream task 의 성능을 안정화</li></ul><p>Prompt ensembling 은 머신 러닝의 긴 역사 중 멀티 시스템을 결합한 앙상블 방법을 이용한 것으로, 현재 효과적인 방법을 도출</p><ul><li><strong>Uniform averaging</strong> : 여러 prompt 의 확률값을 평균<ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mfrac><mn>1</mn><mi>K</mi></mfrac><msubsup><mo>∑</mo><mi>i</mi><mi>K</mi></msubsup><mi>P</mi><mo stretchy="false">(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><msub><mi>f</mi><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(z|x) := \frac{1}{K} \sum^K_i P(z|f_{prompt, i}(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.3262em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">pt</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>m</mi><mi>p</mi><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{prompt, i}(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">pt</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>th prompt</li></ul></li><li><a href="https://doi.org/10.1162/tacl_a_00324" target="_blank" rel="noopener noreferrer">52</a> : 가장 높은 정확도의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> 개 prompt 선택 후, 평균 로그 확률을 사용하여 <!-- -->[Z]<!-- --> 위치의 single token 에 대한 확률 계산</li><li><a href="https://doi.org/10.18653/v1/2021.eacl-main.20" target="_blank" rel="noopener noreferrer">117</a> : unlabeled 데이터셋에 annotate 하기 위해 앙상블 모델을 사용할 때 간단한 평균화 시도</li><li><a href="https://proceedings.neurips.cc/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf" target="_blank" rel="noopener noreferrer">Bartscore</a> : text generation task 에서, 다양한 prompt 를 사용하여 최종 생성 스코어의 평균을 사용</li></ul></li><li><strong>Weighted averaging</strong> : weight 가 있는 각 prompt 로 weight average 를 사용하여 앙상블<ul><li>weight 는 prompt 성능에 따라 다르고 훈련셋으로 최적화된다.</li><li><a href="https://doi.org/10.1162/tacl_a_00324" target="_blank" rel="noopener noreferrer">52</a> : 훈련셋으로 target output 확률값을 최대화하여 각 prompt 의 weight 학습</li><li><a href="https://arxiv.org/pdf/2104.06599.pdf" target="_blank" rel="noopener noreferrer">103</a> : 위와 동일한 approach 지만, 데이터 의존적 전략 사용</li><li><a href="https://doi.org/10.18653/v1/2021.eacl-main.20" target="_blank" rel="noopener noreferrer">117</a>, <a href="https://doi.org/10.18653/v1/2021.naacl-main.185" target="_blank" rel="noopener noreferrer">120</a> : 훈련셋에서 정확도에 비례하여 각 prompt 에 weight 를 설정</li></ul></li><li><strong>Majority voting</strong> : classification task 에서, 다양한 prompt 의 결과를 결합하기 위해 다수결 투표를 사용 (<a href="https://doi.org/10.18653/v1/2021.acl-long.381" target="_blank" rel="noopener noreferrer">40</a>, <a href="https://doi.org/10.18653/v1/2021.emnlp-main.243" target="_blank" rel="noopener noreferrer">67</a>)</li><li><strong>Knowledge distillation</strong> : 성능 향상을 위해 우수한 모델을 단일 모델로의 knowledge distillation<ul><li><a href="https://doi.org/10.18653/v1/2021.eacl-main.20" target="_blank" rel="noopener noreferrer">117</a>, <a href="https://doi.org/10.18653/v1/2021.emnlp-main.32" target="_blank" rel="noopener noreferrer">118</a>, <a href="https://doi.org/10.18653/v1/2021.naacl-main.185" target="_blank" rel="noopener noreferrer">120</a> : 수동 생성된 template-answer 쌍으로 모델 훈련 및 앙상블을 통해 unlabeled 데이터셋을 annotate</li><li><a href="https://arxiv.org/pdf/2012.15723.pdf" target="_blank" rel="noopener noreferrer">32</a> : 자동으로 생성된 template 에서 앙상블 사용</li></ul></li><li><strong>Prompt ensembling for text generation</strong><ul><li>generation task 에 대한 prompt ensembling 연구는 상대적으로 적음</li><li>answer sequence 의 다음 단어의 앙상블될 확률 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><mi>x</mi><mo separator="true">,</mo><msub><mi>z</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mfrac><mn>1</mn><mi>K</mi></mfrac><msubsup><mo>∑</mo><mi>i</mi><mi>K</mi></msubsup><mi>P</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>f</mi><mtext>prompt, i</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>z</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(z_t|x,z_{&lt;t}) := \frac{1}{K} \sum^K_i P(z_t|f_{\textup{prompt, i}}(x), z_{&lt;t})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.3262em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord textup mtight">prompt, i</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 을 기반으로 output 생성</li><li>[118]<!-- -->  : 각 prompt <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mtext>prompt, i</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{\textup{prompt, i}}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord textup mtight">prompt, i</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> 에 대한 별도의 모델을 훈련<ul><li>각각의 finetuned LM 을 메모리에 저장하기 힘듬
대신 각 모델의 생성을 디코드한 다음, 생성 확률의 평균을 사용하여 평가</li></ul></li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-prompt-augmentation">5.2 Prompt Augmentation<a href="#52-prompt-augmentation" class="hash-link" aria-label="Direct link to 5.2 Prompt Augmentation" title="Direct link to 5.2 Prompt Augmentation">​</a></h2><p><strong><em>Prompt Augmentation  ( Demonstration Learning )</em></strong> : LM 에게 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 로 인스턴스화된 실제 프롬프트에 대한 answer 를 제공하는데 사용될 수 있는 <em>answered prompt</em> 를 추가로 제공하는 것</p><p>&quot;중국의 수도는 <!-- -->[Z]<!-- -->&quot; 대신, &quot;영국의 수도는 런던. 일본의 수도는 도쿄. 중국의 수도는 <!-- -->[Z]<!-- -->&quot; 와 같이 prompt 를 제공할 수 있음</p><p>few-shot demonstration 으로 강력한 언어 모델이 반복 패턴을 학습하는 데 활용한다.</p><p>아이디어는 간단하지만 다음 어려움이 있음</p><ul><li><strong>Sample Selection</strong> : 가장 효과적인 예는 어떻게 선택?<ul><li>few-shot 시나리오는 선택에 따라 성능이 천차만별</li><li><a href="https://arxiv.org/pdf/2012.15723.pdf" target="_blank" rel="noopener noreferrer">32</a>. <a href="https://arxiv.org/pdf/2101.06804.pdf" target="_blank" rel="noopener noreferrer">74</a> : 문장 임베딩으로 input 과 가까운 예제를 샘플링</li><li><a href="https://arxiv.org/abs/2104.08773" target="_blank" rel="noopener noreferrer">87</a> : instruction 에 기반한 LM 의 일반화를 측정하기 위해, 주의할 사항을 강조하는 positive, negative sample 을 모두 제공</li></ul></li><li><strong>Sample Ordering</strong> : 선택한 샘플을 올바르게 정렬하는 방법은 무엇?<ul><li><a href="https://doi.org/10.18653/v1/2022.acl-long.556" target="_blank" rel="noopener noreferrer">80</a> : answered prompt 의 정렬은 모델 성능에 중요한 역할하는 점 발견 및 다양한 후보 순열을 평가하기 위해 entropy 기반 방법 제안</li><li><a href="https://doi.org/10.18653/v1/2021.findings-acl.395" target="_blank" rel="noopener noreferrer">62</a> : prompt augmentation 으로 훈련 예제의 좋은 순열을 찾고, prompt 사이의 separator token 을 학습하여 성능 증가</li><li><a href="https://doi.org/10.18653/v1/2021.findings-emnlp.192" target="_blank" rel="noopener noreferrer">145</a> : prompting 을 통한 answered prompt 를 기반으로 meta-prompt 를 생성하는 것을 제안</li><li><a href="https://doi.org/10.1162/tacl_a_00030" target="_blank" rel="noopener noreferrer">37</a> : Prompt Augmentation 은 많은 textual context 를 제공하여 성능을 증가시키는 검색 기법과 관련 있음을 발견</li><li><a href="https://doi.org/10.24432/C5201W" target="_blank" rel="noopener noreferrer">99</a> : 37 방법은 prompt 기반 학습에도 효과적임을 발견<ul><li>37 와의 차이는, prompt augmentation 은 template 과 answer 에 좌우되는 반면, larger context learning 은 그렇지 않다는 것</li></ul></li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="53-prompt-composition">5.3 Prompt Composition<a href="#53-prompt-composition" class="hash-link" aria-label="Direct link to 5.3 Prompt Composition" title="Direct link to 5.3 Prompt Composition">​</a></h2><p><strong><em>Prompt Composition</em></strong> : 여러 하위 프롬프트를 사용하여 각 하위 작업에 수행하고, 해당 하위 프롬프트를 기반으로 composite prompt 를 정의하는 기법</p><p>relation extraction task 의 경우, 두 개체 간의 관계를 추출하는 것으로, 개체 식별 및 개체 분류를 포함한 하위 작업으로 분해할 수 있다.</p><p><a href="https://arxiv.org/pdf/2105.11259.pdf" target="_blank" rel="noopener noreferrer">41</a> : 개체 관계 및 관계 분류에 대한 수동 생성한 여러 sub-prompt 를 생성하고, 관계 추출 로직을 기반으로 완전한 prompt 로 조합</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="54-prompt-decomposition">5.4 Prompt Decomposition<a href="#54-prompt-decomposition" class="hash-link" aria-label="Direct link to 5.4 Prompt Decomposition" title="Direct link to 5.4 Prompt Decomposition">​</a></h2><p>한 샘플로 여러 예측을 수행하는 작업은 전체 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 에 대한 holistic prompt 를 직정 정의하는 것으로 어려움</p><p><strong><em>Prompt Decomposition</em></strong> : holistic prompt 를 여러 sub-prompts 로 분해하여 각 sub-prompts 를 개별적으로 answer 하는 기법</p><p>개체 식별 작업에서, input 을 text span 셋으로 변환하고, 모델은 각 span 에 대한 개체 타입 (&quot;Not an Entity&quot; 포함)을 예측하도록 prompt 될 수 있음. 이는 span 수가 많아서, 각 span 에 대한 여러 prompt 를 생성하고 개별적으로 예측한다.</p><p><a href="https://arxiv.org/pdf/2106.01760.pdf" target="_blank" rel="noopener noreferrer">17</a> :  개체 인식에 대한 prompt decomposition 의 접근법을 조사</p><h1>6 Training Strategies for Prompting Methods</h1><p>prompt 를 통해 모델을 훈련하는 방법을 보자.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="61-training-settings">6.1 Training Settings<a href="#61-training-settings" class="hash-link" aria-label="Direct link to 6.1 Training Settings" title="Direct link to 6.1 Training Settings">​</a></h2><ul><li><strong><em>zero-shot learning</em></strong> : text <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> 확률을 예측하는 간단한 모델로 훈련하지 않고 close / prefix prompt 를 채우는 것을 적용할 수 있다.
이는 특정 task 에 대한 훈련 데이터가 없는 <strong><em>zero-shot learning</em></strong> setting 이라 한다.</li><li><strong><em>full-data learning</em></strong> : 많은 수의 예제를 모델이 훈련</li><li><strong><em>few-shot learning</em></strong> : 적은 수의 예제로 모델 훈련<ul><li>훈련 예제가 충분하지 않고 모델이 올바르게 작동하는데 효과적</li></ul></li></ul><p>annotated 훈련 샘플을 downstream task 훈련 에 사용되지 않지만, downstream task 에 사용할 prompt 생성이나 검증에 사용된다. 이점은 <a href="https://proceedings.neurips.cc/paper/2021/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf" target="_blank" rel="noopener noreferrer">96</a> 에 따르면, downstream task 에 관련한 zero-shot learning 이 아니라고 한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="62-parameter-update-methods">6.2 Parameter Update Methods<a href="#62-parameter-update-methods" class="hash-link" aria-label="Direct link to 6.2 Parameter Update Methods" title="Direct link to 6.2 Parameter Update Methods">​</a></h2><p>prompt 기반 downstream task learning 엔 두 타입의 파라미터다 있다.</p><ul><li>pre-trained LMs</li><li>prompts</li></ul><p>다양한 시나리오에 적용 가능한 수준이 다르기 때문에, 파라미터 결정은 중요하다.</p><p>다음 여부에 따라 5 가지 tuning 전략을 소개</p><ul><li>LM parameter 의 tuning 여부</li><li>prompt 관련 parameter 의 추가 여부</li><li>추가 prompt 가 있는 경우, 해당 parameter 의 tuning 여부</li></ul><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/03aec8fd-5f71-4edd-ac6d-f88c1cba47c6/image.png" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="621-promptless-fine-tuning">6.2.1 Promptless Fine-tuning<a href="#621-promptless-fine-tuning" class="hash-link" aria-label="Direct link to 6.2.1 Promptless Fine-tuning" title="Direct link to 6.2.1 Promptless Fine-tuning">​</a></h3><p><strong><em>Promptless Fine-tuning</em></strong> : pretrained LM 의 모든 parameter (또는 일부 <a href="https://arxiv.org/pdf/1801.06146.pdf" target="_blank" rel="noopener noreferrer">46</a>, <a href="https://doi.org/10.18653/v1/W19-4302" target="_blank" rel="noopener noreferrer">98</a>) 가 downstream task 훈련 샘플에서 prompt 없이 gradient 를 통해 업데이트하는 기법</p><ul><li><a href="https://doi.org/10.18653/v1/N19-1423" target="_blank" rel="noopener noreferrer">BERT</a> 및 <a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noopener noreferrer">RoBERTa</a> : 위 방법으로 pretrained LM 을 finetuning
이 방법은 간단하며 강력하여 널리 사용되지만, 적은 데이터셋에선 과적합 및 안정적인 학습이 안될 수 있음</li><li><a href="https://arxiv.org/pdf/1801.06146.pdf" target="_blank" rel="noopener noreferrer">84</a> : 이러한 모델은 <em>catastrophic forgetting</em> 에 취약. 즉, LM 이 finetuning 전에 한 일을 할 수 없게 되는 것</li><li><strong>Advantages</strong> : 간단, prompt 설계 불필요, LM 의 parameter 를 tuning 하여 큰 데이터셋에 fit 가능</li><li><strong>Disadvantages</strong> : 적은 데이터셋에선 과적합 및 불안정</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="622-tuning-free-prompting">6.2.2 Tuning-free Prompting<a href="#622-tuning-free-prompting" class="hash-link" aria-label="Direct link to 6.2.2 Tuning-free Prompting" title="Direct link to 6.2.2 Tuning-free Prompting">​</a></h3><p><strong><em>Tuning-free Prompting</em></strong> : prompt 기반의 pre-trained LM 의 파라미터를 변경하지 않고 직접 answer 를 생성하는 기법</p><ul><li>answered prompt 를 선택적으로 augmentation 하거나, <strong><em>in-context learning</em></strong> 으로 tuning-free prompting 과 prompt augmentation 을 조합 가능</li><li>일반적으로 tuning-free prompting 의 예로 <a href="https://doi.org/10.18653/v1/D19-1250" target="_blank" rel="noopener noreferrer">LAMA</a> 및 <a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" target="_blank" rel="noopener noreferrer">GPT-3</a> 이 있음</li><li><strong>Advantages</strong> : 효율적, 파라미터 업데이트 과정 없음, catastrophic forgetting 없음, zero-shot 설정 적용 가능</li><li><strong>Disadvantages</strong> : 높은 정확도를 위해선 heavy engineering 필요. in-context learning 에서, 많은 answered prompt 가 제공되면 테스트 시간이 느리고 대규모 훈련셋에 쉽게 사용 불가</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="623-fixed-lm-prompt-tuning">6.2.3 Fixed-LM Prompt Tuning<a href="#623-fixed-lm-prompt-tuning" class="hash-link" aria-label="Direct link to 6.2.3 Fixed-LM Prompt Tuning" title="Direct link to 6.2.3 Fixed-LM Prompt Tuning">​</a></h3><p><strong><em>Fixed-LM Prompt Tuning</em></strong> : prompt 관련 파라미터가 추가되는 상황에, downstream task 훈련 샘플로 얻은 supervision 을 사용하여 prompt 의 파라미터 만을 업데이트하면서도 pretrained LM 은 변하지 않는 기법</p><ul><li>일반적으로 <a href="https://doi.org/10.18653/v1/2021.acl-long.353" target="_blank" rel="noopener noreferrer">Prefix-Tuning</a> 및 <a href="https://doi.org/10.18653/v1/2021.emnlp-main.243" target="_blank" rel="noopener noreferrer">Prompt-Tuning</a> 이 있음</li><li><strong>Advantages</strong> : <em>tuning-free prompting</em> 과 유사하게, LM 의 knowledge 유지 및 few-shot 에 적합. 종종 tuning-free prompting 보다 정확도 높음</li><li><strong>Disadvantages</strong> : zero-shot 불가능, 대규모 데이터셋에선 representation 이 제한됨, hyperparameter / seed prompts 선택을 통한 prompt engineering 필수. 사람이 이해 및 조작할 수 없음</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="624-fixed-prompt-lm-tuning">6.2.4 Fixed-Prompt LM Tuning<a href="#624-fixed-prompt-lm-tuning" class="hash-link" aria-label="Direct link to 6.2.4 Fixed-Prompt LM Tuning" title="Direct link to 6.2.4 Fixed-Prompt LM Tuning">​</a></h3><p><strong><em>Fixed-prompt LM Tuning</em></strong> : pretraining 및 finetuning 으로 LM 을 tuning 하지만, 고정된 파라미터의 prompt 를 사용하여 모델의 동작을 지정하는 기법</p><ul><li>few-shot 상황에 잠재적인 성능 향상을 가져옴</li><li>자연스러운 방법은 모든 훈련 및 테스트 예제에 적용되는 discrete textual template 을 제공하는 것</li><li>일반적으로 <a href="https://doi.org/10.18653/v1/2021.eacl-main.20" target="_blank" rel="noopener noreferrer">117</a>, <a href="https://doi.org/10.18653/v1/2021.emnlp-main.32" target="_blank" rel="noopener noreferrer">118</a> 및 <a href="https://arxiv.org/pdf/2012.15723.pdf" target="_blank" rel="noopener noreferrer">32</a> 이 있음</li><li><a href="https://arxiv.org/pdf/2106.13353.pdf" target="_blank" rel="noopener noreferrer">48</a> : LM finetuning 일부와 prompt answer engineering 의 조합으로 prompt engineering 을 줄일 수 있음을 관찰<ul><li>input 과 mask 를 template word 없는 &quot;<!-- -->[X][Z]<!-- -->&quot; 로 직접 연결한 간단한 템플릿 <em>null prompt</em> 를 정의</li><li>경쟁력 있는 정확도를 달성</li></ul></li><li><strong>Advantages</strong> : Template 및  answer engineering 은 특정 task 에 더 경쟁력 있고 더 효과적인 학습을 함. 특히 few-shot 상황에 좋음.</li><li><strong>Disadvantages</strong> : prompt 가 없으면 Template 및 answer engineering 이 여전히 필요. 한 downstream task 에 finetuning 된 LM 은 다른 downstream task 에 비효율적</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="625-promptlm-tuning">6.2.5 Prompt+LM Tuning<a href="#625-promptlm-tuning" class="hash-link" aria-label="Direct link to 6.2.5 Prompt+LM Tuning" title="Direct link to 6.2.5 Prompt+LM Tuning">​</a></h3><p><strong><em>Prompt+LM Tuning</em></strong> : pretrained LM 의 파라미터의 일부나 모두를 prompt 관련 파라미터와 함께 finetuning 하는 기법</p><ul><li>일반적으로 <a href="https://doi.org/10.1162/tacl_a_00468" target="_blank" rel="noopener noreferrer">PADA</a> 및 <a href="https://arxiv.org/pdf/2103.10385.pdf" target="_blank" rel="noopener noreferrer">P-Tuning</a> 이 있음</li><li>표준 pretraining 과 finetuning 패러다임과 유사하지만, prompt 추가로 모델 훈련 시작 시 추가 부스팅을 제공</li><li><strong>Advantages</strong> : 표현력이 가장 뛰어난 방법, 높은 수준의 데이터에 적합</li><li><strong>Disadvantages</strong> : 모든 모델의 파라미터를 훈련하고 저장이 필요. 적은 데이터셋에는 과적합</li></ul><h1>7 Applications</h1><p>어떠 분야에 사용되었는지 관점으로 섹션 시작</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="71-knowledge-probing">7.1 Knowledge Probing<a href="#71-knowledge-probing" class="hash-link" aria-label="Direct link to 7.1 Knowledge Probing" title="Direct link to 7.1 Knowledge Probing">​</a></h2><ul><li><strong>Factual Probing</strong> : prompting method 를 적용하여 사실을 탐색하는 가장 초기의 시나리오로, LM 의 representation 이 얼마나 사실적 지식을 많이 담는지 정량화하는 것<ul><li>LM parameter 를 고정되고, 수동 / 자동으로 발견될 수 있는 close prompt 로 original input 을 변환하여 지식을 탐색<ul><li><a href="https://doi.org/10.18653/v1/D19-1250" target="_blank" rel="noopener noreferrer">LAMA</a> 및 <a href="https://doi.org/10.18653/v1/2020.emnlp-main.479" target="_blank" rel="noopener noreferrer">X-FACTR</a> 가 관련 데이터셋 포함</li></ul></li><li>answer 가 미리 정의되어, 효과적인 template 및 다양한 모델의 결과 분석에 중점적<ul><li><em>discrete template search</em> [<a href="https://arxiv.org/pdf/2103.05327.pdf" target="_blank" rel="noopener noreferrer">43</a>, <a href="https://doi.org/10.18653/v1/2020.emnlp-main.479" target="_blank" rel="noopener noreferrer">50</a>, <a href="https://doi.org/10.1162/tacl_a_00324" target="_blank" rel="noopener noreferrer">52</a>, <a href="https://proceedings.neurips.cc/paper/2021/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf" target="_blank" rel="noopener noreferrer">96</a>, <a href="https://doi.org/10.24432/C5201W" target="_blank" rel="noopener noreferrer">99</a>, <a href="https://doi.org/10.18653/v1/D19-1250" target="_blank" rel="noopener noreferrer">100</a>, <a href="https://arxiv.org/pdf/2010.15980.pdf" target="_blank" rel="noopener noreferrer">125</a>]  <em>continuous template learning</em> [<a href="https://arxiv.org/pdf/2103.10385.pdf" target="_blank" rel="noopener noreferrer">77</a> <a href="https://arxiv.org/pdf/2104.06599.pdf" target="_blank" rel="noopener noreferrer">103</a> <a href="https://www.sciencedirect.com/science/article/pii/S000437020200190X" target="_blank" rel="noopener noreferrer">152</a>] 및 prompt ensemble learning [<a href="https://doi.org/10.1162/tacl_a_00324" target="_blank" rel="noopener noreferrer">52</a> <a href="https://arxiv.org/pdf/2104.06599.pdf" target="_blank" rel="noopener noreferrer">103</a>] 가 탐구됨</li></ul></li></ul></li><li><strong>Linguistic Probing</strong> : 대규모 pretrained LM 은 analogies <a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" target="_blank" rel="noopener noreferrer">9</a>, negations <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00298/43535/What-BERT-Is-Not-Lessons-from-a-New-Suite-of" target="_blank" rel="noopener noreferrer">25</a>, semantic role sensitivity <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00298/43535/What-BERT-Is-Not-Lessons-from-a-New-Suite-of" target="_blank" rel="noopener noreferrer">25</a>, semantic similarity <a href="https://arxiv.org/pdf/2107.02137.pdf" target="_blank" rel="noopener noreferrer">131</a>, understanding <a href="https://arxiv.org/pdf/2107.02137.pdf" target="_blank" rel="noopener noreferrer">131</a> 및 rare word understanding <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6403" target="_blank" rel="noopener noreferrer">116</a> 가능<ul><li>위 지식은 LM 이 완성해야할 자연어 문장의 형태로 <em>lignuistic probing</em> 작업을 제시하여 도출</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="72-structure-prediction">7.2 Structure Prediction<a href="#72-structure-prediction" class="hash-link" aria-label="Direct link to 7.2 Structure Prediction" title="Direct link to 7.2 Structure Prediction">​</a></h2><ul><li><strong>Semantic Parsing</strong> : 자연어가 주어지면 구조화된 의미있는 representation 을 생성하는 작업<ul><li><a href="https://doi.org/10.18653/v1/2021.emnlp-main.608" target="_blank" rel="noopener noreferrer">124</a> : LM 으로 few-shot semantic parsing 에 대한 task 를 탐구<ol><li>의미있는 파싱 작업을 paraphrasing 작업으로 재구성</li><li>문법에 따라 유효한 출력만 허용하여 디코딩</li></ol><ul><li><em>in-context learning</em> 으로 테스트 예제와 의미있게 가까운 answered prompt 선택</li><li>pretrained LM 으로 의미있는 파싱에 대한 paraphrasing 재구성의 효과를 입증</li></ul></li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="73-classification-based-tasks">7.3 Classification-based Tasks<a href="#73-classification-based-tasks" class="hash-link" aria-label="Direct link to 7.3 Classification-based Tasks" title="Direct link to 7.3 Classification-based Tasks">​</a></h2><p>classification-based tasks 는 텍스트 분류와 자연어 추론과 같이 템플릿을 쉽게 구성할 수 있다.</p><p>핵심 prompting 은 적절한 prompt 를 구성하는 것이다. 예로, <a href="https://doi.org/10.18653/v1/D19-1404" target="_blank" rel="noopener noreferrer">144</a> 에서는 &quot;이 문서의 주제는 <!-- -->[Z]<!-- -->.&quot; 같은 prompt 를 사용하며, 이 prompt 는 슬롯을 채우기 위해 masked pretrained LM 에 입력된다.</p><ul><li><strong>Text Classification</strong><ul><li>이전 연구들에선 대부분 close prompt 사용</li><li>prompt template engineering [<a href="https://arxiv.org/pdf/2012.15723.pdf" target="_blank" rel="noopener noreferrer">32</a>. <a href="https://doi.org/10.18653/v1/2021.acl-long.381" target="_blank" rel="noopener noreferrer">40</a>, <a href="https://doi.org/10.18653/v1/2021.emnlp-main.243" target="_blank" rel="noopener noreferrer">67</a>] 및 prompt answer engineering [<a href="https://arxiv.org/pdf/2012.15723.pdf" target="_blank" rel="noopener noreferrer">32</a>, <a href="https://doi.org/10.18653/v1/2020.coling-main.488" target="_blank" rel="noopener noreferrer">115</a>, <a href="https://doi.org/10.18653/v1/2021.eacl-main.20" target="_blank" rel="noopener noreferrer">117</a>] 이 탐구되어, <em>fixed-prompt LM Tuning</em> 으로 few-shot 에서 텍스트 분류에 대한 prompt 효율성을 탐구</li></ul></li><li><strong>Text Pair Classification</strong> : 두 문장 간의 관계 (유사성, 함축 등)를 예측하는 작업<ul><li>paraphrase 식별, 자연어 추론, 텍스트 유사성 예측 등의 작업 포함</li><li>text classification 과 유사하게, close prompt 일반적으로 사용 [<a href="https://doi.org/10.18653/v1/2021.eacl-main.20" target="_blank" rel="noopener noreferrer">117</a>, <a href="https://doi.org/10.18653/v1/2021.naacl-main.185" target="_blank" rel="noopener noreferrer">120</a>]</li><li>few-shot 의 template 에 중점을 두거나 answer space <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Z</mi></mrow><annotation encoding="application/x-tex">\mathcal{Z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.07944em">Z</span></span></span></span></span> 를 vocabulary 에서 수동으로 선택하는 연구 존재</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="74-information-extraction">7.4 Information Extraction<a href="#74-information-extraction" class="hash-link" aria-label="Direct link to 7.4 Information Extraction" title="Direct link to 7.4 Information Extraction">​</a></h2><p>prompt 를 구성하는 데, 섬세함이 classification task 보다 더 필요</p><ul><li><strong>Relation Extraction</strong> : 문장 내의 두 개체 간의 관계 예측<ul><li><a href="https://doi.org/10.1145/3485447.3511998" target="_blank" rel="noopener noreferrer">13</a> : relation extraction 에서 처음으로 <em>fixed-prompt LM Tuning</em> 기법 적용 및 classification task 로 부터의 prompting 상속을 방해하는 두 가지를 논의<ul><li>더 큰 label space (예; 80개 관계 추출 vs 이진 감정 분류) 는 prompt answer engineering 에 큰 어려움 초래</li><li>관계 추출에서 input sequence 의 여러 token 들은 중요도가 다름.</li></ul></li><li>위 문제 해결을 위해, adaptive answer selection method 제안<ul><li>task-oriented prompt template 구축</li><li>template 에서 entity mention 을 강조하기 위해 특수 마커 (예; <!-- -->[E]<!-- -->) 사용</li></ul></li><li><a href="https://arxiv.org/pdf/2105.11259.pdf" target="_blank" rel="noopener noreferrer">41</a> : 위와 유사하게, multiple prompt 를 통해 개체 유형 정보를 통합</li></ul></li><li><strong>Named Entity Recognition</strong> : 문장 내의 named entity 식별 (예; 사람 이름, 지역)<ul><li>tagging task 에 prompt-based learning 적용이 어려움<ul><li>예측 단위가 text 가 아닌 token 이나 span 임</li><li>token label 간의 잠재적인 관계가 존재</li></ul></li><li><a href="https://arxiv.org/pdf/2106.01760.pdf" target="_blank" rel="noopener noreferrer">17</a> : BART 로 template-based NER 모델 제안<ul><li>text span 열거 및 수동 생성된 template 내에서 각 타입의 생성 확률 고려</li><li>&quot;마이크는 어제 뉴욕에 갔다&quot; 가 주어지면 &quot;마이크는 <!-- -->[Z]<!-- --> 개체다.&quot; 라는 template 으로 결정
answer space <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">Z</mi></mrow><annotation encoding="application/x-tex">\mathcal{Z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.07944em">Z</span></span></span></span></span> 는 &quot;사람&quot;, &quot;조직&quot; 과 같은 값으로 구성</li></ul></li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="75-reasoning-in-nlp">7.5 &quot;Reasoning&quot; in NLP<a href="#75-reasoning-in-nlp" class="hash-link" aria-label="Direct link to 7.5 &quot;Reasoning&quot; in NLP" title="Direct link to 7.5 &quot;Reasoning&quot; in NLP">​</a></h2><p>신경망이 &quot;추론&quot; 을 하는지 &quot;페턴&quot; 을 인식하는지는 아직도 논쟁이다. 추론 능력 조사를 위해 다양한 시나리오를 포괄하는 벤치마크 작업을 정의하는 시도가 많다.</p><ul><li><strong>Commonsense Reasoning</strong> : NLP 의 상식 추론을 테스트 하는 것. 많은 벤치마크 데이터셋 존재 [<a href="https://doi.org/10.18653/v1/D19-1243" target="_blank" rel="noopener noreferrer">47</a>, <a href="https://doi.org/10.18653/v1/2020.findings-emnlp.165" target="_blank" rel="noopener noreferrer">72</a>, <a href="https://doi.org/10.18653/v1/2020.emnlp-main.185" target="_blank" rel="noopener noreferrer">101</a>, <a href="https://doi.org/10.18653/v1/P19-1487" target="_blank" rel="noopener noreferrer">107</a>]<ul><li><a href="https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf" target="_blank" rel="noopener noreferrer">68</a> : 모호한 대명사를 선행 식별하거나 여러 선택 중 문장을 완성하도록 모델에게 요구하여 해결<ul><li>전자의 경우, &quot;트로피가 갈색 가방에 못 들어가. 이것은 너무 커.&quot; 에서 &quot;이것&quot; 이 트로피인지 가방인지 추론</li><li>후자의 경우, &quot;Eleanor 은 손님에게 커피를 제안했다. 그녀는 깨끗한 <!-- -->[Z]<!-- --> 가 없단걸 깨달았다&quot; 에서 후보군은 &quot;컵&quot;, &quot;그릇&quot;, &quot;숟가락&quot; 이다.</li></ul></li><li><a href="https://arxiv.org/pdf/1806.02847.pdf" target="_blank" rel="noopener noreferrer">134</a> : 잠재적 후보의 다양한 선택 확률을 계산하여 pretrained LM 으로 가장 높은 확률을 택하여 전자 해결</li><li><a href="https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00298/1923116/tacl_a_00298.pdf" target="_blank" rel="noopener noreferrer">25</a> : 각 후보의 생성 확률을 평가하여 가장 높은 확률을 선택하여 후자 해결</li></ul></li><li><strong>Mathematical Reasoning</strong> : 산술, 함수 등과 같은 수학 문제를 해결하는 것<ul><li>pretrained embedding 및 LM 은 작은 숫자에서 산술을 수행하지만, 숫자가 크면 실패하는 것을 발견 [<a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" target="_blank" rel="noopener noreferrer">9</a>, <a href="https://doi.org/10.18653/v1/P19-1329" target="_blank" rel="noopener noreferrer">88</a>, <a href="https://doi.org/10.18653/v1/D19-1534" target="_blank" rel="noopener noreferrer">139</a>]</li><li><a href="https://doi.org/10.1145/3411763.3451760" target="_blank" rel="noopener noreferrer">110</a> : 복잡한 수학 추론 문제 탐구 (예; <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mo>∗</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x) = x * x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4653em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(f(3))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord">3</span><span class="mclose">))</span></span></span></span></span> =?) 및 질문에 대한 추론을 직렬화하여 LM 성능 향상</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="76-question-answering">7.6 Question Answering<a href="#76-question-answering" class="hash-link" aria-label="Direct link to 7.6 Question Answering" title="Direct link to 7.6 Question Answering">​</a></h2><p><strong><em>Question answering (QA)</em></strong> : context document 를 기반으로 input question 에 대한 answer 제공을 목표</p><p>QA 는 다양한 형태 존재</p><ul><li>extractive QA [<a href="https://doi.org/10.18653/v1/D16-1264" target="_blank" rel="noopener noreferrer">SQuAD</a>] : context document 에서 answer 을 포함하는 내용 식별</li><li>multiple-choice QA [<a href="https://doi.org/10.18653/v1/D17-1082" target="_blank" rel="noopener noreferrer">RACE</a>] : 모델이 여러 선택지 중에 선택</li><li>free-form QA [<a href="https://doi.org/10.1162/tacl_a_00023" target="_blank" rel="noopener noreferrer">NarrativeQA</a>] : 모델이 임의의 텍스트 문자열을 answer 로 반환</li></ul><p>이런 다양한 형태는 서로 다른 모델링 프레임워크로 처리하지만, prompting 을 통하면 한 프레임워크로 처리할 수 있다는 장점이 있다.</p><ul><li><a href="https://doi.org/10.18653/v1/2020.findings-emnlp.171" target="_blank" rel="noopener noreferrer">55</a> : context 와 question 으로 적절한 프롬프트 및 seq2seq pretrained T5 를 finetuning 하여 QA task 를 text generation 문제로 재구성 </li><li><a href="https://doi.org/10.1162/tacl_a_00407" target="_blank" rel="noopener noreferrer">51</a> : seq2seq pretrained LMs (T5, BART, GPT2) 으로 QA task 를 관찰하여, 이러한 모델들의 확률이 QA 작업에 유용하지 않다는 점 발견</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="77--text-generation">7.7  Text Generation<a href="#77--text-generation" class="hash-link" aria-label="Direct link to 7.7  Text Generation" title="Direct link to 7.7  Text Generation">​</a></h2><p><strong><em>Text Generation</em></strong> : 다른 정보에 따라 텍스트를 생성하는 작업들의 집합. prompting 방법은 <em>prefix prompt</em> 와 함께 autoregressive pretrained LM 으로 쉽게 적용 가능</p><ul><li><a href="https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf" target="_blank" rel="noopener noreferrer">105</a> : &quot;프랑스어 번역, <!-- -->[X]<!-- -->, <!-- -->[Z]<!-- -->.&quot; 같은 prompt 를 사용하여 텍스트 요약 및 번역의 생성 작업의 놀라운 성능 입증</li><li><a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" target="_blank" rel="noopener noreferrer">9</a> : 텍스트 생성에 <em>in-context learning</em> 수행, multiple <em>answered prompt</em> 로 수동 템플릿 및 augmenting 와 함께 prompt 생성</li><li><a href="https://doi.org/10.18653/v1/2021.emnlp-main.32" target="_blank" rel="noopener noreferrer">118</a> : 수동 생성된 template 로 few-shot 텍스트 요약에 대한 <em>fixed-prompt LM tuning</em> 탐구</li><li><a href="https://doi.org/10.18653/v1/2021.acl-long.353" target="_blank" rel="noopener noreferrer">71</a> : few-shot 에서 텍스트 요약 및 data-to-text 생성에 대한 <em>fixed-LM prompt tuning</em> 을 탐구<ul><li>learnable <em>prefix token</em> 을 input 앞에 붙임</li><li>pretrained LM 의 파라미터를 유지</li></ul></li><li><a href="https://doi.org/10.18653/v1/2021.naacl-main.384" target="_blank" rel="noopener noreferrer">23</a> : 텍스트 요약 task 에서 <em>prompt+LM tuning</em> 전략 탐구<ul><li>learnable <em>prefix prompt</em> 사용 및 pretrained LM 의 파라미터와 함께 업데이트</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="78-automatic-evaluation-of-text-generation">7.8 Automatic Evaluation of Text Generation<a href="#78-automatic-evaluation-of-text-generation" class="hash-link" aria-label="Direct link to 7.8 Automatic Evaluation of Text Generation" title="Direct link to 7.8 Automatic Evaluation of Text Generation">​</a></h2><p><a href="https://proceedings.neurips.cc/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf" target="_blank" rel="noopener noreferrer">147</a> : 생성된 텍스트에 자동 평가를 prompt learning 으로 사용될 수 있음을 입증</p><p>pretrained seq2seq 을 사용하여 생성된 텍스트의 평가를 텍스트 생성 문제로 개념화하고 pretraining task 와 가깝게 평가하도록 하는 <em>prefix prompt</em> 사용</p><p>실험적으로 변역된 텍스트에 &quot;such as&quot; 문구를 추가하여, 독일어-영어 번역 평가에서 상당한 관계 개선을 가져올 수 있음을 발견</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="79-meta-applications">7.9 Meta-Applications<a href="#79-meta-applications" class="hash-link" aria-label="Direct link to 7.9 Meta-Applications" title="Direct link to 7.9 Meta-Applications">​</a></h2><p>prompting 기술은 NLP task 뿐 아니라 다른 task 에도 모델을 훈련하는 데 유용한 요소로 작용</p><ul><li><strong>Domain Adaptation</strong> : 한 도메일에서 다른 도메인으로 적응 시키는 것 (예; 뉴스 → 소셜 미디아)<ul><li><a href="https://doi.org/10.1162/tacl_a_00468" target="_blank" rel="noopener noreferrer">5</a> : 원본 텍스트 input 을 augmentation 하기 위해 self-generated DRFs 사용 및 seq2seq 모델로 시퀀스 태깅 수행</li></ul></li><li><strong>Debiasing</strong><ul><li><a href="https://arxiv.org/pdf/2103.00453.pdf" target="_blank" rel="noopener noreferrer">121</a> : LMs 가 biased / debiased instruction 에 따라 self-diagnosis / self-bebiasing 을 수행할 수 있음을 발견<ul><li>self-diagnosis 의 경우</li></ul><ol><li>폭력적인 정보가 포함되었는지 self-diagnosis 하기 위해, &quot;The following text contains violence. <!-- -->[X][Z]<!-- -->&quot; 사용 가능</li><li>ㅤ<!-- -->[X]<!-- --> 를 채우고 <!-- -->[Z]<!-- --> 의 생성 확률을 본다.</li><li>&quot;Yes&quot; 와 &quot;No&quot; 의 확률을 통해 폭력이 포함되었는지 아닌지 추정</li></ol><ul><li>debiasing 의 경우</li></ul><ol><li>input 이 주어지면 다음 단어의 확률 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(x_t | x_{&lt;t}; \theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span></span></span></span> 계산</li><li>self-diagnosis input 을 원본 input 에 추가하여 다음 단어의 확률 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><mo stretchy="false">[</mo><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo separator="true">;</mo><msub><mi>x</mi><mtext>diagnosis</mtext></msub><mo stretchy="false">]</mo><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(x_t | [x_{&lt;t};x_{\textup{diagnosis}}];\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord textup mtight">diagnosis</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span></span></span></span></span> 계산</li><li>다음 토큰에 대한 위의 두 확률 분포를 결합하여 원하지 않는 속성을 막음</li></ol></li></ul></li><li><strong>Dataset Construction</strong><ul><li><a href="https://doi.org/10.18653/v1/2021.eacl-main.20" target="_blank" rel="noopener noreferrer">117</a> : 특정 instruction 이 주어지면 데이터셋을 생성하기 위해 pretrained LM 사용을 제안<ul><li>의미적으로 유사한 문장으로 데이터셋 구성할 경우, 각 input 문장은 다음과 같은 template 을 사용할 수 있다.
&quot;Write two sentences that mean the same thing. <!-- -->[X][Z]<!-- -->&quot;
그리고 같은 의미를 공유하는 문장을 생성할 수 있다.</li></ul></li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="710-multi-modal-learning">7.10 Multi-modal Learning<a href="#710-multi-modal-learning" class="hash-link" aria-label="Direct link to 7.10 Multi-modal Learning" title="Direct link to 7.10 Multi-modal Learning">​</a></h2><p><a href="https://proceedings.neurips.cc/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf" target="_blank" rel="noopener noreferrer">135</a> : NLP 의 prompt learning 을 <em>multi-modal</em> 에서 적용</p><p><em>fixed-LM prompting tuning</em> 과 <em>prompt augmentation</em> 사용</p><p>각 이미지를 continuous embedding 의 시퀀스로 표현하고 파라미터가 고정된 pretrained LM 으로 프롬프트화하여 image caption 을 생성</p><p>위 결과는 few-shot learning 능력을 보여줌</p><p>→ few demonstration (answered prompt) 을 통해 시스템이 새로운 객체와 시각점 카테고리에 대한 단어를 빠르게 학습</p><h1>8 Prompt-Relevant Topics</h1><p>prompt-based learning 의 본질 및 다른 learning method 와의 관계를 알아보자.</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/2d9ca2c5-8403-4ada-ae00-390cf9c40830/image.png" class="img_ev3q"></p><ul><li><strong>Ensemble Learning</strong> : 여러 시스템의 상보성(complementarity) 의 이점을 사용하여 task 의 성능 향상을 목적<ul><li>앙상블은 여러 시스템의 아키텍처, 학습 전략, 데이터 순서 또는 무작위 초기화로 생성</li><li>prompt template 의 선택 또한 여러 결과를 생성하는 하나의 방법<ul><li>여러 번 학습할 필요가 없는 이점</li><li>예로, discrete prompt 사용 시 추론 단계에서 간단히 변경 가능 [<a href="https://doi.org/10.1162/tacl_a_00324" target="_blank" rel="noopener noreferrer">52</a>]</li></ul></li></ul></li><li><strong>Few-shot Learning</strong> : 적은 훈련 샘플로 데이터가 적은 상황에 훈련하는 것을 목표. 다양한 방법 존재<ul><li>model agnostic meta-learning [<a href="http://proceedings.mlr.press/v70/finn17a/finn17a.pdf" target="_blank" rel="noopener noreferrer">29</a>] : 새로운 task 에 빨리 적응하는 feature 학습</li><li>embedding learning [<a href="https://proceedings.neurips.cc/paper/2016/file/839ab46820b524afda05122893c2fe8e-Paper.pdf" target="_blank" rel="noopener noreferrer">8</a>] : 유사한 샘플이 서로 가깝도록 각 샘플을 저차원 공간에 임베딩</li><li>memory-based learning [<a href="https://arxiv.org/pdf/1703.03129" target="_blank" rel="noopener noreferrer">53</a>] : 각 샘플이 메모리에서 내용을 weighted average</li><li>prompt augmentation [<a href="https://doi.org/10.18653/v1/2021.findings-acl.395" target="_blank" rel="noopener noreferrer">62</a>] : few-shot 을 위한 방법으로 볼 수 있음. 파라미터 tuning 없이 pretrained LM 에서 knowledge 를 유도하여 여러 샘플을 더 추가 가능</li></ul></li><li><strong>Larger-context Learning</strong> : input 에 학습 데이터셋 [<a href="https://doi.org/10.18653/v1/P18-1015" target="_blank" rel="noopener noreferrer">11</a>] 또는 외부 데이터 소스 [<a href="https://arxiv.org/pdf/2002.08909.pdf" target="_blank" rel="noopener noreferrer">38</a>] 에서 검색된 추가적인 문맥 정보로 augmentation 하여 성능을 향상하는 것이 목표<ul><li>Prompt Augmentation 은 input 에 관련 라벨 샘플을 추가하지만, <em>larger-context learning</em> 과의 차이점은 라벨 데이터가 반드시 필요하지 않다는 점</li></ul></li><li><strong>Query Reformulation</strong> : input query 를 관련된 용어로 확장하거나 paraphrasing 을 생성하여 관련성 높은 텍스트를 유도하는 것이 목표<ul><li>정보 검색 [<a href="https://doi.org/10.18653/v1/P19-1329" target="_blank" rel="noopener noreferrer">90</a>] 및 QA task [<a href="https://openreview.net/forum?id=S1CChZ-CZ" target="_blank" rel="noopener noreferrer">10</a>, <a href="https://doi.org/10.18653/v1/2020.scai-1.2" target="_blank" rel="noopener noreferrer">136</a>] 에서 사용</li><li>prompt-based learning 와 query reformulation 간의 공통성이 있음<ul><li>기존 지식 베이스를 더 잘 활용하기 위해 올바른 질문을 던짐</li><li>지식 베이스는 일반적으로 black-box 여서, 질문에만 기반한 최적화 방법을 학습해야 함</li></ul></li><li>차이점 또한 존재<ul><li>query reformulation : 지식 베이스는 search engine [<a href="https://doi.org/10.18653/v1/P19-1329" target="_blank" rel="noopener noreferrer">90</a>] / QA system [<a href="https://openreview.net/forum?id=S1CChZ-CZ" target="_blank" rel="noopener noreferrer">10</a>] 에 사용</li><li>prompt-based learning : 지식 베이스를 LM 으로 정의 및 적절한 answer 유도를 위해 적절한 프롬프트 탐색 필요</li></ul></li><li>위 차이점에도 불구하고, query reformulation 은 prompt learning 에 도움이 됨</li></ul></li><li><strong>QA-based Task Reformulation</strong> : 다양한 NLP task 를 question-answer 문제로 개념화 하는 것을 목표<ul><li>어떤 task 를 수행할지 지정하기 위해 text question 을 사용하는 점에서 prompting 방법과 유사</li><li><a href="http://proceedings.mlr.press/v48/kumar16.pdf" target="_blank" rel="noopener noreferrer">61</a>. <a href="https://arxiv.org/pdf/1806.08730.pdf" target="_blank" rel="noopener noreferrer">83</a> : 다양한 NLP task 를 QA 프레임워크로 통합을 시도한 초기 연구</li><li>정보 추출 [<a href="https://doi.org/10.18653/v1/2020.acl-main.519" target="_blank" rel="noopener noreferrer">70</a>, <a href="https://doi.org/10.18653/v1/2020.acl-main.622" target="_blank" rel="noopener noreferrer">142</a>] 및 텍스트 분류 [<a href="http://proceedings.mlr.press/v119/chai20a/chai20a.pdf" target="_blank" rel="noopener noreferrer">12</a>] 로 위 아이디어 더욱 연구</li><li><a href="https://arxiv.org/pdf/1909.09031.pdf" target="_blank" rel="noopener noreferrer">argRanker</a> : 논쟁적인 관계 분류를 수동으로 연결한 두 문장의 랭킹 문제로 개념화</li></ul></li><li><strong>Controlled Generation</strong> : input text 외의 다양한 유형의 가이드를 생성 모델에 통합하는 것을 목표<ul><li>guidance signal 은 <em>style token</em> [<a href="https://arxiv.org/pdf/1711.05217.pdf" target="_blank" rel="noopener noreferrer">27</a>, <a href="https://doi.org/10.18653/v1/P16-1009" target="_blank" rel="noopener noreferrer">123</a>], <em>length spacifications</em> [<a href="https://doi.org/10.18653/v1/D16-1140" target="_blank" rel="noopener noreferrer">56</a>], <em>domain tags</em> [<a href="https://doi.org/10.18653/v1/P17-2061" target="_blank" rel="noopener noreferrer">14</a>] 또는 생성된 텍스트를 제어하기 위해 사용되는 다양한 다른 정보일 수 있다.
생성된 텍스트의 내용을 계획하기 위해 <em>keywords</em> [<a href="https://aclanthology.org/P17-2061.pdf" target="_blank" rel="noopener noreferrer">112</a>], <em>relation triples</em> [<a href="https://doi.org/10.18653/v1/2021.naacl-main.58" target="_blank" rel="noopener noreferrer">154</a>] 또는 <em>highlighted phrases or sentences</em> [<a href="https://doi.org/10.18653/v1/N18-1025" target="_blank" rel="noopener noreferrer">34</a>, <a href="https://doi.org/10.18653/v1/2021.naacl-main.113" target="_blank" rel="noopener noreferrer">78</a>] 일 수도 있다.</li><li>이 작업에서 prompt 는 task 지정에 사용되며, 다음 두 유형 사이에 공통점을 발견할 수 있음<ul><li>나은 생성을 위해 input text 에 정보를 추가하며, 이러한 additional signals 는 learnable parameter 이다</li><li>&quot;controlled generation&quot; 을 seq2seq pretrained LM (예;BART) 로 얻었다면, input 종송적인 prompt 및 <em>prompt+LM fine-tuning</em> 전략을 가진 prompt learning 으로 간주 가능.
예; prompt 및 LM 파라미터로 tuning 가능한 <a href="https://doi.org/10.18653/v1/2021.naacl-main.384" target="_blank" rel="noopener noreferrer">GSum</a></li></ul></li><li>controlled generation 과 prompt-based text generation 의 차이<ul><li>control 은 생성 스타일이나 내용 제어하는데 사용 [<a href="https://doi.org/10.18653/v1/2021.naacl-main.384" target="_blank" rel="noopener noreferrer">23</a>, <a href="https://arxiv.org/pdf/1711.05217" target="_blank" rel="noopener noreferrer">27</a>] 하면서도 동일한 task 상태로 유지. pretrained LM 이 필수적이지 않음</li><li>text generation 의 prompt 사용 동기는 task 명시 및 pretrained LM 활용</li><li>text generation 의 prompt learning 은 최근 연구에서 데이터셋 또는 task-level prompt 를 공유 [<a href="https://doi.org/10.18653/v1/2021.acl-long.353" target="_blank" rel="noopener noreferrer">71</a>]</li><li>몇몇 연구에서만 input 종속성에 대해 탐구하지만, contolled text generation 에서 일반적인 세팅이며 효과적이다. prompt learning 에 대한 미래 연구의 방향을 제공할 수도 있다.</li></ul></li></ul></li><li><strong>Supervised Attention</strong> : 데이터 기반 attention 은 과적합될 수 있어, 모델의 attention 을 supervised 로 제공하는 것을 목표 [<a href="https://aclanthology.org/P17-1164.pdf" target="_blank" rel="noopener noreferrer">76</a>]<ul><li>주요 정보에 attention 하는 것은, long text sequence [<a href="https://arxiv.org/pdf/1609.04186" target="_blank" rel="noopener noreferrer">75</a>, <a href="https://proceedings.neurips.cc/paper/2020/file/460191c72f67e90150a093b4585e7eb4-Paper.pdf" target="_blank" rel="noopener noreferrer">129</a>], images [<a href="https://arxiv.org/pdf/1608.05203.pdf" target="_blank" rel="noopener noreferrer">130</a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7476326/" target="_blank" rel="noopener noreferrer">149</a>] 또는 knowledge bases [<a href="https://doi.org/10.18653/v1/2021.naacl-main.384" target="_blank" rel="noopener noreferrer">23</a>, <a href="https://doi.org/10.1145/3512467" target="_blank" rel="noopener noreferrer">146</a>] 과 같은 객체에서 유용한 정보를 추출하는데 핵심적인 단계</li><li>prompt learning 및 supervised attention 은 어떠한 단서로 주요 정보를 추출하는 아이디어가 같으며, 이 단서는 별도로 제공되야 한다.<ul><li>이를 해결하기 위해, supervised attention 은 수동으로 라벨링된 corpus 에서 gold attention 예측을 위해 추가 손실 함수를 사용하여 학습을 시도 [<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Gan_VQS_Linking_Segmentations_ICCV_2017_paper.pdf" target="_blank" rel="noopener noreferrer">31</a>, <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Jiang_SALICON_Saliency_in_2015_CVPR_paper.pdf" target="_blank" rel="noopener noreferrer">49</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/download/12272/12131" target="_blank" rel="noopener noreferrer">102</a>]</li></ul></li></ul></li><li><strong>Data Augmentation</strong> : 기존 데이터를 수정하여 훈련에 사용될 데이터 양을 늘리는 기술 [<a href="https://doi.org/10.18653/v1/P17-2090" target="_blank" rel="noopener noreferrer">26</a>, <a href="https://proceedings.neurips.cc/paper/2017/file/f26dab9bf6a137c3b6782e562794c2f2-Paper.pdf" target="_blank" rel="noopener noreferrer">109</a>]<ul><li><a href="https://doi.org/10.18653/v1/2021.naacl-main.208" target="_blank" rel="noopener noreferrer">114</a> : prompt 추가가 분류 작업 전반에 걸쳐 100개 이상의 데이터 포인트 추가와 유사한 정확도 향상을 평균적으로 얻을 수 있음을 발견<ul><li>downstream task 에 대한 prompt 사용이 data augmentation 을 암묵적으로 수행하는 것과 유사하다는 것을 시사</li></ul></li></ul></li></ul><h1>9 Challenges</h1><p>prompt-based learning 은 다양한 task 와 상황에 대해 상당한 잠재력을 보여주지만, 아직 몇몇 과제들이 있다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="91-selection-of-pre-trained-lms">9.1 Selection of Pre-trained LMs<a href="#91-selection-of-pre-trained-lms" class="hash-link" aria-label="Direct link to 9.1 Selection of Pre-trained LMs" title="Direct link to 9.1 Selection of Pre-trained LMs">​</a></h2><p>다양한 LMs 가 있어, prompt-based learning 을 더 잘 활용하기 위해 선택하는 법도 과제이다.</p><p>현재까지 다양한 pretrained LM 에 대한 prompt-based learning 이점의 체계적인 비교가 거의 없거나 전무하다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="92-prompt-design">9.2 Prompt Design<a href="#92-prompt-design" class="hash-link" aria-label="Direct link to 9.2 Prompt Design" title="Direct link to 9.2 Prompt Design">​</a></h2><ul><li><strong>Tasks beyond Classification and Generation</strong> : prompt-based learning 은 text classification / generation-based task 에 비해 information extraction / text analysis task 는 덜 다루어졌다. 이유는 prompt 설계가 덜 직관적이기 때문이다.<ul><li>추후, 적절한 텍스트 형태로의 구조화된 출력을 표현하는 효과적인 prompt answer engineering 이나, 텍스트 분류 및 생성 작업처럼 재구성이 필요할 것으로 보임</li></ul></li><li><strong>Prompting with Structured Information</strong> : NLP task 에서 input 은 tree, graph, table, relational structure 등 다양하게 표현할 수 있는데, template / answer engineering 에서 어떻게 잘 표현할지가 과제<ul><li><a href="https://doi.org/10.1145/3485447.3511998" target="_blank" rel="noopener noreferrer">13</a> : 기존 연구는 entity marking 처럼, 어휘 정보를 인코딩하여 추가적인 marks 와 prompt 를 만들어서 단계를 나아간다.</li><li><a href="https://arxiv.org/pdf/2107.06955.pdf" target="_blank" rel="noopener noreferrer">1</a> : fine-grained web text 생성에 대해 hyper text markup language 를 기반으로 구조화된 prompt 제안<ul><li>하지만 이 방법은 복잡한 구조의 다양한 형태로 확장하는 것은 아직 탐구되지 않아, 흥미로운 연구 주제일 수 있다.</li></ul></li></ul></li><li><strong>Entanglement of Template and Answer</strong> : 모델 성능은 사용 중인 template 과 고려 중인 answer 에 따라 달라진다.<ul><li>template 과 answer 의 최상의 조합을 동시에 탐색하거나 학습하는 방법은 여전히 어려운 문제</li><li>최근 template 선택 전에 answer 을 선택하지만 [<a href="https://arxiv.org/pdf/2012.15723.pdf" target="_blank" rel="noopener noreferrer">32</a>, <a href="https://arxiv.org/pdf/2010.15980" target="_blank" rel="noopener noreferrer">125</a>] , <a href="https://doi.org/10.18653/v1/2021.acl-long.381" target="_blank" rel="noopener noreferrer">40</a> 에서는 두 가지의 동시 학습의 잠재력을 입증</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="93-prompt-answer-engineering">9.3 Prompt Answer Engineering<a href="#93-prompt-answer-engineering" class="hash-link" aria-label="Direct link to 9.3 Prompt Answer Engineering" title="Direct link to 9.3 Prompt Answer Engineering">​</a></h2><ul><li><strong>Many-class Classification Tasks</strong> : class 가 너무 많은 경우, 적절한 answer space 를 선택하는 방법은 어려운 최적화 문제</li><li><strong>Long-answer Classification Tasks</strong> : multi-token answer 을 사용하는 경우, LMs 를 사용하여 다중 토큰을 잘 디코딩하는 방법은 아직 알려지지 않았으며, 몇 가지 다중 토큰 디코딩 방법이 제안되었지만 [<a href="https://doi.org/10.18653/v1/2020.emnlp-main.479" target="_blank" rel="noopener noreferrer">50</a>], 여전히 최적이지 않음</li><li><strong>Multiple Answers for Generation Tasks</strong> : text generation 의 경우, 적절한 answer 는 의미는 동등하지만 문법적으로는 다양<ul><li>거의 모든 연구가 single answer 에 의존하여 text generation 을 prompt learning 을 사용하며, 예외적인 경우는 거의 없음 [<a href="https://doi.org/10.1162/tacl_a_00324" target="_blank" rel="noopener noreferrer">52</a>]</li><li>멀티 레퍼런스로 학습 과정을 잘 가이드하는 방법은 여전히 크게 연구되지 않은 문제</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="94-selection-of-tuning-strategy">9.4 Selection of Tuning Strategy<a href="#94-selection-of-tuning-strategy" class="hash-link" aria-label="Direct link to 9.4 Selection of Tuning Strategy" title="Direct link to 9.4 Selection of Tuning Strategy">​</a></h2><p>prompt, LMs, 또는 둘 모두의 파라미터 튜닝에는 다양한 방법이 있다.</p><p>이 연구 분야의 초기 단계에서, 이러한 방법들 사이의 균형에 대한 체계적인 이해가 부족하다.</p><p>다양한 전략들 간의 균형에 대한 체계적인 탐구로 pretrain 및 finetune 패러다임에 수행되는 것과 유사학 이득을 취할 수 있을 것이다 [<a href="https://doi.org/10.18653/v1/W19-4302" target="_blank" rel="noopener noreferrer">98</a>].</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="95-multiple-prompt-learning">9.5 Multiple Prompt Learning<a href="#95-multiple-prompt-learning" class="hash-link" aria-label="Direct link to 9.5 Multiple Prompt Learning" title="Direct link to 9.5 Multiple Prompt Learning">​</a></h2><ul><li><p><strong>Prompt Ensembling</strong> : prompt ensembling 에서, prompt 를 많이 고려할수록 공간 및 시간 복잡도는 증가</p><ul><li>다양한 프롬프트의 knowledge 를 추출하는 방법은 아직 충분히 탐구되지 않음<ul><li><a href="https://doi.org/10.18653/v1/2021.emnlp-main.32" target="_blank" rel="noopener noreferrer">118</a>, <a href="https://doi.org/10.18653/v1/2021.naacl-main.185" target="_blank" rel="noopener noreferrer">120</a> 및 <a href="https://doi.org/10.18653/v1/2021.eacl-main.20" target="_blank" rel="noopener noreferrer">117</a> : 앙상블 모델을 사용하여 다양한 프롬프트의 knowledge 추출을 위해 대규모 데이터셋에 annotation 을 달았다.</li></ul></li><li>앙상블 할만한 프롬프트를 선택하는 방법도 아직 충분히 탐구되지 않음<ul><li>텍스트 생성 작업의 경우, prompt ensemble learning 의 연구가 수행되오지 않았으며, 이는 텍스트 생성에서의 앙상블 학습이 비교적 복잡하기 때문</li><li><em><a href="https://doi.org/10.18653/v1/2021.naacl-main.113" target="_blank" rel="noopener noreferrer">Refactor</a></em> : 위 해결방안으로, neural ensembling method 제안</li></ul></li></ul></li><li><p><strong>Prompt Composition and Decomposition</strong> : 다중 sub-prompt 를 도입하여 복잡한 task input 의 어려움을 제거하는 것이 목표. 좋은 선택을 해야 하는 것이 중요</p><ul><li>token [<a href="https://doi.org/10.18653/v1/P16-1101" target="_blank" rel="noopener noreferrer">81</a>] / span [<a href="https://doi.org/10.18653/v1/2021.acl-long.558" target="_blank" rel="noopener noreferrer">30</a>] 예측 task (예; NER) 의 경우, prompt decomposition 를 고려할 수 있음</li><li>span relation [<a href="https://doi.org/10.18653/v1/D17-1018" target="_blank" rel="noopener noreferrer">66</a>] 예측 task (예; 개체 인식) 의 경우, prompts composition 이 더 좋은 선택일 것임</li></ul></li><li><p><strong>Prompt Augmentation</strong> : 기존의 prompt augmentation 은 입력 길이에 제한이 있다.</p><ul><li>예로, 너무 많은 demonstration 을 input 으로 넣으면 실행 불가능하다.</li><li>정보를 가진 demonstration 을 선택하고 적절하게 정렬하는 방법은 흥미롭고 도전적인 문제다 [<a href="https://doi.org/10.18653/v1/2021.findings-acl.395" target="_blank" rel="noopener noreferrer">62</a>]</li></ul></li><li><p><strong>Prompt Sharing</strong> : 이전엔 주로 single task, domain, language 에 대한 prompt 적용이었지만 multiple 에 대해서도 prompt learning 을 적용하는 <em>prompt sharing</em> 을 고려할 수 있다.</p><ul><li>다양한 task 에서 개별 prompt 를 설계 및 각각의 상호작용을 맞추는 법이 핵심</li><li>지금까지 많이 탐구되지 않은 분야</li><li>Fig. 3 에서 mutiple task 에 대한 multiple prompt learning 전략으로 prompt template 을 공유하는 것을 보여준다</li></ul><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/b26a92b7-1b20-46b7-b600-cf10ffad434d/image.png" class="img_ev3q"></p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="96-theoretical-and-empirical-analysis-of-prompting">9.6 Theoretical and Empirical Analysis of Prompting<a href="#96-theoretical-and-empirical-analysis-of-prompting" class="hash-link" aria-label="Direct link to 9.6 Theoretical and Empirical Analysis of Prompting" title="Direct link to 9.6 Theoretical and Empirical Analysis of Prompting">​</a></h2><p>많은 상황에선 성공하지만, prompt-based learning 의 이론적 분석과 보장은 희박하다.</p><ul><li><a href="https://proceedings.neurips.cc/paper/2021/file/86b3e165b8154656a71ffe8a327ded7d-Paper.pdf" target="_blank" rel="noopener noreferrer">141</a> : soft-prompt tuning 은 downstream recovery (예; downstream task 의 ground-truth labels 을 복원하는 것)를 위해 필요한 non-degeneracy assumptions (각 토큰의 생성 확률은 선형적으로 독립) 을 완화시킬 수 있음을 입증<ul><li>이는 task-specific 정보를 추출하기 쉽게 만들어줌</li></ul></li><li><a href="https://arxiv.org/pdf/2010.03648" target="_blank" rel="noopener noreferrer">113</a> : 텍스트 분류 작업은 문장 완성 작업으로 재구성할 수 있음을 검증하여, 언어 모델링이 의미 있는 pretrained 작업이 될 수 있음을 보여줌</li><li><a href="https://doi.org/10.18653/v1/2021.naacl-main.208" target="_blank" rel="noopener noreferrer">114</a> : 분류 작업 전반에 걸쳐 prompt 가 평균 데이터 포인트 수백 개에 해당하는 가치가 있다는 것을 경험적으로 보여줌</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="97-transferability-of-prompts">9.7 Transferability of Prompts<a href="#97-transferability-of-prompts" class="hash-link" aria-label="Direct link to 9.7 Transferability of Prompts" title="Direct link to 9.7 Transferability of Prompts">​</a></h2><p>prompt 가 모델에 특화된 정도를 이해하고 prompt 의 전이성을 향상시키는 것 또한 중요한 주제</p><p><a href="https://proceedings.neurips.cc/paper/2021/file/5c04925674920eb58467fb52ce4ef728-Paper.pdf" target="_blank" rel="noopener noreferrer">96</a> 에선 tuned few-shot learning 상황 (prompt 를 선택하기 위해 더 큰 검증셋이 있는 경우)에서 선택된 prompt 가 유사한 크기의 모델에 잘 일반화되는 반면, true few-shot 상황 (학습 샘플이 몇 개 뿐일 경우)에서 선택된 prompt 는 전자보다 일반화되지 않는 다는 것을 보여줌.</p><p>모델 크기가 두 상황 모두에서 상당히 다른 전 경우, 전이성이 낮다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="98-combination-of-different-paradigms">9.8 Combination of Different Paradigms<a href="#98-combination-of-different-paradigms" class="hash-link" aria-label="Direct link to 9.8 Combination of Different Paradigms" title="Direct link to 9.8 Combination of Different Paradigms">​</a></h2><p>prompting 패러다임의 성공은 BERT 같은 pretrain 및 finetune 으로 개발된 pretrained LMs 의 top 에서 구축되었다. </p><p>하지만, 후자에 대한 효과적인 pretraining 방법이 전자에 그대로 적용할 수 있는지, 또는 다시 생각하여 정확성이나 prompt-based learning  의 적용 용이성을 더 개선할 수 있는지 중요한 연구 질문으로, 이에 대한 문헌은 충분히 다뤄지지 않았다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="99-calibration-of-prompting-methods">9.9 Calibration of Prompting Methods<a href="#99-calibration-of-prompting-methods" class="hash-link" aria-label="Direct link to 9.9 Calibration of Prompting Methods" title="Direct link to 9.9 Calibration of Prompting Methods">​</a></h2><p>Calibration (보정)는 모델이 좋은 확률적 예측을 할 수 있는 능력을 말한다 <a href="https://www.tandfonline.com/doi/abs/10.1080/00401706.1996.10484418?journalCode=utch20" target="_blank" rel="noopener noreferrer">33</a>.</p><p>answer 예측을 위해 pretrained LMs (예; BART) 의 생성 확률 사용 시, 확률 분포가 일반적으로 잘 보정되어 있지 않아 조심할 필요가 있다.</p><ul><li><a href="https://doi.org/10.1162/tacl_a_00407" target="_blank" rel="noopener noreferrer">51</a> : QA task 에서의 pretrained LMs (예; BART, T5, GPT2) 의 확률이 잘 보정된다는 점 발견</li><li><a href="http://proceedings.mlr.press/v139/zhao21c.html" target="_blank" rel="noopener noreferrer">151</a> : answered prompt 가 제공됐을 때, pretrained LMs 가 특정 answer 로 향하도록 편향되는 세 가지 문제점 (대부분 label bias, receny bias, common token bias)을 식별함<ul><li>예로, 최종 answered prompt 가 positive label 이면, 모델은 positive words 를 예측하도록 편향됨</li><li>이를 해결하기 위해<ol><li>context-free input (예; prompt 가 &quot;Input: Subpar acting. Sentiment: Negative\n Input: Beautiful film. Sentiment: Positive\n Input: N/A. Sentiment:&quot;)을 사용하여 초기 확률 분포 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">P_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 을 얻는다</li><li>real input (예; prompt 가 &quot;Input: Subpar acting. Sentiment: Negative\n Input: Beautiful film. Sentiment: Positive\n Input: Amazing. Sentiment:&quot;) 를 사용하여 확률 분포 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">P_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 를 얻는다.</li><li>이 두 분포를 사용하여 보정된 생성 확률 분포를 얻는다.</li></ol></li><li>이 모델은 두 가지 단점이 있다.<ul><li>적절한 context-free input (예; &quot;N/A&quot; 나 &quot;None&quot; 을 사용할지 여부)을 찾는 추가 비용 발생</li><li>pretrained LMs 의 확률 분포는 여전히 보정되지 않음</li></ul></li></ul></li></ul><p>보정된 확률 분포가 있어도, input 에 대한 single gold answer 추정할 때 조심할 필요가 있다.</p><p>동일한 객체의 표면 형태가 유한한 확률 질량을 경쟁한다는 것을 의미 <a href="https://arxiv.org/pdf/2104.08315.pdf" target="_blank" rel="noopener noreferrer">45</a></p><p>예로, &quot;Whirlpool bath&quot; 가 gold answer 이라면, 해당 생성 확률은 일반적으로 낮을 것이다.
이유는 &quot;Bathtub&quot; 단어는 동일한 의미를 공유하며 더 큰 확률 질량을 차지하기 때문이다.</p><p>이를 해결하기 위해</p><ul><li>paraphrasing 을 사용하여 gold answer set 을 포괄적으로 구성하는 prompt answer angineering 을 수행</li><li>단어 확률을 context 내의 이전 확률에 기반하여 보정</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/prompting">Prompting</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/prompt-based-learning">prompt-based learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/survey">survey</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/prompt-engineering">prompt engineering</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multi-prompt-learning">multi-prompt learning</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Survey/2021-07-Prompting.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/Text Generation/InstructGPT"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Training language models to follow instructions with human feedback (+ ChatGPT)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Survey/PEFT for PVMs"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-supervised-learning-in-nlp" class="table-of-contents__link toc-highlight">2.1 Supervised Learning in NLP</a></li><li><a href="#22-prompting-basics" class="table-of-contents__link toc-highlight">2.2 Prompting Basics</a><ul><li><a href="#221-prompt-addition" class="table-of-contents__link toc-highlight">2.2.1 Prompt Addition</a></li><li><a href="#222-answer-search" class="table-of-contents__link toc-highlight">2.2.2 Answer Search</a></li><li><a href="#223-answer-mapping" class="table-of-contents__link toc-highlight">2.2.3 Answer Mapping</a></li></ul></li><li><a href="#23-design-considerations-for-prompting" class="table-of-contents__link toc-highlight">2.3 Design Considerations for Prompting</a></li><li><a href="#31-prompt-shape" class="table-of-contents__link toc-highlight">3.1 Prompt Shape</a></li><li><a href="#32-manual-template-engineering" class="table-of-contents__link toc-highlight">3.2 Manual Template Engineering</a></li><li><a href="#33-automated-template-learning" class="table-of-contents__link toc-highlight">3.3 Automated Template Learning</a><ul><li><a href="#331-discrete-prompt" class="table-of-contents__link toc-highlight">3.3.1 Discrete Prompt</a></li><li><a href="#332-continuous-prompt" class="table-of-contents__link toc-highlight">3.3.2 Continuous Prompt</a></li></ul></li><li><a href="#41-answer-shape" class="table-of-contents__link toc-highlight">4.1 Answer Shape</a></li><li><a href="#42-answer-space-design-method" class="table-of-contents__link toc-highlight">4.2 Answer Space Design Method</a><ul><li><a href="#421-manual-design" class="table-of-contents__link toc-highlight">4.2.1 Manual Design</a></li><li><a href="#422-discrete-answer-search" class="table-of-contents__link toc-highlight">4.2.2 Discrete Answer Search</a></li><li><a href="#423-continuous-answer-search" class="table-of-contents__link toc-highlight">4.2.3 Continuous Answer Search</a></li></ul></li><li><a href="#51-prompt-ensembling" class="table-of-contents__link toc-highlight">5.1 Prompt Ensembling</a></li><li><a href="#52-prompt-augmentation" class="table-of-contents__link toc-highlight">5.2 Prompt Augmentation</a></li><li><a href="#53-prompt-composition" class="table-of-contents__link toc-highlight">5.3 Prompt Composition</a></li><li><a href="#54-prompt-decomposition" class="table-of-contents__link toc-highlight">5.4 Prompt Decomposition</a></li><li><a href="#61-training-settings" class="table-of-contents__link toc-highlight">6.1 Training Settings</a></li><li><a href="#62-parameter-update-methods" class="table-of-contents__link toc-highlight">6.2 Parameter Update Methods</a><ul><li><a href="#621-promptless-fine-tuning" class="table-of-contents__link toc-highlight">6.2.1 Promptless Fine-tuning</a></li><li><a href="#622-tuning-free-prompting" class="table-of-contents__link toc-highlight">6.2.2 Tuning-free Prompting</a></li><li><a href="#623-fixed-lm-prompt-tuning" class="table-of-contents__link toc-highlight">6.2.3 Fixed-LM Prompt Tuning</a></li><li><a href="#624-fixed-prompt-lm-tuning" class="table-of-contents__link toc-highlight">6.2.4 Fixed-Prompt LM Tuning</a></li><li><a href="#625-promptlm-tuning" class="table-of-contents__link toc-highlight">6.2.5 Prompt+LM Tuning</a></li></ul></li><li><a href="#71-knowledge-probing" class="table-of-contents__link toc-highlight">7.1 Knowledge Probing</a></li><li><a href="#72-structure-prediction" class="table-of-contents__link toc-highlight">7.2 Structure Prediction</a></li><li><a href="#73-classification-based-tasks" class="table-of-contents__link toc-highlight">7.3 Classification-based Tasks</a></li><li><a href="#74-information-extraction" class="table-of-contents__link toc-highlight">7.4 Information Extraction</a></li><li><a href="#75-reasoning-in-nlp" class="table-of-contents__link toc-highlight">7.5 &quot;Reasoning&quot; in NLP</a></li><li><a href="#76-question-answering" class="table-of-contents__link toc-highlight">7.6 Question Answering</a></li><li><a href="#77--text-generation" class="table-of-contents__link toc-highlight">7.7  Text Generation</a></li><li><a href="#78-automatic-evaluation-of-text-generation" class="table-of-contents__link toc-highlight">7.8 Automatic Evaluation of Text Generation</a></li><li><a href="#79-meta-applications" class="table-of-contents__link toc-highlight">7.9 Meta-Applications</a></li><li><a href="#710-multi-modal-learning" class="table-of-contents__link toc-highlight">7.10 Multi-modal Learning</a></li><li><a href="#91-selection-of-pre-trained-lms" class="table-of-contents__link toc-highlight">9.1 Selection of Pre-trained LMs</a></li><li><a href="#92-prompt-design" class="table-of-contents__link toc-highlight">9.2 Prompt Design</a></li><li><a href="#93-prompt-answer-engineering" class="table-of-contents__link toc-highlight">9.3 Prompt Answer Engineering</a></li><li><a href="#94-selection-of-tuning-strategy" class="table-of-contents__link toc-highlight">9.4 Selection of Tuning Strategy</a></li><li><a href="#95-multiple-prompt-learning" class="table-of-contents__link toc-highlight">9.5 Multiple Prompt Learning</a></li><li><a href="#96-theoretical-and-empirical-analysis-of-prompting" class="table-of-contents__link toc-highlight">9.6 Theoretical and Empirical Analysis of Prompting</a></li><li><a href="#97-transferability-of-prompts" class="table-of-contents__link toc-highlight">9.7 Transferability of Prompts</a></li><li><a href="#98-combination-of-different-paradigms" class="table-of-contents__link toc-highlight">9.8 Combination of Different Paradigms</a></li><li><a href="#99-calibration-of-prompting-methods" class="table-of-contents__link toc-highlight">9.9 Calibration of Prompting Methods</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.947e1592.js"></script>
<script src="/assets/js/main.d6888401.js"></script>
</body>
</html>