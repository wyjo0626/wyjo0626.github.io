<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/PEFT/Module/2019-02-Adapter">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Parameter-Efficient Transfer Learning for NLP | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/Adapter"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Parameter-Efficient Transfer Learning for NLP | WYJLab"><meta data-rh="true" name="description" content="논문 이미지 및 출처 :"><meta data-rh="true" property="og:description" content="논문 이미지 및 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/Adapter"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/Adapter" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/Adapter" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.8134e117.js" as="script">
<link rel="preload" href="/assets/js/main.4358ace8.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Attacking/universal-adversarial-prompt">Attacking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Generalization/NoisyTune">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Flan-T5">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Generalization/Flat-LoRA">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Mixture/UF">Mixture</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Module</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Parameter-Efficient Transfer Learning for NLP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/K-Adapter">K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/MAD-X">MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/AdapterFusion">AdapterFusion: Non-Destructive Task Composition for Transfer Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Compacter">COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/MAD-G">MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Meta-Adapter">Meta-Adapters: Parameter Efficient Few-shot Fine-tuning through Meta-Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/AdaMix">AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Hyper-X">Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/AdapterSoup">AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/TLR-Adapter">Cross-Lingual Transfer with Target Language-Ready Task Adapters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/PHA">Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Pruning/SMP">Pruning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Quantization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Selective/AdapterDrop">Selective</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning">Soft Prompt</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Engineering/CoT/Chain-of-Thought">Prompt Engineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Parameter-Efficient Transfer Learning for NLP</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Parameter-Efficient Transfer Learning for NLP</h1></header><p>논문 이미지 및 출처 : <a href="https://arxiv.org/pdf/1902.00751.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1902.00751.pdf</a></p><h1>Abstract</h1><p>pre-trained model fine-tuning 은 NLP 에서 효과적인 transfer mechanism 이다.</p><p>하지만, downstream task 가 많을 경우, 모든 task 마다 entire new model 이 요구되어 비효율적이다.</p><p>이 대안으로 저자는 <strong>adapter module</strong> 을 사용한 transfer 제안</p><ul><li>모델이 간결하고 확장 가능</li><li>task 당 few trainable parameters 만 추가 및 이전 작업 수정 없이 new task 에 추가 가능</li><li>기존 모델의 parameter 는 고정하며, 고도의 parameter sharing 이 가능</li></ul><p>Adapter 효과성 입증을 위해 BERT 를 26 task 를 포함한 GLUE 에서 transfer</p><ul><li>task 당 few parameter 만으로, 거의 SOTA 에 가까운 성능을 얻음</li><li>GLUE 에서 task 당 3.6% parameter 만 추가하여 full fine-tuning 성능 0.4% 안으로 차이남</li><li>반면 full fine-tuning 은 parameter 100% 훈련</li></ul><h1>1. Introduction</h1><p>pre-trained model transfer 은 NLP task 에서 강력한 성능을 내며, 특히 unsupervised loss 로 large text corpora 를 학습한 Transformer network 인 BERT 는 text classification 및 extractive question answering 에서 SOTA 성능을 달성한다.</p><p>본 논문은 oneline setting (tasks arrive in a stream) 에 대처</p><ul><li>목적은 모든 new task 에 entire new model 을 훈련하지 않고 잘 수행하는 시스템 구축</li><li>클라우드 서비스같은 응용 분야에서 task 간 고도의 sharing 이 필요한데, 순차적으로 오는 여러 task 해결</li></ul><p>이를 위해 <em>compact</em> 하고 <em>extensible</em> 한 downstream model 을 생성하는 transfer learning 전략 제안</p><ul><li>Compact Model : task 당 small number additional parameter 를 사용하여 많은 task 해결</li><li>Extensible Model : 이전 작업을 잊지 않고 new task 를 해결할 수 있도록 점진적으로 훈련될 수 있음</li></ul><p>저자의 모델은 성능을 희생하지 않고 모델 생성</p><hr><p>NLP 에서 일반적인 transfer learning 은 feature-based transfer 및 fine-tuning 이다. 대신, 저자는 adapter 에 기반한 대안법 제시 (Rebuffi et al. 2017)</p><ul><li>Feature-based transfer : real-valued embedding vector 를 pre-training<ul><li>이런 embedding vector 는 word (Mikolov et al. 2013), sentence (Cer et al. 2019) or paragraph (Le &amp; Mikolov, 2014) level 에서 이루어질 수 있음</li><li>이후 embedding 은 custom downstream model 에 주입</li></ul></li><li>Fine-tuning : pre-trained network 의 weight 를 복사하고 해당 weight 를 downstream task 에 맞게 tuning<ul><li>최근 연구에 따르면 fine-tuning 은 종종 feature-based transfer 보다 나은 성능 (Howard &amp; Ruder, 2018)</li></ul></li></ul><hr><p>feature-based transfer 및 fine-tuning 모두 각 task 에 대한 new weight set 필요</p><p>network lower layer 이 task 간에 공유되면 fine-tuning 는 parameter-efficient 하지만, 저자의 adapter tuning 은 더욱 parameter-efficient</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-183-e700dd1a0683eeceba2043670be62814.png" width="861" height="892" class="img_ev3q"></p><p>Fig. 1 에서 trade-off 를 보여줌</p><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span>-axis 는 task 당 trained parameter 수 나타냄</li><li>Adapter-based tuning 은 fine-tuning 에 비해 fewer parameters 를 두 자릿수 더 적게 요구하며 비슷한 성능 달성</li></ul><hr><p>Adapter 는 pre-trained network 사이에 추가된 new modules</p><p>Adapter-based tuning 과 feature-based network 에는 다음과 같은 차이가 있다.</p><p>parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span></span></span></span> 를 가진 function (neural network) <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\phi_w(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> 를 고려하자.</p><ul><li>Feature-based transfer 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\phi_w(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> 와 new function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">X</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{X}_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14643em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1464em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 을 결합하여 $ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">X</mi><mi>v</mi></msub><mo stretchy="false">(</mo><msub><mi>ϕ</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{X}_v(\phi_w(w))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14643em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1464em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mclose">))</span></span></span></span></span> 생성<ul><li>이후 new task-specific parameters <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span></span> 만 훈련</li></ul></li><li>Fine-tuning 은 각 new task 에 대해 original parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span></span></span></span> 를 조정하며 수행. 이는 간결성에 제한됨</li><li>Adapter Tuning 의 경우, new function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ψ</mi><mrow><mi>w</mi><mo separator="true">,</mo><mi>v</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\psi_{w,v}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">ψ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> 정의<ul><li>parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span></span></span></span> : pre-training 에서 복사</li><li>initial parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">v_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> : new function 이 original 과 유사하도록 설정: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ψ</mi><mrow><mi>w</mi><mo separator="true">,</mo><msub><mi>v</mi><mn>0</mn></msub></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>≈</mo><msub><mi>ϕ</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\psi_{w, v_0}(x) \approx \phi_w(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">ψ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></li><li>훈련 중, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span></span> 만 tuning</li></ul></li><li>deep network 의 경우, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ψ</mi><mrow><mi>w</mi><mo separator="true">,</mo><mi>v</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\psi_{w,v}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">ψ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 는 일반적으로 original network <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">\phi_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 에 new layer 를 추가하는 것을 포함<ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>v</mi><mi mathvariant="normal">∣</mi><mo>≪</mo><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|v| \ll |w|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mord">∣</span></span></span></span></span> 를 선택하면 많은 task 에 parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∼</mo><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\sim |w|)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mord">∣</span><span class="mclose">)</span></span></span></span></span>  가 필요한 모델이 생성됨</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span></span></span></span> 는 고정되어 있어, model 은 이전 task 에 영향을 미치지 않고 새로운 작업으로 확장 가능</li></ul></li></ul><hr><p>Adapter-based tuning 은 <em>multi-task</em> 및 <em>continual</em> learning 과 관련 있다.</p><ul><li>Multi-task learning 또한 compact model 결과로 가져옴<ul><li>하지만 모든 task 에 동시 액세스가 필요하여 Adapter tuning 에서는 불필요</li></ul></li><li>Continual learning 은 endless stream task 학습을 목표<ul><li>이는 network 가 re-training 후 이전 task 를 forget 하기 때문에 어려움이 있음</li><li>Adapter 는 task 가 상호작용하지 않고 shared parameter 는 freezing 되어 있어 다르다.</li><li>이는 small task-specific parameter 를 사용하여 이전 task 에 대한 완전한 기억을 갖고 있는 특징이 있다.</li></ul></li></ul><hr><p>저자는 adapter 가 NLP 에 parameter-efficient tuning 임을, 크고 다양한 text classification task 에서 입증</p><p>핵심은  effective adapter module 과 base model 의 통합하는 설계이다.</p><ul><li>GLUE 에서 fully fine-tuned BERT 성능과 일치하지만 task-specific parameter 는 3% 에 불과 (fine-tuning 은 100%)</li><li>17 dataset 및 SQuAD extractive questiuon answer 에서 유사한 결과 관찰. 요약하여, adapter tuning 은 single, extensible model 을 제공 가능하며 text classification 에서 SOTA 달성</li></ul><h1>2. Adapter tuning for NLP</h1><p>저자는 여러 downstream task 에서 large text model tuning 전략 제안. 이에는 세 가지 주요 특성이 존재</p><ul><li>좋은 성능 달성</li><li>task 를 순차적으로 훈련. 즉, 모든 데이터셋을 동시에 액세스할 필요가 없음</li><li>task 당 추가 파라미터 수를 적음. </li></ul><p>이러한 특성은 클라우스 서비스에서 유용하며, 많은 downstream task 을 훈련해야 하므로 고도의 공유가 바람직하다.</p><p>이 특성에 달성하기 위해, 저자는 <strong>new bottleneck adapter module</strong> 을 제안</p><ul><li>apdater module 을 사용한 tuning 은 new parameter 를 추가하는 것으로 이루어지며, downstream task 에 훈련</li><li>adapter module 은 pre-trained network 를 downstream task 용으로 재사용되기 위해 architectual 수정<ul><li>특히, adapter tuning 전략은 원래 네트워크에 new layer 주입을 포함</li></ul></li><li>original network 의 weight 는 건드리지 않은 채로, new adapter layer 는 random initialize<ul><li>fine-tuning 에선 new top-layer 및 original weight 을 공동 훈련</li><li>반면 adapter tuning 은 original network 는 freezing 하므로 많은 task 에서 공유될 수 있음</li></ul></li></ul><hr><p>Adapter module 은 두 특징이 있다.</p><ul><li>small parameter<ul><li>adapter module 은 original network 의 layer 에 비해 작아야 함</li><li>이는 많은 task 가 추가될 때 total model size 가 상대적으로 천천히 증가함을 의미</li></ul></li><li>near-identity initialization<ul><li>adapted model 의 안정적인 훈련을 위해 필요</li><li>경험적 실험으로, adapter 를 near-identity function 으로 초기화 시 훈련 시작 때 original network 에 영향을 주지 않음</li><li>훈련 중, adapter 는 network 를 통해 activation distribution 을 바꾸기 위해 활성화될 수 있다</li><li>adapter module 은 필요하지 않은 경우 무시될 수 있음</li><li>어느 adapter 가 많은 영향을 미치는지 관찰하여, 초기화가 far-identity function 이면 훈련에 실패할 수 있다.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-instantiation-for-transformer-networks">2.1 Instantiation for Transformer Networks<a href="#21-instantiation-for-transformer-networks" class="hash-link" aria-label="Direct link to 2.1 Instantiation for Transformer Networks" title="Direct link to 2.1 Instantiation for Transformer Networks">​</a></h2><p>Transformer 에 대한 adapter-based tuning 을 인스턴스화</p><p>이 모델은 다양한 task 의 NLP 에 SOTA 달성. 저자는 standard Transformer architecture 고려한다.</p><p>Adapter module 은 많은 architectural 선택지를 제공한다.</p><ul><li>저자는 좋은 성능을 달성하는 간단한 설계를 제공</li><li>복잡한 설계 여러 가지를 실험해보았지만, 다음 전략이 다양한 dataset 에서 다른 방법들과 마찬가지로 잘 수행됨을 발견</li></ul><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-184-ef11bfd07c7b74f584b27ac220cd24fc.png" width="1480" height="496" class="img_ev3q"></p><p>Fig. 2 는 adapter architecture 및 Transformer 에 적용을 보여줌</p><p>Transformer 의 각 layer 는 two primary sub-layers 포함: attention layer 및 feedforward layer</p><ul><li>두 layer 모두 input layer size 로 매핑하는 projection</li><li>각 sub-layer 는 skip-connection 적용</li><li>각 sub-layer 의 output 은 layer normalization 에 주입</li><li>저자는 이 sub-layer 각각 뒤에 two serial adapter 삽입</li><li>adapter 는 항상 skip-connection 다음에 바고 적용</li><li>projection 을 통해 input size 로 돌아간 후 적용되지만 skip-connection 다음에 삽입</li><li>adapter output 은 그 다음 layer normalization 에 직접 전달</li></ul><p>parameter 수를 제한하기 위해 bottleneck architecture 제안</p><ul><li>adapter 는 먼저 original <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span>-dimensional feature 를 smaller dimension <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span> 으로 projection 한 다음 nonlinearity 적용하고 다시 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> dimension 으로 projection</li><li>layer 당 추가되는 파라미터는 biases 포함하여 총 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>m</mi><mi>d</mi><mo>+</mo><mi>d</mi><mo>+</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">2md+d+m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mord mathnormal">m</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span></li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>≪</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">m \ll d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 로 설정하여, task 당 추가되는 parameter 수를 제한<ul><li>실제로 model parameter 중 약 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.5</mn><mo>−</mo><mn>8</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">0.5 - 8\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">0.5</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">8%</span></span></span></span></span> 사용</li></ul></li><li>bottleneck dimension <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span> 은 성능을 parameter-efficient 와 trade-off 하는 간단한 수단</li><li>adapter module 자체에 내부적으로 skip-connection<ul><li>skip-connection 을 사용하면, projection layer parameter 가 near-zero 로 초기화된 경우 module 이 approximate identity function 으로 초기화된다.</li></ul></li></ul><p>adapter module 의 layer 와 함께 각 task 에 대해 new layer normalization parameter 를 훈련</p><ul><li>이는 conditional batch normalization (De Vries et al. 2017), FiLM (Perez et al. 2018) 및 self-modulation (Chen et al. 2019) 와 유사</li><li>layer 당 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>d</mi></mrow><annotation encoding="application/x-tex">2d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord">2</span><span class="mord mathnormal">d</span></span></span></span></span> parameters 만 사용하여 network 의 parameter-efficient adaptation 제공</li><li>단순히 layer normalization parameter 만을 훈련하는 것은 좋은 성능을 얻기에 충분치 않</li></ul><h1>3. Experiments</h1><p>저자는 adapter 가 text task 에 대해 parameter-efficient transfer 달성을 보여줌</p><p>GLUE 벤치마크에서 BERT 의 fine-tuning 과 0.4% 차이만 나지만, fine-tuning 의 parameter 3% 만 추가</p><p>이 결과로 17 classification task 와 SQuAD question answering 에 대해 확인</p><p>분석 결과 adapter-based tuning 이 network higher layer 에 자동으로 집중하는 것을 보져여</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-experiments-settings">3.1 Experiments Settings<a href="#31-experiments-settings" class="hash-link" aria-label="Direct link to 3.1 Experiments Settings" title="Direct link to 3.1 Experiments Settings">​</a></h2><p>public pre-trained BERT Transformer 를 base model 로 사용</p><ul><li>BERT 사용으로 classification 수행을 위해 Devlin et al (2018) 을 따른다.</li><li>각 sequence 의 first token 은 special &quot;classification token&quot; 이다<ul><li>이 token 의 embedding 에 linear layer 을 연결하여 class label 을 예측</li></ul></li><li>훈련 절차도 Devlin et al (2018) 을 따름<ul><li>optimizer Adam 사용</li><li>Adam 의 learning rate 는 처음 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">10\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">10%</span></span></span></span></span> 를 선형적으로 증가. 그 후 선형적으로 감소하여 0 으로 줆</li><li>4개의 Google Cloud TPUs 에서 32 batch size</li></ul></li><li>hyperparameter sweep 하고 validation set 에 정확도에 따라 best model 선택</li></ul><p>BERT 의 fine-tuning 과 비교하며, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> 개 task 에 대한 fine-tuning 은 pre-trained model parameter 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> 배 필요</p><p>저자는 fine-tuning 보다 적은 parameter 로 동등한 성능 달성을 목표로 한다.</p><p>이상적으로는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mord">×</span></span></span></span></span> 에 가까운 수준이다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-glue-benchmark">3.2 GLUE benchmark<a href="#32-glue-benchmark" class="hash-link" aria-label="Direct link to 3.2 GLUE benchmark" title="Direct link to 3.2 GLUE benchmark">​</a></h2><p>GLUE 에서 평가를 진행</p><ul><li>300M parameter 및 24 layers 를 포함한 pre-trained BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>LARGE</mtext></msub></mrow><annotation encoding="application/x-tex">_{\text{LARGE}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">LARGE</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 에서 transfer</li><li>hyperparameter sweep 수행<ul><li>learning rate <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>3</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup><mo separator="true">,</mo><mn>3</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup><mo separator="true">,</mo><mn>3</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{3 \cdot 10^{-5}, 3 \cdot 10^{-4}, 3 \cdot 10^{-3}\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span></li><li>epochs <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>3</mn><mo separator="true">,</mo><mn>20</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{3, 20\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">20</span><span class="mclose">}</span></span></span></span></span></li><li>fixed adapter size (bottleneck 수) 및 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>8</mn><mo separator="true">,</mo><mn>64</mn><mo separator="true">,</mo><mn>256</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{8,64,256\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">8</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">64</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">256</span><span class="mclose">}</span></span></span></span></span> 를 task 당 best size 선택</li></ul></li><li>adapter size 는 tuning 한 adapter-specific hyperparameter</li><li>훈련 불안정성을 위해 서로 다른 random seed 5번 re-run 후 validation set 에 대한 best model 선택</li></ul><p><img loading="lazy" alt="Table 1" src="/assets/images/image-185-bb7cd4ee6a3e57de5d186d4bd9731138.png" width="1302" height="314" class="img_ev3q"></p><ul><li>Adapter 는 GLUE score 평균 80.0 달성. full fine-tuning 은 80.4</li><li>optimal adapter size 는 dataset 당 다양함<ul><li>MNLI 의 경우 256</li><li>RTE 의 경우 8</li><li>size 를 64 로 제안하면 평균 79.6 으로 약간 감소</li></ul></li><li>fine-tuning 은 BERT 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">9\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">9</span><span class="mord">×</span></span></span></span></span>, Adapter 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.3</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.3\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1.3</span><span class="mord">×</span></span></span></span></span>만 필요하다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-additional-classification-tasks">3.3 Additional Classification Tasks<a href="#33-additional-classification-tasks" class="hash-link" aria-label="Direct link to 3.3 Additional Classification Tasks" title="Direct link to 3.3 Additional Classification Tasks">​</a></h2><p>Adapter 의 간결함과 우수한 모델 생성의 검증을 더 하기 위해, text classification task 를 추가 진행</p><ul><li>example 이 900 ~ 330k 까지, class 는 2 ~ 157 까지, 평균 text length 는 57 ~ 1.9k</li><li>32 batch size 사용</li><li>데이터셋이 다양하여 learning rate 범위를 넓게 sweep: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup><mo separator="true">,</mo><mn>3</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup><mo separator="true">,</mo><mn>3</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{1\cdot10^{-5},3\cdot10^{-5},1\cdot10^{-4},3\cdot10^{-3}\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span></li><li>데이터 수가 많아 epochs sweep: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>20</mn><mo separator="true">,</mo><mn>50</mn><mo separator="true">,</mo><mn>100</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{20, 50, 100\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">20</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">50</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">100</span><span class="mclose">}</span></span></span></span></span></li><li>fine-tuning 과 adapter 의 optimal value 선택</li><li>adapter size <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>2</mn><mo separator="true">,</mo><mn>4</mn><mo separator="true">,</mo><mn>8</mn><mo separator="true">,</mo><mn>16</mn><mo separator="true">,</mo><mn>32</mn><mo separator="true">,</mo><mn>64</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{2, 4, 8, 16, 32, 64\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">8</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">16</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">32</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">64</span><span class="mclose">}</span></span></span></span></span></li><li>일부 dataset 은 작으므로, fine-tuning 하는 것이 optimal 하지 않을 수 있으므로, 추가적인 base model 실행<ul><li>이를 위해 top <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> layer 만 fine-tuning 하고 나머지는 freezing</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mn>3</mn><mo separator="true">,</mo><mn>5</mn><mo separator="true">,</mo><mn>7</mn><mo separator="true">,</mo><mn>9</mn><mo separator="true">,</mo><mn>11</mn><mo separator="true">,</mo><mn>12</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">n \in \{1,2,3,5,7,9,11,12\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">7</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">9</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">11</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">12</span><span class="mclose">}</span></span></span></span></span></li></ul></li><li>이 실험에선 BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>BASE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{BASE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">BASE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> model 및 12 layers 사용</li></ul><hr><p>GLUE task 와 달리 적절한 SOTA set 이 없어, BERT-based model 이 경쟁력있는지 확인을 위해 벤치마크 성능 수집 </p><ul><li>표준 모델에 large-scale hyperparameter 수행</li><li>Zoph &amp; Le (2017); Wong et al (2018) 과 유사한 single-task Neural AutoML algorithm 실행</li><li>이는 TensorFlow Hub 로 공개된 pre-trained text embedding module 위에 샇인 feedforward 및 convolutional network space 에서 search</li><li>각 task 에 대해 30대 machines 로 CPU 에서 일주일간 AutoML 실행</li><li>평균적으로 task 당 10k 개 이상의 모델 탐색</li><li>검증셋의 정확도에 따라 best model 선택</li></ul><p><img loading="lazy" alt="Table 2" src="/assets/images/image-186-3abc0a77a5dadab214bf00f8da032c31.png" width="909" height="564" class="img_ev3q"></p><p>Auto 벤치마크 에 대한 fine-tuning, variable fine-tuning, adapter-tuning 을 Table 2 에 보고</p><ul><li>AutoML baseline 은 BERT 가 경쟁력 있음. 수천 개의 model 을 탐색하여, BERT 가 평균적으로 나은 성능 발휘</li><li>GLUE 와 유사한 결과 패턴 보임</li><li>adapter tuning 의 성능이 fine-tuning 과 거의 비슷 (0.4%)</li><li>fine-tuning 은 BERT<em>$</em>\text{BASE}$ parameter 의 17배 필요</li><li>variable fine-tuning 은 조금 더 나은 성능 발휘하며, fewer layer 로 훈련. </li><li>variable fine-tuning 의 optimal set 은 평균 52% 훈련, 총 9.9배 줄음</li><li>adapter 는 훨씬 더 간결하며, task 당 1.14% new parameter 만 도입하여 모든 17개 task 에 대해 1.19배 parameter 생성</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-parameterperformance-trade-off">3.4 Parameter/Performance trade-off<a href="#34-parameterperformance-trade-off" class="hash-link" aria-label="Direct link to 3.4 Parameter/Performance trade-off" title="Direct link to 3.4 Parameter/Performance trade-off">​</a></h2><p>adapter size 는 parameter-efficient 를 제어</p><p>smaller adapter 는 fewer parameter 를 도입하지만 성능 손실 발생할 수 있음</p><p>이러한 trade-off 를 연구하기 위해 다양한 adapter size 를 고려하고 두 가지 baseline 비교:</p><ul><li>BERT<em>$</em>\text{BASE}$ 의 top <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> layers 만 fine-tuning</li><li>layer normalization parameters 만 tuning</li></ul><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-187-5879a2c0f60f63c72d8ae0add0f8f5db.png" width="910" height="433" class="img_ev3q"></p><p>Fig. 3 은 각 sweep 의 모든 classification task 를 통합한 parameter/performance trade-off 를 보여줌</p><ul><li>GLUE 에선 fewer layers 가 fine-tuning 될 때 성능 급격히 감소. additional task 중 일부는 fewer layers 훈련이 이점을 줌. 따라서 fine-tuning 의 성능 감소는 훨씬 적다</li><li>adapter 는 두 가지 범위의 크기에서 매우 좋은 성능 제공</li></ul><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-188-f66b8133706ed135f359d6acbd42a104.png" width="867" height="386" class="img_ev3q"></p><p>Fig. 4 는 두 GLUE task, MNLI<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3014em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 및 CoLA 에 대해 더 자세히 조사</p><ul><li>top layers 를 tuning 하며 all <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>&gt;</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">k &gt; 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2</span></span></span></span></span> 에 대해 더 많은 task-specific parameter 를 훈련</li><li>비슷한 수의 task-specific 을 사용하여 fine-tuning 할 때 성능 크게 감소</li><li>약 trainable parameter 9M 의 top-layer 만 fine-tuning 하여 MNLI<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3014em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 에 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>77.8</mn><mi mathvariant="normal">%</mi><mo>±</mo><mn>0.1</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">77.8\% \pm 0.1\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.0833em"></span><span class="mord">77.8%</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">±</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">0.1%</span></span></span></span></span> validation accuracy 달성</li><li>반면 64 size 인 adapter tuning 은 약 trainable parameter 2M 으로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>83.7</mn><mi mathvariant="normal">%</mi><mo>±</mo><mn>0.1</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">83.7\% \pm 0.1\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.0833em"></span><span class="mord">83.7%</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">±</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">0.1%</span></span></span></span></span> validation accuracy 달성</li><li>full fine-tuning 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>84.4</mn><mi mathvariant="normal">%</mi><mo>±</mo><mn>0.02</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">84.4\% \pm 0.02\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.0833em"></span><span class="mord">84.4%</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">±</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">0.02%</span></span></span></span></span> 달성</li><li>CoLA 에서도 유사한 추게 관찰</li></ul><hr><p>나아가 비교를 위해, layer normalization 의 parameter 를 tuning</p><ul><li>이 layer 는 point-wise addition 및 multiplication 만 포함하므로 매우 적은 trainable parameter 도입<ul><li>BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>BASE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{BASE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">BASE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 의 경우 40k</li><li>하지만 성능 저하 발생</li></ul></li><li>CoLA 에서 약 3.5%, MNLI<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3014em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 에서 약 4% 성능 감소</li></ul><p>요약하여 adapter tuning 은 parameter-efficient 가 높고 강력한 성능을 가진 compact model 생성</p><p>full fine-tuning 과 비교하여 성능이 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">1\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">1%</span></span></span></span></span> 내외로 유지</p><p>original model 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.5</mn><mi mathvariant="normal">%</mi><mo>−</mo><mn>5</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">0.5\% - 5\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.0833em"></span><span class="mord">0.5%</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">5%</span></span></span></span></span> size 로 adapter 를 훈련시키면, BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>LARGE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{LARGE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">LARGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 의 경쟁력 있는 결과에 근접</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="35-squad-extractive-question-answering">3.5 SQuAD Extractive Question Answering<a href="#35-squad-extractive-question-answering" class="hash-link" aria-label="Direct link to 3.5 SQuAD Extractive Question Answering" title="Direct link to 3.5 SQuAD Extractive Question Answering">​</a></h2><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-189-916a7c1c4fe736937603d2dc8ada26a3.png" width="541" height="435" class="img_ev3q"></p><p>마지막으로, adapter 를 classification 외의 task 에서도 작동하는지 확인을 위해 SQuAD v1.1 에서 실행</p><p>Fig. 5 는 SQuAD validation set 에서 fine-tuning 및 adapter parameter/performance trade-off 를 보여줌</p><ul><li>question 및 Wikipedia paragraph 이 주어지면, 이에 대한 answer span 을 선택</li><li>Fine-tuning 의 경우, trained layers 수를 sweep: <ul><li>learning rate <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>3</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup><mo separator="true">,</mo><mn>5</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{3\cdot10^{-5},5\cdot10^{-5},1\cdot10^{-4}\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span></li><li>epochs <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>2</mn><mo separator="true">,</mo><mn>3</mn><mo separator="true">,</mo><mn>5</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{2,3,5\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">5</span><span class="mclose">}</span></span></span></span></span></li></ul></li><li>adapter 의 sweep:<ul><li>learning rate <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>3</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup><mo separator="true">,</mo><mn>3</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo>⋅</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{3\cdot10^{-5},1\cdot10^{-4},3\cdot10^{-4},1\cdot10^{-3}\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span></li><li>epochs <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>3</mn><mo separator="true">,</mo><mn>10</mn><mo separator="true">,</mo><mn>20</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{3,10,20\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">10</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">20</span><span class="mclose">}</span></span></span></span></span></li></ul></li><li>classification 처럼, adapter 는 fewer parameter 를 사용하면서도 full fine tuning 과 유사한 성능 달성<ul><li>adapter size 64 (2% parameter) 는 F1 90.4% 얻음</li><li>fine-tuning 은 90.7% 달성</li></ul></li><li>SQuAD 는 small adapter (0.1% parameter) 로도 잘 작동. F1 89.9% 얻음</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="36-analysis-and-discussion">3.6 Analysis and Discussion<a href="#36-analysis-and-discussion" class="hash-link" aria-label="Direct link to 3.6 Analysis and Discussion" title="Direct link to 3.6 Analysis and Discussion">​</a></h2><p>저자는 adapter 의 영향을 관찰하기 위해 ablation 실험 수행</p><p>이를 위해 일부 trained adapters 를 제거하고 validation set 에서 model 재평가 (재훈련 없이)</p><p><img loading="lazy" alt="Figure 6" src="/assets/images/image-190-eb63ef0fa7ba0af6d37c7e9fbb19f0af.png" width="825" height="321" class="img_ev3q"></p><p>Fig. 6 은 MNLI 및 CoLA 에서 adapter size 64 로 BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>BASE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{BASE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">BASE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 에서 all continuous layer span 에서 adapter 제거했을 때의 변화를 보여줌</p><ul><li>single layer adapter 제거는 성능 영향을 주지 않음을 관찰<ul><li>heatmap 의 대각선 요소는 single layer 에서 adapter 제거의 성능</li><li>가장 큰 성능 하락은 2%</li><li>대조적으로, all adapter 제거하면 성능 크게 감소</li><li>MNLI 37%, CoLA 는 69% 로 감소</li><li>위 결과는 각 adapter 가 전체 network 에 미치는 영향은 작지만, 전체적인 영향이 큼을 나타냄</li></ul></li><li>Fig. 6 은 lower layer 가 top layer 보다 적은 영향을 미침을 시사<ul><li>MNLI 의 0-4 layer 에서 adapter 제거해도 성능 영향을 거의 미치지 않음</li><li>이는 adapter 가 자동으로 top layer 를 우선 처리하여 adapter 가 잘 작동함을 시사</li><li>실제 top layer 에 집중하는 것은 fine-tuning 에서 널리 사용되는 전략 (Howard &amp; Ruder, 2018)<ul><li>top layer 는 공유되는 lower-level feature 를 추출하고, top layer 는 다른 task 의 고유 feature 를 구축</li><li>이는 일부 task 에서는 top layer 만 fine-tuning 하는 것이 full fine-tuning 보다 우수한 결과를 얻음과 관련 있다 (Table 2 참조)</li></ul></li></ul></li><li>initialization scale 에 대한 robustness 조사<ul><li>메인 실험에서 adapter module weight 의 표준편차가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span> 인 zero-mean Gaussian 으로 추출</li><li>initialization scale 의 영향 분석을 위해 표준 편차를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>7</mn></mrow></msup><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[10^{-7}, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">7</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span> 구간으로 테스트</li><li>두 데이터셋 모두 adapter 성능이 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span> 이하의 표준 편차에 robust</li><li>initialization scale 이 너무 크면 성능 저하. 특히 CoLA</li></ul></li><li>adapter module 의 뉴런 수에 대한 robustness 조사<ul><li>all task 에 대해 일정한 adapter size 를 선택할 때 모델 품질이 안정적임을 발견</li><li>각 adapter size 에 대해 최적의 학습률과 에폭을 선택하여 8 classification task 에서 평균 검증 정확도 계산</li><li>어댑터 크기 8, 64 및 256 에 대한 평균 검증 정확도는 각각 86.2%, 85.8% 및 85.7% </li><li>이는 Fig. 4, 5 의 안정적인 성능을 보여주는 것임</li></ul></li><li>마지막으로, adapter architecture 의 여러 확장을 했지만 성능 향상은 미미<ul><li>adapter 에 batch/layer normalization 추가</li><li>adapter 당 layer 수 증가</li><li>tanh 같은 다양한 activation function</li><li>attention layer 내부에만 adapter 삽입</li><li>main layer 와 병렬로 adapter 추가</li><li>multiplicative interaction 이 있는 경우도 가능 </li><li>모든 경우가 <a href="#21-instantiation-for-transformer-networks">Section 2.1</a> 의 bottleneck 과 유사한 성능</li><li>따라서 간단함과 강력한 성능을 감안할 때, 원래의 adapter architecture 권장</li></ul></li></ul><h1>4. Related Work</h1></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/adapter">Adapter</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/PEFT/Module/2019-02-Adapter.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/PEFT/Mixture/PEFT without Its Gradients"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/PEFT/Module/K-Adapter"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-instantiation-for-transformer-networks" class="table-of-contents__link toc-highlight">2.1 Instantiation for Transformer Networks</a></li><li><a href="#31-experiments-settings" class="table-of-contents__link toc-highlight">3.1 Experiments Settings</a></li><li><a href="#32-glue-benchmark" class="table-of-contents__link toc-highlight">3.2 GLUE benchmark</a></li><li><a href="#33-additional-classification-tasks" class="table-of-contents__link toc-highlight">3.3 Additional Classification Tasks</a></li><li><a href="#34-parameterperformance-trade-off" class="table-of-contents__link toc-highlight">3.4 Parameter/Performance trade-off</a></li><li><a href="#35-squad-extractive-question-answering" class="table-of-contents__link toc-highlight">3.5 SQuAD Extractive Question Answering</a></li><li><a href="#36-analysis-and-discussion" class="table-of-contents__link toc-highlight">3.6 Analysis and Discussion</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.8134e117.js"></script>
<script src="/assets/js/main.4358ace8.js"></script>
</body>
</html>