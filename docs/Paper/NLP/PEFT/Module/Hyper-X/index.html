<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/PEFT/Module/2022-05-Hyper-X">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/Hyper-X"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/Hyper-X"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/Hyper-X" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/Hyper-X" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.4045ea26.js" as="script">
<link rel="preload" href="/assets/js/main.84227156.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Attacking/universal-adversarial-prompt">Attacking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Generalization/NoisyTune">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Flan-T5">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Generalization/Flat-LoRA">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Mixture/UF">Mixture</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Module</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Parameter-Efficient Transfer Learning for NLP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/K-Adapter">K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/MAD-X">MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/AdapterFusion">AdapterFusion: Non-Destructive Task Composition for Transfer Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Compacter">COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/AdapterDrop">AdapterDrop: On the Efficiency of Adapters in Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/MAD-G">MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Meta-Adapter">Meta-Adapters: Parameter Efficient Few-shot Fine-tuning through Meta-Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/AdaMix">AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Hyper-X">Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Pruning/SMP">Pruning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Quantization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning">Soft Prompt</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Engineering/CoT/Chain-of-Thought">Prompt Engineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer</h1></header><p>논문 및 이미지 출처 : <a href="https://aclanthology.org/2022.emnlp-main.541." target="_blank" rel="noopener noreferrer">https://aclanthology.org/2022.emnlp-main.541.</a></p><h1>Abstract</h1><p>대규모 multilingual model 은 task 와 language 전이 학습에 유망하다. 그러나 기존 방법은 서로 다른 task-language 조합에서 training data 가 제공될 때 이를 완전히 활용하지 못한다. 이러한 heterogeneous supervision 을 활용하기 위해, 저자는 multi-task 와 multilingual learning 을 efficient adaptation 으로 통합하는 single hypernetwork 인 <strong>Hyper-X</strong> 를 제안한다. </p><ul><li>이 model 은 task 와 language embedding 에 조건화된 adapter module 의 weight 를 생성한다. </li><li>Task 와 language-specific knowledge 를 결합하는 방법을 학습함으로써, 이 model 은 unseen language 및 task-language 조합에 대해 zero-shot transfer 를 가능하게 한다. </li><li>다양한 language 에서의 실험을 통해 Hyper-X 는 여러 자원의 혼합이 사용 가능할 때 최고의 성능 혹은 경쟁력 있는 향상을 달성하며, 표준 시나리오에서는 strong baseline 과 동등한 성능을 보임을 확인하였다. </li><li>또한 Hyper-X 는 separate adapter 를 학습하는 방법에 비해 parameter 와 resource 면에서 상당히 더 효율적이다. </li><li>마지막으로, Hyper-X 는 새로운 language 에 대한 few-shot 시나리오에서도 일관되게 강력한 결과를 보여주며, zero-shot transfer 를 넘어서는 저자의 접근법의 다양성을 입증한다.</li></ul><h1>1 Introduction</h1><p>Language 와 task 를 넘나드는 transfer learning 은 오랫동안 NLP 의 중요한 초점이었다. 최근 massively multilingual transformers (MMTs) 는 이 영역에서 큰 성과를 보여주었다. 이러한 model 의 장점은 high-resource source language 에서 task-specific information 을 low-resource target language 로 전이할 수 있다는 점이다 (Fig. 1, 1). 또 다른 장점은 여러 task 로부터의 knowledge 를 활용하여 stronger generalization 을 이끌어낼 수 있다는 것이다 (Fig. 1, 2).</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-64-179ba63e8cea660d0e30f87befff3363.png" width="778" height="1092" class="img_ev3q"></p><p>시간이 지나면서 많은 연구 커뮤니티가 특정 language 에 초점을 맞춘 resource 를 개발해 왔다. 실제로는 서로 다른 language 와 task 가 섞여 있는 data 가 이용 가능한 경우가 흔하다. 예를 들어, English 에 대해 POS tagging 과 Named Entity Recognition (NER) data 가 모두 존재한다고 하자. 이때 Turkish 에 대해서는 POS annotation 이 달린 treebank 가 존재할 수 있고, Arabic 에 대해서는 NER data 가 존재할 수 있다. 이러한 예시는 Fig. 1, 3 에 나타나 있다.</p><p>기존 cross-lingual transfer paradigm 인 single-task zero-shot transfer 나 few-shot learning 과는 달리, 이러한 mixture dataset(mixed-language multi-task)에서의 multi-task learning 은 모든 이용 가능한 data 를 활용하고, task 와 language 를 모두 넘어 unseen task-language 조합으로 information 을 전이할 수 있는 기회를 제공한다.</p><p>그러나 standard fine-tuning 전략은 이러한 heterogeneous task 와 language data 를 활용하는 능력에 한계가 있다. 구체적으로, MMTs 는 여러 source 에 대해 fine-tuning 할 경우 catastrophic forgetting 과 interference 에 취약하다. Parameter-efficient fine-tuning 대안인 adapter 는 일반적으로 task 간 transfer 이나 language 간 transfer 을 위해 사용되지만, 새로운 language 마다 새로운 adapter 학습이 필요하다.</p><p>이에 본 논문에서는 single model 내에서 서로 다른 language 와 task 를 포함한 여러 source 의 information 을 활용할 수 있는 통합 hypernetwork, <strong>Hyper-X</strong> 를 제안한다. </p><ul><li>핵심 아이디어는 language 와 task embedding 을 input 으로 받아, 해당 task-language combination 을 위한 adapter parameter 를 hypernetwork 를 통해 생성하는 것이다. </li><li>각 task 와 language 를 별도로 parameterization 함으로써, Hyper-X 는 test 시 unseen 조합에 대한 adaptation 을 가능하게 하면서 모든 이용 가능한 data resource 를 활용할 수 있다.</li><li>추가적으로, Hyper-X 는 labelled data 가 없는 경우에도 masked language modelling (MLM) 을 자연스럽게 활용할 수 있다. <ul><li>이를 통해 pre-training 동안 MMT 가 다루지 않은 language 에 대해서도 zero-shot adaptation 을 수행할 수 있다. </li></ul></li><li>또한 MLM 은 task-specific data 가 없는 경우에도 Hyper-X 가 해당 language representation 을 학습하도록 한다.</li></ul><p><img loading="lazy" alt="Table 1" src="/assets/images/image-65-1d70c7b85f2cf3a0a9784a5c405b4d3d.png" width="1343" height="538" class="img_ev3q"></p><p>요약하면, 본 연구는 최근 문헌에서 탐구된 여러 transfer 의 ‘ingredient’를 하나로 결합한다 (Tab. 1). 즉, multi-task learning, multilingual learning, further pre-training, 그리고 높은 수준의 compute 및 time 효율성을 포함한다.</p><p>저자는 Hyper-X 를 cross-lingual transfer 환경에서 two sequence labelling task, 즉 part-of-speech (POS) tagging 과 named-entity recognition (NER)에 대해 평가한다. 실험은 총 16 개 language (이 중 7 개는 pre-training 에 포함되지 않음)를 대상으로 Fig. 1 에 묘사된 세 가지 setup 에 걸쳐 진행되었다. </p><p>실험 결과, Hyper-X 는 English 로부터의 cross-lingual transfer 에서는 strong baseline 과 동등한 성능을 보였다. Multi-task 와 mixed-language setting 에서는 Hyper-X 가 standard baseline 대비 큰 향상을 보였으며, heterogeneous supervision source 를 활용할 수 있는 능력 덕분에 효율성이 떨어지는 adapter 기반 model 과 동등한 성능을 달성하였다. </p><p>분석 결과, Hyper-X 는 효율성과 성능 간의 trade-off 에서 우수함이 드러났다. 마지막으로 few-shot setting 에서 Hyper-X 는 서로 다른 language 와 task 에 걸쳐 일관되게 경쟁력 있는 성능을 달성하였으며, 이는 저자의 접근법이 continuous learning 시나리오에서도 유용함을 시사한다.</p><h1>2 Background</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-adapters">2.1 Adapters<a href="#21-adapters" class="hash-link" aria-label="Direct link to 2.1 Adapters" title="Direct link to 2.1 Adapters">​</a></h2><p>Adapter 는 MMT 에 삽입되어 new task, language, 또는 domain 에 대해 model 을 fine-tuning 하기 위한 경량 bottleneck layer 이다. Transformer 의 pre-trained weight 는 고정된 상태로 유지되고, adapter parameter 만 업데이트된다. 이러한 설정은 specialized knowledge 를 캡슐화함으로써 catastrophic forgetting 을 방지한다.</p><p>형식적으로, layer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span> 의 adapter module <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">A_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 bottleneck dimension <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span> 를 가진 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>h</mi></msup></mrow><annotation encoding="application/x-tex">z_i \in \mathbb{R}^h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6891em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span></span></span></span></span></span></span></span></span> 의 down-projection <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>h</mi><mo>×</mo><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">D_i \in \mathbb{R}^{h \times b}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">b</span></span></span></span></span></span></span></span></span></span></span></span></span>, non-linear fuction (ReLU), 그리고 up-projection <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>b</mi><mo>×</mo><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">U_i \in \mathbb{R}^{b \times h}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span> 로 구성된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>A</mi><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>U</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi><mtext>ReLU</mtext><mo stretchy="false">(</mo><msub><mi>D</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><msub><mi>z</mi><mi>i</mi></msub></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">A_i(z_i) = U_i.\text{ReLU}(D_i \cdot z_i) + z_i \tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">.</span><span class="mord text"><span class="mord">ReLU</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span><span class="tag"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></div><p>여기서 feed-forward network 는 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 와 연결되는 residual link 를 따른다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-hypernetworks">2.2 Hypernetworks<a href="#22-hypernetworks" class="hash-link" aria-label="Direct link to 2.2 Hypernetworks" title="Direct link to 2.2 Hypernetworks">​</a></h2><p>Hypernetwork 는 larger main network 의 weight 를 생성하는 network 이다. Hypernetwork 를 사용할 때 main model 은 원하는 objective (e.g., classification) 를 학습하는 반면, hypernetwork 는 weight 의 구조를 나타내는 보조 입력 (일반적으로 embedding) 을 받아 main model 의 parameter 를 생성한다.</p><p>따라서 hypernetwork 는 task 나 language 와 같은 여러 transfer 차원에 걸쳐 공유되는 single parameter space 를 학습할 수 있도록 하며, 동시에 input-specific reparametrization 도 가능하게 한다.</p><p>보다 구체적으로, hypernetwork 는 input source 를 나타내는 embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><msub><mi>d</mi><mi>s</mi></msub></msup></mrow><annotation encoding="application/x-tex">s^{(h)} \in \mathbb{R}^{d_s}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9271em;vertical-align:-0.0391em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">h</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 를 받아 model parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Θ</mi></mrow><annotation encoding="application/x-tex">\Theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Θ</span></span></span></span></span> 를 생성하는 generator function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">H</mi></mrow><annotation encoding="application/x-tex">\mathcal{H}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.00965em">H</span></span></span></span></span> 이다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi mathvariant="normal">Θ</mi><mo>≜</mo><mi mathvariant="script">H</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\Theta \triangleq \mathcal{H}(s^{(h)}) \tag{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9987em;vertical-align:-0.082em"></span><span class="mord">Θ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel amsrm">≜</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em"></span><span class="mord mathcal" style="margin-right:0.00965em">H</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">h</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1.188em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">H</mi></mrow><annotation encoding="application/x-tex">\mathcal{H}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.00965em">H</span></span></span></span></span> 는 임의의 differentiable function 일 수 있지만, 일반적으로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mi>h</mi></msup></mrow><annotation encoding="application/x-tex">W^h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span></span></span></span></span></span></span></span></span> 라는 simple linear transform 으로 parameterization 된다. </li><li>이 변환은 model parameter 의 총 개수에 해당하는 dimension <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">d_a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 의 flat vector 를 생성한다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mi>h</mi></msup></mrow><annotation encoding="application/x-tex">W^h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span></span></span></span></span></span></span></span></span> 는 모든 입력 source 에 대해 공유되며, 이를 통해 최대한의 sharing 이 가능해진다.</li></ul><h1>3 Hyper-X</h1><p>저자는 MMT 의 efficient adaptation 방법인 <strong>Hyper-X</strong> 를 제안한다. </p><ul><li>Hyper-X 는 unseen language 나 task-language pair 로의 transfer 를 위해 여러 source 의 information 을 활용한다. </li><li>구체적으로, Hyper-X 는 hypernetwork 를 사용하여 task 와 language-specific knowledge 를 embedding 형태로 결합하는 방법을 학습한다. </li><li>Task 와 language embedding 에 조건화된 hypernetwork 는 해당 task-language combination(e.g., Turkish 에서의 NER)을 위한 composite adapter layer 를 생성하며, 이를 통해 test 시 임의의 task-language pair 로의 transfer 가 가능하다. </li></ul><p>Fig. 2 는 저자의 model 개요를 보여준다.</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-66-a89712045aa018066335b29f3010bc71.png" width="630" height="845" class="img_ev3q"></p><ul><li>Task 와 language information 을 함께 학습함으로써 Hyper-X 는 기존 연구의 한계를 극복한다. <ul><li>예를 들어 adapter 기반 접근법은 cross-lingual information 을 task adapter 의 task 로만 전이할 수 있지만, 저자의 model 은 여러 task 와 language 로부터의 supervision 과 positive transfer 를 모두 활용할 수 있다. </li></ul></li><li>또한 Ponti et al. 의 접근법이 각 language 에 대해 target task 중 하나의 annotated data 를 필요로 하는 것과 달리, Hyper-X 는 각 language 에 대해 MLM 을 auxiliary task 로 사용함으로써 target task 에 annotated data 가 전혀 없는 경우에도 zero-shot transfer 가 가능하다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-a-hypernetwork-for-task-language-adapters">3.1 A Hypernetwork for Task-Language Adapters<a href="#31-a-hypernetwork-for-task-language-adapters" class="hash-link" aria-label="Direct link to 3.1 A Hypernetwork for Task-Language Adapters" title="Direct link to 3.1 A Hypernetwork for Task-Language Adapters">​</a></h2><p>저자는 parameter generator function 으로 standard hypernetwork 를 사용한다. 그러나 전체 model parameter 를 생성하는 대신, hypernetwork 는 각 adapter layer 의 parameter 를 생성한다. 구체적으로, hypernetwork <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">H</mi></mrow><annotation encoding="application/x-tex">\mathcal{H}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.00965em">H</span></span></span></span></span> 는 adapter parameter 를 생성하며, 각 adapter layer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">A_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 down-projection 과 up-projection matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>D</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>U</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(D_i, U_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 로 구성된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>D</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>U</mi><mi>i</mi></msub><mo>≜</mo><mi mathvariant="script">H</mi><mo stretchy="false">(</mo><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(3)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">D_i, U_i \triangleq \mathcal{H}(s^{(h)}) \tag{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1111em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel amsrm">≜</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em"></span><span class="mord mathcal" style="margin-right:0.00965em">H</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">h</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1.188em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">3</span></span><span class="mord">)</span></span></span></span></span></span></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="decoupling-tasks-and-languages">Decoupling Tasks and Languages<a href="#decoupling-tasks-and-languages" class="hash-link" aria-label="Direct link to Decoupling Tasks and Languages" title="Direct link to Decoupling Tasks and Languages">​</a></h4><p>Hyper-X 에서는 parameter 생성을 input task 와 language 에 조건화한다. 따라서 task <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><msub><mi>t</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>t</mi><mi>m</mi></msub></mrow></mrow><annotation encoding="application/x-tex">t \in {t_1, ..., t_m}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6542em;vertical-align:-0.0391em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span></span> 와 language <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>∈</mo><mrow><msub><mi>l</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>l</mi><mi>n</mi></msub></mrow></mrow><annotation encoding="application/x-tex">l \in {l_1, ..., l_n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span></span> 조합이 주어졌을 때, source embedding 은 두 source 의 knowledge 를 포함한다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup><mo>≈</mo><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>l</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s^{(h)} \approx (t, l)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">h</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mclose">)</span></span></span></span></span></p><p>각 task 와 language 는 별도의 embedding 으로 parameterization 되며, 이를 통해 임의의 task-language 조합에 대한 adaptation 이 가능하다. Task 와 language embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(s^{(t)}, s^{(l)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.138em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 은 low-dimensional vector 로서 hypernetwork parameter 와 함께 학습된다. 학습 과정에서 각 mini-batch 에 대해 해당 batch 가 sampling 된 task 와 language 에 따라 이 embedding 이 업데이트된다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mlm-as-auxiliary-task">MLM as Auxiliary Task<a href="#mlm-as-auxiliary-task" class="hash-link" aria-label="Direct link to MLM as Auxiliary Task" title="Direct link to MLM as Auxiliary Task">​</a></h4><p>Hyper-X 는 training 동안 task 와 language 가 나타나기만 하면 각각의 embedding 을 학습한다. 그러나 많은 under-represented language 의 annotated data 는 제한적이므로, 저자는 training 중 MLM 을 auxiliary task 로 사용하여 모든 language 에 대한 embedding 계산을 가능하게 한다. 또한 MLM 은 MMT pre-training 에 포함되지 않은 language 에 대해서도 더 나은 zero-shot 성능을 가능하게 한다 (MLM 의 영향에 대한 상세 분석은 Sec. 6.2 에서 다룬다).</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="sharing-across-layers">Sharing Across Layers<a href="#sharing-across-layers" class="hash-link" aria-label="Direct link to Sharing Across Layers" title="Direct link to Sharing Across Layers">​</a></h4><p>Task 와 language embedding 외에도, Hyper-X 는 transformer layer index <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span> 에 해당하는 layer embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">s^{(i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span> 를 학습한다. </p><p>이는 해당 adapter module 이 삽입되는 위치에 대응된다. Hyper-X 는 각 transformer layer 마다 adapter 를 생성하므로, 독립적인 layer embedding 학습은 layer 간 information sharing 을 가능하게 한다. </p><p>또한 layer embedding 은 모든 transformer layer 에 대해 single hypernetwork 를 사용할 수 있도록 하여, trainable parameter 수(즉, hypernetwork 의 크기)를 main model 의 layer 개수에 해당하는 비율만큼 줄여준다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="combining-multiple-sources">Combining Multiple Sources<a href="#combining-multiple-sources" class="hash-link" aria-label="Direct link to Combining Multiple Sources" title="Direct link to Combining Multiple Sources">​</a></h3><p>Language, task, layer embedding 을 결합하기 위해, 저자는 hypernetwork 의 일부로 source projector network <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">P_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 를 사용한다. 이 모듈은 두 개의 feed-forward layer 와 ReLU activation 으로 구성되며, 세 embedding 의 concatenation 을 입력으로 받아 combined embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><msub><mi>d</mi><mi>p</mi></msub></msup></mrow><annotation encoding="application/x-tex">s^{(p)} \in \mathbb{R}^{d_p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9271em;vertical-align:-0.0391em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 를 학습한다. 이때 차원은 더 작아질 수 있다:</p><div class="math math-display"><span class="katex-error" title="ParseError: KaTeX parse error: Multiple \tag" style="color:#cc0000">\begin{align*} &amp;s^{(h)} = s^{(l)} \oplus s^{(t)} \oplus s^{(i)} \tag{4} \\ &amp;s^{(p)} = \mathcal{P}_s(s^{(h)}) \tag{5} \end{align*}</span></div><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><msub><mi>d</mi><mi>s</mi></msub></msup></mrow><annotation encoding="application/x-tex">s^{(h)} \in \mathbb{R}^{d_s}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9271em;vertical-align:-0.0391em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">h</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">P</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{P}_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.08222em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0822em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 에 입력되기 전 concatenated embedding 을 의미하며, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>s</mi></msub><mo>=</mo><msub><mi>d</mi><mi>l</mi></msub><mo>+</mo><msub><mi>d</mi><mi>t</mi></msub><mo>+</mo><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">d_s = d_l + d_t + d_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 이다.</li><li>이 component 는 source embedding 을 결합하는 방법을 학습할 수 있도록 하며, 동시에 전체 trainable parameter 수를 줄이는 역할을 한다.</li></ul><h1>4 Experiments</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dataset-and-languages">Dataset and Languages<a href="#dataset-and-languages" class="hash-link" aria-label="Direct link to Dataset and Languages" title="Direct link to Dataset and Languages">​</a></h4><p>저자는 두 가지 downstream task, 즉 part-of-speech (POS) tagging 과 named entity recognition (NER) 에 대해 실험을 수행하였다. POS tagging 에는 Universal Dependencies (UD) 2.7 dataset 을 사용하고, NER 에는 WikiANN dataset 을 사용하며, Rahimi et al. 이 제공한 train, dev, test split 을 따른다.</p><p>이 두 task 외에도 Wikipedia article 에 대해 masked language modelling (MLM) 을 auxiliary task 로 사용한다. Dataset 크기의 영향을 통제하고 training 시간을 줄이기 위해 각 language 별 Wikipedia 문장 수를 100K 로 제한하였다.</p><p>Language selection 은 다음 기준에 따라 이루어졌다:</p><ol><li>언어 계통, 문자 체계, 형태통사적 속성에 따른 typological diversity</li><li>downstream task 에 사용 가능한 data 를 기준으로 한 high-resource 와 low-resource language 의 조합</li><li>mBERT pre-training data 에 포함 여부</li><li>두 개의 task-specific dataset 에서 해당 language 의 존재 여부</li></ol><h4 class="anchor anchorWithStickyNavbar_LWe7" id="experimental-setup">Experimental Setup<a href="#experimental-setup" class="hash-link" aria-label="Direct link to Experimental Setup" title="Direct link to Experimental Setup">​</a></h4><p>저자는 Hyper-X 의 zero-shot transfer 성능을 세 가지 setting 에서 평가하였다:</p><ol><li><strong>English single-task</strong>: 각 downstream task 에 대해 English data 만을 사용하여 model 을 학습한다.</li><li><strong>English multi-task</strong>: English POS 와 NER data 를 동시에 사용하여 model 을 학습한다.</li><li><strong>Mixed-language multi-task</strong>: multi-task setup 으로 학습하되, POS 와 NER 모두에 대해 English data 만 사용하는 대신 여러 task-language 조합을 사용한다.</li></ol><p>Zero-shot 성능을 측정하기 위해 Ponti et al. 을 따라, 가능한 모든 language-task 조합으로부터 두 개의 partition 을 만들되, 각 partition 에서는 특정 task-language pair 가 항상 unseen 이 되도록 하였다 (e.g., Fig. 1 의 NER-Turkish 와 POS-Arabic). Partition 및 partitioning strategy 의 세부 사항은 Appendix A 에 제시된다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-baselines-and-model-variants">4.1 Baselines and Model Variants<a href="#41-baselines-and-model-variants" class="hash-link" aria-label="Direct link to 4.1 Baselines and Model Variants" title="Direct link to 4.1 Baselines and Model Variants">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mbert">mBERT<a href="#mbert" class="hash-link" aria-label="Direct link to mBERT" title="Direct link to mBERT">​</a></h4><p>104 개 language 에 대해 pre-trained 된 MMT. 모든 model parameter 를 fine-tuning 하는 방식으로 사용된다. 이 standard approach 는 단일 source 또는 다중 language-task 조합으로부터 cross-lingual transfer 를 가능하게 하므로, 모든 setting 에서 Hyper-X 와 비교된다. 또한 Hyper-X 및 다른 baseline 의 base model 로 mBERT 를 사용한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mad-x">MAD-X<a href="#mad-x" class="hash-link" aria-label="Direct link to MAD-X" title="Direct link to MAD-X">​</a></h4><p>MMT 기반 cross-lingual transfer learning 을 위한 adapter 기반 modular framework. Task-specific adapter 와 각 language 별로 독립적으로 MLM 으로 학습된 language-specific adapter 를 결합한다. Hyper-X 와 동일한 Wikipedia data 로 모든 language adapter 를 학습하였다. 원래 MAD-X 는 standard multi-task training 을 지원하지 않으므로, mixed-language setup 에서는 NER 과 POS 를 분리하여 여러 source language 를 사용해 task adapter 를 학습하였다. 이를 MAD-X MS 라고 한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="parameter-space-factorization">Parameter Space Factorization<a href="#parameter-space-factorization" class="hash-link" aria-label="Direct link to Parameter Space Factorization" title="Direct link to Parameter Space Factorization">​</a></h4><p>Bayesian framework 로, MMT 상단 softmax layer 의 parameter generator 를 여러 task 와 language 로부터 학습한다. 그러나 특정 language 에 annotated training data 가 없는 경우, 해당 language 의 latent variable 을 학습할 수 없다. 따라서 이 baseline 은 Hyper-X 와 동일한 partition 을 사용하는 mixed-language multi-task setting 에 대해서만 평가된다. Original implementation 과 default hyper-parameter, 그리고 low-rank factorization 을 사용하였다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="model-variants">Model Variants<a href="#model-variants" class="hash-link" aria-label="Direct link to Model Variants" title="Direct link to Model Variants">​</a></h4><p>Hypernetwork 크기의 영향을 보기 위해 Hyper-X 의 두 가지 variant 를 평가하였다.</p><ul><li><strong>Hyper-X Base</strong>: 76M parameter ( <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>s</mi></msub><mo>=</mo><mn>192</mn></mrow><annotation encoding="application/x-tex">d_s=192</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">192</span></span></span></span></span> ) 를 fine-tuning 하며, 총 학습 parameter 수 기준으로 MAD-X 와 유사하다.</li><li><strong>Hyper-X Small</strong>: 13M parameter ( <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>s</mi></msub><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">d_s=32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">32</span></span></span></span></span> ) 만 업데이트한다.</li></ul><p>Tab. 3 은 parameter 수와 runtime 을 함께 보여준다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-training-details">4.2 Training Details<a href="#42-training-details" class="hash-link" aria-label="Direct link to 4.2 Training Details" title="Direct link to 4.2 Training Details">​</a></h2><ul><li>모든 실험에서 batch size 는 32, 최대 sequence length 는 256 으로 설정하였다. Hyper-X 는 100,000 step 동안 학습되었으며, learning rate 1e-4 를 선형적으로 감소시키고, 4000 step warm-up 을 사용하였다. 매 5000 step 마다 checkpoint 를 평가하고, validation 평균 점수가 가장 높은 checkpoint 를 최종 test 에 사용하였다.</li><li>Baseline 의 경우, mBERT 와 MAD-X task adapter 는 동일한 scheduler 와 warm-up step 을 사용하여 각각 20 epoch 동안 학습되었으며, learning rate 는 mBERT 1e-5, MAD-X 1e-4 로 설정되었다. MAD-X 는 prerequisite 인 language adapter 가 필요하므로, 각 language 별로 100,000 step 학습하였다.</li><li>Model 크기 측면에서 Hyper-X 는 adapter 학습을 위해 bottleneck dimension 256 을 사용하였다. <ul><li>MAD-X 의 경우 language adapter 와 task adapter 를 각각 dimension 256 과 48 로 학습하여 비교 가능한 baseline 을 만들었다. </li><li>Hyper-X 의 hypernetwork 입력에는 task, language, layer embedding dimension 을 모두 64 로 설정하여 총 192 차원이 된다. </li></ul></li><li>training 중에는 각 task-language 조합에 대해 homogeneous mini-batch 를 구성하여 해당 embedding 과 hypernetwork 를 함께 학습하였다. 또한 Mahabadi et al. 을 따라 original layer-norm parameter 도 업데이트하였다.</li></ul><p>Multi-task training 중에는 각 task-language pair 의 균형을 맞추기 위해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">T=5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">5</span></span></span></span></span> 의 temperature-based sampling 을 사용하였다.</p><h1>5 Zero-shot Transfer Results</h1><p>Tab. 2 는 각각 NER 과 POS tagging 에 대한 zero-shot 결과를 집계한 것이다. 전체 15 개 zero-shot language 에 대한 평균 점수 외에도, mBERT 의 language coverage 를 기준으로 ‘seen’ 8 개 language 와 ‘unseen’ 7 개 language 의 평균을 별도로 제시한다. 결과는 English single-task, English multi-task, Mixed-language multi-task setting 에 대해 보고된다.</p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-67-9f06254e19296902527fe5494522709a.png" width="1264" height="768" class="img_ev3q"></p><ul><li>전반적으로, <strong>Hyper-X Base</strong> 는 English 로부터 전이할 때 가장 strong baseline 과 동등한 성능을 보인다. </li><li>추가적인 source (즉, task-language pair 의 혼합) 가 존재하는 경우 Hyper-X 는 mBERT 와 parameter space factorization (PSF) 모두보다 더 우수하다. </li><li>MAD-X 와 비교하면, Hyper-X 는 일반적으로 seen language 에서 더 좋은 성능을 보인다. <ul><li>이는 unified hypernetwork 가 language 간 최대 sharing 을 가능하게 하고, isolated adapter 와 달리 pre-trained capacity 를 더 잘 활용하기 때문이라고 본다. </li></ul></li><li>반면 unseen language 에 대해서는 대부분의 경우 Hyper-X 가 MAD-X 에게 뒤처진다. 그러나 MAD-X 는 새로운 language 마다 별도의 language adapter 학습이 필요하므로 Hyper-X 에 비해 resource 효율성이 크게 떨어진다 (Sec. 6.1 참조).</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="english-single-task">English Single-Task<a href="#english-single-task" class="hash-link" aria-label="Direct link to English Single-Task" title="Direct link to English Single-Task">​</a></h4><p>English 를 각 task 별로 유일한 source language 로 사용할 때, Hyper-X (Base) 는 NER 에서는 MAD-X 와 동등한 성능을 보인다 (52.7 vs 52.8 F1). 그러나 POS tagging 에서는 MAD-X 보다 낮은 성능을 보인다 (63.5 vs 65.4 Acc. 평균). 두 model 모두 mBERT 를 크게 능가한다.</p><p>individual language 결과를 보면, Hyper-X 는 seen language 에서 MAD-X 보다 약간 더 좋은 성능을 보인다 (NER 과 POS tagging 모두). Unseen language 의 경우, MAD-X 와 Hyper-X 모두 MLM 덕분에 mBERT 대비 큰 성능 향상을 보인다. 그러나 두 model 간 비교에서는 MAD-X 가 NER 과 POS tagging 모두에서 더 높은 평균 점수를 달성한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="english-multi-task">English Multi-Task<a href="#english-multi-task" class="hash-link" aria-label="Direct link to English Multi-Task" title="Direct link to English Multi-Task">​</a></h4><p>English data 만을 사용할 수 있는 multi-task setting 에서, mBERT 를 POS 와 NER 에 대해 동시에 fine-tuning 하면 single-task 학습 대비 성능이 뒤섞인 결과를 보인다. 이는 MMTs 가 catastrophic forgetting 과 interference 에 취약하다는 기존 결과와 일치한다.</p><p>반면 Hyper-X Base 는 대부분의 language 에서 작은 폭이지만 일관된 개선을 보여준다. 평균적으로 NER 에서 0.2 (F1), POS tagging 에서 0.1 (Acc.) 향상을 보였다. 이는 Hyper-X 가 충분한 capacity 가 주어졌을 때 interference 를 완화하면서 task 간 sharing 을 가능하게 한다는 점을 확인시켜 준다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mixed-language-multi-task">Mixed-Language Multi-Task<a href="#mixed-language-multi-task" class="hash-link" aria-label="Direct link to Mixed-Language Multi-Task" title="Direct link to Mixed-Language Multi-Task">​</a></h4><p>이 setting 에서는 두 개의 training partition 을 통해 NER 과 POS 를 위한 language data 혼합이 제공되며, 각 task-language pair 는 한 partition 에서 항상 unseen 이 된다. 이 setup 에서는 mBERT 를 포함한 모든 model 이 이전 setting 대비 더 나은 zero-shot 성능을 보인다.</p><p>Baseline 중에서는 PSF 가 mBERT 대비 두 task 모두에서 더 큰 향상을 보여, MMT adaptation 을 위한 task- 및 language-specific parameterization 의 중요성을 보여준다. Hyper-X Base 는 단일 model 만 학습하는 방법 중 가장 큰 성능 향상을 달성한다. NER 과 POS 에서 각각 평균 9.0 (F1), 4.3 (Acc.) 의 향상을 보였다.</p><p>PSF 와 Hyper-X 모두 task 와 language 조합에 조건화된 adaptation 을 가능하게 하지만, 두 방법의 성능 차이는 parameter 생성 방식의 차이에 기인한다. PSF 는 softmax layer 의 parameter 만 생성하므로 model 의 깊은 layer 는 적응할 수 없다. 반면 Hyper-X 는 전체 model 에 삽입된 adapter layer parameter 를 생성하므로 더 높은 수준의 adaptation flexibility 를 제공한다. 특히 Hyper-X 는 auxiliary task 로 MLM 을 활용할 수 있기 때문에 unseen language 에서 PSF 보다 뛰어난 성능을 보인다.</p><p>마지막으로, Hyper-X 는 seen language 에서 MAD-X 의 multi-source version 보다 약간 더 좋은 성능을 보이는 경향이 있다. 그러나 unseen language 에서는 MAD-X 가 Hyper-X 보다 더 높은 성능을 보였다 (NER 에서 1.2 F1, POS tagging 에서 2.8 Acc. 우위). 이는 MAD-X 가 독립적으로 학습된 language adapter 의 이점을 가진 것뿐 아니라, Hyper-X 가 unseen language 에 대해 cross-task supervision 이 제한된다는 점과도 관련된다. 특히 target task 가 POS 인 경우, 대부분의 unseen language 는 NER dataset 에서 단지 100 문장만 이용 가능하므로 개선의 여지가 매우 제한적이다.</p><h1>6 Analysis</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="61-parameter-and-time-efficiency">6.1 Parameter and Time Efficiency<a href="#61-parameter-and-time-efficiency" class="hash-link" aria-label="Direct link to 6.1 Parameter and Time Efficiency" title="Direct link to 6.1 Parameter and Time Efficiency">​</a></h2><p><img loading="lazy" alt="Table 3" src="/assets/images/image-68-f2fc036412cd088128428b465b75dc2e.png" width="630" height="542" class="img_ev3q"></p><p>Tab. 3 은 baseline 과 Hyper-X model 의 fine-tuned parameter 수와 학습 시간을 보여준다. mBERT, PSF, Hyper-X 와 달리, MAD-X 는 16 개의 language adapter 와 2 개의 task adapter 가 각각 독립적으로 학습된 구조를 가진다.</p><p>Parameter 효율성 측면에서 MAD-X 와 Hyper-X Base model 은 mBERT parameter 의 43% 에 해당한다. 그러나 학습 시간 측면에서 Hyper-X Base 는 단 한 번만 학습되며 약 18 시간 소요된다. 반면 MAD-X 는 총 116 시간으로 상당히 많은 학습 시간이 요구된다. 따라서 다양한 language 와 setting 에서의 경쟁력 있는 zero-shot 성능을 고려했을 때, Hyper-X Base 는 더 나은 efficiency–performance trade-off 를 제공한다. 게다가 새로운 language 를 추가하는 경우, MAD-X 의 parameter 수와 학습 시간은 language 수에 비례해 선형적으로 증가하지만, Hyper-X 의 computation cost 는 동일하게 유지된다.</p><p>Hyper-X 의 model variant 로는 source embedding 크기 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>d</mi><mi>s</mi></msub><mo separator="true">;</mo><mn>32</mn><mo>→</mo><mn>192</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(d_s; 32 \rightarrow 192)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">32</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">192</span><span class="mclose">)</span></span></span></span></span> 가 다른 두 버전을 평가하였다. Hyper-X Small 은 훨씬 더 parameter-efficient 하며 (mBERT parameter 의 7.2%) 학습 시간도 약간 더 짧다 (16h). 그러나 zero-shot 성능은 base model 대비 특히 unseen language 에서 크게 낮다. 그럼에도 Hyper-X Small 은 특히 seen language 에 대해서는 유효한 대안으로 남는다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="62-impact-of-auxiliary-mlm-training">6.2 Impact of Auxiliary MLM Training<a href="#62-impact-of-auxiliary-mlm-training" class="hash-link" aria-label="Direct link to 6.2 Impact of Auxiliary MLM Training" title="Direct link to 6.2 Impact of Auxiliary MLM Training">​</a></h2><p>Fig. 3 은 mixed-language multi-task setting 에서 Hyper-X Base 의 auxiliary MLM training 영향력을 보여준다. 이 setting 은 각 task 와 language 에 대해 training instance 를 제공하므로, 저자는 ‘seen’ language 와 ‘all’ language 각각에 대해 Wikipedia data (MLM) 를 제거하면서 영향을 평가하였다.</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-69-61d687314df44714bcb692b85e7ece2b.png" width="628" height="544" class="img_ev3q"></p><ul><li>결과적으로 MLM data 가 seen language 성능을 소폭 향상시키지만, 주된 효과는 unseen language 에서 나타났다. </li><li>구체적으로, NER 에서는 +6.2 F1, POS 에서는 +10.5 Acc. 향상을 보였다. </li><li>또한 seen language 만 MLM data 를 제거했을 경우 Hyper-X 는 대부분의 성능을 회복할 수 있었으며, 이는 MLM 의 주요 효과가 unseen language 에 있음을 확인시켜 준다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="63-impact-of-source-languages">6.3 Impact of Source Languages<a href="#63-impact-of-source-languages" class="hash-link" aria-label="Direct link to 6.3 Impact of Source Languages" title="Direct link to 6.3 Impact of Source Languages">​</a></h2><p>Mixed-language multi-task setting 에서 저자는 동일 언어 계통에 속한 language 들이 다른 partition 에 포함되지 않도록 하여, 동일 계열 언어 간 transfer 를 제한하고 cross-task supervision 효과를 관찰하였다. 그러나 추가적으로 source language 의 영향을 측정하기 위해, kk, mt, yue 의 partition 을 교체하여 동일 언어 계열의 high-resource language 로부터 target task 에 대한 positive transfer 가능성을 평가하였다.</p><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-70-87f40212487169df1f3f33c95224f26b.png" width="628" height="483" class="img_ev3q"></p><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-71-ddc0dec3960082a8806bf2ff5c1564b7.png" width="621" height="487" class="img_ev3q"></p><ul><li>Fig. 4 와 Fig. 5 는 Hyper-X Base 와 mBERT 의 집계 결과를 보여준다. </li><li>첫째, 두 model 모두 positive transfer 로부터 이득을 본다. </li><li>둘째, mBERT 의 상대적 향상폭이 약간 더 크지만 Hyper-X 는 여전히 큰 차이로 mBERT 를 능가한다. 이는 Hyper-X 가 partition 이 달라져도 강건함을 보임을 나타낸다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="64-few-shot-transfer">6.4 Few-shot Transfer<a href="#64-few-shot-transfer" class="hash-link" aria-label="Direct link to 6.4 Few-shot Transfer" title="Direct link to 6.4 Few-shot Transfer">​</a></h2><p>few target instance 로 MMT 를 fine-tuning 하면 zero-shot 성능이 향상된다는 것이 알려져 있다. 이에 저자는 5 개 language 에 대해 Hyper-X 의 few-shot transfer 를 평가하였다. 이 중 3 개는 high-resource 이며 mBERT 에 포함된 언어이고, 2 개는 low-resource 이며 unseen language 이다.</p><p>실험에서는 English multi-task 로 초기 학습된 Hyper-X 와 baseline 들을 각각 5, 10, 20, 50 개의 training instance 를 사용해 language 별로 NER 과 POS tagging 에 대해 추가 fine-tuning 하였다 (세부 사항은 Appendix §D 참조).</p><p>Fig. 6 은 mBERT 와 MAD-X 대비 평균 결과를 보여준다. </p><p><img loading="lazy" alt="Figure 6" src="/assets/images/image-72-dad16baeaf87678e5193b603f7c47504.png" width="1714" height="569" class="img_ev3q"></p><ul><li>Zero-shot 결과와 유사하게, seen language 에서는 Hyper-X 가 NER 과 POS 모두에서 두 baseline 보다 일관되게 더 나은 adaptation 을 제공한다. </li><li>Unseen language 에서는 MAD-X 가 평균적으로 더 좋은 결과를 보인다. 이는 MAD-X 가 Maltese 와 Uyghur 에 대해 더 나은 초기 representation 을 가지고 있기 때문이다. 그러나 sample 수가 증가할수록 Hyper-X 가 초기 격차를 줄여나간다.</li><li>전체적으로, Hyper-X 는 POS tagging 의 ‘unseen’ language 를 제외한 대부분의 실험에서 최적 혹은 경쟁력 있는 성능을 일관되게 달성하였다.     <ul><li>이는 Hyper-X 가 standard zero-shot transfer 를 넘어 효과적임을 보여준다. </li></ul></li><li>Parameter 와 training efficiency 결과와 함께 고려할 때, Hyper-X 는 큰 computing cost 증가 없이 새로운 language 로 쉽게 확장될 수 있음을 시사한다.</li></ul><h1>7 Related Work</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="adapters">Adapters<a href="#adapters" class="hash-link" aria-label="Direct link to Adapters" title="Direct link to Adapters">​</a></h4><p>standard fine-tuning 의 parameter-efficient 대안으로서, adapter 는 빠른 학습, multi-task learning, knowledge composition 에 사용되어 왔다. 또한 Mahabadi et al. 과 He et al. 은 더 적은 parameter 로 더 나은 성능을 내기 위해 adapter 를 확장하였다. Multilingual transfer 맥락에서, adapter 는 language-specific capacity 를 추가로 할당할 수 있게 하여 ‘curse of multilinguality’를 완화한다. 이러한 language adapter 는 task adapter 와 결합될 때 높은 zero-shot 성능을 달성하며, MLM 기반 adaptation 을 통해 pre-training 에 포함되지 않은 language 로의 generalization 도 가능하게 한다. Philip et al. 과 Üstün et al. 은 monolingual adapter 를 zero-shot 및 unsupervised NMT 에 활용하였다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="hypernetworks-in-nlp">Hypernetworks in NLP<a href="#hypernetworks-in-nlp" class="hash-link" aria-label="Direct link to Hypernetworks in NLP" title="Direct link to Hypernetworks in NLP">​</a></h4><p>Tay et al. 은 input 에 조건화된 hypernetwork 를 사용하여 task-specific reparametrization 을 학습하는 multi-task model 을 제안하였다. 유사하게 Mahabadi et al. 은 hypernetwork 를 통해 task-specific adapter 를 생성하였다. 최근 He et al. 은 hypernetwork 를 사용하여 prompt 를 생성하였다. Multilingual learning 에서는 input source 가 language embedding 에 해당하며, Üstün et al. 과 Ansell et al. 은 language 의 typological feature vector 로부터 embedding 을 학습하여 hypernetwork 기반 unseen language generalization 을 가능하게 하였다. 본 연구와 유사한 맥락에서 Ponti et al. 의 parameter space factorization (PSF) 은 task-language 조합으로부터 task 와 language-specific embedding 을 학습한다. 그러나 저자의 model 과 달리, 이 embedding 은 softmax layer 에서 task/language-specific parameterization 에 사용된다.</p><h1>8 Conclusion</h1><p>저자는 multi-task multilingual transfer learning 을 위한 새로운 접근법인 <strong>Hyper-X</strong> 를 제안하였다. </p><ul><li>Hyper-X 는 unified hypernetwork 를 기반으로 하며, 여러 task 와 language 와 같은 heterogeneous source 의 information 을 활용한다. </li><li>Hyper-X 는 pre-trained multilingual transformer 의 parameter 를 수정하는 각 task-language 조합별 composite adapter 를 생성하도록 학습함으로써, 최대한의 information sharing 을 가능하게 하고 test 시 임의의 task-language pair 에 대해 zero-shot 예측을 수행할 수 있다.</li></ul><p>다양한 실험을 통해 Hyper-X 는 source language 로부터 전이할 때 state-of-the-art 와 경쟁할 수 있음을 보였다. Task 와 language 의 혼합이 가능한 경우, Hyper-X 는 여러 strong baseline 을 많은 language 에서 능가하면서도 더 parameter-efficient 하고 time-efficient 하다. 마지막으로, few-shot transfer 상황에서도 Hyper-X 는 초기 task adaptation 에 필요한 computing cost 가 baseline 대비 적으면서 강력한 선택지임을 보였다.</p><h1>9 Limitations</h1><p>첫째, Hyper-X 가 zero-shot transfer 에서 여러 task 로부터 이점을 얻을 수 있는 잠재력을 보여주었지만, 본 논문은 NER 과 POS tagging 이라는 제한된 task 에 대해서만 model 을 평가하였으므로 다른 task 로의 일반화 가능성이 제한될 수 있다.</p><p>둘째, few-shot transfer 에서는 MLM 을 통해 학습한 language 와 기존 task 로만 실험을 제한하였다. 본 연구에는 MLM data 가 없는 language 나 완전히 새로운 task 는 포함되지 않았다. 그러나 task 와 language embedding 을 분리하여 학습한다는 점은 새로운 language 나 task 를 위해 기존 embedding 을 보간(interpolate)할 가능성을 제공하며, 특히 few-shot learning 에 효과적일 수 있다. 이러한 두 가지 한계의 탐구는 향후 연구로 남긴다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/adapter">Adapter</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/hyper-x">Hyper-X</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multilingual-learning">multilingual learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multi-task-learning">multi-task learning</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/PEFT/Module/2022-05-Hyper-X.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/PEFT/Module/AdaMix"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/PEFT/Pruning/SMP"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Pruning Pre-trained Language Models Without Fine-Tuning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-adapters" class="table-of-contents__link toc-highlight">2.1 Adapters</a></li><li><a href="#22-hypernetworks" class="table-of-contents__link toc-highlight">2.2 Hypernetworks</a></li><li><a href="#31-a-hypernetwork-for-task-language-adapters" class="table-of-contents__link toc-highlight">3.1 A Hypernetwork for Task-Language Adapters</a><ul><li><a href="#combining-multiple-sources" class="table-of-contents__link toc-highlight">Combining Multiple Sources</a></li></ul></li><li><a href="#41-baselines-and-model-variants" class="table-of-contents__link toc-highlight">4.1 Baselines and Model Variants</a></li><li><a href="#42-training-details" class="table-of-contents__link toc-highlight">4.2 Training Details</a></li><li><a href="#61-parameter-and-time-efficiency" class="table-of-contents__link toc-highlight">6.1 Parameter and Time Efficiency</a></li><li><a href="#62-impact-of-auxiliary-mlm-training" class="table-of-contents__link toc-highlight">6.2 Impact of Auxiliary MLM Training</a></li><li><a href="#63-impact-of-source-languages" class="table-of-contents__link toc-highlight">6.3 Impact of Source Languages</a></li><li><a href="#64-few-shot-transfer" class="table-of-contents__link toc-highlight">6.4 Few-shot Transfer</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.4045ea26.js"></script>
<script src="/assets/js/main.84227156.js"></script>
</body>
</html>