<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/PEFT/Module/2020-02-K-Adapter">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/K-Adapter"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/K-Adapter"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/K-Adapter" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Module/K-Adapter" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.1a9c24d0.js" as="script">
<link rel="preload" href="/assets/js/main.21f2c310.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Attacking/universal-adversarial-prompt">Attacking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Generalization/NoisyTune">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Flan-T5">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Generalization/Flat-LoRA">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Mixture/UF">Mixture</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Module</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Parameter-Efficient Transfer Learning for NLP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/K-Adapter">K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/MAD-X">MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/AdapterFusion">AdapterFusion: Non-Destructive Task Composition for Transfer Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Compacter">COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/MAD-G">MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Meta-Adapter">Meta-Adapters: Parameter Efficient Few-shot Fine-tuning through Meta-Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/AdaMix">AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Hyper-X">Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/AdapterSoup">AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/TLR-Adapter">Cross-Lingual Transfer with Target Language-Ready Task Adapters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/PHA">Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Pruning/SMP">Pruning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Quantization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Selective/AdapterDrop">Selective</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning">Soft Prompt</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Engineering/ICL/Chain-of-Thought">Prompt Engineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Reinforce Learning/DPO/RLHF/Self-RLM">Reinforce Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</h1></header><p>논문 및 이미지 출처 : <a href="https://aclanthology.org/2021.findings-acl.121.pdf" target="_blank" rel="noopener noreferrer">https://aclanthology.org/2021.findings-acl.121.pdf</a></p><h1>Abstract</h1><p>저자는 BERT 와 RoBERTa 와 같은 large pre-trained model 에 knowledge 를 주입하는 문제를 연구한다. 기존 방법들은 일반적으로 knowledge 를 주입할 때 pre-trained model 의 originsl parameter 를 업데이트한다. 그러나 여러 종류의 knowledge 가 주입될 경우, 과거에 주입된 knowledge 는 사라지게 된다. 이를 해결하기 위해 저자는 <strong>K-ADAPTER</strong> 를 제안한다. </p><ul><li>이 framework 은 pre-trained model 의 originsl parameter 를 고정한 채 유지하면서 다양한 knowledge-infused model 의 개발을 지원한다. </li><li>RoBERTa 를 backbone model 로 사용하며, K-ADAPTER 는 주입된 각 종류의 knowledge 에 대해 neural adapter 를 제공한다. </li><li>이는 RoBERTa 에 연결된 plug-in 과 같은 구조이다. 서로 다른 adapter 사이에는 information flow 가 존재하지 않으므로, 여러 adapter 를 분산 방식으로 효율적으로 training 할 수 있다.</li></ul><p>case study 로, 저자는 본 연구에서 두 가지 종류의 knowledge 를 주입한다.</p><p>(1) Wikipedia 와 Wikidata 에서 자동으로 정렬된 text-triplets 로부터 얻은 factual knowledge<br>
<!-- -->(2) dependency parsing 을 통해 얻은 linguistic knowledge</p><p>relation classification, entity typing, question answering 의 세 가지 knowledge-driven task 에 대한 결과는 각 adapter 가 성능을 향상시키며, 두 adapter 의 결합은 추가적인 성능 향상을 가져옴을 보여준다. 추가적인 분석을 통해 K-ADAPTER 가 RoBERTa 보다 다양한 knowledge 를 포착함을 확인할 수 있다.</p><h1>1 Introduction</h1><p>Language representation model 은 BERT, GPT, XLNet, RoBERTa, T5 와 같이 large-scale text corpus 에 대해 (masked) language modeling 과 같은 unsupervised objective 로 pre-training 된 모델을 의미하며, 다양한 NLP downstream task 에서 SOTA 성능을 확립하였다. 이러한 pre-trained model 들이 경험적 연구에서 큰 성공을 거두었음에도 불구하고, 최근 연구들은 이러한 unsupervised 방식으로 학습된 model 이 rich knowledge 를 포착하는 데 어려움을 겪는다고 제안한다. </p><ul><li>예를 들어, Poerner et al 은 language model 이 entity name 의 surface form 에 대해 reasoning 은 잘 수행하지만, rich factual knowledge 를 포착하는 데는 실패한다고 주장한다. </li><li>Kassner and Schutze 는 BERT 가 부정(e.g., &quot;not&quot;)의 의미를 대부분 학습하지 못했다고 관찰하였다. 이러한 관찰은 BERT 와 RoBERTa 같은 pre-trained model 에 knowledge 를 주입하는 연구를 촉발한다.</li></ul><p><img loading="lazy" alt="Table 1" src="/assets/images/image-29-f331223bdf69d0da3668f1a1d623f90b.png" width="3820" height="1806" class="img_ev3q"></p><p>최근에는 pre-trained language model 에 knowledge 를 주입하려는 다양한 시도가 이루어졌다. 기존 연구 대부분은 (Tab. 1 에 제시된 바와 같이) standard language modeling objective 에 knowledge-driven objective 를 결합하여 전체 model parameter 를 업데이트하였다. 이러한 방법들은 downstream task 에서 더 나은 성능을 얻지만, 여러 종류의 knowledge 가 주입된 versatile model 개발을 지원하는 데 어려움을 겪는다. 새로운 종류의 knowledge 가 주입되면, model parameter 를 다시 training 해야 하고, 이 과정에서 과거에 주입된 knowledge 가 사라지게 된다. 동시에, 이러한 model 은 서로 얽힌 representation 을 생성하므로 각 knowledge 의 효과를 분석하기 어렵다.</p><p>본 논문에서 저자는 K-ADAPTER 라는 flexible 하고 simple 한 framework 을 제안한다. </p><ul><li>이 framework 은 large pre-trained model 에 여러 종류의 knowledge infusion 을 지원한다. </li><li>K-ADAPTER 는 pre-trained model 의 originsl representation 을 변경하지 않고, 주입된 knowledge 의 종류에 따라 다른 representation 을 생성한다. </li><li>이는 adapter 라 불리는 compact neural model 의 통합을 통해 달성된다. </li><li>Adapter 는 pre-trained model outside plug-in 형태로 연결된 knowledge-specific model 이며, input 은 pre-trained model 의 intermediate layer 의 hidden-state output 이다. </li></ul><p>저자는 RoBERTa 를 base pre-trained model 로 사용하고, 두 가지 종류의 knowledge 를 통합한다. </p><p>(1) Wikipedia text 와 Wikidata triplet 을 정렬하여 얻은 factual knowledge,<br>
<!-- -->(2) web text 에 dependency parser 를 적용하여 얻은 linguistic knowledge. </p><p>pre-training 단계에서 두 adapter 는 독립적으로 training 된다. Adapter 는 RoBERTa 에 비해 trainable parameter 수가 훨씬 적으므로 training 과정은 memory efficient 하다.</p><p>저자는 relation classification, entity typing, question answering 의 세 가지 knowledge-driven task 에 걸쳐 여섯 개 benchmark dataset 에 대해 광범위한 실험을 수행한다. 실험 결과 K-ADAPTER 는 RoBERTa 보다 일관되게 더 나은 성능을 보였으며, 다섯 개 dataset 에서 SOTA 성능을 달성하였다. Case study 와 probing experiment 는 K-ADAPTER 가 RoBERTa 보다 다양한 knowledge 를 포착함을 보여준다.</p><h1>2 Related Work</h1><p>본 연구는 pre-trained model 에 knowledge 를 주입하는 영역과 관련된다. Tab. 1 에서 나타난 바와 같이, 기존 연구들은 주로 knowledge source 와 training 에 사용된 objective 의 차이에서 구분된다.</p><ul><li><strong>ERNIE</strong> 는 knowledge graph 를 BERT 에 주입한다. Wikipedia sentence 의 entity 를 WikiData 의 fact triple 과 정렬하고, entity 가 세 개 미만인 sentence 는 폐기한다. <ul><li>Training 과정에서 input 은 sentence 와 연결된 fact 를 포함하며, knowledge-aware learning objective 는 올바른 token-entity alignment 를 예측하는 것이다. </li><li>Entity embedding 은 WikiData 의 fact triple 에 대해 TransE 로 학습된다.</li></ul></li><li><strong>LIBERT</strong> 는 WordNet 에서 synonym 과 hyponym-hypernym 관계를 가지는 word 쌍을 주입한다. <ul><li>Model 은 special token 으로 구분된 word pair 를 input 으로 사용하고, 특정 relation 이 성립하는지를 예측하는 binary classification 문제로 최적화된다.</li></ul></li><li><strong>SenseBERT</strong> 는 word-supersense knowledge 를 고려한다. Input 에서 masked word 의 supersense 를 예측하는 방식으로 knowledge 를 주입하며, 후보는 noun 과 verb 이고 ground truth 는 WordNet 에서 가져온다.</li><li><strong>KnowBERT</strong> 는 Knowledge attention 과 recontextualization 을 통해 knowledge base 를 BERT 에 통합한다. Knowledge source 는 WordNet 의 synset-synset, lemma-lemma 관계와 Wikipedia 의 entity linking 정보이다. Entity linking supervision 이 가능할 경우, model 은 knowledge-aware log-likelihood 나 max-margin objective 로 학습된다.</li><li><strong>WKLM</strong> 은 originsl document 의 entity mention 을 동일한 type 의 다른 entity name 으로 교체한다. <ul><li>Model 은 올바른 entity mention 과 무작위로 선택된 entity mention 을 구별하도록 학습된다.</li></ul></li><li><strong>BERT-MK</strong> 는 knowledge graph 의 fact triple 을 통합한다. <ul><li>각 entity 에 대해 knowledge graph 상의 이웃으로부터 incoming 과 outcoming instance 를 sampling 하고, head 또는 tail entity 를 교체하여 negative instance 를 생성한다. </li><li>Model 은 real fact 와 fake fact 를 구별하도록 학습된다.</li></ul></li></ul><p>Tab. 1 에서 볼 수 있듯이, 저자의 model 인 K-ADAPTER 는 기존 연구와 세 가지 측면에서 다르다.</p><ol><li>fact-related objective (i.e., predicate/relation prediction) 와 linguistic-related objective (i.e., dependency relation prediction) 를 모두 고려한다.</li><li>knowledge infusion 과정에서 BERT 의 original parameter 를 고정한다.</li><li>저자의 접근법은 continual learning 을 지원한다. 즉, 서로 다른 adapter 의 학습이 얽히지 않는다. 이러한 flexibility 덕분에 서로 다른 종류의 knowledge 를 독립적으로 효율적으로 주입할 수 있으며, 이전에 주입된 knowledge 의 손실 없이 더 많은 종류의 knowledge 를 주입할 수 있다.</li></ol><h1>3 K-ADAPTER</h1><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-30-c90b8acbb67508fc2eba7bacc3dc72e3.png" width="3820" height="2305" class="img_ev3q"></p><p>Fig. 1(a) 에 나타난 바와 같이, 기존 연구 대부분은 pre-trained language model 에 knowledge 를 주입하고 multi-task learning 을 통해 model parameter 를 업데이트하여 이를 강화한다. 그러나 이러한 multi-task learning 기반 knowledge-injected 방법들의 다양한 변형에도 불구하고 공통적으로 충분히 연구되지 않은 문제는 과거 knowledge 의 catastrophic forgetting 이다. </p><p>이를 해결하기 위해 저자는 Fig. 1(b) 와 같은 <strong>K-ADAPTER</strong> 를 제시한다. </p><ul><li>이 방식에서는 여러 종류의 knowledge 를 pre-trained model 에 직접 주입하는 대신, 개별 compact neural model (i.e., 본 논문에서 adapter) 에 각각 주입한다. </li><li>이 방법은 pre-trained model 의 original representation 을 고정한 채 유지하면서 continual knowledge infusion 을 지원한다. </li><li>즉, 각 종류의 knowledge 를 해당 knowledge-specific adapter 에 주입하여 disentangled representation 을 생성한다.</li><li>구체적으로 adapter 는 knowledge-specific model 로 (few parameter 를 가짐), pre-trained model outside plug-in 형태로 연결된다. </li><li>Adapter 의 input 은 pre-trained model 의 intermediate layer hidden-state output 이다. </li><li>각 adapter 는 discriminative knowledge 를 주입하기 위해 서로 다른 task 에 대해 독립적으로 pre-training 되며, 이때 pre-trained model 의 original parameter 는 고정된다. </li></ul><p>본 논문에서는 RoBERTa 를 pre-trained model 로 활용하며, factual knowledge 와 linguistic knowledge 를 두 종류의 adapter (factual adapter 와 linguistic adapter) 로 주입한다. </p><p>Factual adapter 는 relation classification task 로, linguistic adapter 는 dependency relation prediction task 로 각각 pre-training 된다. 이 절에서는 먼저 adapter 의 구조를 설명하고, 이후 knowledge-specific adapter 의 pre-training 과정을 제시한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-adapter-structure">3.1 Adapter Structure<a href="#31-adapter-structure" class="hash-link" aria-label="Direct link to 3.1 Adapter Structure" title="Direct link to 3.1 Adapter Structure">​</a></h2><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-31-f40380c8d1a24dbdce2ca5ccf0f67bf4.png" width="1910" height="1720" class="img_ev3q"></p><p>본 연구에서는 Fig. 2 와 같은 knowledge-specific adapter 구조를 제안한다. </p><ul><li>Houlsby et al. 이 transformer layer 마다 adapter layer 를 추가하는 방식과 달리, 저자의 adapter 는 outside plug-in 으로 동작한다. </li><li>각 adapter model 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> 개의 adapter layer 로 구성되며, 각 adapter layer 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> 개의 transformer layer 와 두 개의 projection layer 를 포함한다. <ul><li>두 projection layer 사이에는 skip-connection 이 적용된다.</li></ul></li><li>구체적으로, 각 adapter model 은 pre-trained model 의 서로 다른 transformer layer 사이에 adapter layer 를 삽입한다. </li><li>Pre-trained model 의 transformer layer output hidden feature 와 이전 adapter layer 의 output feature 를 연결(concatenate)하여 현재 adapter layer 의 input feature 로 사용한다. </li><li>각 knowledge-specific adapter 의 경우, pre-trained model 과 adapter 의 마지막 hidden feature 를 연결하여 해당 adapter model 의 최종 output feature 로 사용한다.</li></ul><p>Pre-training 절차에서는 각 knowledge-specific adapter 를 서로 다른 pre-training task 에 대해 독립적으로 학습한다. </p><ul><li>다양한 downstream task 에 대해서는 K-ADAPTER 가 RoBERTa 와 BERT 와 유사한 fine-tuning 절차를 적용할 수 있다. </li><li>Knowledge-specific adapter 가 하나만 채택된 경우, 해당 adapter model 의 final output feature 를 downstream task 의 task-specific layer 에 input 으로 사용한다. </li><li>Knowledge-specific adapter 가 여러 개 채택된 경우, 서로 다른 adapter model 의 output feature 를 연결하여 downstream task 의 task-specific layer 에 input 으로 사용한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-pre-training-settings">3.2 Pre-training settings<a href="#32-pre-training-settings" class="hash-link" aria-label="Direct link to 3.2 Pre-training settings" title="Direct link to 3.2 Pre-training settings">​</a></h2><ul><li>모든 실험에서 pre-trained model 로 Huggingface 의 RoBERTaLARGE ( <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mn>24</mn></mrow><annotation encoding="application/x-tex">L=24</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">24</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">H=1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1024</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">A=16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">16</span></span></span></span></span>, 355M params) 구현체를 사용한다. </li><li>각 adapter layer 에 대해, transformer layer 의 개수를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span>, transformer layer 의 hidden dimension 을 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">H_A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>, self-attention head 의 개수를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">A_A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>, down-projection 과 up-projection layer 의 hidden dimension 을 각각 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">H_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>u</mi></msub></mrow><annotation encoding="application/x-tex">H_u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">u</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 로 표기한다. <ul><li>구체적으로 adapter 의 크기는 다음과 같다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">N=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>A</mi></msub><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">H_A=768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">768</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>A</mi></msub><mo>=</mo><mn>12</mn></mrow><annotation encoding="application/x-tex">A_A=12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">12</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>u</mi></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">H_u=1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">u</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1024</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>d</mi></msub><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">H_d=768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0813em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">768</span></span></span></span></span>. </li></ul></li><li>Adapter layer 가 삽입되는 RoBERTa layer 는 {0, 11, 23} 이며, 서로 다른 adapter layer 는 parameter 를 공유하지 않는다. <ul><li>따라서 각 adapter model 의 전체 parameter 수는 약 42M 으로, RoBERTaLARGE 보다 훨씬 작아 training 과정에서 memory efficient 하다. </li></ul></li><li>Training 중에는 RoBERTa 가 고정되며, adapter 의 parameter 만이 학습 가능하고 무작위로 초기화된다. </li><li>이후 서로 다른 knowledge 를 knowledge-specific adapter 에 주입하는 방법을 설명한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-factual-adapter">3.3 Factual Adapter<a href="#33-factual-adapter" class="hash-link" aria-label="Direct link to 3.3 Factual Adapter" title="Direct link to 3.3 Factual Adapter">​</a></h2><p>Factual knowledge 는 사실과 관련된 기본 정보로 설명될 수 있다. 본 연구에서는 자연어 내 entity 들 간의 관계에서 factual knowledge 를 획득한다. Wikipedia abstract 와 Wikidata triple 간의 large-scale alignment dataset 인 T-REx 로부터 sub-dataset 인 T-REx-rc 를 추출한다. Entity pair 가 50 미만인 relation 은 모두 제거하고, 최종적으로 430 개 relation 과 5.5M sentence 를 수집한다.</p><p>Factual knowledge 를 주입하기 위해, 저자는 relation classification task 에 대해 facAdapter 라 불리는 knowledge-specific adapter 를 pre-training 한다. 이 task 는 주어진 entity pair 의 context 에 기반하여 relation label 을 분류하는 것을 요구한다. 구체적으로 RoBERTa 와 facAdapter 의 마지막 hidden feature 를 연결하여 input representation 으로 사용하고, 주어진 entity 들의 input representation 에 pooling layer 를 적용한다. 이후 두 entity representation 을 연결하여 relation classification 을 수행한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-linguistic-adapter">3.4 Linguistic Adapter<a href="#34-linguistic-adapter" class="hash-link" aria-label="Direct link to 3.4 Linguistic Adapter" title="Direct link to 3.4 Linguistic Adapter">​</a></h2><p>Linguistic knowledge 는 natural language texts 에 내재적으로 포함되어 있으며, 예를 들어 syntactic 및 semantic 정보가 있다. </p><p>본 연구에서는 natural language texts 내 word 간 dependency 관계에서 linguistic knowledge 를 획득한다. 이를 위해 100 만 개의 example 로 구성된 dataset 을 구축하였다. 구체적으로, Book Corpus 의 일부에 Stanford Parser 의 off-the-shell dependency parser 를 적용한다.</p><p>Linguistic knowledge 를 주입하기 위해, 저자는 dependency relation prediction task 에 대해 linAdapter 라 불리는 또 다른 knowledge-specific adapter 를 pre-training 한다. 이 task 는 주어진 sentence 에서 각 token 의 head index 를 예측하는 것을 목표로 한다. RoBERTa 와 linAdapter 의 마지막 hidden feature 를 연결하여 input representation 으로 사용하고, 각 token 의 input representation 에 linear layer 를 적용하여 classification 을 수행한다. facAdapter 와 linAdapter 의 추가적인 training detail 은 Appendix 에 제시되어 있다.</p><h1>4 Experiments</h1><p>저자는 K-ADAPTER 를 세 가지 knowledge-driven downstream task, 즉 entity typing, question answering, relation classification 에 대해 평가한다. 또한 case study 와 probing experiment 를 통해 factual knowledge 학습에 대한 model 의 효과성과 능력을 탐구한다. </p><p>K-ADAPTER (F+L), K-ADAPTER (F), K-ADAPTER (L) 은 각각 factual adapter 와 linguistic adapter 를 모두 포함한 경우, factual adapter 만 포함한 경우, linguistic adapter 만 포함한 경우를 의미한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-entity-typing">4.1 Entity Typing<a href="#41-entity-typing" class="hash-link" aria-label="Direct link to 4.1 Entity Typing" title="Direct link to 4.1 Entity Typing">​</a></h2><p>저자는 주어진 entity 와 그 context 의 type 을 예측하는 fine-grained entity typing 실험을 수행한다. </p><ul><li>Model 은 OpenEntity 와 FIGER dataset 에 대해 평가되며, Zhang et al. 과 동일한 split setting 을 따른다. </li><li>Entity typing 을 위해 fine-tuning 시, input token sequence 를 수정하여 특정 entity 앞뒤에 special token “@” 를 추가하고, first “@” token 의 representation 을 classification 에 사용한다. </li><li>OpenEntity 의 경우, model 성능 평가는 micro F1 score 를 사용한다. </li><li>FIGER 의 경우, strict accuracy, loose macro, loose micro F1 score 를 평가 metric 으로 사용하며, Ling and Weld 의 동일한 평가 기준을 따른다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="baselines">Baselines<a href="#baselines" class="hash-link" aria-label="Direct link to Baselines" title="Direct link to Baselines">​</a></h4><ul><li>NFGEC 은 attentive recursive neural network 를 사용하여 context representation 을 구성한다.</li><li>KEPLER 는 knowledge embedding objective 의 supervision 으로 factual knowledge 를 통합한다.</li><li>RoBERTa+multitask 은 두 가지 pre-training task 에 대해 multi-task learning (Fig. 1(a)) 으로 학습된 RoBERTa model 이며, 여러 종류의 knowledge 를 주입하기 위해 사용된다.</li><li>K-ADAPTER (w/o knowledge) 는 RoBERTa model 과 knowledge 가 주입되지 않은 adapter 로 구성된다.</li><li>다른 baseline model 은 Sec. 2 에 설명되어 있다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-and-discussion">Results and Discussion<a href="#results-and-discussion" class="hash-link" aria-label="Direct link to Results and Discussion" title="Direct link to Results and Discussion">​</a></h4><p>OpenEntity 와 FIGER 에 대한 결과는 Tab. 2 에 제시되어 있다. </p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-32-607b32e7efffb6fc519df9f030c9a002.png" width="2752" height="1646" class="img_ev3q"></p><ul><li>K-ADAPTER (F+L) 은 두 dataset 모두에서 일관된 성능 향상을 달성하였다. </li><li>OpenEntity 의 경우, 저자의 RoBERTa 는 다른 baseline model 보다 더 나은 성능을 보였으며, K-ADAPTER (F+L) 은 RoBERTa 대비 F1 score 가 1.38% 더 향상되었다. <ul><li>이는 factual knowledge 와 linguistic knowledge 가 entity type 을 더 정확히 예측하는 데 기여함을 의미한다.</li></ul></li><li>FIGER 는 OpenEntity 보다 더 많은 entity type 을 포함하며, 더 fine-grained 하다. </li><li>WKLM 과 비교했을 때, K-ADAPTER (F+L) 은 macro F1 에서 2.88%, micro F1 에서 2.54%, accuracy 에서 1.60% 향상되었다. 이는 K-ADAPTER (F+L) 이 fine-grained entity typing 에 효과적임을 보여준다.</li></ul><p>추가적으로, 저자는 ablated model 인 K-ADAPTER (w/o knowledge) 에 대해 실험을 수행하여 성능 향상이 knowledge 도입 때문인지, 아니면 parameter 증가 때문인지를 검증하였다. </p><ul><li>결과는 K-ADAPTER (F) 가 K-ADAPTER (w/o knowledge) 보다 유의미하게 우수함을 보여준다. </li><li>더욱이 OpenEntity dataset 에서 K-ADAPTER (w/o knowledge) 는 RoBERTa 보다 오히려 약간 낮은 성능을 보였다. </li><li>이러한 결과는 저자의 model 이 parameter 증가가 아니라 knowledge 주입으로 인한 성능 향상을 달성했음을 입증한다. </li><li>따라서 이후의 실험에서는 단순화를 위해 K-ADAPTER (w/o knowledge) 는 논의하지 않는다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-question-answering">4.2 Question Answering<a href="#42-question-answering" class="hash-link" aria-label="Direct link to 4.2 Question Answering" title="Direct link to 4.2 Question Answering">​</a></h2><p>저자는 두 가지 QA task, 즉 commonsense QA 와 open-domain QA 에 대해 실험을 수행한다.</p><p><strong>Commonsense QA</strong> 는 commonsense 를 활용하여 질문에 답하는 것을 목표로 한다. 이를 위해 CosmosQA dataset 을 사용하여 model 을 평가한다. CosmosQA 는 commonsense 기반 reading comprehension 문제로, multiple-choice question 형태로 구성된다. Fine-tuning 시, input token sequence 는</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">&lt;SEP&gt;context&lt;/SEP&gt;question&lt;/SEP&gt;answer&lt;/SEP&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>형식으로 수정되며, first token 의 representation 을 사용하여 classification 을 수행하고 각 answer 에 대해 score 를 얻는다. 네 개의 score 중 가장 높은 값을 가지는 answer 를 최종 선택한다. 최종 성능은 leaderboard accuracy score 로 보고한다.</p><p><strong>Open-domain QA</strong> 는 document 나 webpage 와 같은 외부 자원을 사용하여 질문에 답하는 것을 목표로 한다. 저자는 Quasar-T 와 SearchQA 두 public dataset 에 대해 model 을 평가한다. 구체적으로, 먼저 information retrieval system 을 통해 question 에 대응하는 paragraph 를 검색한 후, reading comprehension 기법으로 검색된 paragraph 에서 answer 를 추출한다. 이전 연구와 동일하게 Wang et al. 이 제공한 retrieval paragraph 를 사용한다. Fine-tuning 시 input token sequence 는</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">&lt;SEP&gt;question&lt;/SEP&gt;paragraph&lt;/SEP&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>형식으로 수정한다. Model 의 마지막 hidden feature 에 linear layer 를 적용하여 answer span 의 시작과 끝 position 을 예측한다. 평가 metric 으로는 ExactMatch (EM) 과 loose F1 score 를 사용한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="baselines-1">Baselines<a href="#baselines-1" class="hash-link" aria-label="Direct link to Baselines" title="Direct link to Baselines">​</a></h4><ul><li>BERT-FTRACE+SWAG: BERT 를 RACE 와 SWAG dataset 에 sequential fine-tuning 한 model.</li><li>BiDAF: bi-directional attention network 기반 model.</li><li>AQA: 질문을 재작성하고, 재작성된 질문들로부터 생성된 answer 를 집계.</li><li>R³: ranker 를 활용하여 가장 confident 한 paragraph 를 선택하는 reinforced model.</li><li>Evidence Agg.: 여러 paragraph 에 걸친 evidence 를 집계하여 활용.</li><li>WKLM: 여러 paragraph 를 읽고 단일 answer 를 예측하는 reader model.</li><li>WKLM+Ranking: WKLM 기반 paragraph reader 에 BERT 기반 paragraph ranker 를 추가하여 각 paragraph 에 relevance score 를 할당.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-and-discussion-1">Results and Discussion<a href="#results-and-discussion-1" class="hash-link" aria-label="Direct link to Results and Discussion" title="Direct link to Results and Discussion">​</a></h4><p>CosmosQA 결과는 Tab. 3 에 제시되어 있다. </p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-33-0e23f70455a873228758bc6bc49815f2.png" width="3272" height="1820" class="img_ev3q"></p><ul><li>BERT-FTRACE+SWAG 과 비교했을 때, 저자의 RoBERTa 는 accuracy 에서 11.89% 의 향상을 달성하였다. </li><li>RoBERTa 와 비교했을 때, K-ADAPTER (F+L) 은 accuracy 가 추가적으로 1.24% 향상되었으며, 이는 K-ADAPTER 가 더 나은 commonsense inference 능력을 획득했음을 보여준다. </li><li>또한 K-ADAPTER (F), K-ADAPTER (L) 은 RoBERTa 보다 확실히 성능이 좋지만, RoBERTa+multitask 보다는 약간 낮은 성능을 보였다. </li><li>그러나 K-ADAPTER (F+L) 은 RoBERTa+multitask 와 비교하여 뚜렷한 성능 향상을 달성하였다. </li><li>이는 여러 knowledge-specific adapter 의 결합이 더 나은 성능을 이끌어낼 수 있음을 입증한다.</li></ul><p>Open-domain QA 결과도 Tab. 3 에 제시되어 있다. </p><ul><li>K-ADAPTER model 은 다른 baseline 에 비해 더 나은 성능을 달성하였다. <ul><li>이는 K-ADAPTER 가 주입된 knowledge 를 효과적으로 활용하여 검색된 paragraph 를 더 잘 이해하고 question 에 답할 수 있음을 나타낸다. </li><li>특히 SearchQA dataset 에서, K-ADAPTER (F+L) 은 WKLM 대비 F1 score 가 4.01% 향상되었으며, WKLM+Ranking 과 비교해도 약간의 성능 향상을 보였다. </li></ul></li><li>주목할 점은 K-ADAPTER model 은 검색된 각 paragraph 의 confidence 를 고려하지 않음에도, WKLM+Ranking (BERT 기반 ranker 의 ranking score 활용) 과 유사하거나 더 나은 성능을 보였다는 것이다. </li><li>Quasar-T dataset 에서도 K-ADAPTER (F+L) 은 WKLM 대비 F1 score 가 3.1% 향상되었으며, WKLM+Ranking 보다 약간 더 우수한 성능을 달성하였다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-relation-classification">4.3 Relation Classification<a href="#43-relation-classification" class="hash-link" aria-label="Direct link to 4.3 Relation Classification" title="Direct link to 4.3 Relation Classification">​</a></h2><p>Relation classification 은 주어진 sentence 내 두 entity 간의 올바른 relation 을 판별하는 task 이다. </p><p>본 연구에서는 large-scale relation classification dataset 인 TACRED 를 사용한다. Fine-tuning 시, input token sequence 를 수정하여 first entity 앞뒤에 special token “@” 를 추가하고, second entity 앞뒤에 special token “#” 를 추가한다. 이후 first “@” 와 “#” token 의 representation 을 연결하여 relation classification 을 수행한다. 평가 metric 은 기존 연구와 동일하게 micro F1 score 를 사용한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="baselines-2">Baselines<a href="#baselines-2" class="hash-link" aria-label="Direct link to Baselines" title="Direct link to Baselines">​</a></h4><ul><li>C-GCN 은 dependency tree 를 model 하기 위해 graph convolutional network 을 사용한다.</li><li>BERT-large 는 baseline BERT-large model.</li><li>BERT+MTB 는 knowledge base supervision 없이 relation representation 을 학습하기 위해 matching the blanks 방법을 사용한다.</li><li>다른 baseline model 은 Sec. 2 와 Sec. 4.1 에 기술되어 있다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-and-discussion-2">Results and Discussion<a href="#results-and-discussion-2" class="hash-link" aria-label="Direct link to Results and Discussion" title="Direct link to Results and Discussion">​</a></h4><p>Tab. 4 는 TACRED 에서 서로 다른 model 의 성능을 보여준다. </p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-34-7d190db4bc6897ecaecdbf172a8a769a.png" width="1926" height="1615" class="img_ev3q"></p><p>결과는 K-ADAPTER model 이 모든 baseline 을 크게 능가함을 나타내며, 이는 model 이 relation classification 에서 knowledge infusion 으로부터 이득을 얻을 수 있음을 직접적으로 입증한다. 특히,</p><ol><li>K-ADAPTER model 은 RoBERTa 보다 우수하며, 이는 adapter 를 통해 pre-trained model 에 knowledge infusion 하는 방식의 효과성을 증명한다.</li><li>K-ADAPTER model 은 RoBERTa+multitask 와 비교했을 때 더 큰 성능 향상을 보인다. 이는 K-ADAPTER 방식처럼 knowledge 를 개별적으로 주입하는 것이 model 이 knowledge 를 최대한 활용하는 데 유리함을 직접적으로 보여준다.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="44-case-study">4.4 Case Study<a href="#44-case-study" class="hash-link" aria-label="Direct link to 4.4 Case Study" title="Direct link to 4.4 Case Study">​</a></h2><p>Tab. 5 는 TACRED relation classification dataset 에서 K-ADAPTER 와 RoBERTa 의 정성적 비교 예시를 제공한다. </p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-35-494c613e19885dd41f59e084c793ebaf.png" width="3808" height="2131" class="img_ev3q"></p><p>결과는 대부분의 경우 RoBERTa 가 잘못 예측한 logit 값과 true label 의 logit 값이 실제로 매우 가깝다는 것을 보여준다. 예를 들어, 주어진 sentence “<strong>New Fabris</strong> closed down <strong>June 16</strong>” 에 대해 RoBERTa 는 “no relation” 을 예측하지만, true label 인 “city of birth” 는 두 번째 순위에 위치한다. 만약 model 이 “New Fabris” 와 “June 16” 간의 relation 을 올바르게 예측하려면 “New Fabris” 가 회사라는 사실을 알아야 한다. K-ADAPTER 에 포함된 factual knowledge 덕분에 model 이 “no relation” 대신 올바른 category label 을 예측할 수 있다.</p><p>추가적으로, 저자는 LAMA (LAnguage Model Analysis) probe 를 사용하여 model 이 factual knowledge 를 기억하는 능력을 검증한다. 구체적으로, LAMA probing task 는 zero-shot setting 에서 수행되며, language model 이 fine-tuning 없이 relational fact 에 대한 cloze-style question 에 답하도록 요구한다. 예를 들어, input 이 “Simon Bowman was born in <!-- -->[MASK]<!-- -->” 일 때, model 은 mask 된 올바른 token 을 예측해야 한다. </p><p><img loading="lazy" alt="Table 6" src="/assets/images/image-36-30000885be90b12d96492958aaf974f9.png" width="3808" height="1694" class="img_ev3q"></p><ul><li>Tab. 6 은 RoBERTaLARGE 와 K-ADAPTER 의 LAMA query 에 대한 예시 결과를 보여준다. </li><li>이 예시들에서 K-ADAPTER 가 예측한 object 가 더 정확함을 확인할 수 있으며, 이는 K-ADAPTER 가 RoBERTa 보다 더 rich 한 factual knowledge 를 포착함을 입증한다.</li></ul><h1>5 Conclusion</h1><p>본 논문에서는 large pre-trained model 에 knowledge 를 주입하기 위한 flexible 하고 simple 한 접근법인 K-ADAPTER 를 제안하였다. K-ADAPTER 는 pre-trained model 의 original parameter 를 변경하지 않고 유지하며, continual knowledge infusion 을 지원한다. 즉, 새로운 종류의 knowledge 가 주입되더라도 기존 knowledge 를 위해 학습된 parameter 는 영향을 받지 않는다. 구체적으로, factual knowledge 와 linguistic knowledge 는 각각 relation classification task 와 dependency relation prediction task 에 대해 pre-training 된 두 종류의 adapter 를 통해 RoBERTa 에 주입된다.</p><p>세 가지 knowledge-driven downstream task 에 대한 광범위한 실험은 각 adapter 가 개별적으로 유의미한 성능 향상을 달성하며, 두 adapter 를 결합했을 때 더욱 큰 향상을 이끈다는 것을 보여준다. 또한 세부 분석은 K-ADAPTER 가 RoBERTa 보다 더 rich 한 factual 및 commonsense knowledge 를 포착함을 시사하며, knowledge infusion 의 효과성에 대한 통찰을 제공한다. 향후 연구에서는 더 다양한 종류의 knowledge 를 주입하고, 본 framework 을 더 많은 pre-trained model 에 적용할 예정이다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/adapter">Adapter</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/k-adapter">K-Adapter</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/knowledge-infused-model">knowledge-infused model</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/PEFT/Module/2020-02-K-Adapter.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/PEFT/Module/Adapter"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Parameter-Efficient Transfer Learning for NLP</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/PEFT/Module/MAD-X"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-adapter-structure" class="table-of-contents__link toc-highlight">3.1 Adapter Structure</a></li><li><a href="#32-pre-training-settings" class="table-of-contents__link toc-highlight">3.2 Pre-training settings</a></li><li><a href="#33-factual-adapter" class="table-of-contents__link toc-highlight">3.3 Factual Adapter</a></li><li><a href="#34-linguistic-adapter" class="table-of-contents__link toc-highlight">3.4 Linguistic Adapter</a></li><li><a href="#41-entity-typing" class="table-of-contents__link toc-highlight">4.1 Entity Typing</a></li><li><a href="#42-question-answering" class="table-of-contents__link toc-highlight">4.2 Question Answering</a></li><li><a href="#43-relation-classification" class="table-of-contents__link toc-highlight">4.3 Relation Classification</a></li><li><a href="#44-case-study" class="table-of-contents__link toc-highlight">4.4 Case Study</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.1a9c24d0.js"></script>
<script src="/assets/js/main.21f2c310.js"></script>
</body>
</html>