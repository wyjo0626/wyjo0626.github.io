<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-11-BitNet-a4.8">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">BitNet a4.8: 4-bit Activations for 1-bit LLMs | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="BitNet a4.8: 4-bit Activations for 1-bit LLMs | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.52ac8645.js" as="script">
<link rel="preload" href="/assets/js/main.12d93f77.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Attacking/universal-adversarial-prompt">Attacking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Chain-of-Thought">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Mixture/UniPELT">Mixture</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Module</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Pruning/SMP">Pruning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Quantization</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Fine-Tuning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/QuIP">QuIP: 2-Bit Quantization of Large Language Models With Guarantees</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet">BitNet: Scaling 1-bit Transformers for Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/GIFT-SW">GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/LLM-FP4">LLM-FP4: 4-Bit Floating-Point Quantized Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8">BitNet a4.8: 4-bit Activations for 1-bit LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/fp4">Optimizing Large Language Model Training Using FP4 Quantization</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/LoRA/QLoRA">LoRA</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning">Soft Prompt</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Quantization</span><meta itemprop="position" content="4"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Fine-Tuning</span><meta itemprop="position" content="5"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">BitNet a4.8: 4-bit Activations for 1-bit LLMs</span><meta itemprop="position" content="6"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>BitNet a4.8: 4-bit Activations for 1-bit LLMs</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2411.04965" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2411.04965</a></p><h1>Abstract</h1><p>최근 1-bit Large Language Models (LLMs) 에 대한 연구, 예를 들어 BitNet b1.58, 는 LLM 의 inference cost 를 줄이면서도 성능을 유지하는 유망한 방향을 제시한다. </p><p>본 논문에서는 1-bit LLM 을 위한 4-bit activations 를 가능하게 하는 <strong>BitNet a4.8</strong> 을 소개한다. </p><ul><li>BitNet a4.8 은 outlier channels 로 인해 발생하는 quantization errors 를 줄이기 위해 <strong>hybrid quantization 및 sparsification 전략</strong>을 활용한다. <ul><li>구체적으로, attention 및 feed-forward network (FFN) layer 의 inputs 에 대해 4-bit activation 을 사용하고, intermediate states 는 sparsification 후 8-bit quantization 을 적용한다.</li></ul></li><li>광범위한 실험 결과, BitNet a4.8 은 BitNet b1.58 과 동등한 training costs 로 유사한 성능을 달성하면서도 4-bit (INT4/FP4) kernel 을 활성화하여 faster inference 를 보인다. </li><li>또한, BitNet a4.8 은 전체 parameter 의 55% 만 활성화하며, 3-bit KV cache 를 지원하여 large-scale LLM 배포 및 추론의 효율성을 더욱 향상시킨다.  </li></ul><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-28-295f03e80ae8e66eefdea1f7144219d9.png" width="1702" height="1007" class="img_ev3q"></p><h1>1 Introduction</h1><p>최근 연구에 따르면, <strong>1-bit LLM</strong> 은 동일한 parameter 및 training token 수를 사용할 경우 full-precision model 과 유사한 성능을 보이면서도 <strong>latency, memory, throughput, energy consumption</strong> 면에서 상당한 비용 절감을 가능하게 한다.</p><p><strong>1.58-bit weight</strong> (i.e., <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{-1, 0, 1\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">}</span></span></span></span></span>) 를 사용하는 경우, inference bottleneck 이 제한된 memory bandwidth 에서 high computational cost 로 이동한다. 따라서, LLM 에서 low-bit 또는 sparse activation 을 활용하는 것은 computational budget 을 더욱 줄이면서도 downstream task 의 성능을 유지하는 유망한 접근 방식이다.  </p><p>Sparsification 외에도, <strong>activation quantization</strong> 은 matrix multiplication 연산을 가속하는 또 다른 방법이다.  그러나, low-bit activation 을 사용하는 neural network 최적화는 몇 가지 어려움이 동반한다.</p><p>특히, 학습이 진행되고 model 크기가 커질수록 outlier dimension 이 나타나는 현상이 발생한다. 이러한 outlier 들은 전체 activation 중 극히 일부에 불과하지만, much larger magnitude 를 가지므로 quantization error 가 증가하며,<br>
<!-- -->결과적으로 downstream task 의 성능 저하를 초래한다.  </p><p>기존 연구에서는 이러한 문제를 해결하기 위해 <strong>Hadamard 또는 learnable rotary transformation</strong> 을 활용하여<br>
<strong>outlier feature 를 other entries 로 분산시키는 방법</strong>을 사용했다. 그러나, 이러한 기법들은 주로 higher precision LLM (e.g., 4-bit) 을 대상으로 설계되었으며, 1-bit LLM 에서는 weights bit-width 가 극도로 낮기 때문에 transformation matrix 를 직접 weight 에 흡수하는 것이 어렵다. 또한, 이러한 변환을 <strong>online transformation</strong> 으로 적용할 경우, 추가적인 연산 비용이 발생하여 전체적인 inference 성능이 저하되는 문제가 있다.  </p><p>본 연구에서는 <strong>BitNet a4.8</strong> 을 도입하여 1-bit LLM 에서 4-bit activation 을 가능하게 하는 <strong>hybrid quantization 및 sparsification 전략</strong> 을 제안한다.</p><p>1-bit LLM 의 activation distribution 을 면밀히 분석한 후, activation distribution pattern 에 따라 4-bit quantization 또는 sparsification 을 선택적으로 적용한다.  </p><p>구체적으로, <strong>BitNet a4.8</strong> 은 다음과 같은 방식을 사용한다:</p><ul><li><strong>Attention 및 FFN layer 의 inputs</strong> 에 <strong>4-bit activation</strong> 을 적용  </li><li><strong>intermediate state</strong> 에 대해서는 <strong>8-bit sparsification</strong> 을 적용  </li></ul><p>또한, training efficiency 를 높이기 위해, BitNet a4.8 은 8-bit activation 에서 시작하여 4-bit activation 으로 전환하는 two-stage recipe 를 을 따른다. 이 접근 방식 덕분에, <strong>BitNet b1.58 을 low-bit activation 에 빠르게 적응시키면서도 few training token 만으로도 높은 성능을 유지</strong> 할 수 있다.  </p><p>광범위한 실험 결과, <strong>BitNet a4.8 은 BitNet b1.58 과 동일한 training cost 로 동등한 성능을 유지하면서도 inference  efficiency 가 크게 향상됨</strong>을 확인했다. 또한, <strong>BitNet a4.8 은 전체 parameter 의 55% 만 활성화하며, 3-bit KV cache 를 지원</strong>하여 large-scale LLM 배포 및 추론의 효율성을 더욱 높인다.</p><h1>2 BitNet a4.8</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-architecture">2.1 Architecture<a href="#21-architecture" class="hash-link" aria-label="Direct link to 2.1 Architecture" title="Direct link to 2.1 Architecture">​</a></h2><p>Fig. 1 에서 볼 수 있듯이, BitNet a4.8 은 BitNet b1.58 과 동일한 구조를 따른다.</p><p>기존 연구를 참고하여, attention 과 feed-forward network (FFN) 에 있는 linear projections 를 <code>BitLinear</code> 로 대체하여 1.58-bit weights 를 처음부터 학습할 수 있도록 설계했다. 또한, activations 에 대해 <strong>hybrid quantization 및 sparsification 전략</strong>을 적용하여 outlier dimension 으로 인해 발생하는 오류를 최소화했다.</p><p>Fig. 2 는 7B model size 의 BitNet b1.58 model 을 사용하여 각 구성 요소의 input distribution 을 시각화한 결과다.  </p><p>Attention 및 FFN layer 의 input distribution  일반적으로 Gaussian-like 한 형태를 보이는 반면, FFN down projection 및 self-attention output projection 의 경우 outlier channel 이 많고, 대부분의 값이 zero 주변에 집중되어 있다. 이러한 현상은 full-precision LLM 에서도 유사하게 관찰된다.  </p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-29-d9144041e5f52130023b79d09ad88837.png" width="1227" height="904" class="img_ev3q"></p><p>Fig. 3 에서 확인할 수 있듯이, 이러한 intermediate states 에 대해 low-bit quantization 을 직접 적용하면 심각한 quantization error 가 발생할 수 있다. 따라서, Q-Sparse 기법을 사용하여 intermediate states 를 8-bit 로 유지하면서 sparsification 을 적용하여 computation bottleneck 을 제거했다.</p><p>Self-attention layers 의 output projection 에 대해서는 아래와 같은 <strong>sparsify-then-quantize function</strong> 을 사용했다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>Q</mi><mtext>INT8</mtext></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>⊙</mo><mi>M</mi><mo stretchy="false">)</mo><mo>⋅</mo><msub><mi>Q</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>W</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mo separator="true">,</mo><mspace width="1em"></mspace><mi>M</mi><mo>=</mo><msub><mtext>Top</mtext><mi>k</mi></msub><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>X</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} Y = (Q_\text{INT8}(X) \odot M) \cdot Q_w(W)^T , \quad M = \text{Top}_k(|X|) \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2513em;vertical-align:-0.3757em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8757em"><span style="top:-2.9843em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em">Y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">INT8</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord text"><span class="mord">Top</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.242em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord">∣</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3757em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8757em"><span style="top:-2.8757em"><span class="pstrut" style="height:2.8913em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3757em"><span></span></span></span></span></span></span></span></span></div><p>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q_w(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mtext>INT8</mtext></msub><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q_\text{INT8}(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">INT8</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> 는 각각 weight <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span></span> 와 activations <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span></span> 의 quantization function 을 의미하며, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span></span> 은 activations <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span></span> 의 abs values 가 maximum top-<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> elements 를 나타내는 mask tensor 이다. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊙</mo></mrow><annotation encoding="application/x-tex">\odot</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord">⊙</span></span></span></span></span> 는 element-wise multiplication 을 의미한다.  </p><p>특히, Weight quantization 과 activation quantization function 은 아래와 같이 정의된다:  </p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>Q</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><mi>α</mi><mtext>RoundClip</mtext><mrow><mo fence="true">(</mo><mfrac><mi>W</mi><mrow><mi>α</mi><mo>+</mo><mi>ϵ</mi></mrow></mfrac><mo separator="true">,</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mspace width="1em"></mspace><mi>α</mi><mo>=</mo><mtext>mean</mtext><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>W</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>Q</mi><mtext>INT8</mtext></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>γ</mi><mn>127</mn></mfrac><mtext>RoundClip</mtext><mrow><mo fence="true">(</mo><mfrac><mn>127</mn><mrow><mi>γ</mi><mo>+</mo><mi>ϵ</mi></mrow></mfrac><mi>X</mi><mo separator="true">,</mo><mo>−</mo><mn>128</mn><mo separator="true">,</mo><mn>127</mn><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mspace width="1em"></mspace><mi>γ</mi><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>X</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext>RoundClip</mtext><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mtext>round</mtext><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align} &amp;Q_w(W) = \alpha \text{RoundClip} \left( \frac{W}{\alpha + \epsilon}, -1, 1 \right), \quad \alpha = \text{mean}(|W|) \\ &amp;Q_\text{INT8}(X) = \frac{\gamma}{127} \text{RoundClip} \left( \frac{127}{\gamma + \epsilon} X, -128, 127 \right), \quad \gamma = \max(|X|) \\ &amp;\text{RoundClip}(X, a, b) = \min(\max(\text{round}(X), a), b) \end{align}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:6.9001em;vertical-align:-3.2em"></span><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.7em"><span style="top:-5.7em"><span class="pstrut" style="height:3.45em"></span><span class="mord"></span></span><span style="top:-3em"><span class="pstrut" style="height:3.45em"></span><span class="mord"></span></span><span style="top:-0.91em"><span class="pstrut" style="height:3.45em"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.2em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.7em"><span style="top:-5.7em"><span class="pstrut" style="height:3.45em"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mord text"><span class="mord">RoundClip</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord text"><span class="mord">mean</span></span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mord">∣</span><span class="mclose">)</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3.45em"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">INT8</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">127</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord text"><span class="mord">RoundClip</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">127</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">−</span><span class="mord">128</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">127</span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord">∣</span><span class="mclose">)</span></span></span><span style="top:-0.91em"><span class="pstrut" style="height:3.45em"></span><span class="mord"><span class="mord"></span><span class="mord text"><span class="mord">RoundClip</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop">min</span><span class="mopen">(</span><span class="mop">max</span><span class="mopen">(</span><span class="mord text"><span class="mord">round</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">b</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.2em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.7em"><span style="top:-5.7em"><span class="pstrut" style="height:3.45em"></span><span class="eqn-num"></span></span><span style="top:-3em"><span class="pstrut" style="height:3.45em"></span><span class="eqn-num"></span></span><span style="top:-0.91em"><span class="pstrut" style="height:3.45em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.2em"><span></span></span></span></span></span></span></span></span></div><p>FFN 에 대해서는 activation sparsity 를 극대화하기 위해 <strong>squared ReLU 및 gated linear unit (GLU) 기법</strong> 을 적용했다:  </p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mtext>ReLU</mtext><mn>2</mn></msup><mtext>GLU</mtext><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>X</mi><msubsup><mi>W</mi><mtext>up</mtext><mi>T</mi></msubsup><mo>⊙</mo><msup><mtext>ReLU</mtext><mn>2</mn></msup><mo stretchy="false">(</mo><mi>X</mi><msubsup><mi>W</mi><mtext>gate</mtext><mi>T</mi></msubsup><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \text{ReLU}^2\text{GLU}(X) = XW_{\text{up}}^T \odot \text{ReLU}^2(XW_{\text{gate}}^T) \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2744em;vertical-align:-0.3872em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8872em"><span style="top:-2.9959em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord text"><span class="mord">ReLU</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8873em"><span style="top:-3.1362em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord text"><span class="mord">GLU</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">up</span></span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord text"><span class="mord">ReLU</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8873em"><span style="top:-3.1362em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">gate</span></span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3872em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8872em"><span style="top:-2.8872em"><span class="pstrut" style="height:2.8913em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3872em"><span></span></span></span></span></span></span></span></span></div><p>사전 실험 결과, squared ReLU 를 적용했을 때 FFN down projection 의 input sparsity 가 80% 이상 증가했으며, 성능에 미치는 영향은 미미했다. 또한, gate projection 의 outputs <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>ReLU</mtext><mn>2</mn></msup><mo stretchy="false">(</mo><mi>X</mi><msubsup><mi>W</mi><mtext>gate</mtext><mi>T</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{ReLU}^2(XW_{\text{gate}}^T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2704em;vertical-align:-0.3831em"></span><span class="mord"><span class="mord text"><span class="mord">ReLU</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8873em"><span style="top:-3.1362em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">gate</span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 도 high activation sparsity 를 보였다 (e.g., 7B models 에서 67.5%).</p><p>이 특성을 활용하면 gate projection 을 먼저 수행한 후, gate 에서 non-zero 로 선택된 채널에 대해서만 up projection 연산을 수행할 수 있다. 이로 인해 <strong>FFN up projection 의 FLOPs 를 더욱 감소</strong>시킬 수 있다.  </p><p>Attention 및 FFN 의 input activation 은 <strong>outlier features 가 훨씬 적으므로</strong> 4-bit integers 로 quantize 하는 absmean function 을 사용했다:  </p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>Y</mi><mo>=</mo><msub><mi>Q</mi><mtext>INT4</mtext></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>⋅</mo><msub><mi>Q</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>W</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>Q</mi><mtext>INT4</mtext></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>β</mi><msqrt><mn>7</mn></msqrt></mfrac><mtext>RoundClip</mtext><mrow><mo fence="true">(</mo><mfrac><msqrt><mn>7</mn></msqrt><mrow><mi>β</mi><mo>+</mo><mi>ϵ</mi></mrow></mfrac><mi>X</mi><mo separator="true">,</mo><mo>−</mo><mn>8</mn><mo separator="true">,</mo><mn>7</mn><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mspace width="1em"></mspace><mi>β</mi><mo>=</mo><mtext>mean</mtext><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>X</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align} &amp;Y = Q_\text{INT4}(X) \cdot Q_w(W)^T\\ &amp;Q_\text{INT4}(X) = \frac{\beta}{\sqrt{7}} \text{RoundClip} \left( \frac{\sqrt{7}}{\beta + \epsilon} X, -8, 7 \right), \quad \beta = \text{mean}(|X|) \end{align}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.8514em;vertical-align:-2.1757em"></span><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6757em"><span style="top:-5.5343em"><span class="pstrut" style="height:3.75em"></span><span class="mord"></span></span><span style="top:-3.1243em"><span class="pstrut" style="height:3.75em"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1757em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6757em"><span style="top:-5.5343em"><span class="pstrut" style="height:3.75em"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">INT4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span></span></span><span style="top:-3.1243em"><span class="pstrut" style="height:3.75em"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">INT4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em"><span style="top:-2.2028em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9072em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord">7</span></span></span><span style="top:-2.8672em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1328em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord text"><span class="mord">RoundClip</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size4">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5842em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9072em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord">7</span></span></span><span style="top:-2.8672em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1328em"><span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">−</span><span class="mord">8</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">7</span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size4">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord text"><span class="mord">mean</span></span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord">∣</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1757em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.6757em"><span style="top:-5.5343em"><span class="pstrut" style="height:3.75em"></span><span class="eqn-num"></span></span><span style="top:-3.1243em"><span class="pstrut" style="height:3.75em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.1757em"><span></span></span></span></span></span></span></span></span></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-training">2.2 Training<a href="#22-training" class="hash-link" aria-label="Direct link to 2.2 Training" title="Direct link to 2.2 Training">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="continue-training-from-bitnet-b158">Continue-training from BitNet b1.58<a href="#continue-training-from-bitnet-b158" class="hash-link" aria-label="Direct link to Continue-training from BitNet b1.58" title="Direct link to Continue-training from BitNet b1.58">​</a></h4><p>BitNet a4.8 은 W1.58A8 에서 W1.58A4 로 변환하는 two-stage recipe 로 학습된다.</p><ol><li>1 stage : <strong>8-bit activation 및 ReLU<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>GLU</strong> 를 사용하여 모델을 학습한다.  </li><li>2 stage : <strong>Sec. 2.1 에서 설명한 hybrid quantization 및 sparsification 기법</strong> 을 적용한다.</li></ol><p>이러한 학습 과정 덕분에, BitNet a4.8 은 few training token 만으로도 4-bit 및 sparse activation 에 빠르게 적응하며,<br>
<!-- -->성능 저하는 거의 발생하지 않는다.  </p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="gradient-approximation">Gradient approximation<a href="#gradient-approximation" class="hash-link" aria-label="Direct link to Gradient approximation" title="Direct link to Gradient approximation">​</a></h4><p>기존 연구를 참고하여, BitNet a4.8 의 gradient approximation 을 수행하기 위해 <strong>straight-through estimator (STE)</strong> 를 사용한다. 또한, <strong>mixed precision training</strong> 기법을 적용하여 parameter 를 업데이트한다.  </p><p>Backward propagation 시, <strong>quantization function 및 top-K sparsification function 과 같은 non-differentiable functions</strong> 들을 직접 우회한다. 즉, 이러한 함수들의 gradient 는 그대로 전달되지 않고, STE 를 사용하여 근사적으로 전파된다.</p><p>Mixed precision training 을 위해, <strong>full-precision latent weight</strong> 를 유지하며 parameter update 를 누적한다. Forward pass 동안, <strong>latent weight 를 1.58-bit 로 on-the-fly quantization</strong> 하여 연산을 수행한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-floating-point-quantization">2.3 Floating-point quantization<a href="#23-floating-point-quantization" class="hash-link" aria-label="Direct link to 2.3 Floating-point quantization" title="Direct link to 2.3 Floating-point quantization">​</a></h2><p>Floating-point quantization 은 <strong>integer-based quantization 보다 broader dynamic range</strong> 를 제공하며, 이는 <strong>long-tailed activation distribution 을 처리하는 데 필수적</strong> 이다.</p><p>BitNet a4.8 은 <strong>FFN down projection 의 inputs 를 8-bit integers 로 유지</strong> 하고, 나머지 activation 은 <strong>FP4</strong> 로 quantize 한다.</p><p>이를 위해 <strong>MinMax quantizer</strong> 를 사용하며, 다음과 같이 정의된다:  </p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>Q</mi><mtext>FP4</mtext></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>γ</mi><msup><mn>2</mn><mrow><mi>M</mi><mo>+</mo><mi>b</mi></mrow></msup></mfrac><mtext>Round</mtext><mrow><mo fence="true">(</mo><mfrac><msup><mn>2</mn><mrow><mi>M</mi><mo>+</mo><mi>b</mi></mrow></msup><mi>γ</mi></mfrac><mi>X</mi><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mspace width="1em"></mspace><mi>γ</mi><mo>=</mo><msup><mn>2</mn><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo stretchy="false">⌊</mo><mo stretchy="false">⌊</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mi mathvariant="normal">∣</mi><mi>X</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">⌋</mo><mo>+</mo><mi>b</mi><mo stretchy="false">⌋</mo><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>b</mi><mo>=</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mrow><mo fence="true">(</mo><mfrac><mrow><mn>2</mn><mo>−</mo><msup><mn>2</mn><mrow><mo>−</mo><mi>M</mi></mrow></msup></mrow><mrow><mi mathvariant="normal">∣</mi><mi>X</mi><msub><mi mathvariant="normal">∣</mi><mi>max</mi><mo>⁡</mo></msub></mrow></mfrac><mo fence="true">)</mo></mrow><mo>+</mo><msup><mn>2</mn><mi>E</mi></msup><mo>−</mo><mn>1</mn></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align} &amp;Q_\text{FP4}(X) = \frac{\gamma}{2^{M+b}} \text{Round} \left( \frac{2^{M+b}}{\gamma} X \right), \quad \gamma = 2^{\max(\lfloor \lfloor \log_2 |X| \rfloor + b \rfloor,1)}\\ &amp;b = \log_2 \left( \frac{2 - 2^{-M}}{|X|_{\max}} \right) + 2^E - 1 \end{align}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:5.5445em;vertical-align:-2.5222em"></span><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.0222em"><span style="top:-5.0222em"><span class="pstrut" style="height:3.5261em"></span><span class="mord"></span></span><span style="top:-2.2539em"><span class="pstrut" style="height:3.5261em"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.5222em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.0222em"><span style="top:-5.0222em"><span class="pstrut" style="height:3.5261em"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">FP4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7751em"><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord text"><span class="mord">Round</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">a</span><span class="mtight">x</span></span><span class="mopen mtight">(⌊⌊</span><span class="mop mtight"><span class="mop mtight"><span class="mtight">l</span><span class="mtight">o</span><span class="mtight" style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span style="top:-2.2341em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2659em"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em"></span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.07847em">X</span><span class="mord mtight">∣</span><span class="mclose mtight">⌋</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span><span class="mclose mtight">⌋</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span><span style="top:-2.2539em"><span class="pstrut" style="height:3.5261em"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">m</span><span class="mtight">a</span><span class="mtight">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">E</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.5222em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.0222em"><span style="top:-5.0222em"><span class="pstrut" style="height:3.5261em"></span><span class="eqn-num"></span></span><span style="top:-2.2539em"><span class="pstrut" style="height:3.5261em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.5222em"><span></span></span></span></span></span></span></span></span></div><p>여기서, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span></span> 은 각각 exponent 와 mantissa 의 bit-width 를 나타낸다.</p><p>저자는 <strong>E2M1 format</strong> 을 사용하며, 이는 larger dynamic range 를 제공한다.  </p><p>Tab. 1 에서 확인할 수 있듯이, <strong>BitNet a4.8 에 FP4 quantization 을 적용한 경우, hybrid quantization 및 sparsification 기반 integer quantization 과 유사한 성능을 보인다</strong>.  </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-experiments">3 Experiments<a href="#3-experiments" class="hash-link" aria-label="Direct link to 3 Experiments" title="Direct link to 3 Experiments">​</a></h2><p>BitNet a4.8 을 BitNet b1.58 및 저자가 재현한 FP16 LLaMA LLM 과 비교했다.</p><ul><li>1.58-bit model 의 경우, BitNet b1.58 의 training recipe 를 따르며 two-stage weight decay 및 learning rate scheduling 을 적용했다. 자세한 사항은 Appendix A 에서 확인할 수 있다.  </li><li>all models 는 RedPajama dataset 에서 100B training tokens 을 사용하여 학습되었다.</li><li>BitNet a4.8 의 경우, <strong>8-bit activations</strong> 로 <strong>95B tokens</strong> 학습한 후, optimizer state 를 재사용하여 <strong>hybrid quantization 및 sparsification</strong> 을 적용하는 <strong>5B tokens 추가 학습</strong> 을 수행했다.</li><li>또한, attention output projection 에 대한 <strong>Top-K sparsification 비율을 50%</strong> 로 설정했다.  </li></ul><p>모델 성능 평가를 위해 <strong>lm-evaluation-harness toolkit</strong> 을 사용하여 다양한 <strong>zero-shot language task</strong> 에 대한 정확도를 측정했다.</p><p>사용한 task 는 다음과 같다:  </p><ul><li>ARC-Easy (ARCe)  </li><li>ARC-Challenge (ARCc)  </li><li>HellaSwag (HS)  </li><li>Winogrande (WGe)  </li><li>PIQA (PQ)  </li><li>C4 validation set 에 대한 perplexity 도 측정했다.  </li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-main-results">3.1 Main Results<a href="#31-main-results" class="hash-link" aria-label="Direct link to 3.1 Main Results" title="Direct link to 3.1 Main Results">​</a></h2><p>Tab. 1 에 BitNet a4.8, BitNet b1.58, FP16 LLaMA LLM 의 평가 결과를 정리했다.</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-30-a90ca1b8fade013ccfb86e98c8755b7b.png" width="1325" height="815" class="img_ev3q"></p><ul><li>모델 크기가 증가할수록, <strong>full-precision (FP16) LLaMA LLM 과 BitNet b1.58 간의 성능 차이</strong> 가 점점 좁혀지는 경향을 보인다.</li><li>특히, <strong>7B model 의 경우 BitNet b1.58 은 LLaMA LLM 과 perplexity 및 downstream task average accuracy 측면에서 거의 동일한 성능</strong> 을 달성한다.  </li><li>또한, <strong>BitNet a4.8 은 BitNet b1.58 과 동등한 성능을 유지하면서도 inference efficiency 가 크게 향상</strong> 된다.  </li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="sparsity">Sparsity<a href="#sparsity" class="hash-link" aria-label="Direct link to Sparsity" title="Direct link to Sparsity">​</a></h4><p>Tab. 2 는 BitNet a4.8, BitNet b1.58, FP16 LLaMA LLM 의 각 component 별 sparsity 를 보여준다. 여기서 sparsity 는 C4 validation set 에서 non-embedding parameter 를 기준으로 계산되었다.  </p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-31-966abe886c4d640fd72987bbc0e028f4.png" width="1415" height="716" class="img_ev3q"></p><ul><li>BitNet a4.8 은 <strong>BitNet b1.58 및 LLaMA LLM 보다 훨씬 higher sparsity</strong> 를 달성했다.<ul><li>예를 들어, 7B model 의 경우 BitNet a4.8 은 overall sparsity 가 44.5% 이며, 오직 3.4B active parameters 에 불과하다.</li><li>특히, FFN down projection 의 inputs 는 매우 높은 sparsity 를 보였으며, 이는 intermediate state disbributions 이 0 을 중심으로 급격하게 쏠리는 현상과 일치한다.</li></ul></li><li>또한, <strong>gate projection output sparsity 가 높아 up projection 의 sparsity 도 함께 증가</strong> 하는 경향을 보였다. <ul><li>예를 들어, 7B BitNet a4.8 의 경우:  <ul><li>Gate projection 출력 sparsity: 67.5%</li><li>Up projection 입력 sparsity: 12.0%</li><li>따라서 up projection 전체 sparsity 는: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>12.0</mn><mi mathvariant="normal">%</mi><mo stretchy="false">)</mo><mo>×</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mn>67.5</mn><mi mathvariant="normal">%</mi><mo stretchy="false">)</mo><mo>=</mo><mn>71.4</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">1 - (1 - 12.0\%) \times (1 - 67.5\%) = 71.4\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">12.0%</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">67.5%</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">71.4%</span></span></span></span></span></li></ul></li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="low-bit-attention">Low-bit Attention<a href="#low-bit-attention" class="hash-link" aria-label="Direct link to Low-bit Attention" title="Direct link to Low-bit Attention">​</a></h4><p>Tab. 3 에는 3B 및 7B model size 의 BitNet a4.8 에 대해 low-bit attention 을 적용한 결과를 정리했다. </p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-32-991e5b95b945ab5e99736ec683513591.png" width="1478" height="559" class="img_ev3q"></p><p>Low-bit attention 은 long sequence 를 효율적으로 modeling 하는 데 필수적이다. 이는 KV cache 의 memory usage 를 줄이고 I/O 비용을 낮추며, attention 연산을 가속시킨다. 실험에서는 <strong>post-RoPE quantization</strong> 을 적용했다.</p><p>QKV head 는 별도의 calibration dataset 없이 absmax function 을 사용하여 unsigned integer 로 quantize 했다. 또한, 3-bit KV quantization 시에는 bos token head 를 4-bit 로 유지하여 outlier feature 를 보존했다.  </p><ul><li>Tab. 3 에서 확인할 수 있듯이, BitNet a4.8 은 3B 및 7B model 에서 4-bit KV 또는 QKV head 를 적용해도 정확도 감소가 거의 없음을 보인다.  </li><li>또한, KV cache 를 3-bit 정수로 quantize 해도 평균 정확도 저하가 거의 발생하지 않는다.  </li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-ablation-study">3.2 Ablation Study<a href="#32-ablation-study" class="hash-link" aria-label="Direct link to 3.2 Ablation Study" title="Direct link to 3.2 Ablation Study">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="hybrid-architecture">Hybrid architecture<a href="#hybrid-architecture" class="hash-link" aria-label="Direct link to Hybrid architecture" title="Direct link to Hybrid architecture">​</a></h4><p>Fig. 4 는 700M BitNet a4.8 에 대해 full INT4/FP4 quantization 과 hybrid quantization, sparsification 을 적용했을 때의 training loss curve 를 보여준다. </p><p>이 모델들은 RedPajama dataset 의 25B tokens 에 대해 first-stage scheduling 으로 학습했다. full INT4 와 FP4 quantization 을 위해 각각 absmean quantizer 와 MinMax quantizer 를 사용했다. 게다가 full INT4 quantization 의 경우, FFN 의 down projection 에는 inputs 의 larger outliers 로 인해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mn>2</mn><mtext>mean</mtext><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>X</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\beta = 2\text{mean}(|X|)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2</span><span class="mord text"><span class="mord">mean</span></span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord">∣</span><span class="mclose">)</span></span></span></span></span> 를 적용한 absmean quantizer 를 사용했다.</p><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-33-0615af620f7495df5e69ab66a9e0d7cc.png" width="1268" height="597" class="img_ev3q"></p><ul><li>Fig. 4 에서 보이듯, full INT4 quantization 은 divergence 로 이어진다. </li><li>또한 hybrid architecture 는 training perplexity 측면에서 full FP4 architecture 보다 훨씬 우수한 성능을 보인다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="down-projection-of-ffn">Down projection of FFN<a href="#down-projection-of-ffn" class="hash-link" aria-label="Direct link to Down projection of FFN" title="Direct link to Down projection of FFN">​</a></h4><p>1.3B BitNet a4.8 을 대상으로 FFN 의 down projection 에서 사용하는 quantization 이나 activation function 을 달리했을 때를 비교했다. 모든 모델은 RedPajama dataset 의 50B tokens 에 대해 first-stage scheduling 으로 학습했다. </p><p>공정한 비교를 위해 다른 activations 은 8-bits 로 유지했다. INT8 quantization 에는 absmax quantizer 를, FP4 quantization 에는 MinMax quantizer 를 사용했다. absmean quantizer 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> 값은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mtext>mean</mtext><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi>X</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">2\text{mean}(|X|)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2</span><span class="mord text"><span class="mord">mean</span></span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord">∣</span><span class="mclose">)</span></span></span></span></span> 로 설정했다. </p><p>Fig. 5 는 이러한 모델들의 training loss curve 를 보여준다. </p><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-34-4ceade63198377eef8d55a3eef45f7a9.png" width="1468" height="643" class="img_ev3q"></p><ul><li>Squared ReLU 는 Swish 와 비교했을 때, higher sparsity 를 가능하게 하면서도 약간 더 나은 training perplexity 를 달성한다. </li><li>또한 down projection 의 입력에 FP4 quantization 을 적용하면 성능이 크게 저하되며, INT4 activations 에 STE 를 적용하면 divergence 가 발생한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="output-projection-of-attention">Output projection of attention<a href="#output-projection-of-attention" class="hash-link" aria-label="Direct link to Output projection of attention" title="Direct link to Output projection of attention">​</a></h4><p>Tab. 4 는 attention 의 output projection 에 들어가는 입력에 대해 Top-K sparsification 을 적용했을 때와 적용하지 않았을 때의 3B BitNet a4.8 결과를 자세히 보여준다. </p><p>두 모델 모두 8-bit 에서 4-bit activations 로 전환하는 동일한 two stage recipe 로 학습했다. sparsification 의 K 는 50% 로 설정했다. baseline 은 output projection 의 입력에 대해 INT8 absmax quantizer 를 사용했다. </p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-36-7e17f95e13b1a533c7cd5d51c821ff8b.png" width="1456" height="272" class="img_ev3q"></p><p>결과에 따르면, TopK sparsification 은 perplexity 와 accuracy 의 손실을 무시할 수 있을 정도로 미미하게 만든다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="4-bit-quantization">4-bit quantization<a href="#4-bit-quantization" class="hash-link" aria-label="Direct link to 4-bit quantization" title="Direct link to 4-bit quantization">​</a></h4><p>3B BitNet a4.8 의 attention 과 FFN 에 들어가는 입력에 대해 서로 다른 4-bit quantizer 를 적용했을 때의 loss curves 를 Fig. 6 에 제시했다. </p><p>BitNet a4.8 을 MinMax quantizer 로 E2M1 과 E1M2 format 의 floating-point quantization 과 absmax, absmean quantizer 를 사용한 integer quantization 으로 비교했다. </p><p><img loading="lazy" alt="Figure 6" src="/assets/images/image-35-ba1f3bb36d523e7c265feeb76971e8e3.png" width="548" height="561" class="img_ev3q"></p><p>Fig. 6 에서 보이듯, FP4 quantization 의 E2M1 format 과 absmean quantizer 를 사용한 INT4 가 작은 크기의 activation entries 를 다루는 데 유리하여, 약간 더 나은 training perplexity 를 달성한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="33-more-training-tokens">3.3 More Training Tokens<a href="#33-more-training-tokens" class="hash-link" aria-label="Direct link to 3.3 More Training Tokens" title="Direct link to 3.3 More Training Tokens">​</a></h3><p>이전 연구에서는 training token 수가 증가할수록 activation outlier 의 빈도도 증가한다는 사실을 보고했다.</p><p>BitNet a4.8 의 <strong>scalability</strong> 를 검증하기 위해, 2B parameter 모델을 2T tokens 로 학습하는 추가 실험을 수행했다. 동일한 training data 및 설정을 유지하면서 BitNet b1.58 과 비교했다.  </p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-37-410e0dfd1fdcbbdf78088d24d4df8473.png" width="1450" height="252" class="img_ev3q"></p><p>Tab. 5 에서 확인할 수 있듯이, BitNet a4.8 은 4-bit activation compression 을 적용한 상태에서도 BitNet b1.58 과 거의 동일한 정확도를 유지한다. 이는 large-scale training token 환경에서도 hybrid quantization 및 sparsification 기법이 효과적임을 보여준다.  </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-conclusion">4 Conclusion<a href="#4-conclusion" class="hash-link" aria-label="Direct link to 4 Conclusion" title="Direct link to 4 Conclusion">​</a></h2><p>본 논문에서는 <strong>BitNet a4.8</strong> 을 제안하여 <strong>1-bit LLM 을 위한 4-bit activation</strong> 을 가능하게 했다.</p><p>BitNet a4.8 은 <strong>hybrid quantization 및 sparsification</strong> 기법을 활용하여 activation 의 <strong>outlier channel 로 인해 발생하는 quantization error 를 줄이는 새로운 아키텍처</strong>를 사용했다.  </p><p>특히, 다음 방식을 도입했다.  </p><ul><li><strong>Attention 및 FFN layers 의 input</strong> 에는 <strong>4-bit quantization</strong> 적용  </li><li><strong>intermediate state</strong> 에는 <strong>8-bit sparsification</strong> 적용  </li></ul><p>BitNet a4.8 은 W1.58A8 모델을 W1.58A4 모델로 continue-training 하는 방식으로 학습되었다.</p><p>실험 결과, <strong>BitNet a4.8 은 BitNet b1.58 과 동일한 training 비용으로 동등한 성능을 달성</strong> 하면서도, <strong>inference efficiency 가 크게 향상됨</strong>을 확인했다.  </p><h1>A Hyper-parameters</h1><p><img loading="lazy" alt="Table 6, 7" src="/assets/images/image-38-36675e4ae74ec31958943b3b28e8e949.png" width="1386" height="834" class="img_ev3q"></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/quantization">Quantization</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/1-58-bit">1.58-bit</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/bit-net">BitNet</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/bit-net-b-1-58">BitNet b1.58</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/1-bit-activation">1-bit activation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/bit-net-a-4-8">BitNet a4.8</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-11-BitNet-a4.8.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/LLM-FP4"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">LLM-FP4: 4-Bit Floating-Point Quantized Transformers</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/fp4"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Optimizing Large Language Model Training Using FP4 Quantization</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-architecture" class="table-of-contents__link toc-highlight">2.1 Architecture</a></li><li><a href="#22-training" class="table-of-contents__link toc-highlight">2.2 Training</a></li><li><a href="#23-floating-point-quantization" class="table-of-contents__link toc-highlight">2.3 Floating-point quantization</a></li><li><a href="#3-experiments" class="table-of-contents__link toc-highlight">3 Experiments</a></li><li><a href="#31-main-results" class="table-of-contents__link toc-highlight">3.1 Main Results</a></li><li><a href="#32-ablation-study" class="table-of-contents__link toc-highlight">3.2 Ablation Study</a><ul><li><a href="#33-more-training-tokens" class="table-of-contents__link toc-highlight">3.3 More Training Tokens</a></li></ul></li><li><a href="#4-conclusion" class="table-of-contents__link toc-highlight">4 Conclusion</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.52ac8645.js"></script>
<script src="/assets/js/main.12d93f77.js"></script>
</body>
</html>