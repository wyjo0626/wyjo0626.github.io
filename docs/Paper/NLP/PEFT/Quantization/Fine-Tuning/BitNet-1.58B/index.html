<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-02-BitNet-1.58b">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.74164ba1.js" as="script">
<link rel="preload" href="/assets/js/main.82abd802.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Attacking/universal-adversarial-prompt">Attacking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Generalization/NoisyTune">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Flan-T5">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Generalization/Flat-LoRA">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Mixture/UniPELT">Mixture</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Module</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Pruning/SMP">Pruning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Quantization</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Fine-Tuning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/QuIP">QuIP: 2-Bit Quantization of Large Language Models With Guarantees</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet">BitNet: Scaling 1-bit Transformers for Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/GIFT-SW">GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/LLM-FP4">LLM-FP4: 4-Bit Floating-Point Quantized Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8">BitNet a4.8: 4-bit Activations for 1-bit LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/fp4">Optimizing Large Language Model Training Using FP4 Quantization</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/LoRA/QLoRA">LoRA</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning">Soft Prompt</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Engineering/CoT/Chain-of-Thought">Prompt Engineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Quantization</span><meta itemprop="position" content="4"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Fine-Tuning</span><meta itemprop="position" content="5"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</span><meta itemprop="position" content="6"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2402.17764" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2402.17764</a></p><h1>Abstract</h1><p>BitNet 같은 최근 연구는 1-bit Large Language Models (LLMs) 의 새로운 시대를 열어가고 있다.</p><p>본 논문에서 저자는 LLM 의 모든 single parameter (or weight) 가 ternary <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{-1, 0, 1\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">}</span></span></span></span></span> 인 <strong>BitNet b1.58</strong> 이라는 1-bit LLM variant 를 소개한다. 이는 perplexity 및 end-task performance 두 측면에서 동일한 모델 크기와 training tokens 에서 full-precision (i.e., FP16 or BF16) Transformer LLM 과 동일한 성능을 내면서도 latency, memory, throughput 및 energy consumption 측면에서 상당히 cost-effective 하다.</p><p>더 깊게 말해, 1.58-bit LLM 은 high-performance 및 cost-effective 인 차세대 LLMs 를 훈련하기 위해 scaling law 및 recipe 를 정의한다. 게다가, 이는 1-bit LLMs 에 최적화된 특정 하드웨어 설계에 대한 새로운 계산 패러다임과 문을 열어준다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-21-adbce13ff63e89ac281fe8111c41e5d0.png" width="1106" height="880" class="img_ev3q"></p><h1>1 The Era of 1-bit LLMs</h1><p>최근 몇 년 동안 AI 에서는 Large Language Models (LLMs) 의 크기와 성능이 급격히 성장했다. 이러한 모델들은 다양한 natural language processing tasks 에서 뛰어난 성능을 보여주었지만, 모델 크기의 증가로 인해 배포에 어려움이 따르고, high energy consumption 로 인한 환경적, 경제적 영향에 대한 우려가 제기되었다. 이러한 문제를 해결하기 위한 한 가지 접근 방식은 <strong>post-training quantization</strong> 을 사용하여 <strong>low-bit models</strong> 를 생성하는 것이다. 이는 weights 와 activations 의 precision 을 줄여 LLMs 의 memory 및 computational requirements 를 크게 줄이는 기법이다. 최근에는 16-bit 에서 4-bit 로 이동하는 경향이 있다. 하지만, post-training quantization 은 industy LLMs 에 널리 사용되고 있음에도 불구하고 최적의 방법은 아니다.</p><p>1-bit model architectures 와 관련된 최근 연구, 예를 들어 BitNet 은 LLMs 의 비용을 줄이면서도 성능을 유지할 수 있는 유망한 방향을 제시한다. Vanilla LLMs 는 16-bit floating-point values (i.e., FP16 or BF16) 을 사용하며, 대부분의 연산은 <strong>matrix multiplication</strong> 에 의해 이루어진다. 따라서, 주요 computation cost 는 floating-point addition 과 multiplication 에서 발생한다. 반면, BitNet 의 matrix multiplication 은 integer addition 만 포함하므로, LLMs 의 energy cost 를 대폭 절감할 수 있다. 많은 칩에서 연산 성능의 근본적인 한계는 전력 소비이므로, energy sasvings 는 연산 속도의 증가로도 이어질 수 있다.</p><p>연산뿐만 아니라, DRAM 에서 on-chip accelerator (e.g., SRAM) 로 model parameters 를 전송하는 과정도 inference 중에 큰 비용이 든다. SRAM 을 확대하여 throughput 을 높이려는 시도가 있었지만, 이는 DRAM 보다 훨씬 높은 비용이 든다. 반면, 1-bit LLMs 는 full-precision models 보다 훨씬 낮은 memory footprint 와 bandwidth standpoint 를 가지므로, DRAM 에서 weight 를 로드하는 데 필요한 비용과 시간을 크게 줄일 수 있어 더 빠르고 효율적인 inference 가 가능해진다.</p><p>이 연구에서는 <strong>BitNet b1.58</strong> 이라는 중요한 1-bit LLM 변형을 소개한다. 이 모델의 모든 parameter 는 ternary values $<!-- -->{<!-- -->-1, 0, 1}<!-- -->$<!-- --> 을 갖는다. 기존 1-bit BitNet 에 0 값을 추가하여, binary system 에서 1.58-bit 를 제공한다. BitNet b1.58 은 기존 1-bit BitNet 의 모든 장점을 유지하면서도, 거의 multiplication 이 필요 없는 matrix multiplication 방식을 사용하여 최적화할 수 있다. 또한, energy consumption 은 기존 1-bit BitNet 과 동일하며, FP16 LLM 대비 memory consumption, throughput 및 latency 면에서 훨씬 효율적이다. </p><p>게다가, BitNet b1.58 은 두 가지 추가적인 장점을 제공한다. 1) model weights 에 0 을 포함함으로써 feature filtering 을 명시적으로 지원할 수 있어, 1-bit LLMs 의 성능을 크게 향상시킨다. 2) 실험 결과 BitNet b1.58 은 3B 이상의 모델 크기에서 full-precision (FP16) LLM 과 동일한 perplexity 및 end-task 성능을 달성할 수 있음을 보여주었다.</p><h1>2 BitNet b1.58</h1><p>BitNet b1.58 은 BitNet architecture 를 기반으로 하며, Transformer 구조에서 <code>nn.Linear</code> 를 <code>BitLinear</code> 로 대체한다. 이 모델은 처음부터 1.58-bit weights 와 8-bit activations 로 훈련되었다. 기존 BitNet 과 비교하여, 다음과 같은 몇 가지 수정 사항이 추가되었다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="quantization-function">Quantization Function<a href="#quantization-function" class="hash-link" aria-label="Direct link to Quantization Function" title="Direct link to Quantization Function">​</a></h4><p>weights 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">−</span><span class="mord">1</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span></span>, or <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">+</span><span class="mord">1</span></span></span></span></span> 로 제한하기 위해, <strong>absmean quantization function</strong> 을 적용하였다. 이 방법은 weight matrix 를 average absolute value 로 scaling 후, 각 값을 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>0</mn><mo separator="true">,</mo><mo>+</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{-1, 0, +1\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">+</span><span class="mord">1</span><span class="mclose">}</span></span></span></span></span> 중 nearest integer 로 반올림한다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mover accent="true"><mi>W</mi><mo>~</mo></mover><mo>=</mo><mtext>RoundClip</mtext><mrow><mo fence="true">(</mo><mfrac><mi>W</mi><mrow><mi>γ</mi><mo>+</mo><mi>ϵ</mi></mrow></mfrac><mo separator="true">,</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo fence="true">)</mo></mrow></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \tilde{W} = \text{RoundClip}\left( \frac{W}{\gamma + \epsilon}, -1, 1 \right) \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em"><span style="top:-3.45em"><span class="pstrut" style="height:3.45em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span><span style="top:-3.6023em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">~</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord text"><span class="mord">RoundClip</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3603em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em"><span style="top:-3.45em"><span class="pstrut" style="height:3.45em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.95em"><span></span></span></span></span></span></span></span></span></div><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>RoundClip</mtext><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>b</mi><mo separator="true">,</mo><mtext>round</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \text{RoundClip}(x, a, b) = \max(a, \min(b, \text{round}(x))) \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord text"><span class="mord">RoundClip</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord mathnormal">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord text"><span class="mord">round</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)))</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-2.85em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span></span></div><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>γ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><mi>m</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi>i</mi><mi>j</mi></mrow></munder><mi mathvariant="normal">∣</mi><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi mathvariant="normal">∣</mi><mi mathvariant="normal">.</mi></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \gamma = \frac{1}{nm} \sum_{ij} |W_{ij}|. \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.7352em;vertical-align:-1.1176em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6176em"><span style="top:-3.6176em"><span class="pstrut" style="height:3.3214em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">nm</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">ij</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord">∣.</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1176em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6176em"><span style="top:-3.6176em"><span class="pstrut" style="height:3.3214em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1176em"><span></span></span></span></span></span></span></span></span></div><p>activations 에 대한 quantization function 은 BitNet 의 기존 구현을 따른다. 다만, non-linear function 전에 activations 을 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi>Q</mi><mi>b</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0, Qb]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">Q</span><span class="mord mathnormal">b</span><span class="mclose">]</span></span></span></span></span> 범위로 scaling 하지 않고, 대신 token 단위로 activations 을 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mi>Q</mi><mi>b</mi><mo separator="true">,</mo><mi>Q</mi><mi>b</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-Qb, Qb]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord mathnormal">Q</span><span class="mord mathnormal">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">Q</span><span class="mord mathnormal">b</span><span class="mclose">]</span></span></span></span></span> 범위로 scaling 하여 zero-point quantization 을 제거하였다. 이는 구현과 시스템 수준 최적화를 더 간편하게 만들면서도 성능에는 거의 영향을 미치지 않는다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="llama-alike-components">LLaMA-alike Components<a href="#llama-alike-components" class="hash-link" aria-label="Direct link to LLaMA-alike Components" title="Direct link to LLaMA-alike Components">​</a></h4><p>LLaMA architecture 는 현재 open-source LLMs 의 대표적인 backbone 이다. 따라서, open-source community 를 고려하여 BitNet b1.58 은 LLaMA 와 유사한 구성 요소를 사용한다. 구체적으로, RMSNorm, SwiGLU, rotary embedding 을 적용하며, 모든 bias 항을 제거하였다. 이를 통해 BitNet b1.58 은 Huggingface, vLLM, llama.cpp 등의 popular open-source 소프트웨어에 쉽게 통합될 수 있다.</p><h1>3 Results</h1><p>BitNet b1.58 을 FP16 LLaMA LLM 과 다양한 크기로 비교하였다. 공정한 비교를 위해, 두 모델 모두 RedPajama dataset 에 대해 100 billion tokens 로 pre-training 하였다. 평가에는 다양한 language tasks 를 포함하였으며, ARC-Easy, ARC-Challenge, Hellaswag, Winogrande, PIQA, OpenbookQA, BoolQ 등의 zero-shot 성능을 측정하였다. 또한, WikiText2 와 C4 dataset 에 대한 validation perplexity 도 보고하였다.</p><p>LLaMA LLM 과 BitNet b1.58 의 runtime GPU memory 및 latency 도 비교하였다. 결과는 LLM inference latency 에 최적화된 FasterTransformer 코드베이스를 사용하여 측정되었다. 또한, BitNet b1.58 의 2-bit kernel 에 Ladder 를 통합하여 평가하였다. 주요 비용이 발생하는 output token 당 처리 시간을 보고하였다.</p><p>Tab. 1 은 BitNet b1.58 과 LLaMA LLM 의 perplexity 및 비용을 요약한 것이다. </p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-22-b95def5402b9ba2234e50fe7036367a9.png" width="1145" height="465" class="img_ev3q"></p><p>BitNet b1.58 은 3B 모델 크기에서 full-precision LLaMA LLM 과 perplexity 면에서 동등한 성능을 나타내며, 2.71 배 빠르고 3.55 배 적은 GPU memory 를 사용한다. 특히, 3.9B 모델 크기의 BitNet b1.58 은 3B LLaMA LLM 보다 2.4 배 빠르고 3.32 배 적은 memory 를 사용하면서도 더 나은 성능을 보여준다.</p><p>Tab. 2 는 end tasks 에 대한 zero-shot accuracy 의 상세한 결과를 보여준다.</p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-23-8597066587b26e1c1300fa374ef31cb5.png" width="1250" height="455" class="img_ev3q"></p><p><em>lm-evaluation-harness</em> pipeline 을 따라 평가를 수행하였다. 결과에 따르면, 모델 크기가 증가할수록 BitNet b1.58 과 LLaMA LLM 간의 성능 차이가 줄어든다. 더욱 중요한 점은, BitNet b1.58 이 3B 크기부터 full-precision baseline 과 동등한 성능을 달성할 수 있다는 것이다.  </p><p>Perplexity 관찰 결과와 마찬가지로, end-task 결과에서도 BitNet b1.58 3.9B 가 LLaMA LLM 3B 를 뛰어넘으며, lower memory 및 latency cost 를 가진다는 것이 확인되었다. 이는 BitNet b1.58 이 최신 LLM models 대비 Pareto improvement 를 제공한다는 것을 보여준다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="memory-and-latency">Memory and Latency<a href="#memory-and-latency" class="hash-link" aria-label="Direct link to Memory and Latency" title="Direct link to Memory and Latency">​</a></h4><p>모델 크기를 7B, 13B, 70B 로 확장하여 비용을 평가하였다. </p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-24-c3ba2c9a8095940b0d19b03b2161a303.png" width="1361" height="602" class="img_ev3q"></p><p>Fig. 2 는 latency 와 memory 소비 경향을 나타내며, 모델 크기가 증가할수록 속도 향상이 더욱 커진다는 점을 보여준다. 특히, 70B BitNet b1.58 은 LLaMA LLM baseline 대비 4.1 배 빠르다. 이는 <code>nn.Linear</code> 연산 시간이 모델 크기에 따라 증가하기 때문이다. memory 소비도 유사한 경향을 보이며, embedding 이 full-precision 으로 유지되면서 큰 모델에서는 memory 비율이 작아진다. 모든 latency 및 memory 는 2-bit kernel 로 측정되었으며, 추가적인 최적화를 통해 비용을 더욱 줄일 여지가 있다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="energy">Energy<a href="#energy" class="hash-link" aria-label="Direct link to Energy" title="Direct link to Energy">​</a></h4><p>BitNet b1.58 과 LLaMA LLM 의 연산 energy consumption 를 추정하였다. matrix multiplication 은 LLMs 의 가장 큰 비용 요소이므로, 주요 분석 대상으로 삼았다. Fig. 3 은 computation cost  구성 요소를 나타낸다.</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-25-d48c2dd771f0dc646d1b70efb016a1a7.png" width="1366" height="639" class="img_ev3q"></p><p>BitNet b1.58 은 대부분 INT8 addition 으로 구성된 반면, LLaMA LLM 은 FP16 addition 및 FP16 multiplication 을 포함한다. 기존 연구의 energy model 을 참고하면, BitNet b1.58 은 7nm 칩에서 matrix multiplication 연산 energy consumption 를 71.4 배 절감할 수 있다.</p><p>512 tokens 에 대한 end-to-end energy consumption 도 측정하였다. 결과에 따르면, 모델 크기가 증가할수록 BitNet b1.58 은 FP16 LLaMA LLM 대비 더욱 효율적으로 에너지를 소비한다. 이는 모델 크기가 증가함에 따라 <code>nn.Linear</code> 연산 비율이 커지기 때문이며, 다른 구성 요소에서 발생하는 비용은 상대적으로 감소하기 때문이다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="throughput">Throughput<a href="#throughput" class="hash-link" aria-label="Direct link to Throughput" title="Direct link to Throughput">​</a></h4><p>70B parameters 를 가진 BitNet b1.58 과 LLaMA LLM 의 throughput 을 비교하였다. 두 개의 80GB A100 GPU 를 사용하고, pipeline parallelism 기법을 활용하여 LLaMA LLM 70B 모델이 실행 가능하도록 하였다. GPU memory 한계에 도달할 때까지 batch size 를 증가시키며, sequence length 는 512 로 설정하였다. </p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-26-59d26e6135274b69e593b29bbbe7ffb0.png" width="1251" height="248" class="img_ev3q"></p><p>Tab. 3 은 70B BitNet b1.58 이 LLaMA LLM 대비 최대 11 배의 batch size 를 지원하며, 8.9 배 높은 throughput 을 제공함을 보여준다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="bitnet-b158-is-enabling-a-new-scaling-law-with-respect-to-model-performance-and-inference-cost">BitNet b1.58 is enabling a new scaling law with respect to model performance and inference cost.<a href="#bitnet-b158-is-enabling-a-new-scaling-law-with-respect-to-model-performance-and-inference-cost" class="hash-link" aria-label="Direct link to BitNet b1.58 is enabling a new scaling law with respect to model performance and inference cost." title="Direct link to BitNet b1.58 is enabling a new scaling law with respect to model performance and inference cost.">​</a></h4><p>Fig. 2 와 Fig. 3 의 결과를 기반으로, 1.58-bit 모델과 16-bit 모델 간의 크기 대응 관계는 다음과 같다.  </p><ul><li>13B BitNet b1.58 은 latency, memory usage, energy consumption 측면에서 3B FP16 LLM 보다 더 효율적이다.  </li><li>30B BitNet b1.58 은 latency, memory usage, energy consumption 측면에서 7B FP16 LLM 보다 더 효율적이다.  </li><li>70B BitNet b1.58 은 latency, memory usage, energy consumption 측면에서 13B FP16 LLM 보다 더 효율적이다.  </li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="training-with-2t-tokens">Training with 2T Tokens<a href="#training-with-2t-tokens" class="hash-link" aria-label="Direct link to Training with 2T Tokens" title="Direct link to Training with 2T Tokens">​</a></h4><p>LLMs 에서 training tokens 의 수는 중요한 요소이다. BitNet b1.58 의 tokens 규모에 따른 확장성을 테스트하기 위해, StableLM-3B 의 data recipe 를 따르며 2T tokens 로 BitNet b1.58 모델을 훈련하였다. StableLM-3B 는 SOTA open-source 3B model 이다.  </p><p>두 모델은 Winogrande, PIQA, SciQ, LAMBADA, ARC-easy 로 구성된 benchmark 에서 평가되었다. zero-shot accuracy 결과는 Tab. 4 에 보고되었으며, accuracy 와 normalized accuracy 로 측정된 작업에서는 두 값의 평균을 사용하였다. StableLM-3B 의 2T tokens 결과는 해당 모델의 technical report 에서 직접 가져왔다.</p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-27-8e8e27b6b0e5f4eebdf5322472894504.png" width="1349" height="225" class="img_ev3q"></p><p>실험 결과, BitNet b1.58 은 모든 end-task 에서 더 우수한 성능을 달성하였다. 이는 1.58-bit LLMs 가 높은 generalization capabilities 를 가지고 있음을 시사한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-discussion-and-future-work">4 Discussion and Future Work<a href="#4-discussion-and-future-work" class="hash-link" aria-label="Direct link to 4 Discussion and Future Work" title="Direct link to 4 Discussion and Future Work">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-bit-mixture-of-experts-moe-llms">1-bit Mixture-of-Experts (MoE) LLMs<a href="#1-bit-mixture-of-experts-moe-llms" class="hash-link" aria-label="Direct link to 1-bit Mixture-of-Experts (MoE) LLMs" title="Direct link to 1-bit Mixture-of-Experts (MoE) LLMs">​</a></h4><p>Mixture-of-Experts (MoE) 는 LLMs 에서 computation cost 을 절감할 수 있는 효과적인 방법으로 입증되었다. MoE 는 연산 FLOPs 를 크게 줄일 수 있지만, 높은 memory consumption 과 inter-chip communication overhead 가 발생하여 배포 및 활용에 제한이 있다. 이러한 문제는 1.58-bit LLMs 를 통해 해결할 수 있다.  </p><p>1) 1.58-bit LLMs 는 memory footprint 가 작아져, MoE models 배포에 필요한 장치 수를 줄일 수 있다. 2) activations 을 네트워크를 통해 전송하는 데 드는 비용을 크게 줄일 수 있다. 궁극적으로, 만약 전체 model 이 single chip 에 배치될 수 있다면 inter-chip communication overhead 는 완전히 제거될 것이다.  </p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="native-support-of-long-sequence-in-llms">Native Support of Long Sequence in LLMs<a href="#native-support-of-long-sequence-in-llms" class="hash-link" aria-label="Direct link to Native Support of Long Sequence in LLMs" title="Direct link to Native Support of Long Sequence in LLMs">​</a></h4><p>LLMs 시대에서 long sequence 를 처리하는 능력은 중요한 요구 사항이 되었다. long sequence inference 의 주요 도전 과제 중 하나는 KV caches 가 차지하는 memory consumption 이다. BitNet b1.58 은 activations 을 16-bit 에서 8-bit 로 줄여 같은 자원에서 context length 를 두 배로 늘릴 수 있도록 지원한다.  </p><p>추가적으로, 1.58-bit LLMs 의 경우 lossless compression 을 통해 activations 을 4-bit 이하로 줄일 수도 있다. 이는 더욱 효율적인 long sequence 처리 방식으로 이어질 수 있으며, 향후 연구에서 다룰 계획이다.  </p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="llms-on-edge-and-mobile">LLMs on Edge and Mobile<a href="#llms-on-edge-and-mobile" class="hash-link" aria-label="Direct link to LLMs on Edge and Mobile" title="Direct link to LLMs on Edge and Mobile">​</a></h4><p>1.58-bit LLMs 는 edge 및 mobile devices 에서의 LLMs 성능을 크게 향상시킬 가능성이 있다. 이러한 장치들은 종종 memory 와 computational power 가 제한적이기 때문에 LLMs 의 성능과 규모가 제약을 받는다. 그러나, 1.58-bit LLMs 의 낮은 memory 와 energy consumption 덕분에 edge 및 mobile 환경에서도 실행할 수 있게 된다.  </p><p>이를 통해 이전에는 불가능했던 다양한 응용 사례들이 가능해질 것이다. 또한, 1.58-bit LLMs 는 CPU 에서 더욱 효율적으로 실행될 수 있는데, CPU 는 대부분의 edge 및 mobile devices 에서 주요 연산 장치로 사용된다. 따라서 BitNet b1.58 은 이러한 장치에서 높은 성능을 발휘할 수 있으며, 이를 통해 edge AI 및 mobile AI 응용이 더욱 발전할 것으로 기대된다.  </p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-hardware-for-1-bit-llms">New Hardware for 1-bit LLMs<a href="#new-hardware-for-1-bit-llms" class="hash-link" aria-label="Direct link to New Hardware for 1-bit LLMs" title="Direct link to New Hardware for 1-bit LLMs">​</a></h4><p>최근 연구들은 LLMs 에 최적화된 새로운 hardware, 예를 들어 LPUs (Language Processing Units) 를 개발하는 방향을 탐색하고 있다. 이를 한 단계 더 발전시켜, BitNet 이 제공하는 새로운 computation paradigm 을 고려하여 1-bit LLMs 에 최적화된 hardware 및 system 설계를 추진할 필요가 있다.  </p><p>1-bit 연산 방식을 기반으로 하는 새로운 hardware 는 inference latency, memory usage 및 energy consumption 을 더욱 획기적으로 줄일 수 있다. 이를 통해 LLMs 의 확장성이 더욱 향상될 것이며, 대규모 모델을 더욱 효율적으로 운용할 수 있을 것이다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/quantization">Quantization</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/1-58-bit">1.58-bit</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/bit-net">BitNet</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/bit-net-b-1-58">BitNet b1.58</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-02-BitNet-1.58b.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">BitNet: Scaling 1-bit Transformers for Large Language Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/GIFT-SW"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#4-discussion-and-future-work" class="table-of-contents__link toc-highlight">4 Discussion and Future Work</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.74164ba1.js"></script>
<script src="/assets/js/main.82abd802.js"></script>
</body>
</html>