<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-11-SmoothQuant">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models | WYJLab"><meta data-rh="true" name="description" content="Large language models (LLMs) 은 뛰어난 성능을 보여주지만 compute 와 memory 가 많이 필요하다. Quantization 은 memory 를 줄이고 inference 를 빠르게 할 수 있다. 하지만 기존 방법들은 accuracy 및 hardware efficiency 를 동시에 유지하지 못한다."><meta data-rh="true" property="og:description" content="Large language models (LLMs) 은 뛰어난 성능을 보여주지만 compute 와 memory 가 많이 필요하다. Quantization 은 memory 를 줄이고 inference 를 빠르게 할 수 있다. 하지만 기존 방법들은 accuracy 및 hardware efficiency 를 동시에 유지하지 못한다."><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.31c2725c.js" as="script">
<link rel="preload" href="/assets/js/main.6a697ae5.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Attacking/universal-adversarial-prompt">Attacking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Flan-T5">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Generalization/Flat-LoRA">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Mixture/UniPELT">Mixture</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Module</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Pruning/SMP">Pruning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Quantization</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Fine-Tuning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/QuIP">QuIP: 2-Bit Quantization of Large Language Models With Guarantees</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet">BitNet: Scaling 1-bit Transformers for Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/GIFT-SW">GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/LLM-FP4">LLM-FP4: 4-Bit Floating-Point Quantized Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8">BitNet a4.8: 4-bit Activations for 1-bit LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/fp4">Optimizing Large Language Model Training Using FP4 Quantization</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/LoRA/QLoRA">LoRA</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning">Soft Prompt</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Engineering/CoT/Chain-of-Thought">Prompt Engineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Quantization</span><meta itemprop="position" content="4"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Fine-Tuning</span><meta itemprop="position" content="5"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</span><meta itemprop="position" content="6"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Abstract</h1><p>Large language models (LLMs) 은 뛰어난 성능을 보여주지만 compute 와 memory 가 많이 필요하다. Quantization 은 memory 를 줄이고 inference 를 빠르게 할 수 있다. 하지만 기존 방법들은 accuracy 및 hardware efficiency 를 동시에 유지하지 못한다. </p><p>저자는 <strong>SmoothQuant</strong> 라는 training 없이 정확도를 유지하는 범용적인 post-training quantization (PTQ) 솔루션을 제안한다. </p><p>8-bit weight, 8-bit activation (W8A8) quantization 을 LLM 에 가능하게 해준다. Weight 는 quantize 하기 쉽지만 activation 은 그렇지 않다는 점을 바탕으로, SmoothQuant 는 activation 의 outlier 를 smoothing 해서 quantization 어려움를 weight 로 offline 에서 <em>옮기는</em> mathematically equivalent transformation 을 사용한다. </p><ul><li>SmoothQuant 는 OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, Mixtral 같은 LLM 의 모든 matrix multiplication 에서 weight 와 activation 의 INT8 quantization 을 가능하게 한다. </li><li>저자는 최대 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.56</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.56 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1.56</span><span class="mord">×</span></span></span></span></span> speedup 과 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">2 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mord">×</span></span></span></span></span> memory reduction 을 보여주면서 정확도 손실은 거의 없음을 입증한다. </li><li>SmoothQuant 는 single node 에서 530B LLM 을 서비스할 수 있게 한다. 이 연구는 hardware costs 를 줄이고 LLM 을 대중화하는 turn-key 솔루션을 제공한다.</li></ul><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-52-0b5d96026a7eee6cf8a35fbbe3c90fbb.png" width="984" height="795" class="img_ev3q"></p><h1>1 Introduction</h1><p>Large-scale language models (LLMs) 은 다양한 task 에서 뛰어난 성능을 보여준다. 하지만 LLM 서비스는 예산과 에너지를 많이 잡아먹는다. 이는 model size 가 크기 때문이다. </p><ul><li>예로 GPT-3 model 은 175B parameter 를 가지고 있어서 FP16 으로 저장하고 실행하려면 최소 350GB 의 memory 가 필요. 그러려면 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn><mo>×</mo><mn>48</mn></mrow><annotation encoding="application/x-tex">8 \times 48</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">48</span></span></span></span></span>GB A6000 GPU 나 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>×</mo><mn>80</mn></mrow><annotation encoding="application/x-tex">5 \times 80</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">80</span></span></span></span></span>GB A100 GPU 가 inference 에 필요하다. </li><li>게다가 computation 과 communication overhead 때문에 inference latency 도 실세계 응용에 맞지 않을 수 있다.</li></ul><p>Quantization 은 LLM 비용을 줄이는 유망한 방법이며 Weight 와 activation 을 low-bit integer 로 quantize 하면 GPU memory 요구량 (size 및 bandwidth) 을 줄이고 compute-intensive operation (linear layer 의 <code>GEMM</code>, attention 의 <code>BMM</code>) 을 빠르게 할 수 있다. </p><ul><li>예로 INT8 quantization 은 weight 와 activation 의 GPU memory 사용량을 FP16 대비 절반으로 줄이고 matrix multiplication throughput 을 거의 두 배로 늘릴 수 있다.</li></ul><p>하지만 CNN model 이나 BERT 같은 smaller transformer model 과 달리 LLM 의 activation 은 quantize 하기 어렵다. </p><ul><li>LLM 을 6.7B parameter 이상으로 키우면 <em>activations</em> 에 large magnitude 의 systematic outlier 가 나타나서 quantization error 가 커지고 정확도가 떨어진다. </li><li>ZeroQuant 는 dynamic per-token activation quantization 과 group-wise weight quantization 을 적용한다. <ul><li>이는 효율적으로 구현할 수 있고 GPT-3-350M 이나 GPT-J-6B 에서 좋은 정확도를 보여준다. </li><li>하지만 175B parameter 의 OPT model 에서는 정확도를 유지하지 못한다 (Sec. 5.2 참조). </li></ul></li><li><code>LLM.int8()</code> 은 mixed-precision decomposition (outlier 는 FP16 으로 유지하고 나머지 activation 은 INT8 사용) 으로 정확도 문제를 해결한다. <ul><li>하지만 이는 hardware accelerator 에서 효율적으로 구현하기 어렵다. </li><li>그래서 <em>efficient/hardware-friendly</em> 면서 LLM 의 모든 compute-intensive operation 에 INT8 을 사용할 수 있는 <em>training-free quantization scheme</em> 을 찾는 건 여전히 풀리지 않은 도전이다.</li></ul></li></ul><p>저자는 <strong>SmoothQuant</strong> 라는 정확하고 efficient post-training quantization (PTQ) 솔루션을 제안한다. </p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-53-16931a1700a4bbd4495ea6ae9dc0c542.png" width="971" height="1073" class="img_ev3q"></p><ul><li>SmoothQuant 는 activation 이 weight 보다 quantize 하기 훨씬 어렵다는 관찰에 기반한다. 특히 outlier 때문에 그렇다. <ul><li>그런데 다른 token 은 channel 간 비슷한 변화를 보인다. </li><li>이 관찰을 바탕으로 SmoothQuant 는 quantization 어려움을 activation 에서 weight 로 offline 에서 옮긴다 (Fig. 2). </li></ul></li><li>SmoothQuant 는 per-channel scaling transformation 을 제안하는데, 이는 channel 간 magnitude 를 크게 smoothing 해서 model 을 quantization 친화적으로 만든다. </li><li>SmoothQuant 는 다양한 quantization scheme 과 호환되기 때문에 저자는 세 가지 efficiency levels (Tab. 2, O1-O3) 을 구현했다. <ul><li>실험에서 SmoothQuant 는 hardware efficient 하다. </li><li>OPT-175B, BLOOM-176B, GLM-130B, MT-NLG 530B 의 성능을 유지하면서 PyTorch 에서 최대 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.51</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.51 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1.51</span><span class="mord">×</span></span></span></span></span> speedup 과 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.96</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.96 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1.96</span><span class="mord">×</span></span></span></span></span> memory reduction 을 달성한다. </li></ul></li><li>SmoothQuant 는 구현하기 쉽다. 저자는 SmoothQuant 를 state-of-the-art transformer serving framework 인 FasterTransformer 에 통합해서 FP16 대비 최대 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.56</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.56 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1.56</span><span class="mord">×</span></span></span></span></span> speedup 과 memory 사용량 절반을 달성했다. <ul><li>놀랍게도 SmoothQuant 는 OPT-175B 같은 large model 을 FP16 대비 절반 GPU 로 더 빠르게 서비스할 수 있게 하고, 530B model 을 한 8-GPU node 에서 서비스할 수 있게 한다. </li><li>이 연구는 LLM 사용을 대중화하고 serving 비용을 줄이는 turn-key 솔루션을 제공한다.</li></ul></li></ul><h1>2 Preliminaries</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="quantization">Quantization<a href="#quantization" class="hash-link" aria-label="Direct link to Quantization" title="Direct link to Quantization">​</a></h4><p>Quantization 은 high-precision value 를 discrete level 로 매핑한다. 저자는 integer uniform quantization (특히 INT8) 을 연구하는데, 이는 hardware 지원과 efficiency 가 더 좋다. </p><p>Quantization 과정은 이렇게 표현할 수 있다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mover accent="true"><mi mathvariant="bold">X</mi><mo stretchy="true">‾</mo></mover><mrow><mi mathvariant="normal">I</mi><mi mathvariant="normal">N</mi><mi mathvariant="normal">T</mi><mn>8</mn></mrow></msup><mo>=</mo><mrow><mo fence="true">⌈</mo><mfrac><msup><mi mathvariant="bold">X</mi><mrow><mi mathvariant="normal">F</mi><mi mathvariant="normal">P</mi><mn>16</mn></mrow></msup><mi mathvariant="normal">Δ</mi></mfrac><mo fence="true">⌋</mo></mrow><mo separator="true">,</mo><mspace width="1em"></mspace><mi mathvariant="normal">Δ</mi><mo>=</mo><mfrac><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><mi mathvariant="bold">X</mi><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow><mrow><msup><mn>2</mn><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>−</mo><mn>1</mn></mrow></mfrac></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \overline{\mathbf{X}}^{\mathrm{INT8}} = \left\lceil \frac{\mathbf{X}^{\mathrm{FP16}}}{\Delta} \right\rfloor, \quad \Delta = \frac{\max(|\mathbf{X}|)}{2^{N-1}-1} \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4684em;vertical-align:-0.9842em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4842em"><span style="top:-3.4842em"><span class="pstrut" style="height:3.5183em"></span><span class="mord"><span class="mord"><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8861em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathbf">X</span></span></span><span style="top:-3.8061em"><span class="pstrut" style="height:3em"></span><span class="overline-line" style="border-bottom-width:0.04em"></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1173em"><span style="top:-3.339em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">INT8</span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">⌈</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">Δ</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathbf">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">FP16</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">⌋</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">Δ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7673em"><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord">1</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop">max</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord mathbf">X</span><span class="mord">∣</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9842em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4842em"><span style="top:-3.4842em"><span class="pstrut" style="height:3.5183em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9842em"><span></span></span></span></span></span></span></span></span></div><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em"></span><span class="mord mathbf">X</span></span></span></span></span> 는 floating-point tensor 이고, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi mathvariant="bold">X</mi><mo stretchy="true">‾</mo></mover></mrow><annotation encoding="application/x-tex">\overline{\mathbf{X}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8861em"></span><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8861em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathbf">X</span></span></span><span style="top:-3.8061em"><span class="pstrut" style="height:3em"></span><span class="overline-line" style="border-bottom-width:0.04em"></span></span></span></span></span></span></span></span></span></span> 는 quantize 된 결과다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span></span></span></span></span> 는 quantization step size 이고, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⌈</mo><mo>⋅</mo><mo stretchy="false">⌋</mo></mrow><annotation encoding="application/x-tex">\lceil \cdot \rfloor</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">⌈</span><span class="mord">⋅</span><span class="mclose">⌋</span></span></span></span></span> 는 rounding function 이다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> 은 bit 수 (여기선 8) 다. </li><li>단순하게 tensor 가 0 에서 대칭이라고 가정했다. </li><li>Asymmetric 경우 (e.g., ReLU 후) 는 zero-point 를 추가하면 비슷하다.</li></ul><p>이런 quantizer 는 maximum absolute value 로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span></span></span></span></span> 를 계산해서 activation 의 outlier 를 보존한다. 이는 정확도에 중요하다. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span></span></span></span></span> 는 calibration sample 의 activation 으로 offline 에서 계산할 수 있다. 이를 <strong>static quantization</strong> 이라고 한다. </p><p>또 runtime 에 activation statistics 로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span></span></span></span></span> 를 구할 수도 있다. 이는 <strong>dynamic quantization</strong> 이다. </p><p>Fig. 3 에서 quantization 은 granularity level 이 다르다. <strong>Per-tensor quantization</strong> 은 entire matrix 에 single step size 를 사용한다. finer-grained quantization 은 token 별 activation (<strong>per-token quantization</strong>) 이나 weight 의 output channel 별 (<strong>per-channel quantization</strong>) 다른 step size 를 사용할 수 있다. Per-channel quantization 의 coarse-grained version 은 channel group 별 다른 step size 를 사용하는 <strong>group-wise quantization</strong> 이다.</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-54-d2e6924b6d053778455b3e019765db6b.png" width="967" height="1031" class="img_ev3q"></p><p>Transformer 의 linear layer 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Y</mi><mo>=</mo><mi mathvariant="bold">X</mi><mo>⋅</mo><mi mathvariant="bold">W</mi></mrow><annotation encoding="application/x-tex">\mathbf{Y} = \mathbf{X} \cdot \mathbf{W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em"></span><span class="mord mathbf" style="margin-right:0.02875em">Y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6861em"></span><span class="mord mathbf">X</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6861em"></span><span class="mord mathbf" style="margin-right:0.01597em">W</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Y</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>T</mi><mo>×</mo><msub><mi>C</mi><mi>o</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{Y} \in \mathbb{R}^{T \times C_o}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7252em;vertical-align:-0.0391em"></span><span class="mord mathbf" style="margin-right:0.02875em">Y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.0715em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>T</mi><mo>×</mo><msub><mi>C</mi><mi>i</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{X} \in \mathbb{R}^{T \times C_i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7252em;vertical-align:-0.0391em"></span><span class="mord mathbf">X</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em"><span style="top:-2.357em;margin-left:-0.0715em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>C</mi><mi>i</mi></msub><mo>×</mo><msub><mi>C</mi><mi>o</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{W} \in \mathbb{R}^{C_i \times C_o}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7252em;vertical-align:-0.0391em"></span><span class="mord mathbf" style="margin-right:0.01597em">W</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em"><span style="top:-2.357em;margin-left:-0.0715em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.0715em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 라고 하면 (여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span></span></span></span></span> 는 token 수, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">C_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 input channel, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">C_o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 output channel, batch dimension 은 생략, Fig. 3 참조), weight 를 INT8 로 quantize 하면 FP16 대비 storage 를 절반으로 줄일 수 있다. 하지만 inference 를 빠르게 하려면 weight 와 activation 모두 INT8 (i.e., W8A8) 로 quantize 해야 integer kernel (e.g., INT8 <code>GEMM</code>) 을 활용할 수 있다. 이는 다양한 hardware (NVIDIA GPU, Intel CPU, Qualcomm DSP 등) 에서 지원된다.</p><h1>3 Review of Quantization Difficulty</h1><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-55-f2cc92bc6f5864cd85084d2f521dec50.png" width="1970" height="842" class="img_ev3q"></p><p>LLM 은 activation 의 outlier 때문에 quantize 하기 어렵다. </p><p>먼저 activation quantization 의 어려움을 리뷰하고 outlier pattern 을 찾아본다. Fig. 4 (left) 에서 quantization error 가 큰 linear layer 의 input activation 과 weight 를 시각화했다. 몇 가지 패턴을 발견했는데, 이게 저자의 방법의 동기가 된다:</p><ol><li><strong>Activation 은 weight 보다 quantize 하기 어렵다.</strong> <ul><li>Weight distribution 은 꽤 uniform 하고 flat 해서 quantize 하기 쉽다. </li><li>이전 연구에서 LLM 의 weight 를 INT8 이나 INT4 로 quantize 해도 정확도가 떨어지지 않는다고 했다. 저자의 관찰과 일치한다.</li></ul></li><li><strong>Outlier 때문에 activation quantization 이 어렵다.</strong> <ul><li>Activation 의 outlier size 는 대부분 값보다 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><mn>100</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">\sim 100 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">100</span><span class="mord">×</span></span></span></span></span> 크다. Per-tensor quantization (Eq. 1) 에서 large outlier 가 maximum magnitude 를 지배해서 non-outlier channel 의 effective quantization <em>bit/level</em> 이 적어진다 (Fig. 2). </li><li>Channel <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span> 의 maximum magnitude 가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">m_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 이고 whole matrix 의 maximum 값이 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span> 이라면, channel <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span> 의 effective quantization level 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>8</mn></msup><mo>⋅</mo><msub><mi>m</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">2^8 \cdot m_i / m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal">m</span></span></span></span></span> 이다. Non-outlier channel 은 level 이 아주 작아서 (2-3) quantization error 가 커진다.</li></ul></li><li><strong>Outlier 는 특정 channel 에 지속된다.</strong> <ul><li>Outlier 는 소수 channel 에 나타난다. 한 channel 에 outlier 가 있으면 all tokens 에 지속적으로 나타난다 (Fig. 4, 빨간색). </li><li>Token 별 channel 간 variance 는 크지만 (일부 channel 은 크고 대부분은 작다), channel 별 token 간 magnitude variance 는 작다 (outlier channel 은 계속 크다). </li><li>Outlier 의 지속성과 channel 내 작은 variance 때문에 <em>per-channel</em> quantization 을 하면 quantization error 가 <em>per-tensor</em> quantization 보다 훨씬 작아진다. 반면 <em>per-token</em> quantization 은 별로 도움이 안 된다. </li><li>Tab. 1 에서 per-channel activation quantization 이 FP16 baseline 과 정확도를 맞춘다고 검증했다.</li></ul></li></ol><p><img loading="lazy" alt="Table 1" src="/assets/images/image-56-dbcfe7d846faf37287ed6395856c4d65.png" width="977" height="572" class="img_ev3q"></p><p>하지만 per-channel activation quantization 은 hardware-accelerated <code>GEMM</code> kernel 에 잘 맞지 않다. 이 kernel 은 고속으로 연속 연산 (e.g., Tensor Core MMA) 을 실행하고 lower throughput 의 instruction (e.g., conversion, CUDA Core FMA) 삽입을 허용하지 않는다. 이런 kernel 에서 scaling 은 matrix multiplication 의 outer dimension (activation 의 token dimension <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span></span></span></span></span>, weight 의 output channel dimension <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">C_o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>, Fig. 3 참조) 에서만 가능하다. 이는 matrix multiplication 후에 적용된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="bold">Y</mi><mo>=</mo><mi mathvariant="normal">diag</mi><mo>⁡</mo><mo stretchy="false">(</mo><msubsup><mi mathvariant="bold">Δ</mi><mi mathvariant="bold">X</mi><mrow><mi mathvariant="normal">F</mi><mi mathvariant="normal">P</mi><mn>16</mn></mrow></msubsup><mo stretchy="false">)</mo><mo>⋅</mo><mo stretchy="false">(</mo><msup><mover accent="true"><mi mathvariant="bold">X</mi><mo stretchy="true">‾</mo></mover><mrow><mi mathvariant="normal">I</mi><mi mathvariant="normal">N</mi><mi mathvariant="normal">T</mi><mn>8</mn></mrow></msup><mo>⋅</mo><msup><mover accent="true"><mi mathvariant="bold">W</mi><mo stretchy="true">‾</mo></mover><mrow><mi mathvariant="normal">I</mi><mi mathvariant="normal">N</mi><mi mathvariant="normal">T</mi><mn>8</mn></mrow></msup><mo stretchy="false">)</mo><mo>⋅</mo><mi mathvariant="normal">diag</mi><mo>⁡</mo><mo stretchy="false">(</mo><msubsup><mi mathvariant="bold">Δ</mi><mi mathvariant="bold">W</mi><mrow><mi mathvariant="normal">F</mi><mi mathvariant="normal">P</mi><mn>16</mn></mrow></msubsup><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \mathbf{Y} = \operatorname{diag}(\boldsymbol{\Delta}_{\mathbf{X}}^{\mathrm{FP16}}) \cdot (\overline{\mathbf{X}}^{\mathrm{INT8}} \cdot \overline{\mathbf{W}}^{\mathrm{INT8}}) \cdot \operatorname{diag}(\boldsymbol{\Delta}_{\mathbf{W}}^{\mathrm{FP16}}) \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4773em;vertical-align:-0.4887em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9887em"><span style="top:-2.9887em"><span class="pstrut" style="height:3.1173em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.02875em">Y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">diag</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">Δ</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9173em"><span style="top:-2.453em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight">X</span></span></span></span><span style="top:-3.139em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">FP16</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mopen">(</span><span class="mord"><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8861em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathbf">X</span></span></span><span style="top:-3.8061em"><span class="pstrut" style="height:3em"></span><span class="overline-line" style="border-bottom-width:0.04em"></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1173em"><span style="top:-3.339em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">INT8</span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8861em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">W</span></span></span><span style="top:-3.8061em"><span class="pstrut" style="height:3em"></span><span class="overline-line" style="border-bottom-width:0.04em"></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1173em"><span style="top:-3.339em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">INT8</span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">diag</span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">Δ</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9173em"><span style="top:-2.453em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathbf mtight" style="margin-right:0.01597em">W</span></span></span></span><span style="top:-3.139em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">FP16</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4887em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9887em"><span style="top:-2.9887em"><span class="pstrut" style="height:3.1173em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4887em"><span></span></span></span></span></span></span></span></span></div><p>그래서 이전 연구들은 linear layer 에 per-token activation quantization 을 사용했다. 하지만 이는 activation quantization 의 어려움을 해결하지 못한다 (per-tensor 보다 약간 나을 뿐).</p><h1>4 SmoothQuant</h1><p>Per-channel activation quantization (불가능한) 대신, 저자는 input activation 을 per-channel smoothing factor <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">s</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><msub><mi>C</mi><mi>i</mi></msub></msup></mrow><annotation encoding="application/x-tex">\mathbf{s} \in \mathbb{R}^{C_i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathbf">s</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em"><span style="top:-2.357em;margin-left:-0.0715em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 로 나누어 &quot;smoothing&quot; 한다고 제안한다. Linear layer 의 mathematical equivalence 를 유지하려면 weight 를 반대 방향으로 scale 한다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="bold">Y</mi><mo>=</mo><mo stretchy="false">(</mo><mi mathvariant="bold">X</mi><mi mathvariant="normal">diag</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="bold">s</mi><msup><mo stretchy="false">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo><mo>⋅</mo><mo stretchy="false">(</mo><mi mathvariant="normal">diag</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="bold">s</mi><mo stretchy="false">)</mo><mi mathvariant="bold">W</mi><mo stretchy="false">)</mo><mo>=</mo><mover accent="true"><mi mathvariant="bold">X</mi><mo>^</mo></mover><mover accent="true"><mi mathvariant="bold">W</mi><mo>^</mo></mover></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \mathbf{Y} = (\mathbf{X} \operatorname{diag}(\mathbf{s})^{-1}) \cdot (\operatorname{diag}(\mathbf{s}) \mathbf{W}) = \hat{\mathbf{X}} \hat{\mathbf{W}} \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3095em;vertical-align:-0.4048em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9048em"><span style="top:-2.9552em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.02875em">Y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mopen">(</span><span class="mord mathbf">X</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">diag</span></span><span class="mopen">(</span><span class="mord mathbf">s</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mopen">(</span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">diag</span></span><span class="mopen">(</span><span class="mord mathbf">s</span><span class="mclose">)</span><span class="mord mathbf" style="margin-right:0.01597em">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9495em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathbf">X</span></span><span style="top:-3.2551em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9495em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathbf" style="margin-right:0.01597em">W</span></span><span style="top:-3.2551em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4048em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9048em"><span style="top:-2.9048em"><span class="pstrut" style="height:2.9495em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4048em"><span></span></span></span></span></span></span></span></span></div><p>Input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em"></span><span class="mord mathbf">X</span></span></span></span></span> 는 보통 이전 linear operation (e.g., linear layer, layer norm 등) 에서 나오니까 smoothing factor 를 previous layer 의 parameter 에 <em>offline</em> 으로 fuse 할 수 있다. 이는 extra scaling 에서 kernel call overhead 를 만들지 않는다. Residual add 에서 input 이 오는 경우엔 residual branch 에 extra scaling 을 넣을 수 있다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="migrate-the-quantization-difficulty-from-activations-to-weights">Migrate the quantization difficulty from activations to weights.<a href="#migrate-the-quantization-difficulty-from-activations-to-weights" class="hash-link" aria-label="Direct link to Migrate the quantization difficulty from activations to weights." title="Direct link to Migrate the quantization difficulty from activations to weights.">​</a></h4><p>저자는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi mathvariant="bold">X</mi><mo>^</mo></mover><mo>=</mo><mi mathvariant="bold">X</mi><mi mathvariant="normal">diag</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="bold">s</mi><msup><mo stretchy="false">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\hat{\mathbf{X}} = \mathbf{X} \operatorname{diag}(\mathbf{s})^{-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9495em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9495em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathbf">X</span></span><span style="top:-3.2551em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord mathbf">X</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">diag</span></span><span class="mopen">(</span><span class="mord mathbf">s</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span> 가 quantize 하기 쉽게 per-channel smoothing factor <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">s</mi></mrow><annotation encoding="application/x-tex">\mathbf{s}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4444em"></span><span class="mord mathbf">s</span></span></span></span></span> 를 선택하려 한다. Quantization error 를 줄이려면 all channels 의 <em>effective quantization bit</em> 를 늘려야 한다. all channels 의 maximum magnitude 가 같을 때 total effective quantization bit 가 가장 크다. 그래서 간단한 선택은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">s</mi><mi>j</mi></msub><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><msub><mi mathvariant="bold">X</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>j</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>C</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{s}_j = \max(|\mathbf{X}_j|), j = 1, 2, \ldots, C_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7305em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathbf">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 이다. 여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span> 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span>-th input channel 이다. </p><p>이 선택은 나눗셈 후 all activation channels 의 maximum value 가 같게 해서 quantize 하기 쉽게 한다. Activation 의 range 는 dynamic 하다. 다른 input sample 에 따라 변한다. 여기선 pre-training dataset 의 calibration sample 로 activation channel 의 scale 을 추정한다. 하지만 이 공식은 모든 quantization 어려움 를 weight 로 밀어넣는다. 이 경우 weight 의 quantization error 가 커져서 정확도가 많이 떨어진다 (Fig. 10 참조). 반대로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">s</mi><mi>j</mi></msub><mo>=</mo><mn>1</mn><mi mathvariant="normal">/</mi><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><msub><mi mathvariant="bold">W</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{s}_j = 1 / \max(|\mathbf{W}_j|)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7305em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathbf">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord">1/</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mclose">)</span></span></span></span></span> 로 모든 quantization 어려움를 activation 에 밀어넣을 수도 있다. 마찬가지로 activation quantization error 때문에 성능이 나쁘다. 그래서 weight 와 activation 간에 quantization 어려움를 나누는 게 필요하다.</p><p>여기서 migration strength <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 라는 hyper-parameter 를 도입해서 activation 에서 weight 로 옮기는 난이도를 조절한다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi mathvariant="bold">s</mi><mi>j</mi></msub><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><msub><mi mathvariant="bold">X</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi><msup><mo stretchy="false">)</mo><mi>α</mi></msup><mi mathvariant="normal">/</mi><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><msub><mi mathvariant="bold">W</mi><mi>j</mi></msub><mi mathvariant="normal">∣</mi><msup><mo stretchy="false">)</mo><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow></msup></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \mathbf{s}_j = \max(|\mathbf{X}_j|)^\alpha / \max(|\mathbf{W}_j|)^{1-\alpha} \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2241em;vertical-align:-0.3621em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8621em"><span style="top:-2.9979em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathbf">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord"><span class="mord mathbf">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em">α</span></span></span></span></span></span></span></span><span class="mord">/</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">∣</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.0037em">α</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3621em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8621em"><span style="top:-2.8621em"><span class="pstrut" style="height:2.8641em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3621em"><span></span></span></span></span></span></span></span></span></div><p>대부분 model (e.g., OPT, BLOOM) 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.5</span></span></span></span></span> 가 quantization 어려움를 균등히 나누는 잘 맞는 지점이다. 특히 weight 와 activation 에 같은 quantizer (e.g., per-tensor, static quantization) 를 사용할 때 그렇다. 이 공식은 해당 channel 의 weight 와 activation 의 maximum value 가 비슷하게 해서 quantization 어려움를 공유한다. </p><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-57-7361f523cb6d764073f129f366680a90.png" width="967" height="576" class="img_ev3q"></p><p>Fig. 5 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.5</span></span></span></span></span> 일 때 smoothing transformation 을 보여준다. </p><p>Activation outlier 가 더 심한 model (e.g., GLM-130B, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><mn>30</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\sim 30\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">30%</span></span></span></span></span> outlier) 에선 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 를 더 크게 (e.g., 0.75) 해서 weight 로 더 많은 난이도를 옮길 수 있다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="applying-smoothquant-to-transformer-blocks">Applying SmoothQuant to Transformer blocks.<a href="#applying-smoothquant-to-transformer-blocks" class="hash-link" aria-label="Direct link to Applying SmoothQuant to Transformer blocks." title="Direct link to Applying SmoothQuant to Transformer blocks.">​</a></h4><p>Linear layer 는 LLM 의 대부분 parameter 와 computation 을 차지한다. 기본으로 self-attention 과 feed-forward layer 의 input activation 에 scale smoothing 을 하고 all linear layers 를 W8A8 로 quantize 한다. Attention computation 의 <code>BMM</code> operator 도 quantize 한다. </p><p><img loading="lazy" alt="Figure 6" src="/assets/images/image-58-2e6aaf60978ee7e6d948f9eefa15ef1b.png" width="972" height="748" class="img_ev3q"></p><p>Fig. 6 에서 transformer block 의 quantization flow 를 설계했다. </p><p>Linear layer 와 attention layer 의 <code>BMM</code> 같은 compute-heavy operator 의 input 과 weight 를 INT8 로 quantize 하고, ReLU, Softmax, LayerNorm 같은 lightweight element-wise operation 의 activation 은 FP16 으로 유지한다. 이런 설계는 정확도와 inference 효율성을 균형 있게 한다.</p><h1>5 Experiments</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-setups">5.1 Setups<a href="#51-setups" class="hash-link" aria-label="Direct link to 5.1 Setups" title="Direct link to 5.1 Setups">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="baselines">Baselines<a href="#baselines" class="hash-link" aria-label="Direct link to Baselines" title="Direct link to Baselines">​</a></h4><p>저자는 INT8 post-training quantization 설정에서 네 가지 baseline 과 비교한다. 즉, model parameter 를 retraining 하지 않는다: W8A8 naive quantization, ZeroQuant, <code>LLM.int8()</code>, Outlier Suppression 이다. </p><p>SmoothQuant 는 quantization scheme 과 orthogonal 하니까 점진적으로 공격적이고 efficienct quantization level (O1-O3) 을 제공한다. Baseline 과 SmoothQuant 의 quantization scheme 은 Tab. 2 에 자세히 나온다.</p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-59-2d5c15b58a7591cd031fdb507ead369b.png" width="973" height="641" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models-and-datasets">Models and datasets.<a href="#models-and-datasets" class="hash-link" aria-label="Direct link to Models and datasets." title="Direct link to Models and datasets.">​</a></h4><p>SmoothQuant 를 평가하려고 세 가지 LLM family 를 선택했다: OPT, BLOOM, GLM-130B 이다. </p><ul><li>OPT 와 BLOOM model 은 LAMBADA, HellaSwag, PIQA, WinoGrande, OpenBookQA, RTE, COPA (7 zero-shot 평가 task) 와 WikiText (1 language modeling dataset) 으로 평가한다. </li><li>GLM-130B 은 MMLU, MNLI, QNLI, LAMBADA 로 평가한다. 왜냐면 앞의 일부 benchmark 가 GLM-130B 의 training set 에 포함돼 있기 때문이다. </li><li>OPT 와 BLOOM model 은 lm-eval-harness 로 평가하고, GLM-130B 은 공식 repo 로 평가한다. </li><li>마지막으로 MT-NLG 530B 에 방법론을 확장해서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo><mn>500</mn></mrow><annotation encoding="application/x-tex">&gt;500</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">500</span></span></span></span></span>B model 을 single node 에서 서비스할 수 있게 한다. 저자는 quantization 전후의 상대적 성능 변화에 초점을 맞춘다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="activation-smoothing">Activation smoothing.<a href="#activation-smoothing" class="hash-link" aria-label="Direct link to Activation smoothing." title="Direct link to Activation smoothing.">​</a></h4><p>Migration strength <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\alpha=0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.5</span></span></span></span></span> 는 OPT 와 BLOOM model 에서 일반적인 sweet spot 이고, GLM-130B 에선 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.75</mn></mrow><annotation encoding="application/x-tex">\alpha=0.75</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.75</span></span></span></span></span> 이다. GLM-130B 의 activation 은 quantize 하기 더 어렵다. 적당한 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 는 Pile validation set 의 subset 에서 grid search 로 빠르게 찾는다. </p><p>Activation statistics 를 얻으려면 pre-training dataset Pile 에서 512 random sentences 로 smoothing factor 와 static quantization step size 를 한 번 calibrate 한다. 그 후 같은 smoothed, quantized model 을 all downstream tasks 에 적용한다. 이렇게 quantize 된 LLM 의 generality 와 zero-shot 성능을 benchmark 한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="implementation">Implementation.<a href="#implementation" class="hash-link" aria-label="Direct link to Implementation." title="Direct link to Implementation.">​</a></h4><p>SmoothQuant 는 두 backend 로 구현했다: (1) PyTorch Huggingface 로 개념 증명, (2) FasterTransformer 로 production 환경에서 사용하는 고성능 framework 예시다. </p><p>두 framework 에서 INT8 linear module 과 <code>BMM</code> function 을 CUTLASS INT8 <code>GEMM</code> kernel 로 구현했다. original FP16 linear module 과 <code>BMM</code> function 을 INT8 kernel 로 교체해서 INT8 model 을 만든다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-accurate-quantization">5.2 Accurate Quantization<a href="#52-accurate-quantization" class="hash-link" aria-label="Direct link to 5.2 Accurate Quantization" title="Direct link to 5.2 Accurate Quantization">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-of-opt-175b">Results of OPT-175B.<a href="#results-of-opt-175b" class="hash-link" aria-label="Direct link to Results of OPT-175B." title="Direct link to Results of OPT-175B.">​</a></h4><p>SmoothQuant 는 activation quantize 가 더 어려운 very large LLMs 의 quantization 을 처리할 수 있다. OPT-175B 에서 quantization 을 연구했다. Tab. 3 에서 SmoothQuant 는 all quantization schemes 로 모든 평가 dataset 에서 FP16 정확도를 맞춘다. </p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-60-2c74cfc69c9ec3d2bc59e94a04222514.png" width="1951" height="727" class="img_ev3q"></p><ul><li><code>LLM.int8()</code> 도 floating-point 정확도를 맞출 수 있다. 왜냐면 outlier 를 floating-point 값으로 나타내기 때문이다. 하지만 이는 latency overhead 가 크다 (Tab. 11). </li><li>W8A8, ZeroQuant, Outlier Suppression baseline 은 거의 랜덤 결과를 낸다. </li><li>LLM 의 activation 을 naive 하게 quantize 하면 성능이 망가진다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-of-different-llms">Results of different LLMs.<a href="#results-of-different-llms" class="hash-link" aria-label="Direct link to Results of different LLMs." title="Direct link to Results of different LLMs.">​</a></h4><p>SmoothQuant 는 다양한 LLM 디자인에 적용할 수 있다. Tab. 4 에서 SmoothQuant 가 100B 이상의 모든 공개 LLM 을 quantize 할 수 있음을 보여준다. </p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-61-77fb6f100e47cd740bd653d24376b9ff.png" width="962" height="888" class="img_ev3q"></p><ul><li>OPT-175B 에 비해 BLOOM-176B 은 quantize 하기 쉽다. Baseline 중 어느 것도 model 을 완전히 망가뜨리지 않는다. </li><li>Naive W8A8 per-tensor dynamic quantization 도 정확도를 4% 만 떨어뜨린다. </li><li>SmoothQuant 의 O1, O2 level 은 floating-point 정확도를 유지하고, O3 level (per-tensor static) 은 평균 정확도를 0.8% 떨어뜨린다. <ul><li>이는 statically 수집된 통계와 실제 평가 sample 의 activation 통계 간 차이 때문이라고 본다. </li></ul></li><li>반대로 GLM-130B 은 quantize 하기 더 어렵다. 그럼에도 SmoothQuant-O1 은 FP16 정확도를 맞추고, SmoothQuant-O3 은 정확도를 1% 만 떨어뜨린다. Baseline 보다 훨씬 낫다. </li><li>GLM-130B 의 static quantization step size 를 calibrate 할 때 상위 2% token 을 clip 한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-on-llms-of-different-sizes">Results on LLMs of different sizes.<a href="#results-on-llms-of-different-sizes" class="hash-link" aria-label="Direct link to Results on LLMs of different sizes." title="Direct link to Results on LLMs of different sizes.">​</a></h4><p>SmoothQuant 는 100B 이상의 very large LLMs 뿐 아니라 smaller LLM 에도 일관되게 작동한다. </p><p><img loading="lazy" alt="Figure 7" src="/assets/images/image-62-a9993d7ab2e870de4345f2f54cd532c0.png" width="972" height="756" class="img_ev3q"></p><p>Fig. 7 에서 SmoothQuant 가 모든 OPT model scale 에서 FP16 정확도를 INT8 quantization 으로 맞춘다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-on-instruction-tuned-llm">Results on Instruction-Tuned LLM.<a href="#results-on-instruction-tuned-llm" class="hash-link" aria-label="Direct link to Results on Instruction-Tuned LLM." title="Direct link to Results on Instruction-Tuned LLM.">​</a></h4><p>Tab. 5 에서 SmoothQuant 가 instruction-tuned LLM 에도 작동함을 보여준다.</p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-63-b3b4e742bdda4923c789df4e6d11dd1b.png" width="953" height="505" class="img_ev3q"></p><ul><li>OPT-IML-30B model 을 WikiText-2 와 LAMBADA dataset 으로 테스트했다. 결과는 SmoothQuant 가 W8A8 quantization 으로 model 정확도를 성공적으로 유지함을 보여준다. </li><li>반면 baseline 은 실패한다. SmoothQuant 는 Transformer model 의 quantization 어려움를 균형 있게 하는 범용 방법이다. </li><li>Instruction-tuned LLM 의 architecture 는 vanilla LLM 과 근본적으로 다르지 않고 pre-training 과정도 비슷해서 SmoothQuant 가 여기에도 적용된다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-on-llama-models">Results on LLaMA models.<a href="#results-on-llama-models" class="hash-link" aria-label="Direct link to Results on LLaMA models." title="Direct link to Results on LLaMA models.">​</a></h4><p>LLaMA model 은 성능이 뛰어난 새로운 공개 language model 이다. 초기 실험에서 LLaMA model 은 OPT 나 BLOOM 같은 model 에 비해 activation outlier 문제가 덜 심각하다. 그럼에도 SmoothQuant 는 LLaMA model 에 잘 작동한다. </p><p><img loading="lazy" alt="Table 6" src="/assets/images/image-64-9e2bf35383f2f4c9bc2a1d5e339b2e70.png" width="983" height="472" class="img_ev3q"></p><p>Tab. 6 에서 LLaMA W8A8 quantization 의 초기 결과를 보여준다. SmoothQuant 는 성능 저하 거의 없이 W8A8 quantization 을 가능하게 한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-on-llama-2-falcon-mistral-and-mixtral-models">Results on Llama-2, Falcon, Mistral, and Mixtral models.<a href="#results-on-llama-2-falcon-mistral-and-mixtral-models" class="hash-link" aria-label="Direct link to Results on Llama-2, Falcon, Mistral, and Mixtral models." title="Direct link to Results on Llama-2, Falcon, Mistral, and Mixtral models.">​</a></h4><p>저자는 SmoothQuant 를 Llama-2, Falcon, Mistral, Mixtral 같은 다양한 architecture 의 최근 LLM 에 적용했다. 특히 Mixtral 은 Mixture of Experts (MoE) model 이다. </p><p><img loading="lazy" alt="Table 7" src="/assets/images/image-65-22a6c4ef07318385ed07fb72e774d347.png" width="965" height="1138" class="img_ev3q"></p><p>Tab. 7 의 결과는 SmoothQuant 가 이런 다양한 architecture 에서 W8A8 quantization 을 성능 손실 거의 없이 가능하게 함을 보여준다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="53-speedup-and-memory-saving">5.3 Speedup and Memory Saving<a href="#53-speedup-and-memory-saving" class="hash-link" aria-label="Direct link to 5.3 Speedup and Memory Saving" title="Direct link to 5.3 Speedup and Memory Saving">​</a></h2><p>이 섹션에서 SmoothQuant-O3 를 PyTorch 와 FasterTransformer 에 통합한 speedup 과 memory 절감을 측정한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="context-stage-pytorch-implementation">Context-stage: PyTorch Implementation.<a href="#context-stage-pytorch-implementation" class="hash-link" aria-label="Direct link to Context-stage: PyTorch Implementation." title="Direct link to Context-stage: PyTorch Implementation.">​</a></h4><p>4 sentences batch 의 all hidden states 를 한 번에 생성하는 end-to-end latency (context stage latency) 를 측정한다. 이 과정에서 peak GPU memory 사용량을 기록한다. SmoothQuant 는 <code>LLM.int8()</code> 과만 비교한다. 이는 all scales 에서 LLM 정확도를 유지하는 유일한 기존 quantization 방법이다. </p><p>Huggingface 에서 model parallelism 지원이 없어서 PyTorch 구현은 single GPU 에서만 측정한다. 그래서 OPT-6.7B, OPT-13B, OPT-30B 로 평가한다. FasterTransformer 에선 SmoothQuant 가 Tensor Parallelism 알고리즘과 잘 작동해서 OPT-13B, OPT-30B, OPT-66B, OPT-175B 를 single 및 multi GPU benchmark 로 테스트한다. 모든 실험은 NVIDIA A100 80GB GPU 서버에서 한다.</p><p><img loading="lazy" alt="Figure 8" src="/assets/images/image-66-64a96e56a90ba5129ccaa6ca61d57470.png" width="961" height="754" class="img_ev3q"></p><p>Fig. 8 에서 PyTorch 구현 기반 inference latency 와 peak memory 사용량을 보여준다. </p><ul><li>SmoothQuant 는 FP16 baseline 보다 항상 빠르다. Sequence length 256 일 때 OPT-30B 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.51</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.51 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1.51</span><span class="mord">×</span></span></span></span></span> speedup 을 얻는다. </li><li>Model 이 클수록 가속이 더 두드러진다. 반면 <code>LLM.int8()</code> 은 mixed-precision activation representation 의 large overhead 때문에 거의 항상 FP16 baseline 보다 느리다. </li><li>Memory 면에서 SmoothQuant 와 <code>LLM.int8()</code> 모두 FP16 model 의 memory 사용량을 거의 절반으로 줄인다. </li><li>SmoothQuant 가 완전히 INT8 <code>GEMM</code> 을 사용해서 약간 더 memory 를 절약한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="context-stage-fastertransformer-implementation">Context-stage: FasterTransformer Implementation.<a href="#context-stage-fastertransformer-implementation" class="hash-link" aria-label="Direct link to Context-stage: FasterTransformer Implementation." title="Direct link to Context-stage: FasterTransformer Implementation.">​</a></h4><p><img loading="lazy" alt="Figure 9" src="/assets/images/image-69-31b2d29fb73b619338737c080209f824.png" width="1976" height="706" class="img_ev3q"></p><ul><li>Fig. 9 (top) 에서 FasterTransformer 의 FP16 구현 대비 SmoothQuant-O3 가 single GPU 에서 OPT-13B, OPT-30B 의 실행 latency 를 최대 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.56</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.56 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1.56</span><span class="mord">×</span></span></span></span></span> 줄인다. </li><li>FasterTransformer 가 PyTorch 구현보다 OPT-30B 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">3 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">3</span><span class="mord">×</span></span></span></span></span> 이상 빠른데도 이는 도전적이다. 놀랍게도 larger model (OPT-66B, OPT-175B) 에서 SmoothQuant 는 절반 GPU (OPT-66B 는 2개 대신 1개, OPT-175B 는 8개 대신 4개) 로 비슷하거나 더 빠른 inference 를 한다. 이는 LLM serving 비용을 크게 줄일 수 있다. </li><li>Fig. 9 (bottom) 에서 SmoothQuant-O3 의 FasterTransformer 에서 memory 사용량이 거의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">2 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mord">×</span></span></span></span></span> 줄어든다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="decoding-stage">Decoding-stage.<a href="#decoding-stage" class="hash-link" aria-label="Direct link to Decoding-stage." title="Direct link to Decoding-stage.">​</a></h4><p>Tab. 8 에서 SmoothQuant 가 LLM 의 autoregressive decoding stage 를 크게 가속함을 보여준다.</p><p><img loading="lazy" alt="Table 8" src="/assets/images/image-67-e1b22af49d706b0fe9f55ab4595b1dc6.png" width="977" height="636" class="img_ev3q"></p><ul><li>SmoothQuant 는 FP16 대비 per-token decoding latency 를 지속적으로 줄인다 (최대 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.42</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.42 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1.42</span><span class="mord">×</span></span></span></span></span> speedup). </li><li>또 SmoothQuant 는 LLM inference 의 memory footprint 을 절반으로 해서 배포 비용을 크게 줄인다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="54-scaling-up-530b-model-within-a-single-node">5.4 Scaling Up: 530B Model Within a Single Node<a href="#54-scaling-up-530b-model-within-a-single-node" class="hash-link" aria-label="Direct link to 5.4 Scaling Up: 530B Model Within a Single Node" title="Direct link to 5.4 Scaling Up: 530B Model Within a Single Node">​</a></h2><p>SmoothQuant 를 500B 이상 model 로 확장해서 MT-NLG 530B 의 효율적이고 정확한 W8A8 quantization 을 가능하게 한다.</p><p><img loading="lazy" alt="Table 9" src="/assets/images/image-68-79718b04aabf26b49195a25009e219d1.png" width="975" height="499" class="img_ev3q"></p><p><img loading="lazy" alt="Table 10" src="/assets/images/image-70-a151fe67e1473ee8386e6b30e789aee9.png" width="972" height="703" class="img_ev3q"></p><ul><li>Tab. 9 와 10 에서 SmoothQuant 가 530B model 의 W8A8 quantization 을 정확도 손실 거의 없이 가능하게 한다. </li><li>줄어든 model 크기는 절반 GPU (16 to 8) 로 비슷한 latency 에 서비스할 수 있게 해서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo><mn>500</mn></mrow><annotation encoding="application/x-tex">&gt;500</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">500</span></span></span></span></span>B model 을 single node (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">8 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">8</span><span class="mord">×</span></span></span></span></span> A100 80GB GPU) 에서 서비스할 수 있다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="55-ablation-study">5.5 Ablation Study<a href="#55-ablation-study" class="hash-link" aria-label="Direct link to 5.5 Ablation Study" title="Direct link to 5.5 Ablation Study">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="quantization-schemes">Quantization schemes.<a href="#quantization-schemes" class="hash-link" aria-label="Direct link to Quantization schemes." title="Direct link to Quantization schemes.">​</a></h4><p>Tab. 11 에서 PyTorch 구현 기반으로 quantization scheme 별 inference latency 를 보여준다.</p><p><img loading="lazy" alt="Table 11" src="/assets/images/image-71-bff6b2136ce8e4d0a03d3abff86b053d.png" width="558" height="395" class="img_ev3q"></p><ul><li>Quantization granularity 가 coarse 할수록 (O1 에서 O3 로, per-token 에서 per-tensor 로, dynamic 에서 static 으로) latency 가 낮아진다. </li><li>Static quantization 은 runtime 에서 quantization step size 계산이 필요 없어서 dynamic quantization 보다 inference 를 크게 가속한다. </li><li>SmoothQuant 는 모든 설정에서 FP16 baseline 보다 빠르고, <code>LLM.int8()</code> 은 보통 느리다. 정확도가 허용한다면 coarse scheme 을 추천한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="migration-strength">Migration strength.<a href="#migration-strength" class="hash-link" aria-label="Direct link to Migration strength." title="Direct link to Migration strength.">​</a></h4><p>Weight 와 activation 의 quantization 어려움를 균형 맞추려면 적당한 migration strength <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> (Eq. 4) 를 찾아야 한다. Fig. 10 에서 OPT-175B 에서 LAMBADA 로 다른 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 의 효과를 ablation 했다. </p><p><img loading="lazy" alt="Figure 10" src="/assets/images/image-72-9c6ab8d900b6e1e4ef8f2b109c497273.png" width="873" height="616" class="img_ev3q"></p><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 가 너무 작으면 (&lt;0.4) activation 이 quantize 하기 어렵고, 너무 크면 (&gt;0.6) weight 가 quantize 하기 어렵다. </li><li>Sweet spot 지역 (0.4-0.6) 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 를 선택해야 weight 와 activation 모두 quantization error 가 작고 model 성능을 유지한다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="6-related-work">6 Related Work<a href="#6-related-work" class="hash-link" aria-label="Direct link to 6 Related Work" title="Direct link to 6 Related Work">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="large-language-models-llms">Large language models (LLMs).<a href="#large-language-models-llms" class="hash-link" aria-label="Direct link to Large language models (LLMs)." title="Direct link to Large language models (LLMs).">​</a></h4><p>Pre-trained language model 은 scale-up 으로 다양한 benchmark 에서 놀라운 성능을 달성했다. GPT-3 은 100B 이상의 첫 LLM 으로 few-shot/zero-shot learning 에서 인상적인 결과를 냈다. </p><p>이후 연구들은 500B 이상으로 scaling frontier 를 밀어붙였다. 하지만 language model 이 커질수록 inference serving 이 비싸고 도전적이 된다. 이 작업에서 저자는 제안된 방법이 OPT-175B, BLOOM-176B, GLM-130B, MT-NLG 530B 같은 공개된 가장 큰 LLM 을 quantize 해서 memory 비용을 줄이고 inference 를 가속함을 보여준다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="model-quantization">Model quantization.<a href="#model-quantization" class="hash-link" aria-label="Direct link to Model quantization." title="Direct link to Model quantization.">​</a></h4><p>Quantization 은 model 크기를 줄이고 inference 를 가속하는 효과적인 방법이다. CNN 과 transformer 에 효과적임이 입증됐다. </p><p>Weight equalization 과 channel splitting 은 weight 의 outlier 를 억제해서 quantization error 를 줄인다. 하지만 이 기술들은 LLM 의 주요 quantization bottleneck 인 activation outlier 를 해결하지 못한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="quantization-of-llms">Quantization of LLMs.<a href="#quantization-of-llms" class="hash-link" aria-label="Direct link to Quantization of LLMs." title="Direct link to Quantization of LLMs.">​</a></h4><p>GPTQ 는 weight 만 quantize 하고 activation 은 안 한다. ZeroQuant 와 nuQmm 은 per-token 과 group-wise quantization scheme 을 LLM 에 사용한다. 이는 customized CUDA kernel 이 필요하다. 이들의 가장 큰 평가 model 은 각각 20B, 2.7B 이고 OPT-175B 같은 LLM 의 성능을 유지하지 못한다. </p><ul><li><code>LLM.int8()</code> 은 mixed INT8/FP16 decomposition 으로 activation outlier 를 해결한다. 하지만 구현이 latency overhead 를 만들어 FP16 inference 보다 느릴 수 있다. </li><li>Outlier Suppression 은 non-scaling LayerNorm 과 token-wise clipping 으로 activation outlier 를 다룬다. 하지만 BERT, BART 같은 smaller language model 에서만 성공하고 LLM 에선 정확도를 유지하지 못한다 (Tab. 4). </li><li>저자의 알고리즘은 최대 176B (우리가 찾은 가장 큰 open-source LLM) 의 LLM 성능을 효율적인 per-tensor, static quantization scheme 으로 유지한다. Retraining 없이 off-the-shelf INT8 <code>GEMM</code> 을 사용해서 높은 hardware 효율성을 달성한다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="7-conclusion">7 Conclusion<a href="#7-conclusion" class="hash-link" aria-label="Direct link to 7 Conclusion" title="Direct link to 7 Conclusion">​</a></h3><p>저자는 <strong>SmoothQuant</strong> 라는 정확하고 효율적인 post-training quantization 방법을 제안한다. </p><p>이는 최대 530B parameter 의 LLM 에서 lossless 8-bit weight 와 activation quantization 을 가능하게 한다. </p><p>SmoothQuant 는 LLM 의 all <code>GEMM</code> 에 weight 와 activation quantization 을 가능하게 해서 mixed-precision activation quantization baseline 대비 inference latency 와 memory 사용량을 크게 줄인다. </p><p>SmoothQuant 를 PyTorch 와 FasterTransformer 에 통합해서 최대 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.56</mn><mo>×</mo></mrow><annotation encoding="application/x-tex">1.56 \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1.56</span><span class="mord">×</span></span></span></span></span> inference 가속과 memory footprint 절반을 달성했다. SmoothQuant 는 serving 비용을 줄이는 turn-key 솔루션으로 LLM 응용을 대중화한다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/quantization">Quantization</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/llm">LLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/offline-migrating">offline migrating</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/8-bit">8-bit</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-11-SmoothQuant.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/QuIP"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">QuIP: 2-Bit Quantization of Large Language Models With Guarantees</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#51-setups" class="table-of-contents__link toc-highlight">5.1 Setups</a></li><li><a href="#52-accurate-quantization" class="table-of-contents__link toc-highlight">5.2 Accurate Quantization</a></li><li><a href="#53-speedup-and-memory-saving" class="table-of-contents__link toc-highlight">5.3 Speedup and Memory Saving</a></li><li><a href="#54-scaling-up-530b-model-within-a-single-node" class="table-of-contents__link toc-highlight">5.4 Scaling Up: 530B Model Within a Single Node</a></li><li><a href="#55-ablation-study" class="table-of-contents__link toc-highlight">5.5 Ablation Study</a><ul><li><a href="#6-related-work" class="table-of-contents__link toc-highlight">6 Related Work</a></li><li><a href="#7-conclusion" class="table-of-contents__link toc-highlight">7 Conclusion</a></li></ul></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.31c2725c.js"></script>
<script src="/assets/js/main.6a697ae5.js"></script>
</body>
</html>