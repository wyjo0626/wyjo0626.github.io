<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-08-GIFT-SW">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/GIFT-SW"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/GIFT-SW"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/GIFT-SW" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/GIFT-SW" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.a1722e09.js" as="script">
<link rel="preload" href="/assets/js/main.481c1241.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Attacking/universal-adversarial-prompt">Attacking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Generalization/NoisyTune">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Flan-T5">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Generalization/Flat-LoRA">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Mixture/UF">Mixture</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Module</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Pruning/SMP">Pruning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Quantization</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Fine-Tuning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/QuIP">QuIP: 2-Bit Quantization of Large Language Models With Guarantees</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet">BitNet: Scaling 1-bit Transformers for Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/GIFT-SW">GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/LLM-FP4">LLM-FP4: 4-Bit Floating-Point Quantized Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8">BitNet a4.8: 4-bit Activations for 1-bit LLMs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/fp4">Optimizing Large Language Model Training Using FP4 Quantization</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/LoRA/QLoRA">LoRA</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning">Soft Prompt</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Engineering/CoT/Chain-of-Thought">Prompt Engineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Quantization</span><meta itemprop="position" content="4"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Fine-Tuning</span><meta itemprop="position" content="5"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs</span><meta itemprop="position" content="6"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2408.15300v1" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2408.15300v1</a></p><h1>Abstract</h1><p>Parameter Efficient Fine-Tuning (PEFT) 방법은 large language model(LLM)의 사용을 널리 확산시키며 인기를 얻었다. </p><p>최근 연구에 따르면, 성능에 큰 영향을 미치는 가중치의 small subset 이 있다는 사실이 밝혀졌다. </p><p>이러한 관찰을 바탕으로, 저자는 <strong>Gaussian noise Injected Fine Tuning of Salient Weights (GIFT-SW)</strong> 라는 새로운 PEFT 방법을 소개한다. </p><ul><li>이 방법은 salient columns 만 업데이트하고, non-salient columns 에는 Gaussian noise 를 주입하는 방식이다. </li><li>이러한 columns 를 식별하기 위해 이전 연구에서 사용된 다양한 metric 을 확장하고 통합하는 generalized sensitivity metric 을 개발했다. </li><li>LLaMA 모델을 사용한 실험에서 GIFT-SW 는 동일한 계산 자원 내에서 full fine-tuning 및 SOTA PEFT 방법을 능가하는 성능을 보였다. </li><li>또한, GIFT-SW 는 mixed-precision quantization 를 적용한 모델의 성능을 salient weights 를 full precision 으로 유지하면서 복원하는 실질적인 이점을 제공한다.</li></ul><h1>1 Introduction</h1><p>현대의 LLM 들은 unseen task 에서도 뛰어난 generalization 능력을 보여주지만, fine-tuning 은 여전히 이러한 모델의 성능을 향상시키거나, quantization, pruning, tensor decomposition 과 같은 compression techniques 가 적용된 이후 성능을 복원하기 위해 필수적이다. </p><p>하지만 large LLM 의 all parameters 를 fine-tuning 하는 것은 계산 및 메모리 측면에서 매우 부담이 크다. </p><p>이를 해결하기 위해 <strong>Parameter Efficient Fine-Tuning (PEFT)</strong> 방법들이 개발되어, 제한된 계산 및 메모리 자원 내에서 모델 성능을 향상시키는 것을 목표로 하고 있다.</p><p>하지만, 현재까지 PEFT 방법들은 full fine-tuning 의 정확도를 따라잡지 못하고 있으며, 이는 자원 사용을 최소화하면서도 성능 격차를 줄일 수 있는 새로운 접근법의 필요성을 강조한다. </p><p>또한 대부분의 PEFT 방법은 추가적인 parameter 를 더함으로써 계산 부담을 증가시키는 단점이 있다.</p><p>이를 해결하고 efficiently trained LLM 의 성능을 향상시키기 위해, 저자는 <strong>GIFT-SW</strong> 라는 새로운 PEFT 방법을 소개한다. </p><ul><li>이는 salient weights 의 small subset 만 업데이트하고, non-salient weights 에는 noise 를 주입하는 방식에 초점을 맞춘다.</li><li>이 방법의 개발은 이전 연구에서 도출된 관찰과 그들이 제기한 관련 질문에 기초하고 있다:<ul><li>이전 연구들은 small salient weights (subset of salient weights)가 post-training quantization (PTQ) 및 pruning 기법의 효과에 큰 영향을 미친다는 사실을 보여주었다. </li><li>또한, 일부 연구에서는 모델 기능에 중요한 &quot;universal neurons&quot; 을 식별하였으며, 이러한 salient weights 를 선택하고 업데이트하는 것이 중요함을 강조했다.</li></ul></li><li><strong>Question 1:</strong> salient weights 의 small subset 만 업데이트하는 것으로 모델의 조정이 충분히 가능한가?<ul><li>또한, 최근 연구들은 gradient step 전후로 noise 를 주입한 Perturbed Gradient Descent (PGD)가 수렴을 안정화하고 over-fitting 을 방지할 수 있음을 입증했다.</li></ul></li><li><strong>Question 2:</strong> noise injection 이 수렴에 도움이 되는가?<ul><li>PGD 는 quantization 과정을 근사하여 모델의 robustness 를 높이는 데 주로 사용된다.</li></ul></li><li><strong>Question 3:</strong> noise injection 이 robustness 를 향상시키는가?<ul><li>salient weights 를 선택하는 것은 quantization 및 pruning 에서 큰 과제이며, 이는 우리 방법의 핵심이 된다. </li><li>본 논문에서는 기존에 제안된 all saliency metric 을 generalization 한 공식을 도출하고, 그 효과를 비교하는 실험을 제시한다.</li></ul></li></ul><p>우리 연구의 주요 기여는 다음과 같이 요약할 수 있다:</p><ul><li>저자는 <strong>GIFT-SW</strong>라는 pre-trained 및 quantized LLM 을 위한 새로운 PEFT 방법을 소개한다. <ul><li>이 방법은 salient columns 의 가중치만 fine-tuning 하며, non-salient weights 에는 Gaussian noise 를 주입하고 training 중에는 동결시킨다.</li></ul></li><li>저자는 pre-trained LLM 에서 salient columns 를 식별하기 위한 sensitivity metric 을 generalization 했다. <ul><li>본 논문에서 제안된 일반형의 다양한 기존 및 새로운 인스턴스를 비교하고, 문헌에서 연구된 metric 을 능가하는 new metric 을 식별했다.</li></ul></li><li>실험을 통해 GIFT-SW 가 SOTA PEFT 방법 및 full fine-tuning baseline 을 대다수 zero-shot task 에서 능가함을 입증했다. <ul><li>LLaMA 모델에서 GIFT-SW 는 full parameters 의 3% 만 fine-tuning 하면서도, 사용되는 계산 자원은 10/1 수준으로 TÜLU2 모델과 유사한 정확도를 달성했다.</li></ul></li><li>GIFT-SW 는 low-rank adapters 에 비해 training dataset 의 size 에 대한 안정성이 더 우수함을 보여주었다.</li></ul><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-230-ceabb32640213487094ada3ef1e6eff7.png" width="1549" height="698" class="img_ev3q"></p><h1>2 Related Work</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-parameter-efficient-fine-tuning-of-llm">2.1 Parameter efficient fine-tuning of LLM<a href="#21-parameter-efficient-fine-tuning-of-llm" class="hash-link" aria-label="Direct link to 2.1 Parameter efficient fine-tuning of LLM" title="Direct link to 2.1 Parameter efficient fine-tuning of LLM">​</a></h2><p>가장 효율적인 방법 중 하나로는 <strong>LoRA</strong> 가 있는데, 이는 low-rank adapters 를 학습하는 방법이다. </p><ul><li>최근에는 adapters 의 initialization 개선하거나, sparse adapters 를 추가하여 pre-trained weights 의 low-rank representation 을 강화하는 방식이 제안되었다. </li><li>또한, <strong>DoRA</strong> 는 pre-trained weights 의 magnitude 와 direction components 를 함께 fine-tuning 하는 방법으로, 다양한 fine-tuning task 에서 상당한 성능을 달성했다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-salient-weights-in-llms">2.2 Salient Weights in LLMs<a href="#22-salient-weights-in-llms" class="hash-link" aria-label="Direct link to 2.2 Salient Weights in LLMs" title="Direct link to 2.2 Salient Weights in LLMs">​</a></h2><p><strong>Salient weights</strong> 의 식별은 weight pruning 의 주요 문제 중 하나다. </p><ul><li>최근, LLM 에서 이러한 가중치를 식별하는 여러 방법들이 제안되었는데, 대표적으로 <strong>SparseGPT</strong>, <strong>Wanda</strong>, <strong>OWL</strong> 이 있다.</li><li>또한, small subset 의 input activation 에서 발생하는 <strong>outliers</strong> 가 LLM 의 성능에 큰 영향을 미친다는 연구가 있었으며, 이는 activation outliers 와 salient weights 사이의 연관성을 강조한다. </li><li>이후 여러 <strong>Post-Training Quantization (PTQ)</strong> 방법들이 similar 또는 identical pruning metrics 를 사용하여 이러한 salient weights 를 식별했다. </li><li>본 연구에서는 pruning 및 quantization 관련 문헌에서 제시된 metric 을 활용하여 salient weights 를 식별하는 방법을 generalization 했다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-structured-and-non-structured-salient-weights-selection">2.3 Structured and Non-structured Salient Weights selection<a href="#23-structured-and-non-structured-salient-weights-selection" class="hash-link" aria-label="Direct link to 2.3 Structured and Non-structured Salient Weights selection" title="Direct link to 2.3 Structured and Non-structured Salient Weights selection">​</a></h2><ul><li>Salient weights 는 all weights 의 few percent 에 불과하므로, 이를 저장하는 간단한 방법은 sparse matrix 를 사용하는 것이다. <ul><li>이 접근법은 계산적으로 합리적이며, 성능 향상을 가져온다는 것이 입증되었다. </li></ul></li><li>반면, activation 에서 발생하는 outliers 는 small weight channels 에만 집중된다는 것이 밝혀졌으며, 이를 기반으로 <strong>SmoothQuant</strong> 에선 calibration dataset 을 사용하여 outlier columns 을 식별하고, 이러한 columns 를 full precision 으로 유지한 채 다른 columns 는 quantized 상태로 변환했다. </li><li>이는 <strong>QUIK</strong> 에서 더 발전되었으며, outlier columns 을 full precision 으로 유지하고 나머지 columns 는 <strong>GPTQ</strong> 를 사용하여 quantization 했다. </li><li>유사한 절차가 <strong>OWQ</strong> 에서도 사용되었으며, <strong>OBD</strong> 기반 metric 이 사용되었다.  </li><li>계산 효율성을 고려하여 structured columns-wise salient weight selection 방식을 따랐다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="24-noise-injections">2.4 Noise Injections<a href="#24-noise-injections" class="hash-link" aria-label="Direct link to 2.4 Noise Injections" title="Direct link to 2.4 Noise Injections">​</a></h2><p>이 섹션에서는 <strong>Gaussian Noise Injections (GNI)</strong> 와 그 이점을 간략히 설명하고, quantization noise 와 GNI 가 동일하다는 것을 보인다. </p><p>따라서 GNI 는 further model quantization 에도 이점을 제공할 수 있다.</p><p>저자는 third question 을 검토하기 위해, quantization level 에 따라 noise 를 sampling 한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="guassian-noise-injections-gni">Guassian Noise Injections (GNI)<a href="#guassian-noise-injections-gni" class="hash-link" aria-label="Direct link to Guassian Noise Injections (GNI)" title="Direct link to Guassian Noise Injections (GNI)">​</a></h4><p>Perturbed Gradient Descent (PGD) 는 optimization procedure 중에 random distribution 의 sample 을 사용하여 weights 를 추가하거나 곱하는 방법을 수반한다.</p><p>Gaussian noise injection (GNI)은 gradient step 이후에 적용되며, non-convex optimization 에서 모델이 saddle point 에서 효율적으로 탈출하는 데 도움을 준다. </p><p>반면, gradient step 이전에 gaussian noise 를 주입하면 모델이 spurious local optimum 에서 탈출하는 데 도움이 된다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>←</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>τ</mi><mo stretchy="false">(</mo><mi mathvariant="normal">∇</mi><mi>f</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>ξ</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \theta_{t+1} \leftarrow \theta_t - \tau (\nabla f(\theta_t) + \xi) \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mopen">(</span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.04601em">ξ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-2.85em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span></span></div><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>←</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mi>τ</mi><mo stretchy="false">(</mo><mi mathvariant="normal">∇</mi><mi>f</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>+</mo><mi>ξ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \theta_{t+1} \leftarrow \theta_t - \tau (\nabla f(\theta_t + \xi)) \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mopen">(</span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.04601em">ξ</span><span class="mclose">))</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-2.85em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span></span></div><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>ξ</mi><mo>∼</mo><mi>N</mi><mo stretchy="false">(</mo><mi>μ</mi><mo separator="true">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \xi \sim N(\mu, \sigma^2) \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2241em;vertical-align:-0.3621em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8621em"><span style="top:-2.9979em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04601em">ξ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mopen">(</span><span class="mord mathnormal">μ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3621em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8621em"><span style="top:-2.8621em"><span class="pstrut" style="height:2.8641em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3621em"><span></span></span></span></span></span></span></span></span></div><ul><li>실용적인 측면에서 noise injections 는 자주 <strong>regularization</strong> 기법으로 논의되며, adversarial robustness 를 촉진하는 방법 또는 data augmentation 을 위한 방법으로 사용될 수 있다. </li></ul><p>본 연구에서는 gradient 를 평가하기 전에 GNI 를 사용한다. </p><ul><li>Orvieto et al. (2023) 은 training iteration 에서 단 one layer 에만 noise 를 추가해 <strong>variance explosion</strong> 을 방지할 것을 제안했으며, 이는 GNI 가 regularization 으로 작용한다는 사실을 이론적으로 증명했다. </li><li>Liu et al. (2023) 는 GNI 를 사용해 pre-trained LLM 을 fine-tuning 하는 방법을 연구했으며, 이때 noise distribution 의 layer-wise variance parameters 를 먼저 학습한 후 all weights 에 noise 를 추가했다. <ul><li>해당 방법이 layer-wise 로 independent noise 를 주입하는 방식보다 더 우수한 결과를 보였다.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="quantization-noise-injections-qni">Quantization Noise Injections (QNI).<a href="#quantization-noise-injections-qni" class="hash-link" aria-label="Direct link to Quantization Noise Injections (QNI)." title="Direct link to Quantization Noise Injections (QNI).">​</a></h4><p>networks 의 Quantization aware training (QAT) 는 quantization 후 성능 저하를 완화하기 위한 방법이다. 그러나 uniform quantization 는 non-differentiable operation 이다.</p><ul><li>이를 간단히 표현하면, scaling 과 rounding operations 의 조합으로 나타낼 수 있다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="normal">Δ</mi><mrow><mo fence="true">⌊</mo><mfrac><mi>W</mi><mi mathvariant="normal">Δ</mi></mfrac><mo fence="true">⌉</mo></mrow><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">Q(W) = \Delta \left\lfloor \frac{W}{\Delta} \right\rceil.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2223em;vertical-align:-0.35em"></span><span class="mord">Δ</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size1">⌊</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">Δ</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size1">⌉</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">.</span></span></span></span></span></li><li>QAT operation <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span></span> 측면에서 quantization noise <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi></mrow><annotation encoding="application/x-tex">\Omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Ω</span></span></span></span></span> 로 효율적으로 근사될 수 있으며, 이는 다음과 같다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo>=</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>−</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">\Omega = Q(W) - W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Ω</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span></span><ul><li>따라서 QNI 로 모델을 훈련하는 것은 gradient 평가 전에 GNI 를 사용하는 PGD 와 정확히 동일하다.</li></ul></li><li>uniform quantization 으로 유도된 noise <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi></mrow><annotation encoding="application/x-tex">\Omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Ω</span></span></span></span></span> 는 종종 input signal 과 상관이 없고, uniform distribution 을 가지며, white spectrum 을 가진 additive noise 로 modeling 될 수 있다. 그러나 실제로 이러한 조건은 종종 만족되지 않는다. <ul><li>따라서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi></mrow><annotation encoding="application/x-tex">\Omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Ω</span></span></span></span></span> 에 대해 gaussian distribution <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mi>μ</mi><mo separator="true">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(\mu, \sigma^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord mathcal" style="margin-right:0.14736em">N</span><span class="mopen">(</span><span class="mord mathnormal">μ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 를 사용하는 것이 일반적으로 개선된 결과를 낳는다.</li></ul></li><li>GNI 가 model training 에 유익하지만, noise parameters 를 선택하는 방법에 대한 명확한 답은 없다. </li><li>Liu et al.는 original weights 와 perturbed weights 간의 KL divergence 를 최소화하는 noise parameters 를 결정하였다. </li><li>Shin et al.는 gaussian distribution 의 parameter 를 quantization step 에 비례하는 스케일로 weight distribution 과 유사하게 식별하였다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="25-straight-through-estimator">2.5 Straight Through Estimator<a href="#25-straight-through-estimator" class="hash-link" aria-label="Direct link to 2.5 Straight Through Estimator" title="Direct link to 2.5 Straight Through Estimator">​</a></h2><p>가장 널리 사용되는 <strong>QAT</strong> 기법은 <strong>Straight Through Estimation (STE)</strong> 으로, gradient 를 다시 parameterization 하는 방법이다. </p><p>하지만, STE 는 <strong>biased</strong> 하며, quantization step 사이에서 weights 가 <strong>oscillation</strong> 할 수 있다는 단점이 있다. </p><p><strong>QNI</strong> 를 사용한 pre-trained model 은 STE 대신 QNI 를 사용하여 더 나은 성능을 달성했다.</p><h1>3 Method</h1><p>GIFT-SW 는 다음과 같은 단계로 이루어진다:</p><ol><li>small calibration dataset 을 바탕으로 선택된 sensitivity metric 을 사용하여 fixed number 의 salient columns 를 식별한다. <ul><li>이 수는 all layer 에서 일관되게 유지된다.</li></ul></li><li>weight matrix 의 columns 를 salient columns 와 regular columns 로 나눈다.</li><li>training 중, non-salient columns 의 weights 에 noise 를 추가하고 salient columns 의 가중치만 업데이트한다.</li></ol><p>따라서 이 방법은 두 가지 주요 설계 선택에 의존한다: 1) salient columns 를 선택하는 방법과 2) noise injection 의 parameters 이다. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-generalizing-parameter-sensitivity-metrics">3.1 Generalizing parameter sensitivity metrics<a href="#31-generalizing-parameter-sensitivity-metrics" class="hash-link" aria-label="Direct link to 3.1 Generalizing parameter sensitivity metrics" title="Direct link to 3.1 Generalizing parameter sensitivity metrics">​</a></h2><p>최근 여러 접근법들이 quantization 에 sensitive weights 를 식별하는 방법이나 pruning 에 대한 sensitivity 를 식별하는 방법을 제안하였다. </p><p>저자는 이러한 접근법을 perturbation 에 대한 sensitivity metric 으로 generalization 하고, 이를 적용하여 어떤 columns 이 성능 저하에 더 취약한지를 결정한다. </p><p>따라서, 저자는 이러한 columns 에 noise 를 추가하는 것을 피하고, 이를 model fine-tuning 에 사용한다.</p><p>제안된 sensitivity metric 은 weight matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span></span> 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span> column 에 대해 다음과 같이 표현된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>s</mi><mi>j</mi></msub><mo>=</mo><mi mathvariant="normal">∥</mi><msub><mi>D</mi><mi>j</mi></msub><msub><mi mathvariant="normal">∥</mi><mi>τ</mi></msub><mi mathvariant="normal">∥</mi><msub><mi>X</mi><mi>j</mi></msub><msubsup><mi mathvariant="normal">∥</mi><mi>ρ</mi><mi>γ</mi></msubsup><mo separator="true">,</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} s_j = \|D_j\|_{\tau}\|X_j\|^\gamma_\rho, \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2231em;vertical-align:-0.3616em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8616em"><span style="top:-3.0216em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.1132em">τ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7144em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ρ</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05556em">γ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3616em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8616em"><span style="top:-2.8616em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3616em"><span></span></span></span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">D_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> : weights perturbation 을 측정한 것</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">s_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> : 해당 column 의 perturbation 에 대한 sensitivity 를 나타낸다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span></span> : input feature </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span></span> : 1/2, 1, 2 중 하나를 가진다.</li></ul><p>Sec. 2.4 에서 논의한 바와 같이 GNI 를 perturbation source 로 사용할 수 있으며, 이 경우 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>j</mi></msub><mo>=</mo><msub><mi>W</mi><mrow><mo>:</mo><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>+</mo><mi>ξ</mi></mrow><annotation encoding="application/x-tex">D_j = W_{:,j} + \xi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">:</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.04601em">ξ</span></span></span></span></span> 로 계산된다. </p><p>그러나 noise <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ξ</mi></mrow><annotation encoding="application/x-tex">\xi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.04601em">ξ</span></span></span></span></span> 를 sampling 하는 것은 deterministic 하지 않다. </p><ul><li>noise <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ξ</mi></mrow><annotation encoding="application/x-tex">\xi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.04601em">ξ</span></span></span></span></span> 의 영향을 근사하기 위해 quantization 으로 인한 perturbation 을 활용하여 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>j</mi></msub><mo>=</mo><msub><mi>W</mi><mrow><mo>:</mo><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>−</mo><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mo>:</mo><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_j = W_{:,j} - Q(W_{:,j})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">:</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">:</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 로 설정한다. <ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mo>:</mo><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q(W_{:,j})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">:</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> : uniform symmetric quantization 이 적용된 weights 를 나타낸다.</li></ul></li></ul><p>각 layer 의 input feature <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span></span></span></span></span> 는 calibration dataset 에서 random sentence 들을 사용하여 계산된다. </p><p>이후 각 column 에 대한 sensitivity values <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">s_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 가 추정되고, highest values columns 가 salient columns 로 식별된다. </p><hr><p>Eq. 4 에서 제시된 metric 은 최근 quantization 에 대한 연구에서 다루어진 metrics 와 밀접한 관련이 있다. </p><ul><li>예로, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><mi>X</mi><msub><mi mathvariant="normal">∥</mi><mi mathvariant="normal">∞</mi></msub></mrow><annotation encoding="application/x-tex">\|X\|_{\infty}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∥</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∞</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> metric 은 QUIK 과 SmoothQuant 에서 사용되었으며, OWQ 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>j</mi></msub><mi mathvariant="normal">∥</mi><msub><mi>D</mi><mi>j</mi></msub><msubsup><mi mathvariant="normal">∥</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\lambda_j \|D_j\|_2^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1002em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em"><span></span></span></span></span></span></span></span></span></span></span> metric 을 채택했다.<ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>j</mi></msub><mo>=</mo><mi mathvariant="normal">∥</mi><msub><mi>X</mi><mi>j</mi></msub><msubsup><mi mathvariant="normal">∥</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\lambda_j = \|X_j\|_2^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1002em;vertical-align:-0.2861em"></span><span class="mord">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em"><span></span></span></span></span></span></span></span></span></span></span> : layer 의 quantization error 에 대한 Hessian matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span></span> 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span>-th diagonal element</li><li>이는 OWQ 에서 사용된 sensitivity metric 이 OBD 에서 제안된 network pruning 의 salience measure 를 column quantization 에 맞게 수정한 것임을 보여준다.</li></ul></li><li>Wanda 에서 제안된 metric 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><msub><mi>D</mi><mi>j</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>1</mn></msub><mi mathvariant="normal">∥</mi><msub><mi>X</mi><mi>j</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\|D_j\|_1\|X_j\|_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 로 element-wise variant<ul><li>이는 pruning 을 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">D_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 에 대한 perturbation source 로 사용한 Eq. 4 에서 쉽게 도출할 수 있다.</li></ul></li></ul><p>Wanda 와 달리, 저자는 general Eq. 4 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mi mathvariant="normal">∞</mi></msub></mrow><annotation encoding="application/x-tex">l_\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> norm 을 사용하는데, 그 이유는 calibration dataset 에 포함된 예시들이 input feature values 의 차이를 유도하기 때문이다. </p><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">l_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> norm 을 사용하면 input channel 전반에 걸쳐 값이 평균화되어 activation 의 outlier values 가 lower values 에 의해 가려질 수 있다. <ul><li>동일한 결론을 weight error 에도 적용할 수 있다. </li></ul></li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">l_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0197em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> norm 의 경우, 각 channel 에 대한 error 는 quantized weights 와 original weights 간의 모든 차이를 포함한다. </li><li>따라서 rare considerable error 는 많은 small deviation 에 의해 완화될 수 있다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-quantization-noise-injection">3.2 Quantization Noise Injection<a href="#32-quantization-noise-injection" class="hash-link" aria-label="Direct link to 3.2 Quantization Noise Injection" title="Direct link to 3.2 Quantization Noise Injection">​</a></h2><p>QNI 를 활용한 fine-tuning 절차를 개선하기 위해 저자는 sensitive weights 에 perturbation 을 적용하지 않는다. </p><p>따라서 fine-tuning procedure 에서 perturbation 에 민감하거나 salient columns 를 식별한 후, all layers 에 걸쳐 non-salient columns 에만 quantization noise 를 주입한다 (Fig. 2 참고).</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-231-1b91028fe2a13193b56fcc44ab684627.png" width="571" height="815" class="img_ev3q"></p><ul><li>Gaussian noise 의 scale parameters 는 각 layer 의 quantization step size 에 따라 결정된다. </li><li>모델의 주어진 layer 의 weight matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span></span> 에 대해 noise injection process 는 다음과 같이 설명할 수 있다. <ul><li>training phase 의 forward pass 동안 저자는 먼저 standard normal distribution <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(0,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathcal" style="margin-right:0.14736em">N</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span> 에서 noise matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi></mrow><annotation encoding="application/x-tex">\Omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Ω</span></span></span></span></span> 의 element 를 sampling 한다. </li><li>그 후 matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi></mrow><annotation encoding="application/x-tex">\Omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Ω</span></span></span></span></span> 는 quantization step size <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span></span></span></span></span> 로 스케일링된다. </li><li>마지막으로, scaled noise 를 non-salient columns 의 weight <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mo stretchy="false">[</mo><mo>:</mo><mo separator="true">,</mo><mi>n</mi><mi>o</mi><mi>n</mi><mo>−</mo><mi>s</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo stretchy="false">]</mo></mrow></msub></mrow><annotation encoding="application/x-tex">W_{[:,non-salient]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0385em;vertical-align:-0.3552em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mrel mtight">:</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">]</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span></span></span></span></span> 에 추가한다. </li></ul></li></ul><p>noise injection operation <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">℧</mi></mrow><annotation encoding="application/x-tex">\mho</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6889em"></span><span class="mord amsrm">℧</span></span></span></span></span> 는 다음과 같이 주어진다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="normal">℧</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>W</mi><mrow><mo stretchy="false">[</mo><mo>:</mo><mo separator="true">,</mo><mi>s</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo stretchy="false">]</mo></mrow></msub><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>W</mi><mrow><mo stretchy="false">[</mo><mo>:</mo><mo separator="true">,</mo><mi>n</mi><mi>o</mi><mi>n</mi><mo>−</mo><mi>s</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo stretchy="false">]</mo></mrow></msub><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mtext>diag</mtext><mo stretchy="false">(</mo><mi mathvariant="normal">Δ</mi><mo stretchy="false">)</mo><mi mathvariant="normal">Ω</mi></mrow></mstyle></mtd></mtr></mtable></mrow><mo separator="true">,</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \mho(W) = \begin{cases} W_{[:,salient]}, \\ W_{[:,non-salient]} + \frac{1}{2} \text{diag}(\Delta) \Omega \end{cases}, \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.75em"><span style="top:-3.75em"><span class="pstrut" style="height:3.75em"></span><span class="mord"><span class="mord amsrm">℧</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em"><span style="top:-3.69em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mrel mtight">:</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">]</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span><span class="mpunct">,</span></span></span><span style="top:-2.25em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mrel mtight">:</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">]</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord text"><span class="mord">diag</span></span><span class="mopen">(</span><span class="mord">Δ</span><span class="mclose">)</span><span class="mord">Ω</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.25em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.75em"><span style="top:-3.75em"><span class="pstrut" style="height:3.75em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.25em"><span></span></span></span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>diag</mtext><mo stretchy="false">(</mo><mi mathvariant="normal">Δ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{diag}(\Delta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">diag</span></span><span class="mopen">(</span><span class="mord">Δ</span><span class="mclose">)</span></span></span></span></span> : vector <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span></span></span></span></span> 의 elements 로 이루어진 diagonal matrix</li><li>training 중에는 salient columns <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mo stretchy="false">[</mo><mo>:</mo><mo separator="true">,</mo><mi>s</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo stretchy="false">]</mo></mrow></msub></mrow><annotation encoding="application/x-tex">W_{[:,salient]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0385em;vertical-align:-0.3552em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mrel mtight">:</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">]</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span></span></span></span></span> 의 wegihts 만 업데이트되며, 다른 columns <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mo stretchy="false">[</mo><mo>:</mo><mo separator="true">,</mo><mi>n</mi><mi>o</mi><mi>n</mi><mo>−</mo><mi>s</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo stretchy="false">]</mo></mrow></msub></mrow><annotation encoding="application/x-tex">W_{[:,non-salient]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0385em;vertical-align:-0.3552em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mrel mtight">:</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">]</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span></span></span></span></span> 의 weights 는 고정된다. </li><li>salient weights 에 noise 를 주입하지 않는 이유는 small perturbation 이 모델 성능에 큰 영향을 미칠 수 있기 때문이다. </li><li>quantization step size <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span></span></span></span></span> 는 non-salient columns <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mo stretchy="false">[</mo><mo>:</mo><mo separator="true">,</mo><mi>n</mi><mi>o</mi><mi>n</mi><mo>−</mo><mi>s</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo stretchy="false">]</mo></mrow></msub></mrow><annotation encoding="application/x-tex">W_{[:,non-salient]}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0385em;vertical-align:-0.3552em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">[</span><span class="mrel mtight">:</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">]</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span></span></span></span></span> 의 weights 에만 적용된다.</li><li>weights 의 initial distribution 과 더 일치시키기 위해, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span></span></span></span></span> 에 포함된 quantization scale factors 는 각 row 에 대해 개별적으로 추정된다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>-s row 에 대한 scale factor <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Δ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Delta_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord">Δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 다음과 같이 계산된다:</li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi mathvariant="normal">Δ</mi><mi>i</mi></msub><mo>=</mo><mfrac><msub><mi>α</mi><mi>i</mi></msub><mrow><msup><mn>2</mn><mrow><mi>b</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>−</mo><mn>1</mn></mrow></mfrac><mo separator="true">,</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \Delta_i = \frac{\alpha_i}{2^{b-1} - 1}, \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.8769em;vertical-align:-0.6884em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1884em"><span style="top:-3.1884em"><span class="pstrut" style="height:3.1076em"></span><span class="mord"><span class="mord"><span class="mord">Δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7751em"><span style="top:-2.989em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord">1</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6884em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1884em"><span style="top:-3.1884em"><span class="pstrut" style="height:3.1076em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6884em"><span></span></span></span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span> : bit-width</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> : quantization parameter</li><li>quantization 방법에서 smaller bit-width <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span> 는 higher quantization noise 와 대응한다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> parameter 는 Appendix A 설명처럼 linear search 를 통해 weight error 를 최적화하여 추정된다.</li></ul><p>Eq. 5 와 6 에 따라 rows 전역의 injected noise variance 는 non-salient weights 의 distribution 에 의해 결정된다. </p><ul><li>salient columns 는 이 distribution 에서 제외되는데, salient weights 가 large quantization error 를 유발하여 row-wise scale factors 를 왜곡할 수 있기 때문이다. </li><li>이 접근법은 noise variance 를 최소화하는 데 도움을 주며, 이는 training 중 non-salient weights 의 deviation 을 줄이는 데 기여한다.</li></ul><p>이와 같이 noise 를 sampling 하여, 저자는 Sec. 6.3 에서 논의된 quantization pre-training 실험에 사용할 수 있다.</p><h1>4. Experiments</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-data">4.1 Data<a href="#41-data" class="hash-link" aria-label="Direct link to 4.1 Data" title="Direct link to 4.1 Data">​</a></h2><p>이전 연구를 따라, 저자는 instruction tuning task 에 집중한다. </p><ul><li>이를 위해 TULU-V2-Mix 를 주요 data source 로 사용하며, 이는 다양한 출처의 광범위한 instruction 을 포함하고 있다. </li><li>이 dataset 은 filtering 되어 있으며, 너무 크지 않으면서도 상당량의 data 를 포함하고 있으며, 이 집합에 조정된 모델은 우수한 성능을 보인다. </li><li>또한, OpenOrca dataset 을 사용하여 저자의 방법이 specific instruction set 에 의존하지 않음을 입증한다. </li><li>salient columns 를 찾기 위한 sensitivity metrics 는 Pile validation dataset 에서 512 random sentences 를 기반으로 추정된다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-baselines">4.2 Baselines<a href="#42-baselines" class="hash-link" aria-label="Direct link to 4.2 Baselines" title="Direct link to 4.2 Baselines">​</a></h2><p>저자는 full precision 및 quantization 실험 모두에 대해 여러 baseline 을 고려한다. 모든 baseline 은 LLaMA2-7b, LLaMA2-13b 및 LLaMA3-8b 에 적용된다. </p><p><strong>Full precision</strong> 버전에는 다음과 같은 baseline 이 포함된다:</p><ul><li><strong>LoRA</strong>: 널리 사용되는 adapter-based method</li><li><strong>DoRA</strong>: 현재의 all PEFT 를 초월하는 LoRA 의 수정된 버전이다.</li><li><strong>FT</strong>: all parameters 의 full fine-tuning 이다.</li></ul><p>prompt tuning 과 관련된 PEFT 방법은 adapter-based method 에 비해 성능이 저조하므로 포함하지 않는다. </p><p><strong>Quantized</strong> 버전은 {4, 3, 2} bit-widths 의 weight quantization baseline 으로 제시된다:</p><ul><li><strong>STE</strong>: pre-trained model 의 all parameters 에 대한 quantization-aware fine-tuning 이다. <ul><li>fine-tuning 중 all parameters 가 훈련되지만, 128 salient columns 은 quantization 없이 full precision 로 업데이트된다.</li></ul></li><li><strong>QUIK + LoRA</strong>: LoRA 를 QUIK quantization model 에 적용한 것이다. <ul><li>low-rank adapter 만 훈련되며, quantized weights 와 salient weights 는 고정된다. </li><li>QUIK 은 non-salient columns 의 quantization 를 위해 GPTQ 를 활용하면서 salient weights 는 full precision 을 유지하는 mixed-precision quantization method 이다. </li><li>QUIK 은 PTQ 방법 중 가장 높은 성능을 달성한다.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-evaluation-and-datasets">4.3 Evaluation and Datasets<a href="#43-evaluation-and-datasets" class="hash-link" aria-label="Direct link to 4.3 Evaluation and Datasets" title="Direct link to 4.3 Evaluation and Datasets">​</a></h2><p>저자는 HellaSwag, BoolQ, WinoGrande, PiQA, ARC-easy, ARC-challenge 에서 zero-shot 성능을 측정하기 위한 종합적인 평가를 수행한다. </p><p>평가에는 LM Eval Harness 를 사용하며, baseline 선택은 이전 연구와 유사하다. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="44-compute-budget">4.4 Compute Budget<a href="#44-compute-budget" class="hash-link" aria-label="Direct link to 4.4 Compute Budget" title="Direct link to 4.4 Compute Budget">​</a></h2><ul><li>모든 실험에서 모델의 salient columns 수는 128 로 고정된다. </li><li>또한, training budget 은 500 training iterations 로 고정되며, 별도로 명시되지 않는 한 다른 설정은 없다. </li><li>최근 연구에 따르면, larger dataset 으로 one epoch training 이 multiple epochs 로 less data 로 훈련하는 것보다 더 효과적이다. </li><li>따라서 all 500 iterations 은 instruction repetitions 없이 one epoch 내에서 수행된다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="45-training-details">4.5 Training Details<a href="#45-training-details" class="hash-link" aria-label="Direct link to 4.5 Training Details" title="Direct link to 4.5 Training Details">​</a></h2><ul><li>training 은 4 GPU(각 40GB) 로 500 iterations 수행되었다. </li><li>batch size 는 7b 모델의 경우 128, 13b 모델의 경우 64 로 설정되었다. </li><li>baselines 의 경우, LLaMA2 모델의 learning rate 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">3 \times 10^{-5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span></span>로, LLaMA3 모델의 경우 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1 \times 10^{-5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span></span> 로 설정하였다. </li><li>다양한 learning rate 를 실험하여 baselines 에 가장 유익한 값을 찾았다. </li><li>저자는 0.03 warmup ratio 로 cosine annealing scheduler 를 사용하였다. </li><li>LoRA 와 DoRA 의 alpha 및 dropout 은 원본 논문에서 명시된 대로 설정하였으며, rank 는 저자의 방법에서 learnable parameter 수와 맞추기 위해 64 로 설정하였다. </li><li>따라서 learnable parameter 수는 LLaMA2-7b 의 경우 160M, LLaMA2-13b 의 경우 250M, LLaMA3-8b 의 경우 167M 이다.</li><li>저자의 경우, LLaMA2 모델의 salient columns 에 대한 learning rate 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1 \times 10^{-4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></span> 로, LLaMA3 모델의 경우 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1 \times 10^{-5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span></span> 로 설정하였다. </li><li>salient columns 수는 128 로 고정하였으며, learnable parameter 수는 LLaMA2-7b 의 경우 174M, LLaMA2-13b 의 경우 272M, LLaMA3-8b 의 경우 176M 이다.</li><li>noise injection 을 사용한 full fine-tuning 의 경우, learning rate 은 LLaMA2 및 LLaMA3 모델에 대해 각각 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">3 \times 10^{-5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span></span> 및 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1 \times 10^{-5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span></span> 로 설정하였다. </li></ul><h1>5. Results</h1><p>이 섹션에서는 GIFT-SW 의 성능을 보여주고 Sec. 1 의 질문에 답한다. 간단히 말해, 저자의 결과는 다음과 같다:</p><ul><li><strong>Q1</strong>: salient weights 의 일부를 fine-tuning 하는 것이 low-rank adapter 를 사용하는 것과 유사한 결과를 생성한다는 것을 확인하였다.</li><li><strong>Q2</strong>: noise injection 이 모델 성능을 향상시킨다.</li><li><strong>Q3</strong>: noise injection 으로 훈련된 모델이 추가적인 성능 저하에 더 강건하다는 것을 확인할 수 없었다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-full-precision">5.1 Full Precision<a href="#51-full-precision" class="hash-link" aria-label="Direct link to 5.1 Full Precision" title="Direct link to 5.1 Full Precision">​</a></h2><p>full precision 모델에 대한 평가 기준에서의 평균 성능은 Tab. 1 에 제시되어 있다. </p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-232-fa67dbb38550dde0177e1e3da82b7cce.png" width="1543" height="452" class="img_ev3q"></p><ul><li>GIFT-SW 는 대부분의 model 과 instruction set 에서 일반적으로 우수한 메트릭을 보인다. </li><li>그러나 OpenOrca subset 에서 LLaMA3 의 성능이 다소 낮은 것을 관찰하였다. </li><li>이 문제는 아마도 learning rate 과 scheduler selection 에서 기인하며, 이는 outlier tuning 에 영향을 미칠 수 있다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-quantized-models">5.2 Quantized Models<a href="#52-quantized-models" class="hash-link" aria-label="Direct link to 5.2 Quantized Models" title="Direct link to 5.2 Quantized Models">​</a></h2><p>Tab. 2 에서는 다양한 precision (4, 3, 2)로 quantized model 의 평균 성능을 제시한다. </p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-233-2710200ec8e4ade3a19a0e3d09f320e4.png" width="1417" height="544" class="img_ev3q"></p><ul><li>4 및 3 bits 의 경우 GIFT-SW 는 STE 와 유사한 품질을 달성하지만, 후자는 상당히 더 많은 계산 자원을 요구한다. </li><li>2 bits 설정에서는 GIFT-SW 가 상당한 품질 개선을 보여주며, 두 번째 순위 모델을 5 points 이상 초과한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="53-comparison-with-tülu2">5.3 Comparison with TÜLU2<a href="#53-comparison-with-tülu2" class="hash-link" aria-label="Direct link to 5.3 Comparison with TÜLU2" title="Direct link to 5.3 Comparison with TÜLU2">​</a></h2><p>GIFT-SW 를 TÜLU2 모델과 비교한다. TÜLU2 모델은 instruction 과 DPO 를 사용한 fine-tuned LLaMA2 모델이다. </p><p>이러한 모델은 LLaMA2 수정 중에서 상위 성능을 보이지만, 상당한 계산 자원을 요구한다. </p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-234-3981e4e62e3152847fdf9a211947ab82.png" width="737" height="394" class="img_ev3q"></p><ul><li>Tab. 3 에서는 GIFT-SW 를 사용하여 상당한 lower computational budget (parameter 수와 iterations 수가 적음)으로 LLaMA2-7b 에 대해 유사한 결과를 얻고, LLaMA2-13b 에 대해서는 TÜLU2 를 초과하는 성능을 달성함을 보여준다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="54-scaling-properties">5.4 Scaling Properties<a href="#54-scaling-properties" class="hash-link" aria-label="Direct link to 5.4 Scaling Properties" title="Direct link to 5.4 Scaling Properties">​</a></h2><p>LLaMA2 및 LLaMA3 모델을 사용하여 GIFT-SW 와 baseline 의 성능을 data scaling 을 통해 탐구하는 실험을 수행하였다. </p><p>Fig. 1 에 보고된 결과는 LoRA 와 DoRA 가 data scaling 에 따라 불안정한 성능을 보이는 반면, 저자의 방법과 full fine-tuning 은 더 안정적임을 보여준다. </p><p>또한, 저자의 방법은 거의 all data budget 에서 일관되게 1 위를 차지하였다.</p><h1>6. Ablation</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="61-comparison-sensitivity-metrics">6.1 Comparison sensitivity metrics<a href="#61-comparison-sensitivity-metrics" class="hash-link" aria-label="Direct link to 6.1 Comparison sensitivity metrics" title="Direct link to 6.1 Comparison sensitivity metrics">​</a></h2><p>저자는 다양한 noise level (various perturbations magnitudes)에 대한 sensitivity metrics 를 연구하였으며, 이는 quantization precision 의 변화를 나타낸다. </p><ul><li>이 실험에서는 LLaMA2 와 TÜLU2 의 non-salient weights 에 대해 7B 및 13B parameter 를 가진 모델을 quantization 하였다.</li><li>QUIK 을 사용하여 모델이 quantization 되며, salient weights 는 업데이트되지 않는다. </li><li>salient weights 128 columns 를 선택하였다. </li></ul><p><img loading="lazy" alt="Table 5" src="/assets/images/image-236-be9075fdfae03266777927913f101b65.png" width="1533" height="768" class="img_ev3q"></p><ul><li>Tab. 5 의 zero-shot 작업에 대한 평균 결과는 대부분의 precision 에서, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi><mo>=</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\rho = \infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ρ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord">∞</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>=</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\tau = \infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord">∞</span></span></span></span></span> 로 Eq. (4) 에 의해 식별된 salient columns 이 가장 좋은 성능을 달성함을 보여준다. </li><li>input feature 의 squared <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">l2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord">2</span></span></span></span></span> norm (OWQ metric)으로 식별된 열은 3 및 2 bits 로 quantized TÜLU2 에 대해서만 더 나은 성능을 보인다. </li><li>input feature 만으로 salient columns 를 선택하는 것은(QUIK metric) 특히 2 bits 에서 성능 저하를 초래한다.</li><li>따라서 quantization noise 에 민감한 salient columns 를 식별하기 위해서는 weight quantization error 와 input activation 의 maximum values 를 모두 고려해야 한다. </li><li>결과를 바탕으로, 저자는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi><mo>=</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\rho = \infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ρ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord">∞</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo>=</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\tau = \infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord">∞</span></span></span></span></span> 로 설정된 sensitivity metrics 을 선택하였다. </li><li>그러나 결과는 optimal sensitivity metrics 을 선택하는 명확한 규칙을 제시하지 않으며, 성능은 bit-widths 와 모델에 따라 다르게 나타난다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="62-noise-injection-impact">6.2 Noise Injection Impact<a href="#62-noise-injection-impact" class="hash-link" aria-label="Direct link to 6.2 Noise Injection Impact" title="Direct link to 6.2 Noise Injection Impact">​</a></h2><p>full precision setting 에서 QNI 의 중요성을 분석하기 위해, 저자는 GIFT-SW 가 적용된 LLaMA2 모델의 평균 성능을 noise injection 이 있는 경우와 없는 경우로 측정하였다. 후자의 경우, noise 는 entire weight matrix 에 적용된다. </p><p><img loading="lazy" alt="Table 6" src="/assets/images/image-237-4662e91626f28adac637aed32cd37cb0.png" width="736" height="378" class="img_ev3q"></p><ul><li>Tab. 6 의 결과는 QNI 가 outlier fine-tuning 의 성능을 일관되게 향상시킨다는 것을 보여준다. </li><li>QNI 는 entire network 에 적용할 경우 성능을 저하시킬 수 있지만, LLaMA3-8b 에는 여전히 이점이 있다. </li><li>특히, outlier fine-tuning 은 entire fine-tuning 보다 성능이 우수하지만, QNI 를 사용할 때만 그렇다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="63-quantization-before-and-after-training">6.3 Quantization Before and After Training<a href="#63-quantization-before-and-after-training" class="hash-link" aria-label="Direct link to 6.3 Quantization Before and After Training" title="Direct link to 6.3 Quantization Before and After Training">​</a></h4><p>QAT 관련 연구에 따르면, noise injection 으로 모델을 pre-training 하면 quantization 후 예측 능력이 향상된다고 알려져 있다. </p><p>이러한 관찰을 바탕으로, 이 섹션에서는 여러 설정에서 full precision salient columns 를 fine-tuning 한 후 quantized LLaMA2-7b 의 성능을 검토한다:</p><ul><li><strong>Pre-GIFT-SW</strong>: quantization 전에 GIFT-SW 를 적용한다.</li><li><strong>Post-GIFT-SW</strong>: quantization 후 GIFT-SW 를 적용한다.</li><li><strong>Salient FT</strong>: noise injection 없이 quantization 후 salient columns 를 fine-tuning 한다.</li></ul><p>pre-training 의 경우, model quantization 의 bit-widths 는 training 중 injected noise level 에 해당한다. </p><p>post-training 의 경우, noise injection 은 항상 4 bits 에서 수행된다.</p><p>Tab. 4 는 평가 기준에서 모델이 달성한 평균 점수를 제시한다. </p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-235-c831d18748886132f7e1d4aabd00785f.png" width="740" height="353" class="img_ev3q"></p><ul><li>4 bits quantization 의 경우, Pre-GIFT-SW 모델이 다른 모델보다 상당히 우수한 성능을 보인다. </li><li>그러나 3 및 2 bits 의 경우, quantization 후 salient columns 를 fine-tuning 하면 quantized model 의 더 나은 생성 능력을 달성할 수 있다. </li><li>이는 극도로 low-bit quantization 에 의해 유도된 quantization weight 가 original value 에서 상당한 편차를 보이기 때문이다. </li><li>그 결과 salient weights 와 quantized weights 간의 상관관계가 깨지고, pre-training 의 긍정적인 효과가 사라진다. </li><li>그러나 salient weights 의 post-training 은 다른 가중치와의 새로운 관계를 형성할 수 있으므로 모델이 부분적으로 생성 능력을 회복하게 된다.</li><li>또한 3 bits 에서 <strong>Post-GIFT-SW</strong> 및 <strong>Salient FT</strong> 를 적용한 모델이 유사한 점수를 주는 것을 관찰할 수 있다. </li><li>그러나 2 bits quantization 의 경우, noise injection 이 quantized model 의 fine-tuning 을 개선한다.</li></ul><h1>7 Conclusion</h1><p>본 논문에서는 GIFT-SW 를 소개한다. GIFT-SW 는  salient columns 의 weights 만 훈련하고, fixed weights 에는 quantization noise 를 주입하는 parameter-efficient fine-tuning 방법이다. </p><p>GIFT-SW 는 full precision 및 quantization setting 모두에서 이전의 fine-tuning 전략보다 우수하며, less compute budget 을 요구한다. </p><p>data scaling 실험에서 GIFT-SW 는 이전 PEFT 방법보다 더 큰 안정성을 보여주고, 거의 all data budget 에서 PEFT 및 full fine-tuning 보다 우수한 성능을 발휘한다. </p><p>저자의 ablation study 는 QNI 가 유익하지만, salient weights 와 함께 사용할 때만 그렇다는 것을 보여준다. </p><p>GIFT-SW 가 이전 방법보다 뛰어난 성능을 보이지만, quantization setting 에서 성능을 극대화하는 방법에 대한 추가 연구가 필요하다. </p><p>저자는 이전 연구에서 salient columns 선택 기준을 generalization 하고 다양한 parameter 를 경험적으로 비교하였다. </p><p>실험 결과 일부 기준이 다른 기준보다 더 나은 성능을 보이지만, 명확한 우세 선택이 없음을 보여준다. </p><p>이 중요한 발견은 이러한 기준을 개선하기 위한 추가 연구의 필요성을 강조한다.</p><h1>8 Limitations</h1><p>우리 연구의 주요 한계는 다음과 같다:</p><ol><li>GIFT-SW 의 결과는 LLaMA 모델에 대해서만 보고한다. 현재 고성능 생성 능력을 가진 다양한 open-source pre-trained LLM 이 존재하지만, LLaMA 모델이 현대 PEFT 및 quantization 방법의 효율성을 연구하는 데 가장 일반적으로 선택된다. 대부분의 LLM 간의 구조적 유사성에도 불구하고, 다양한 모델을 대상으로 한 향후 실험이 필요하다.</li><li>모델을 quantization 하는 데 GPTQ 방법만 사용하였다. 이 방법은 LLM 의 mixed precision quantization 에 널리 사용되며, quantization error 를 full precision 로 저장된 열에 집계하여 quantized model 의 성능을 향상시킨다. 그러나 GIFT-SW 는 기존의 RTN 이나 QuantEase 와 같은 다른 방법과 쉽게 통합될 수 있다.</li><li>GIFT-SW 실험 결과는 sensitivity metrics (4) 을 사용하여 선택된 salient columns 에 대한 것이다. 저자의 제안된 metric 은 분석을 기반으로 하여 대부분의 LLaMA2 사례에서 quantization 에 대한 salient columns 의 high sensitivity 를 보여준다. 그러나 다른 sensitivity metrics 이 GIFT-SW 와 mixed precision quantization 에서 더 나은 성능을 낼 수 있다.</li><li>salient weights 를 fine-tuning 하기 위한 noise parameter 는 QNI 접근 방식을 사용하여 결정된다. 그러나 다른 noise distribution 도 fine-tuning process 를 향상시킬 수 있다. optimal noise distribution 을 식별하는 것은 본 논문의 범위를 벗어난다.</li><li>본 연구에서는 LLM 의 효과적인 fine-tuning 을 위한 GIFT-SW 알고리즘 개발에 초점을 맞추었지만, 알고리즘에 대한 CUDA 커널의 계산 효율적인 구현을 제공하지 않는다. 향후 GIFT-SW 에 대한 CUDA 커널은 QUIK 및 OWQ 의 코드를 기반으로 개발할 수 있다.</li><li>GIFT-SW 는 크기와 이전 연구에서의 높은 벤치마크 결과를 기준으로 선택된 몇 가지 fine-tuning instruction set 으로만 훈련되었다. 그러나 fine-tuning set 의 수를 확장하면 실험을 더욱 포괄적으로 만들 수 있다.</li><li>저자는 다양한 이전 연구에서 유래된 6 benchmark 를 사용하여 방법을 평가하였다.</li></ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/gift-sw">GIFT-SW</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/lo-ra">LoRA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/low-rank">Low-Rank</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/shifted-sparse-attention">Shifted Sparse Attention</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/s-2-attn">S2-Attn</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/sf-attn">SF-Attn</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-08-GIFT-SW.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/LLM-FP4"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">LLM-FP4: 4-Bit Floating-Point Quantized Transformers</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-parameter-efficient-fine-tuning-of-llm" class="table-of-contents__link toc-highlight">2.1 Parameter efficient fine-tuning of LLM</a></li><li><a href="#22-salient-weights-in-llms" class="table-of-contents__link toc-highlight">2.2 Salient Weights in LLMs</a></li><li><a href="#23-structured-and-non-structured-salient-weights-selection" class="table-of-contents__link toc-highlight">2.3 Structured and Non-structured Salient Weights selection</a></li><li><a href="#24-noise-injections" class="table-of-contents__link toc-highlight">2.4 Noise Injections</a></li><li><a href="#25-straight-through-estimator" class="table-of-contents__link toc-highlight">2.5 Straight Through Estimator</a></li><li><a href="#31-generalizing-parameter-sensitivity-metrics" class="table-of-contents__link toc-highlight">3.1 Generalizing parameter sensitivity metrics</a></li><li><a href="#32-quantization-noise-injection" class="table-of-contents__link toc-highlight">3.2 Quantization Noise Injection</a></li><li><a href="#41-data" class="table-of-contents__link toc-highlight">4.1 Data</a></li><li><a href="#42-baselines" class="table-of-contents__link toc-highlight">4.2 Baselines</a></li><li><a href="#43-evaluation-and-datasets" class="table-of-contents__link toc-highlight">4.3 Evaluation and Datasets</a></li><li><a href="#44-compute-budget" class="table-of-contents__link toc-highlight">4.4 Compute Budget</a></li><li><a href="#45-training-details" class="table-of-contents__link toc-highlight">4.5 Training Details</a></li><li><a href="#51-full-precision" class="table-of-contents__link toc-highlight">5.1 Full Precision</a></li><li><a href="#52-quantized-models" class="table-of-contents__link toc-highlight">5.2 Quantized Models</a></li><li><a href="#53-comparison-with-tülu2" class="table-of-contents__link toc-highlight">5.3 Comparison with TÜLU2</a></li><li><a href="#54-scaling-properties" class="table-of-contents__link toc-highlight">5.4 Scaling Properties</a></li><li><a href="#61-comparison-sensitivity-metrics" class="table-of-contents__link toc-highlight">6.1 Comparison sensitivity metrics</a></li><li><a href="#62-noise-injection-impact" class="table-of-contents__link toc-highlight">6.2 Noise Injection Impact</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.a1722e09.js"></script>
<script src="/assets/js/main.481c1241.js"></script>
</body>
</html>