<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/PEFT/Composition/2023-09-LongLoRA">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/LongLoRA"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models | My Site"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/LongLoRA"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/LongLoRA" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/LongLoRA" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.d2ad26d0.css">
<link rel="preload" href="/assets/js/runtime~main.a733c2c9.js" as="script">
<link rel="preload" href="/assets/js/main.484c2b15.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Chain-of-Thought">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">Composition</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">LoRA: Low-Rank Adaptation of Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoHa">FedPara: Low-Rank Hadamard Product For Communication-Efficient Federated Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/BitFit">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/IA³">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/DyLoRA">DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoKr">KronA: Parameter Efficient Tuning with Kronecker Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/AdaLoRA">Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ReLoRA">ReLoRA: High-Rank Training Through Low-Rank Updates</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/IncreLoRA">IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/Delta-LoRA">Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LongLoRA">LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SaLoRA">Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SoRA">Sparse Low-rank Adaptation of Pre-trained Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/COLA">Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/DoRA">DoRA: Weight-Decomposed Low-Rank Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/FLoRA">FLoRA: Low-Rank Adapters Are Secretly Gradient Compressors</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA+">LoRA+: Efficient Low Rank Adaptation of Large Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MeLoRA">MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ResLoRA">ResLoRA: Identity Residual Mapping in Low-Rank Adaption</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SIBO">SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ALoRA">ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/AutoLoRA">AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/BiLoRA">BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/HiddenKey">LoRA Meets Dropout under a Unified Framework</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA-Dropout">LoRA Dropout as a Sparsity Regularizer for Overfitting Control</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/PISSA">PISSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/DoRA2">DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MoRA">MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MiLoRA">MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MoSLoRA">Mixture-of-Subspaces in Low-Rank Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/OLoRA">OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/RoseLoRA">RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SinkLoRA">SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/GIFT-SW">GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Mixture/UniPELT">Mixture</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Module</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Pruning/SMP">Pruning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/fp4">Quantization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning">Soft Prompt</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Composition</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2309.12307" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2309.12307</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="초록">초록<a href="#초록" class="hash-link" aria-label="Direct link to 초록" title="Direct link to 초록">​</a></h3><p>LongLoRA 는 limited computation cost 로 pre-trained large language model(LLM)의 context sizes 를 확장하는 efficient fine-tuning 접근 방식이다. </p><p>일반적으로 long context sizes 로 LLM 을 훈련하는 것은 계산적으로 비용이 많이 들며, 상당한 training time 과 GPU 자원을 필요로 한다. 예로, 8192 의 context length 로 훈련하면 2048 의 context length 에 비해 16x computation cost 이 든다. </p><p>본 논문에서는 두 가지 측면에서 LLM 의 context extension 을 가속화한다. </p><p>한편, inference 시에 <em>dense global</em> attention 가 필요하지만, model 의 fine-tuning 은 <em>sparse local</em> attention 로 효과적이고 효율적으로 수행할 수 있다. </p><ul><li>shifted sparse attention(S<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn) 는 context extension 을 효과적으로 가능하게 하며, vanilla attention 와 유사한 성능을 유지하면서 computation cost 을 실질적으로 절감한다. <ul><li>특히, 이는 training 시 <em>두 줄의 코드</em> 만으로 구현할 수 있으며, inference 시에는 선택적이다. </li></ul></li><li>다른 한편으로, context extension 을 위한 parameter efficient fine-tuning 방식을 재검토한다. <ul><li>특히, context extension 을 위한 LoRA 는 trainable embedding 및 normalization 가 전제되어야 잘 작동한다는 것을 발견하였다. </li></ul></li><li>LongLoRA 는 improved LoRA 와 S<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn 을 결합한다. </li><li>LongLoRA 는 Llama2 model(7B/13B to 70B)에서 다양한 task 에 대해 강력한 실험적 결과를 보여준다. <ul><li>LongLoRA 는 Llama2 7B 를 4k context 에서 100k 로 확장하거나, Llama2 70B 를 single 8× A100 에서 32k 로 확장할 수 있다. </li><li>LongLoRA 는 model 의 context 를 확장하면서 original architecture 를 유지하며, Flash-Attention2 와 같은 대부분의 기존 기술과 호환된다.</li></ul></li><li>추가로, LongLoRA 와 저자의 long instruction-following LongAlpaca dataset 을 사용하여 supervised fine-tuning 을 수행한다. </li></ul><h1>1 Introduction</h1><p>large language model(LLM)은 일반적으로 pre-defined context sizes 로 훈련된다. </p><p>예로, LLaMA 는 2048 tokens, Llama2 는 4096 tokens 를 사용한다. </p><p>그러나 이러한 pre-defined size 는 long documents 를 요약하거나 long questions 에 답하는 등 많은 응용에서 LLM 을 제한한다. </p><p>이 제한을 해결하기 위해, 최근 몇 가지 연구는 LLM 을 longer context 로 훈련하거나 fine-tuning 한다. </p><p>그러나 long sequence 의 LLM 을 scratch training 하는 것은 computational challenges 를 제시하며, 기존의 pre-trained LLM 을 fine-tuning 하는 것도 상당히 비싸다. </p><ul><li>예로, Position Interpolation 은 LLaMA model 을 2k to 8k context 로 확장하기 위해 32 A100 GPU 를 사용하고, longer context 의 fine-tuning 에는 128 A100 GPU 를 사용하였다. </li><li>FOT 은 standard transformer training 을 위해 32 TPU 를 사용하고, LongLLaMA 에는 128 TPU 를 사용하였다. </li><li>이러한 computation resources 은 일반 연구자들에게는 감당하기 어려운 경우가 많아, 저자는 LLM 의 context window 를 효율적으로 확장할 수 있는 방법을 모색하게 된다.</li></ul><p>하나의 간단한 접근 방식은 pre-trained LLM 을 low-rank adaptation(LoRA) 을 통해 fine-tuning 하는 것이다. </p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-34-4e72459d213265927468d291214d9e6f.png" width="1660" height="678" class="img_ev3q"></p><ul><li>LoRA 는 self-attention blocks 의 linear projection layers 를 low-rank matrix 를 사용하여 수정하며, 일반적으로 efficient 하며 trainable parameters 수를 줄인다. </li><li>그러나 저자의 경험적 발견에 따르면, long context model 을 이 방식으로 훈련하는 것은 effective 및 efficient 하지 않다. <ul><li><em>effectiveness</em> 측면에선, plain low-rank adaptation 이 long context extension 에서 high perplexity 를 초래한다 (Tab. 2).<ul><li>e.g., <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">rank = 256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal" style="margin-right:0.03148em">ank</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">256</span></span></span></span></span> 의 higher value 로 증가시켜도 안화되지 않는다.</li></ul></li><li><em>efficiency</em> 측면에선, LoRA 를 사용하든 사용하지 않든, context sizes 가 확장됨에 따라 computation cost 이 극적으로 증가하며, 이는 주로 standard self-attention 메커니즘 때문이다. <ul><li>Fig. 1 처럼, LoRA 를 사용하더라도 standard Llama2 model 의 training time 이 context window 가 확장될 때 상당히 증가한다.</li></ul></li></ul></li></ul><p>이 논문에서는 LongLoRA 를 소개한다. </p><ul><li>LongLoRA 는 pre-trained LLM 의 context window 를 확장하는 efficient fine-tuning 접근 방식이다. </li><li>LoRA 는 full fine-tuning 을 근사하기 위해 low-rank weights update 를 사용한다. <ul><li>마찬가지로, 저자는 short attention 가 training 중 long context 를 근사할 수 있음을 발견하였다. </li></ul></li><li>저자는 shifted sparse attention (S<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn) 를 standard self-attention 의 efficient substitute 으로 제시한다. </li><li>Fig. 2 처럼, 저자는 context length 를 여러 groups 으로 나누고 각 group 에서 attention 를 개별적으로 수행한다. <ul><li>half attention head 에서, 저자는 tokens 를 half group size 만큼 shift 하여 neighboring groups 간의 information flow 을 보장한다. </li><li>예로, S<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn 을 group size 2048 로 사용하여 총 8192 context length training 을 근사한다. </li><li>이는 Swin Transformer 와 high-level spirit 을 공유한다.</li></ul></li></ul><p>S<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn 을 통해 fine-tuned model 은 inference 중 original attention architecture 를 유지한다. </p><ul><li>이는 대부분의 기존 optimization 및 infrastructure 를 용이하게 한다. </li><li>일반 LLM 에 대한 기술은 저자의 방법에도 적용할 수 있다.<ul><li>예로, Flash-Attention2 는 training 및 inference time 모두에서 저자의 방법과 호환된다. </li><li>그 이유는 short attention 가 LLM 의 pre-training steps 에서의 attention 방식과 유사하기 때문이다. </li><li>dilated attention 또는 sparse attention 과 같은 다른 efficient attention 방식은 standard style 과 큰 차이가 있으며, Tab. 6 에서와 같이 저자의 방식처럼 잘 작동하지 않는다.</li></ul></li></ul><p>저자는 trainable embedding 및 normalization layers 가 long context LoRA fine-tuning 의 핵심임을 경험적으로 보여준다. </p><ul><li>Tab. 2 처럼, embeddings (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo><mn>2</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\le 2\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7719em;vertical-align:-0.136em"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">2%</span></span></span></span></span>) 및 normalization layers (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo><mn>0.004</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">\le 0.004\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7719em;vertical-align:-0.136em"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8056em;vertical-align:-0.0556em"></span><span class="mord">0.004%</span></span></span></span></span>) 는 entire LLM 에서 parameters 의 작은 비율을 차지한다. </li><li>예로, Llama2 7B 에서 embeddings 은 2% 미만의 parameters 를 가지며, normalization 는 0.004% 이하의 parameters 를 가진다. </li><li>이 비율은 larger LLM 에서는 더 줄어든다.</li></ul><p>실험에서 LongLoRA 가 효과적이고 효율적임을 보여준다. </p><ul><li>저자는 Llama2 7B, 13B 및 70B 의 context window 확장에 대한 실험 결과를 제시한다. </li><li>Position Interpolation 의 실험 설정을 따르며, proper position embeddings 으로 model fine-tuning 한다. </li><li>trained models 은 full-attention 및 fully fine-tuning 결과와 비교할 수 있는 성능을 달성하며, computation cost 은 훨씬 적다 (Fig. 1 참조). </li><li>LongLoRA 는 single 8× A100 에서 Llama2 7B 를 최대 100k context 로, 또는 70B model 을 최대 32k 로 fine-tuning 할 수 있다.</li><li>또한, 저자는 LongAlpaca 라는 long instruction-following dataset 을 사용하여 supervised fine-tuning(SFT)을 위한 솔루션을 제시한다. </li><li>저자의 LongLoRA model 은 long questions 과 해당 답변으로 additional fine-tuning 된다. </li><li>저자는 기술 논문, 공상 과학 및 기타 책을 위한 다양한 유형의 질문을 설계한다. SFT 는 LLM 의 대화 능력을 향상시키는 데 중요하다.</li></ul><h1>2 RELATED WORK</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="long-context-transformers">Long-context Transformers<a href="#long-context-transformers" class="hash-link" aria-label="Direct link to Long-context Transformers" title="Direct link to Long-context Transformers">​</a></h4><p>long context 를 지원하기 위해 많은 연구가 진행되었다. </p><p>이러한 접근 방식 중 일부는 retrieval-based, related documents 를 가져오고 retrieved results 를 context 에 포함시켜 language models 를 보강한다. </p><p>저자의 연구는 이러한 연구와 상호 보완적이며, 저자의 attention 메커니즘은 inference 중 변경되지 않는다. </p><p>많은 연구들은 multi-head attention 를 근사화된 방식으로 수정한다. 이러한 연구들은 self-attention 계산의 quadratic complexity 을 완화한다. </p><ul><li>예로, Longformer 와 BigBird 는 sparse attention 를 사용하여 long sequence 를 처리한다. </li><li>다른 연구들은 memory 메커니즘을 활용하여 past inputs 를 compression 하고 related tokens 를 검색한다. </li><li>이러한 연구들의 한계는 compression 이 full attention 과 큰 차이가 있어 pre-trained LLM 을 fine-tuning 하는 데에 비현실적이라는 점이다. </li></ul><p>비록 저자의 연구도 attention 메커니즘의 근사화를 포함하지만, standard attention 과 유사한 형태와 작은 차이를 가지며, 이는 pre-trained LLM 을 S<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn 으로 fine-tuning 하고 inference 중 full attention 를 유지할 수 있게 한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="long-context-llms">Long-context LLMs.<a href="#long-context-llms" class="hash-link" aria-label="Direct link to Long-context LLMs." title="Direct link to Long-context LLMs.">​</a></h4><p>LLM 은 일반적으로 pre-defined context length 로 훈련된다. </p><ul><li>예로, LLaMA 는 2048, Llama2 는 4096 tokens 를 사용한다. </li><li>long context 로 LLM 을 scratch training 하는 것은 대부분의 연구자들에게는 너무 비용이 크다. </li><li>최근 몇몇 연구들은 fine-tuning 을 통해 LLM 의 context length 를 확장하려고 시도했다. </li><li>Position Interpolation 은 rotary position encoding 을 수정하여 LLaMA 의 context length 를 32768 로 확장한다. </li><li>Focused Transformer 은 contrastive learning 을 사용하여 LongLLaMA 를 훈련한다. <ul><li>이들 모두 full fine-tuning 에 의존하며, 이는 계산적으로 비싸다(training 에 128 A100 GPU / 128 TPUv3 사용). </li></ul></li><li>Landmark attention 은 efficient approach 이지만 다소 손실이 있다. <ul><li>이는 long context input 을 retrievaled tokens 으로 compression 한다. </li></ul></li></ul><p>저자의 방법은 fine-tuning cost 를 대폭 절감하면서 original attention 의 품질을 유지한다. </p><p>저자의 방법은 inference 중 entire input 에 대한 접근을 유지하며, fine-tuning 시 original architecture 를 보존한다.</p><p>일부 문헌은 long context extension 을 위한 LLM 의 position embeddings 수정에 초점을 맞추고 있다. </p><ul><li>여기에는 Position Interpolation, positional Skipping, 그리고 out-of-distribution-based methods 등이 포함된다. </li><li>저자의 방법은 efficient fine-tuning 과 inference 중 original architecture 유지에 초점을 맞추며, position embeddings 방법들과는 orthogonal 하다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="efficient-fine-tuning">efficient fine-tuning<a href="#efficient-fine-tuning" class="hash-link" aria-label="Direct link to efficient fine-tuning" title="Direct link to efficient fine-tuning">​</a></h4><p>이 연구는 LoRA 를 기반으로 한다.</p><p>LoRA 외에도 여러 다른 parameter efficient fine-tuning 방법이 있다. </p><ul><li>여기에는 prompt tuning, prefix tuning, hidden state tuning, bias tuning, 그리고 masked weight learning 이 포함된다. </li><li>Input-tuning 은 input embeddings 을 tuning 하기 위해 adapter 를 도입한다. </li><li>input embeddings layers 도 저자의 방법에서는 trainable 하지만, long context extension 에는 충분하지 않다. </li></ul><p>저자는 실험에서 layer types 에 대한 포괄적인 분석을 수행하였다(Tab. 2). </p><p>기존 연구는 sparse mask 가 training cost 를 효과적으로 절감하고 성능 저하를 방지할 수 있음을 보여준다.</p><h1>3 LongLoRA</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-background">3.1 Background<a href="#31-background" class="hash-link" aria-label="Direct link to 3.1 Background" title="Direct link to 3.1 Background">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="transformer">transformer<a href="#transformer" class="hash-link" aria-label="Direct link to transformer" title="Direct link to transformer">​</a></h4><p>LLM 은 일반적으로 transformer 를 기반으로 구축된다. </p><ul><li>Llama2 를 보면, LLM model 은 embedding input layer 과 여러 decoder layers 로 구성된다. </li><li>각 decoder layers 는 self-attention module 을 포함한다. </li><li>이 module 은 input features 를 query, key, value set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>q</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><mi>v</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{q, k, v\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mclose">}</span></span></span></span></span> 로 mapping 하여, linear projection layers 와 weight matrices <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>W</mi><mi>q</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>v</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{W_q, W_k, W_v\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span> 를 사용한다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>q</mi><mo separator="true">,</mo><mi>k</mi><mo separator="true">,</mo><mi>v</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{q, k, v\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mclose">}</span></span></span></span></span> 를 주어진 후, output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi></mrow><annotation encoding="application/x-tex">o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">o</span></span></span></span></span> 는 다음과 같이 계산된다:</li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>o</mi><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>q</mi><msup><mi>k</mi><mi>T</mi></msup><mo stretchy="false">)</mo><mi>v</mi></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} o = \text{softmax}(qk^T)v \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2513em;vertical-align:-0.3757em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8757em"><span style="top:-2.9843em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">o</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3757em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8757em"><span style="top:-2.8757em"><span class="pstrut" style="height:2.8913em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3757em"><span></span></span></span></span></span></span></span></span></div><ul><li>output 은 linear layer 와 weight matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">W_o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 를 통해 projection 된다. 그 후 MLP layers 가 이어진다. </li><li>self-attention module 전후에 layer normalization 가 적용되며, 모든 decoder layers 후에 final normalization 가 수행된다.</li></ul><p>long sequence 의 경우, self-attention 는 computation cost 이 quadratic 에 비례하여 문제를 겪는다. </p><p>이는 training procedure 를 극적으로 늦추고 GPU memory costs 를 증가시킨다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="low-rank-adaptation">Low-rank Adaptation<a href="#low-rank-adaptation" class="hash-link" aria-label="Direct link to Low-rank Adaptation" title="Direct link to Low-rank Adaptation">​</a></h4><p>LoRA 는 pre-trained model 의 weights update 가 adaptation 중에 low intrinsic rank 를 가진다고 가정한다. </p><p>pre-trained weight matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W \in \mathbb{R}^{d \times k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span></span></span></span></span></span></span></span></span></span> 는 low-rank decomposition <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>W</mi><mo>=</mo><mi>W</mi><mo>+</mo><mi>B</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">W + \Delta W = W + BA</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">A</span></span></span></span></span> 로 업데이트된다. </p><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation encoding="application/x-tex">B \in \mathbb{R}^{d \times r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span></span></span></span></span></span></span></span></span></span></span></span></span> 및 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>r</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in \mathbb{R}^{r \times k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span></span></span></span></span></span></span></span></span></span></li><li>rank <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>≪</mo><mtext>min</mtext><mo stretchy="false">(</mo><mi>d</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r \ll \text{min}(d, k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">min</span></span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mclose">)</span></span></span></span></span></li><li>training 중에는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span></span> 는 고정되며 gradient updates 가 없고, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span> 만 learnable.<ul><li>이는 LoRA training 이 full fine-tuning 보다 훨씬 efficient 이유이다.</li></ul></li></ul><p>transformer 구조에서는 LoRA 가 attention weights (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">W_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">W_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">W_o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>)만 적응하고, MLP 및 normalization layers 를 포함한 다른 all layers 는 고정된다. </p><p>이러한 방식은 simple 및 parameter efficient 이다. </p><p>그러나, 저자는 attention weights 에 대한 low-rank adaptation 만으로는 long context extension 이 잘 작동하지 않음을 경험적으로 보여준다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-shifted-sparse-attention">3.2 Shifted Sparse Attention<a href="#32-shifted-sparse-attention" class="hash-link" aria-label="Direct link to 3.2 Shifted Sparse Attention" title="Direct link to 3.2 Shifted Sparse Attention">​</a></h2><p>standard self-attention 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 의 computation cost 가 드므로, long sequence 에서 LLM 의 memory costs 이 높고 속도가 느려진다. </p><p>이 문제를 해결하기 위해, 저자는 Shifted Sparse Attention (S<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn)을 제안한다. </p><p>이 방식은 Fig. 2 에 나타나 있다.</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-35-9713e42aca4ee28f311ae24e48a64bfc.png" width="1614" height="795" class="img_ev3q"></p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-36-f842851e898197f593a61f549d1db7ef.png" width="1635" height="723" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="pilot-study">Pilot Study.<a href="#pilot-study" class="hash-link" aria-label="Direct link to Pilot Study." title="Direct link to Pilot Study.">​</a></h4><p>Tab. 1 에서 full attention 과 fine-tuning 을 사용하여 훈련하고 테스트한 standard baseline model 을 구축하였다. </p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-37-00684409e6e8d8d11a347083c5fcbea5.png" width="1594" height="483" class="img_ev3q"></p><ul><li>이 model 은 다양한 context length 에서 일관되게 좋은 품질을 보인다. </li><li>첫 번째 시도는 short attention, 즉 Fig. 2 의 <em>pattern</em> 1 만을 사용하여 훈련하는 것이다. </li><li>long context 의 경우, high cost 는 주로 self-attention module 에서 발생한다. <ul><li>따라서 이 시도에서는 input 이 길어 여러 groups 로 나누어 self-attention 를 수행한다. </li><li>예로, model 은 training 과 test 단계에서 8192 tokens 를 input 으로 사용하지만, self-attention 는 각 group 에서 2048 size 로 수행된다. </li><li>group 수는  4 이다. </li><li>이 pattern 은 효율적이지만 very long context 에서는 여전히 작동하지 않는다. </li></ul></li><li>Tab. 1 에서 볼 수 있듯이, context length 가 증가함에 따라 perplexity 가 커진다. <ul><li>그 이유는 서로 다른 group 간의 information exchange 이 없기 때문이다.</li></ul></li></ul><p>information exchange 도입을 위해, Fig. 2 에 나타난 shifted pattern 을 포함한다. </p><ul><li>저자는 group partition 을 half self-attention head 에서 half group size 만큼 shift 한다. </li><li>8192 context length 를 예로, pattern 1 에선 first group 이 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>1</mn><mrow><mi>s</mi><mi>t</mi></mrow></msup></mrow><annotation encoding="application/x-tex">1^{st}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7936em"></span><span class="mord"><span class="mord">1</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7936em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span></span></span></span></span> 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>204</mn><msup><mn>8</mn><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">2048^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord">204</span><span class="mord"><span class="mord">8</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span> tokens 까지 self-attention 를 수행한다. </li><li>pattern 2 에선 group partition 이 1024 만큼 shift 된다. </li><li>first attention group 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>102</mn><msup><mn>5</mn><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">1025^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord">102</span><span class="mord"><span class="mord">5</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span> 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>307</mn><msup><mn>2</mn><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">3072^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord">307</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span> tokens 까지 시작되며, first 와 last 1024 tokens 는 동일한 group 에 속한다. </li><li>저자는 pattern 1 과 2 를 각각 half self-attention head 에서 사용한다. </li><li>이 방식은 additional computation cost 를 증가시키지 않으면서도 서로 다른 group 간의 information flow 를 가능하게 한다. </li><li>이 방식이 standard attention baseline 에 가까운 결과를 얻는다고 보여준다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="consistency-to-full-attention">Consistency to Full Attention.<a href="#consistency-to-full-attention" class="hash-link" aria-label="Direct link to Consistency to Full Attention." title="Direct link to Consistency to Full Attention.">​</a></h4><p>기존 efficient attention 설계는 long context LLM 의 효율성을 향상시킬 수 있다. </p><p>그러나 대부분은 long context fine-tuning 에 적합하지 않다. </p><p>이러한 transformer 는 scratch training 하기 위해 설계되었기 때문에 pre-training 에서 사용되는 standard full attention 과 차이가 있기 때문이다. </p><ul><li>Tab. 6 에선 S<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn 이 efficient fine-tuning 을 가능하게 할 뿐만 아니라 full attention test 도 지원함을 보여준다. </li><li>다른 attention 방식도 long context fine-tuning 에 사용할 수 있지만, model 은 fine-tuning 중에 사용된 attention 방식으로만 테스트할 수 있다. </li><li>shift 방식은 model 이 specific attention pattern 에 over-fitting 됨을 방지한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="easy-implementation">Easy Implementation.<a href="#easy-implementation" class="hash-link" aria-label="Direct link to Easy Implementation." title="Direct link to Easy Implementation.">​</a></h4><p>S<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn 은 구현이 쉽다. 두 가지 단계만 포함된다: (1) half self-attention head 에서 tokens shift, (2) token dimension 에서 batch dimension 으로 feature transposing. </p><p>두 줄의 코드면 충분하다. 저자는 Algorithm 1 에서 PyTorch style 의 코드를 제공한다.</p><p><img loading="lazy" alt="Algorithm 1" src="/assets/images/image-33-8684f15fdef7cfec605e5134087424d0.png" width="1709" height="624" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="33-long-context-를-위한-개선된-lora">3.3 long context 를 위한 개선된 LoRA<a href="#33-long-context-를-위한-개선된-lora" class="hash-link" aria-label="Direct link to 3.3 long context 를 위한 개선된 LoRA" title="Direct link to 3.3 long context 를 위한 개선된 LoRA">​</a></h3><p><img loading="lazy" alt="Table 2" src="/assets/images/image-38-57eedfb0c9d4ce7d1fc6af0da6b3e1c8.png" width="1585" height="521" class="img_ev3q"></p><p>LoRA 는 LLM 을 다른 dataset 에 적응시키기 위한 효율적이고 인기 있는 방법이다. </p><p>이는 full fine-tuning 보다 trainable parameters 와 memory costs 를 크게 절감한다. </p><p>그러나 short context length 에서 long context 로 LLM 을 적응시키는 것은 쉽지 않다. </p><p>저자는 LoRA 와 full fine-tuning 간의 명백한 차이를 경험적으로 관찰했다. </p><ul><li>Tab. 2 처럼, LoRA 와 full fine-tuning 간의 차이는 target context length 가 커질수록 커진다. </li><li>그리고 larger rank 를 가진 LoRA 도 이 차이를 줄일 수 없다.</li></ul><h1>4 Experiment</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-experimental-settings">4.1 Experimental Settings<a href="#41-experimental-settings" class="hash-link" aria-label="Direct link to 4.1 Experimental Settings" title="Direct link to 4.1 Experimental Settings">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models">Models<a href="#models" class="hash-link" aria-label="Direct link to Models" title="Direct link to Models">​</a></h4><p>저자는 pre-trained 7B, 13B, 70B Llama2 model 을 확장한다. </p><ul><li>maximum extended context windows size 는 7B model 의 경우 100k, 13B model 의 경우 65536, 70B model 의 경우 32768 이다. </li><li>이 model 들에 대한 position index 는 Position Interpolation 으로 재조정된다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="training-procedure">Training Procedure<a href="#training-procedure" class="hash-link" aria-label="Direct link to Training Procedure" title="Direct link to Training Procedure">​</a></h4><ul><li>저자는 Position Interpolation 에서 대부분의 training hyper-parameters 를 따르지만, 일부 경우에는 single 8× A100 GPU 을 사용하므로 batch size 가 더 작다. </li><li>all model 은 next tokens prediction objective 를 통해 fine-tuning 된다. </li><li>AdamW 를 사용하며, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">\beta_1 = 0.9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.9</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding="application/x-tex">\beta_2 = 0.95</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.95</span></span></span></span></span> 를 설정한다.</li><li>learning rate 는 7B 및 13B model 에는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2 \times 10^{-5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span></span>, 70B model 에는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span></span> 로 설정된다. </li><li>또한 linear learning rate warmup 을 사용한다. </li><li>weights decay 는 0 으로 설정된다. </li><li>per-device batch size 는 1 로 설정하고, gradient accumulation step 은 8 로 설정하여, 8 GPUs 를 사용할 때 global batch size 는 64 가 된다. </li><li>저자는 model 을 1000 steps 동안 훈련한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dataset">Dataset<a href="#dataset" class="hash-link" aria-label="Direct link to Dataset" title="Direct link to Dataset">​</a></h4><ul><li>저자는 Redpajama dataset 을 training 에 사용한다. </li><li>저자의 fine-tuned model 의 long sequence language modeling 성능을 평가하기 위해, book corpus dataset 인 PG19 와 cleaned Arxiv Math proof-pile dataset 에서 평가한다. </li><li>PG19 의 test split 을 사용하며, 이 split 은 100 documents 로 구성된다. </li><li>proof-pile dataset 의 경우, 이 dataset 의 test split 을 평가에 사용한다. </li><li>proof-pile data preprocessing 에는 Position Interpolation 을 따른다. </li><li>저자는 sliding window approach 를 사용하여 perplexity 를 평가하며, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">S = 256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">256</span></span></span></span></span> 을 따른다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-main-results">4.2 Main Results<a href="#42-main-results" class="hash-link" aria-label="Direct link to 4.2 Main Results" title="Direct link to 4.2 Main Results">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="long-sequence-language-modeling">Long-sequence Language Modeling.<a href="#long-sequence-language-modeling" class="hash-link" aria-label="Direct link to Long-sequence Language Modeling." title="Direct link to Long-sequence Language Modeling.">​</a></h4><p><img loading="lazy" alt="Table 3" src="/assets/images/image-39-ab91df54f6249bbb8f404e741900e2d0.png" width="1589" height="1035" class="img_ev3q"></p><p>Tab. 3 에서 proof-pile 과 PG19 dataset 에서 model 과 baseline 의 perplexity 를 보고한다. </p><ul><li>특정 training context length 에서, 저자의 model 은 longer context sizes 로 더 나은 perplexity 를 달성한다. <ul><li>이는 저자의 efficient fine-tuning 방법의 효과를 나타낸다. </li></ul></li><li>동일한 training 및 evaluation context length 의 경우, context sizes 가 증가함에 따라 perplexity 가 감소한다. <ul><li>예로, Llama2 7B model 의 경우, context window size 를 8192 에서 32768 로 증가시키면 perplexity 가 2.72 에서 2.50 으로 -0.22 감소한다. </li><li>Llama2 13B model 의 경우, perplexity 가 -0.28 감소한다.</li></ul></li></ul><p><img loading="lazy" alt="Table 4" src="/assets/images/image-40-61924f3a4a7a54560ad17adf893bb266.png" width="1594" height="530" class="img_ev3q"></p><p>Tab. 4 에서는 single 8× A100 에서 fine-tuning 할 수 있는 maximum context length 를 추가로 조사한다. </p><ul><li>Llama2 7B, 13B, 및 70B 를 각각 100k, 65536, 및 32768 context length 로 확장한다. </li><li>LongLoRA 는 이러한 극단적으로 큰 설정에서 유망한 결과를 얻는다. </li><li>또한, extended model 에서 작은 context sizes 에서 약간의 perplexity 저하를 발견했다. </li><li>이는 Position Interpolation 의 알려진 한계이다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="retrieval-based-evaluation">Retrieval-based Evaluation.<a href="#retrieval-based-evaluation" class="hash-link" aria-label="Direct link to Retrieval-based Evaluation." title="Direct link to Retrieval-based Evaluation.">​</a></h4><p>long context 에서 Retrieval experiments 를 수행하였다. </p><p>Tab. 5 에서는 LongChat 에서 도입된 topic retrieval task 에서 다른 open LLM 과 저자의 model 을 비교한다. </p><ul><li>이 task 는 매우 long chat 에서 target topic 을 검색하는 것이며, length 는 3k, 6k, 10k, 13k, 16k 로 변동된다. </li><li>LongChat 의 일부 question 이 16k 를 초과하므로, 18k context length 로 Llama2 13B 를 fine-tuning 하였다. </li><li>training cost 는 16k 와 유사하다. </li></ul><p><img loading="lazy" alt="Table 5" src="/assets/images/image-41-623fd311733687c1dc19b5eae7b735f9.png" width="1597" height="574" class="img_ev3q"></p><ul><li>저자의 model 은 이 작업에서 SOTA model 인 LongChat-13B 와 유사한 성능을 달성한다. </li><li>LongChat-13B 는 self-collected long context conversation text 에서 fully fine-tuned 반면, 저자의 model 은 next tokens generation 방식을 통해 RedPajama 에서 효율적으로 적응되었다. </li><li>저자의 model 은 16k evaluation 에서 LongChat-13B 를 약간 초과한다.</li></ul><p>Fig. 4 에선 Landmark Attention 을 따르는 passkey retrieval accuracy 를 제시한다. </p><ul><li>model 은 long documents 에서 random passkey 를 찾는 필요가 있다. </li><li>저자는 Llama2 7B 와 32768 context length 로 fine-tuned LongLoRA model 을 연구하였다. </li><li>passkey retrieval accuracy 를 1k to 34k, 대략 1k 간격으로 테스트하였다 (sentence length 를 정밀하게 조절할 수 없기 때문에). </li><li>각 document length 에 대해 model 을 10 번 테스트하며, random passkey value 가 다르다. </li></ul><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-42-ab47d621cecb904ae0a0a48383d25e98.png" width="1621" height="648" class="img_ev3q"></p><ul><li>저자의 model 은 33k 또는 34k 까지 합리적인 passkey retrieval accuracy 를 달성한다. </li><li>추가 fine-tuning 없이, max position embeddings 을 48k 로 수정하여 position interpolation 을 확장한 model 이 있다. 이는 Fig. 4 의 Ours 7B (extended PI)로 나타낸다. <ul><li>이 model 은 position interpolation 을 단순히 확장하여 longer documents 를 처리할 수 있다. </li><li>Fig. 4 의 주황색 점선처럼, 32k context length 로 fine-tuned model 은 33k to 45k range 에서 중간 정도의 검색 능력을 보인다(60%-90% 정확도).</li></ul></li><li>position interpolation 을 확장하더라도, Llama2 7B 는 4k context length 이후에 급격한 정확도 저하를 겪는다 (파란색 점선).</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-ablation-study">4.3 Ablation Study<a href="#43-ablation-study" class="hash-link" aria-label="Direct link to 4.3 Ablation Study" title="Direct link to 4.3 Ablation Study">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ablation-on-fine-tuning-steps">Ablation on Fine-tuning Steps.<a href="#ablation-on-fine-tuning-steps" class="hash-link" aria-label="Direct link to Ablation on Fine-tuning Steps." title="Direct link to Ablation on Fine-tuning Steps.">​</a></h4><p>Fig. 5 에서는 PG19 validation set 에서 8192 context length 로 extended Llama2 7B model 의 perplexity 와 fine-tuning 단계 간의 관계를 보고한다. </p><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-43-556ad1560ae9a757360c780f80540e81.png" width="1619" height="637" class="img_ev3q"></p><ul><li>fine-tuning 없이, step 0 에서 model 의 long context 능력은 제한적이다(e.g., 15.82 perplexity). </li><li>저자는 perplexity 가 빠르게 감소하는 것을 보여준다. </li><li>full fine-tuning 은 low-rank training 보다 더 빠르게 수렴한다. </li><li>두 방법은 200 steps 후에 가까워지며, 끝에서는 큰 차이가 없다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="attention-patterns">Attention Patterns<a href="#attention-patterns" class="hash-link" aria-label="Direct link to Attention Patterns" title="Direct link to Attention Patterns">​</a></h4><p>Tab. 6 에서는 fine-tuning 동안 다양한 attention pattern 의 영향을 보여준다.</p><ul><li>저자는 Llama2 7B model 을 32768 context length 로 fine-tuning 하고 PG19 validation set 에서 perplexity 를 평가한다. </li><li>저자는 다양한 설정 간의 exchange 방식을 조사한다.</li></ul><p><img loading="lazy" alt="Table 6" src="/assets/images/image-44-7cbe111f3df43cd3b327b98d8fd6bfee.png" width="1606" height="698" class="img_ev3q"></p><ul><li>LongLoRA 에서 사용한 shift 연산의 경우, 세 가지 선택이 있다: disabling, sequential layers 간의 shifting, attention head 간의 shifting.<ul><li>layers 간 shifting 은 허용되지만 최선은 아니다. </li><li>또한, all attention head 를 pattern 1 또는 pattern 2 로 설정하는 것은 작동하지 않는다.</li><li>left or right shifting 은 성능 차이가 미미하다.</li></ul></li><li>저자는 또한 dilated attention, block sparse attention, stride sparse attention 등의 다른 efficient attention 설계를 테스트하였다. <ul><li>dilated attention 에선 attention head 간에 dilate rate 를 1 to 2 로 균일하게 변동시켰다. </li><li>block sparse attention 에서는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">n = 4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">4</span></span></span></span></span> block-wise masking matrix 를 사용하고 block 을 left shifting 하여 원인적 방식으로 만들었다. </li><li>stride sparse attention 은 local 및 stride pattern 을 포함한다. 이러한 설정은 유사한 computation cost 을 공유한다. <ul><li>이러한 pattern 은 training-fromscratch transformers 에서 발명되었다. </li></ul></li></ul></li><li>이 실험은 pre-trained LLM 에서 long context adaptation 을 위한 이들의 능력을 조사하기 위함이다. <ul><li>dilated attention 은 full fine-tuning 에서 잘 작동하지만 low-rank adaptation 에는 잘 맞지 않는다. </li><li>stride sparse attention 으로 fine-tuning 하는 것은 해롭다. </li><li>이들은 full attention 과 큰 차이를 보이며, 이는 pre-training 단계에서 적용된다.</li></ul></li></ul><p><img loading="lazy" alt="Figure 7" src="/assets/images/image-45-abaacea5f9456009b21d59842fb940f1.png" width="1336" height="981" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-결론">5 결론<a href="#5-결론" class="hash-link" aria-label="Direct link to 5 결론" title="Direct link to 5 결론">​</a></h3><p>이 연구에서는 LLM 의 context length 를 효율적으로 확장할 수 있는 LongLoRA 를 제안한다. </p><ul><li>LongLoRA 는 standard full fine-tuning 보다 GPU memory costs 과 training time 을 덜 소모하며, accuracy loss 를 최소화한다. </li><li>architecture-level 에서, 저자는 training 중 standard self-attention pattern 을 근사화하기 위해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn 을 제안한다. <ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn 은 구현이 쉽고 코드 두 줄만 필요하다. </li><li>또한, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">S^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>-Attn 으로 훈련된 model 은 inference 중에도 original standard attention architecture 를 유지하므로 대부분의 기존 인프라와 최적화를 재사용할 수 있다. </li></ul></li><li>training-level 에서, 저자는 LoRA 와 full fine-tuning 간의 gap 을 trainable normalization 및 embeddings 으로 연결한다. <ul><li>저자의 방법은 single 8× A100 에서 Llama2 7B 를 100k context length 로, 70B model 을 32k context length 로 확장할 수 있다. </li><li>또한, long instruction dataset 인 LongAlpaca 를 제시하고 LongLoRA 를 사용하여 supervision fine-tuning 을 수행하였다.</li></ul></li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/long-lo-ra">LongLoRA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/lo-ra">LoRA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/low-rank">Low-Rank</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/long-context">LongContext</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/PEFT/Composition/2023-09-LongLoRA.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/PEFT/Composition/Delta-LoRA"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/PEFT/Composition/SaLoRA"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#초록" class="table-of-contents__link toc-highlight">초록</a></li><li><a href="#31-background" class="table-of-contents__link toc-highlight">3.1 Background</a></li><li><a href="#32-shifted-sparse-attention" class="table-of-contents__link toc-highlight">3.2 Shifted Sparse Attention</a><ul><li><a href="#33-long-context-를-위한-개선된-lora" class="table-of-contents__link toc-highlight">3.3 long context 를 위한 개선된 LoRA</a></li></ul></li><li><a href="#41-experimental-settings" class="table-of-contents__link toc-highlight">4.1 Experimental Settings</a></li><li><a href="#42-main-results" class="table-of-contents__link toc-highlight">4.2 Main Results</a></li><li><a href="#43-ablation-study" class="table-of-contents__link toc-highlight">4.3 Ablation Study</a><ul><li><a href="#5-결론" class="table-of-contents__link toc-highlight">5 결론</a></li></ul></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.a733c2c9.js"></script>
<script src="/assets/js/main.484c2b15.js"></script>
</body>
</html>