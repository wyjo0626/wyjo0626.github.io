<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/PEFT/Composition/2023-10-VeRA">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">VeRA: Vector-Based Random Matrix Adaptation | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/VeRA"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="VeRA: Vector-Based Random Matrix Adaptation | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/VeRA"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/VeRA" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/VeRA" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.3298527c.js" as="script">
<link rel="preload" href="/assets/js/main.aca6605f.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Attacking/universal-adversarial-prompt">Attacking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Generalization/NoisyTune">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Flan-T5">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">Composition</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">LoRA: Low-Rank Adaptation of Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoHa">FedPara: Low-Rank Hadamard Product For Communication-Efficient Federated Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/BitFit">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/IA³">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/DyLoRA">DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoKr">KronA: Parameter Efficient Tuning with Kronecker Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/AdaLoRA">Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ReLoRA">ReLoRA: High-Rank Training Through Low-Rank Updates</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/IncreLoRA">IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/Delta-LoRA">Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LongLoRA">LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SaLoRA">Structure-Aware Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/VeRA">VeRA: Vector-Based Random Matrix Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SoRA">Sparse Low-rank Adaptation of Pre-trained Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/COLA">Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ApiQ">ApiQ: Finetuning of 2-Bit Quantized Large Language Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ApiQ">ApiQ: Finetuning of 2-Bit Quantized Large Language Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/DoRA">DoRA: Weight-Decomposed Low-Rank Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/FLoRA">FLoRA: Low-Rank Adapters Are Secretly Gradient Compressors</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA+">LoRA+: Efficient Low Rank Adaptation of Large Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MeLoRA">MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ResLoRA">ResLoRA: Identity Residual Mapping in Low-Rank Adaption</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SIBO">SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ALoRA">ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/AutoLoRA">AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/BiLoRA">BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/HiddenKey">LoRA Meets Dropout under a Unified Framework</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA-Dropout">LoRA Dropout as a Sparsity Regularizer for Overfitting Control</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/PISSA">PISSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/DoRA2">DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ETHER">ETHER: Efficient Finetuning of Large-Scale Models with Hyperplane Reflections</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MoRA">MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MiLoRA">MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MoSLoRA">Mixture-of-Subspaces in Low-Rank Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/OLoRA">OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/RoseLoRA">RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SinkLoRA">SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/RandLoRA">RandLoRA: Full-Rank Parameter-Efficient Fine-Tuning of Large Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/DeLoRA">DeLoRA: Decoupling Angles and Strength in Low-Rank Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/DenseLoRA">DenseLoRA: Dense Low-Rank Adaptation of Large Language Models</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Generalization/Flat-LoRA">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Mixture/UF">Mixture</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Module</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Pruning/SMP">Pruning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Quantization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Selective/AdapterDrop">Selective</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning">Soft Prompt</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Engineering/ICL/Chain-of-Thought">Prompt Engineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Reinforce Learning/DPO/RLHF/Self-RLM">Reinforce Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Composition</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">VeRA: Vector-Based Random Matrix Adaptation</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>VeRA: Vector-Based Random Matrix Adaptation</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2310.11454" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2310.11454</a></p><h1>Abstract</h1><p>Low-rank adapation (LoRA) 는 large language models 의 finetuning 시 trainable parameter 수를 줄이는 인기 있는 방법이지만, 더 큰 model 로 확장하거나 사용자별 혹은 task 별로 수많은 adapted model 을 배포할 때 여전히 심각한 저장소 문제에 직면한다. </p><p>본 연구에서는 LoRA 와 동일한 성능을 유지하면서도 trainable parameter 수를 크게 줄이는 <strong>Vector-based Random Matrix Adaptation (VeRA)</strong> 를 제안한다. </p><ul><li>VeRA 는 모든 layer 에 걸쳐 공유되는 하나의 low-rank matrix pairs 를 사용하고, 대신 small scaling vector 를 학습함으로써 이를 달성한다. </li><li>저자는 GLUE 및 E2E benchmark, image classification task 에서 그 효과를 입증하였으며, 7B 및 13B language models 의 instruction-tuning 에서의 적용을 보여준다.</li></ul><h1>1 Introduction</h1><p>점점 더 크고 복잡해지는 language model 시대에서 특정 task 에 대한 efficient adaptation 문제는 그 어느 때보다 중요해지고 있다. 이러한 model 들은 강력한 기능을 제공하지만, 막대한 memory 요구량은 특히 personalized 사용을 위해 adaptation 할 때 심각한 병목이 된다. 예를 들어, 개별 사용자의 행동과 feedback 에서 지속적으로 학습하고 이를 반영하는 cloud 기반 운영체제 assistant 를 생각해 보자. 사용자별로 finetuned model 의 checkpoint 를 저장해야 하는 필요성은 저장 공간 요구량을 빠르게 증가시키며, 여러 task 가 결합될 경우 그 문제는 더욱 심각해진다.</p><p>이 문제는 GPT-4 와 같은 최신 SOTA model 에서 더욱 두드러진다. LoRA 와 같은 finetuning 기법은 효과적이지만 여전히 상당한 memory overhead 를 발생시킨다. 예를 들어, GPT-3 의 query 및 value layer 에 rank 16 의 LoRA 를 적용할 경우 single-precision 으로 저장했을 때 최소 288MB 의 memory 가 필요하다. 만약 이를 백만 개의 finetuned weight (e.g., 사용자별 1 개) 로 확장하면, 총 275TB 가 된다. </p><p>최근 language model 의 확산과 personalized assistant, edge device 등에서의 활용을 고려할 때, efficient adaptation 방법은 필수적이다. 저자는 더 효율적인 접근이 가능하다고 본다. Aghajanyan et al 은 pretrained model feature 의 intrinsic dimensionality 가 매우 낮다고 지적하였다. 이 연구들은 LoRA 에서 사용되는 trainable parameter 수보다 훨씬 낮은 수치를 보고하였으며, 이는 개선 여지가 있음을 시사한다.</p><p>동시에, 최근 연구들은 random weight 와 projection 을 사용하는 model 이 놀라운 효과를 보인다는 사실을 보여주었다. 이러한 model 들은 저자의 제안인 Vector-based Random Matrix Adaptation (VeRA) 의 기반이 된다. </p><ul><li>VeRA 는 weight matrix 를 reparametrization 함으로써 finetuning 동안 도입되는 trainable parameter 수를 최소화한다. </li><li>구체적으로, 저자는 layer 간에 공유되는 fixed random matrix pairs 를 두고 이를 적응시키기 위해 “scaling vector” 를 사용한다. </li><li>이 접근을 통해 single GPU 의 제한된 memory 안에 훨씬 더 많은 model 버전을 담을 수 있다.</li></ul><p>요약하면, 저자의 주요 기여는 다음과 같다.</p><ul><li>additional inference time cost 가 없는 새로운 finetuning 방법을 제안한다. 이 방법은 SOTA 인 LoRA 와 비교해 trainable parameter 수를 더 줄이면서도 유사한 성능을 낸다.</li><li>저자의 방법을 LoRA 및 다른 parameter-efficient adaptation 방법과 비교하여 natural language understanding (GLUE) 및 natural language generation (E2E) benchmark 에서 평가하고, instruction-following 및 image classification task 에서도 LoRA 와 비교한다.</li><li>방법의 개별 구성 요소와 그것이 성능에 미치는 영향을 더 잘 이해하기 위해 ablation study 를 수행한다.</li></ul><h1>2 Related Work</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="low-rank-adaptation-lora">Low-Rank Adaptation (LoRA).<a href="#low-rank-adaptation-lora" class="hash-link" aria-label="Direct link to Low-Rank Adaptation (LoRA)." title="Direct link to Low-Rank Adaptation (LoRA).">​</a></h4><p>LoRA 는 large pretrained language model 의 finetuning 과정에서 발생하는 계산적 문제에 대한 혁신적인 해결책을 제시한다. Hu et al. 이 제안한 이 방법은 finetuning 동안의 weight 변화량을 low-rank matrix 로 근사하여 학습해야 하는 parameter 수를 효과적으로 줄인다. </p><ul><li>LoRA 의 장점 중 하나는 대부분의 parameter 에 대해 gradient 계산 및 optimizer state 유지 필요성을 줄임으로써 finetuning 의 hardware 장벽을 크게 낮춘다는 점이다. </li><li>또한 quantized model weight 와도 함께 사용할 수 있어 요구 사항을 더욱 줄일 수 있다. </li><li>더 나아가 LoRA module 은 쉽게 교체 가능하여 task 전환이 효율적이고 자원 소모가 적다. </li><li>중요한 점은 adapter-based finetuning 접근과 달리, LoRA 는 배포 시 additional inference time cost 가 발생하지 않는다. 이는 trainable matrix 가 fixed weight 와 병합될 수 있기 때문이다.</li></ul><p>이를 기반으로 AdaLoRA 는 finetuning 중 low-rank matrix 의 rank 를 동적으로 조정하는 기법을 도입하여 LoRA 를 확장하였다. 핵심 아이디어는 importance metric 에 기반해 matrix 의 덜 중요한 구성 요소를 선택적으로 pruning 하여 parameter budget 을 최적으로 분배하는 것이다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="parameter-efficiency-in-existing-methods">Parameter Efficiency in Existing Methods.<a href="#parameter-efficiency-in-existing-methods" class="hash-link" aria-label="Direct link to Parameter Efficiency in Existing Methods." title="Direct link to Parameter Efficiency in Existing Methods.">​</a></h4><p>LoRA 와 같은 방법들은 finetuning 성능에서 상당한 향상을 보여주었지만 여전히 상당한 수의 trainable parameter 를 요구한다. Aghajanyan et al. 은 intrinsic dimension 의 upper bound 가 이러한 방법에서 일반적으로 사용되는 것보다 훨씬 작다고 보고하였다. 예를 들어, RoBERTa base 의 경우 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>90</mn></msub></mrow><annotation encoding="application/x-tex">d_{90}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">90</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 이 896 으로 보고되었으나, LoRA 논문에서는 해당 model 에 대해 0.3M 개의 trainable parameter 를 사용했다고 보고하였다. 이는 parameter 수를 더 줄일 수 있음을 시사한다. </p><p>AdaLoRA 는 parameter 를 더 중요한 layer 에 동적으로 할당함으로써 이 방향으로 나아가지만, 저자는 다른 접근을 통해 약간의 성능 저하를 감수하면서도 상당한 parameter 감소를 달성할 수 있다고 본다. 이는 저자가 다음 섹션에서 소개하는 방법의 기반이 된다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="random-models-and-projections">Random Models and Projections.<a href="#random-models-and-projections" class="hash-link" aria-label="Direct link to Random Models and Projections." title="Direct link to Random Models and Projections.">​</a></h4><p>Random matrix 와 projection 을 활용한 model efficiency 는 여러 연구에서 뒷받침되고 있다. </p><ul><li>Frankle &amp; Carbin 은 무작위로 초기화된 neural network 안에 학습 시 높은 성능에 도달할 수 있는 subnetwork 가 존재함을 확인하였다. </li><li>Ramanujan et al. 은 학습이 전혀 없는 경우에도 놀라운 성능을 낼 수 있는 subnetwork 가 존재함을 밝혔다. </li><li>Aghajanyan et al. 은 전체 공간으로 random projected few parameter 만 학습해도 full-parameter model 의 90% 성능을 달성할 수 있음을 보였다. </li><li>Ruiz et al. 은 text-to-image model 의 personalization 을 위해 random frozen matrix 를 LoRA 내부에 활용하는 parameter-efficient finetuning 방법을 제안하였다. </li><li>또한 Lu et al., Schrimpf et al., Frankle et al. 의 연구는 frozen, 무작위 초기화된 model 에서 작은 부분만 finetuning 해도 놀라운 성능을 낼 수 있음을 보여주었다.</li></ul><p>종합적으로, 이러한 연구들은 finetuning 방법에서 frozen random matrix 활용에 대한 강력한 근거를 제시하며, 본 논문에서 채택한 접근의 이론적 및 경험적 기반을 제공한다.</p><h1>3 Method</h1><p>이 절에서는 <strong>Vector-based Random Matrix Adaptation (VeRA)</strong> 를 소개한다. </p><p>VeRA 는 SOTA 방법인 LoRA 를 기반으로 확장한 parameter-efficient finetuning 방법이다. VeRA 의 핵심 혁신은 low-rank matrix 의 reparameterization 에 있다. 구체적으로, 모든 adapted layer 에 걸쳐 공유되는 무작위 초기화된 matrix pairs 를 고정(freeze)하고, 각 layer 별 adaptation 을 가능하게 하는 trainable scaling vector 를 도입한다 (Fig. 1 참조). LoRA 와 마찬가지로, 학습된 scaling vector 와 low-rank matrix 는 원래 weight 와 병합될 수 있어 additional inference latency 가 발생하지 않는다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-383-0742e0d663680d9c346ee66665f58631.png" width="1183" height="666" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-method-formulation">3.1 Method Formulation<a href="#31-method-formulation" class="hash-link" aria-label="Direct link to 3.1 Method Formulation" title="Direct link to 3.1 Method Formulation">​</a></h2><p>LoRA 는 새로운 task 에 large-language model 을 적응시키기 위해 두 개의 low-rank matrix 곱을 finetuning 한다. 정식적으로, pretrained weight matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W_0 \in \mathbb{R}^{m \times n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7713em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span></span> 에 대해 weight update <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">\Delta W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span></span> 는 다음과 같이 low-rank decomposition 으로 제한된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>h</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mi>x</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>W</mi><mi>x</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mi>x</mi><mo>+</mo><munder accentunder="true"><mrow><mi>B</mi><mi>A</mi></mrow><mo stretchy="true">‾</mo></munder><mi>x</mi><mo separator="true">,</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">h = W_0 x + \Delta W x = W_0 x + \underline{BA}x, \tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8833em;vertical-align:-0.2em"></span><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6833em"><span style="top:-2.84em"><span class="pstrut" style="height:3em"></span><span class="underline-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2em"><span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mpunct">,</span></span><span class="tag"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></div><p>여기서 gradient descent 를 통해 업데이트되는 parameter 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span> 이다. 이 근사는 원래 weight <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">W_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 를 고정(freeze)하면서 새로운 low-rank matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span> 만 최적화할 수 있게 한다. 이러한 matrix 는 rank 감소 특성 때문에 원래 matrix 보다 크기가 훨씬 작다. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>m</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in \mathbb{R}^{m \times r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7713em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>r</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">B \in \mathbb{R}^{r \times n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7713em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span></span> 이고, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>≪</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r \ll \min(m, n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">min</span><span class="mopen">(</span><span class="mord mathnormal">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span></span> 이 bottleneck 차원으로 사용된다.</p><p>반면, VeRA 는 다음과 같이 표현된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>h</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mi>x</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>W</mi><mi>x</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mi>x</mi><mo>+</mo><munder accentunder="true"><msub><mi mathvariant="normal">Λ</mi><mi>b</mi></msub><mo stretchy="true">‾</mo></munder><mi>B</mi><munder accentunder="true"><msub><mi mathvariant="normal">Λ</mi><mi>d</mi></msub><mo stretchy="true">‾</mo></munder><mi>A</mi><mi>x</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">h = W_0 x + \Delta W x = W_0 x + \underline{\Lambda_b} B \underline{\Lambda_d} A x \tag{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0333em;vertical-align:-0.35em"></span><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6833em"><span style="top:-2.69em"><span class="pstrut" style="height:3em"></span><span class="underline-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord">Λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6833em"><span style="top:-2.69em"><span class="pstrut" style="height:3em"></span><span class="underline-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord">Λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span><span class="mord mathnormal">A</span><span class="mord mathnormal">x</span></span><span class="tag"><span class="strut" style="height:1.1em;vertical-align:-0.35em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></div><ul><li>이 접근에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span> 는 고정되고(random, frozen), layer 간 공유되며, </li><li>scaling vector <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 가 trainable parameter 가 된다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 는 각각 diagonal matrices <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Λ</mi><mi>b</mi></msub></mrow><annotation encoding="application/x-tex">\Lambda_b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord">Λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Λ</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">\Lambda_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord">Λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 로 표현된다. </li><li>이 방식은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span> 의 행과 열을 효과적으로 scale 하거나 disable 할 수 있게 하여, 매우 적은 수의 trainable parameter 로 layer 별 adaptation 을 가능하게 한다.</li></ul><p>이때 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>m</mi><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation encoding="application/x-tex">B \in \mathbb{R}^{m \times r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7713em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>r</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in \mathbb{R}^{r \times n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7713em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span></span> 은 low-rank 일 필요가 없다. 왜냐하면 이들은 static 하며 별도로 값을 저장할 필요가 없기 때문이다. 대신 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span></span></span></span> 을 변화시키면 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>1</mn><mo>×</mo><mi>r</mi></mrow></msup></mrow><annotation encoding="application/x-tex">d \in \mathbb{R}^{1 \times r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.0391em"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span></span></span></span></span></span></span></span></span></span></span></span></span> 를 통해 trainable parameter 수가 선형적으로 증가한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-parameter-count">3.2 Parameter Count<a href="#32-parameter-count" class="hash-link" aria-label="Direct link to 3.2 Parameter Count" title="Direct link to 3.2 Parameter Count">​</a></h2><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mtext>tuned</mtext></msub></mrow><annotation encoding="application/x-tex">L_{\text{tuned}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">tuned</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 를 finetuned layer 수, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub></mrow><annotation encoding="application/x-tex">d_{\text{model}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 을 해당 layer 의 차원으로 두자. VeRA 의 trainable parameter 수는 다음과 같이 표현된다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Θ</mi><mi mathvariant="normal">∣</mi><mo>=</mo><msub><mi>L</mi><mtext>tuned</mtext></msub><mo>×</mo><mo stretchy="false">(</mo><msub><mi>d</mi><mtext>model</mtext></msub><mo>+</mo><mi>r</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">|\Theta| = L_{\text{tuned}} \times (d_{\text{model}} + r),</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣Θ∣</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">tuned</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span></span> 반면 LoRA 는 다음과 같다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Θ</mi><mi mathvariant="normal">∣</mi><mo>=</mo><mn>2</mn><mo>×</mo><msub><mi>L</mi><mtext>tuned</mtext></msub><mo>×</mo><msub><mi>d</mi><mtext>model</mtext></msub><mo>×</mo><mi>r</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">|\Theta| = 2 \times L_{\text{tuned}} \times d_{\text{model}} \times r.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣Θ∣</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">tuned</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord">.</span></span></span></span></span></p><p>특히 lowest rank (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>) 에서는 VeRA 가 LoRA 의 절반 정도의 trainable parameter 만 필요하다. 또한 rank 가 증가할수록 VeRA 의 parameter 수는 각 증가마다 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mtext>tuned</mtext></msub></mrow><annotation encoding="application/x-tex">L_{\text{tuned}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">tuned</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 만큼만 늘어나지만, LoRA 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><msub><mi>L</mi><mtext>tuned</mtext></msub><msub><mi>d</mi><mtext>model</mtext></msub></mrow><annotation encoding="application/x-tex">2 L_{\text{tuned}} d_{\text{model}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord">2</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">tuned</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 만큼 증가한다. 이는 GPT-3 와 같이 96 개 attention layer 와 hidden size 가 12288 인 매우 깊고 넓은 model 맥락에서 특히 큰 절약으로 이어진다.</p><p>이 효율성을 기반으로, VeRA 의 주요 장점은 trained weight 조정을 저장할 때 필요한 memory footprint 가 매우 작다는 점이다. random frozen matrix 는 random number generator (RNG) seed 로 재생성될 수 있기 때문에 memory 에 저장할 필요가 없다. 따라서 memory 요구량은 학습된 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> vector 와 하나의 RNG seed 를 저장하는 데 필요한 byte 로 제한된다. LoRA 와 비교한 memory 효율성은 Tab. 1 에 제시된다.</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-384-e1a1122a10d94c1a06a3e3982d885f5f.png" width="1169" height="514" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-initialization-strategies">3.3 Initialization Strategies<a href="#33-initialization-strategies" class="hash-link" aria-label="Direct link to 3.3 Initialization Strategies" title="Direct link to 3.3 Initialization Strategies">​</a></h2><ul><li><strong>Shared Matrices:</strong> 저자의 방법에서는 frozen low-rank matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span> 를 위해 Kaiming initialization 을 사용한다. 이는 matrix 차원에 따라 값을 scaling 하여, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">AB</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span> 의 matrix product 가 모든 rank 에 대해 일관된 분산을 유지하게 한다. 따라서 rank 별로 learning rate 를 다시 조정할 필요가 없다.</li><li><strong>Scaling Vectors:</strong> scaling vector <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span> 는 0 으로 초기화되며, 이는 LoRA 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span> 초기화와 일치하고 첫 forward pass 에서 weight matrix 가 영향을 받지 않도록 한다. scaling vector <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 는 모든 요소에 동일한 비영(非零) 값으로 초기화되며, 이는 성능 향상을 위해 조정할 수 있는 새로운 hyperparameter 를 도입한다.</li></ul><p>Fig. 1 은 VeRA 의 low-rank matrix 와 scaling vector 초기화 예시를 보여준다. 구체적으로, low-rank matrix 는 normal distribution 으로 초기화되며, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> vector 는 1 로 초기화된다. 또한 uniform distribution 을 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span> 초기화에 사용하거나, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 에 대해 다른 비영 상수를 사용하는 대안 초기화도 실험에서 탐구되었다.</p><h1>4 Experiments</h1><p>이 절에서는 제안된 finetuning 방법을 평가하기 위한 일련의 실험을 수행한다. 먼저 GLUE 와 E2E benchmark 에서 저자의 방법을 LoRA 및 다른 baseline 과 비교한다. 이어서 Llama model 의 instruction-tuning 과 Vision Transformer 를 이용한 image classification 에 실험을 확장한다. 다음으로, 하나의 task 를 선택하여 LoRA 와 VeRA 모두에 대해 rank 를 변화시키며 trainable parameter 수에 따른 성능 변화를 분석한다. 마지막으로 ablation study 를 통해 방법의 각 구성 요소와 초기화 방식이 성능에 미치는 영향을 탐구한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="baselines">Baselines.<a href="#baselines" class="hash-link" aria-label="Direct link to Baselines." title="Direct link to Baselines.">​</a></h4><p>VeRA 와 비교한 baseline 은 다음과 같다:</p><ul><li><strong>Full finetuning</strong>: pretrained weight 로 초기화한 후 모든 parameter 를 학습한다.</li><li><strong>BitFit</strong>: bias vector 만 finetuning 하고 나머지 parameter 는 고정한다. Zaken et al. 이 심층적으로 연구하였다.</li><li><strong>Adapter tuning</strong>: Houlsby et al. 이 처음 제안한 방법으로, self-attention 과 MLP module 사이에 adapter layer 를 삽입하고 residual connection 을 추가한다. <ul><li>이 구조는 두 개의 fully connected layer 와 nonlinearity 로 이루어지며 <strong>Adapter<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">^H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span></span></span></span></span></span></span></span></span></span></span></span></strong> 로 표기된다. </li><li>Lin et al. 은 adapter layer 를 MLP module 뒤와 LayerNorm 이후에만 사용하는 변형을 제안했으며, 이는 Pfeiffer et al. 이 제안한 Adapter<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mi>P</mi></msup></mrow><annotation encoding="application/x-tex">^P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span></span></span></span></span></span></span></span></span></span></span></span> 와 유사하다. </li><li>Rucklé et al. 은 특정 adapter layer 를 생략하여 효율성을 높이는 AdapterDrop (Adapter<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">^D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span></span></span></span></span></span></span></span></span></span></span></span>) 을 제안하였다.</li></ul></li><li><strong>LoRA</strong>: 앞 절에서 소개한 Hu et al. 의 방법.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-glue-benchmark">4.1 GLUE BENCHMARK<a href="#41-glue-benchmark" class="hash-link" aria-label="Direct link to 4.1 GLUE BENCHMARK" title="Direct link to 4.1 GLUE BENCHMARK">​</a></h2><p>저자는 RoBERTa base 및 RoBERTa large model 을 사용하여 GLUE benchmark 에서 VeRA 를 평가하였다. RoBERTa base 에는 rank 1024, RoBERTa large 에는 rank 256 을 사용하였다. shared matrix 는 PyTorch 에 구현된 uniform Kaiming initialization 으로 초기화하였고, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> vector 의 초기값은 0.1 로 설정하였다.</p><ul><li>실험 세팅은 전반적으로 Hu et al. 의 설정과 유사하게, 각 self-attention module 의 query 및 value projection matrix 에 저자의 방법을 적용하고 classification head 는 전체 학습하였다. </li><li>Hu et al. 이 adapted layer 의 gradient 를 조정하기 위해 추가 hyperparameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 를 사용한 것과 달리, 저자는 classification head 와 adapted layer 에 서로 다른 learning rate 를 도입하였다. </li><li>learning rate 와 training epoch 수는 hyperparameter tuning 으로 결정하였으며, 구체적인 설정은 Appendix A 의 Tab. 8 에 제시된다. </li><li>batch size 는 RoBERTa base 의 경우 64, RoBERTa large 의 경우 32 로 설정하였으며, 최대 sequence 길이는 각각 512, 128 이다.</li></ul><p>시간 및 비용 제약으로 인해 MNLI 와 QQP task 는 생략하였으며, 따라서 MRPC, RTE, STS-B task 에서 MNLI trick 을 사용하지 않았다. Hu et al. 과 동일하게, finetuned layer 에 해당하는 trainable parameter 수만 보고하며 classification head 는 표준 방식으로 학습했기 때문에 제외하였다. 5 개의 서로 다른 random seed 로 실험을 반복하고, 각 run 에서 best epoch 결과를 기록한 후 median 값을 보고하였다.</p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-403-3d176f2e23d1134d678cf57ef7268f28.png" width="1173" height="686" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results.<a href="#results" class="hash-link" aria-label="Direct link to Results." title="Direct link to Results.">​</a></h4><p>Tab. 2 에 따르면 VeRA 는 두 model 모두에서 LoRA 와 경쟁력 있는 성능을 보였으며, 필요한 parameter 수는 한 자릿수 수준 더 적었다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-e2e-benchmark">4.2 E2E BENCHMARK<a href="#42-e2e-benchmark" class="hash-link" aria-label="Direct link to 4.2 E2E BENCHMARK" title="Direct link to 4.2 E2E BENCHMARK">​</a></h2><p>E2E benchmark 에 대해서는 Hu et al. 의 실험 설정을 따라 GPT-2 Medium 및 Large model 을 finetuning 하였다. LoRA 에 대해서는 Hu et al. 이 제공한 구현과 hyperparameter 설정을 그대로 사용하였고, VeRA 에 대해서는 rank 와 learning rate 를 조정하였다. 사용된 전체 hyperparameter 는 Appendix A 에 제시되어 있다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-1">Results.<a href="#results-1" class="hash-link" aria-label="Direct link to Results." title="Direct link to Results.">​</a></h4><p><img loading="lazy" alt="Table 3" src="/assets/images/image-404-9105ee80d8fcb78e5a9573b4f82d1e61.png" width="1172" height="662" class="img_ev3q"></p><p>마지막 epoch 결과를 보고한다. Tab. 3 에 따르면 VeRA 는 GPT-2 Medium 에서 약 3 배, GPT-2 Large 에서 약 4 배 더 적은 trainable parameter 로 LoRA 를 능가하였다.</p><h1>4 EXPERIMENTS (continued)</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-instruction-tuning">4.3 INSTRUCTION TUNING<a href="#43-instruction-tuning" class="hash-link" aria-label="Direct link to 4.3 INSTRUCTION TUNING" title="Direct link to 4.3 INSTRUCTION TUNING">​</a></h2><p>Instruction tuning 은 language model 이 주어진 instruction 을 더 효과적으로 따르도록 finetuning 하는 과정이다. 저자는 VeRA 가 Llama 및 Llama2 model 의 instruction-following 능력을 극히 적은 수의 trainable parameter 만으로 가능하게 함을 보였다. 구체적으로, 7B 와 13B variant 에 대해 VeRA 는 각각 1.6M, 2.4M 개의 trainable parameter 만 사용하지만, LoRA (rank 64, Dettmers et al. 방식) 는 각각 159.9M, 250.3M 개가 필요하다.</p><p>실험에서는 LoRA 와 VeRA 를 모두 적용하였으며, 두 방법 모두 Dettmers et al. 과 유사하게 최상위 layer 를 제외한 모든 linear layer 에 적용하였다. 또한 동일 연구의 quantization 기법을 활용하여 단일 GPU 에서 학습을 수행하였다.</p><p>데이터셋으로는 Alpaca dataset 의 정제된 버전을 사용하였다. 이 데이터셋은 51K 개의 instruction 및 demonstration 으로 구성되어 있으며 hallucination, 병합된 instruction, 빈 output 과 같은 문제를 수정한 버전이다. 학습은 learning rate sweep 후 1 epoch 동안 수행하였다.</p><p>평가는 MT-Bench 를 사용하였다. 여기서는 80 개의 multi-turn 질문에 대해 model 응답을 생성하고 이를 GPT-4 로 평가하였다. GPT-4 는 각 응답에 대해 10 점 만점의 정량적 점수를 부여하였다. Tab. 4 에 trainable parameter 수와 평균 점수를 함께 제시한다.</p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-405-f9eb61253c8fe96ecc213d6691a86188.png" width="1175" height="516" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-2">Results.<a href="#results-2" class="hash-link" aria-label="Direct link to Results." title="Direct link to Results.">​</a></h4><p>결과적으로, VeRA 는 trainable parameter 수를 100 배 줄였음에도 불구하고 LoRA 기반 finetuning 성능과 근접한 결과를 보였다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="44-image-classification">4.4 IMAGE CLASSIFICATION<a href="#44-image-classification" class="hash-link" aria-label="Direct link to 4.4 IMAGE CLASSIFICATION" title="Direct link to 4.4 IMAGE CLASSIFICATION">​</a></h2><p>Image classification task 를 평가하기 위해, Vision Transformer (ViT) Base 및 Large variant 를 CIFAR100, Food101, Flowers102, RESISC45 dataset 에 적용하였다. 각 dataset 은 클래스당 10 개의 sample 로 학습하고, CIFAR100, Food101, Flowers102 는 전체 test set, RESISC45 는 나머지 sample 로 평가하였다. ViT weight 는 ImageNet-21k 로 사전학습된 모델을 사용하였다.</p><p>LoRA 와 VeRA 는 ViT 의 query 및 value layer 에 적용하였고, baseline 으로는 fully finetuned model (Full) 과 classification head 만 학습한 경우 (Head) 를 포함하였다. GLUE benchmark 와 마찬가지로 LoRA 는 rank 8, VeRA 는 rank 256 을 사용하였다. 모든 방법에 대해 learning rate 를 조정하고, 10 epoch 학습 후 결과를 Tab. 5 에 제시하였다. 여기서 보고된 parameter 수는 classification head 를 제외한다.</p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-406-18e97c1cbe79ca0e3ea4ef356d1efe15.png" width="1183" height="469" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-3">Results.<a href="#results-3" class="hash-link" aria-label="Direct link to Results." title="Direct link to Results.">​</a></h4><p>결과적으로, VeRA 는 ViT-Base 의 세 dataset 에서 LoRA 성능에 근접하고 Flowers102 에서는 이를 능가하였다. ViT-Large 에서는 CIFAR100, Flowers102, RESISC45 세 dataset 에서 LoRA 를 능가하였다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="45-scaling-the-number-of-trainable-parameters">4.5 SCALING THE NUMBER OF TRAINABLE PARAMETERS<a href="#45-scaling-the-number-of-trainable-parameters" class="hash-link" aria-label="Direct link to 4.5 SCALING THE NUMBER OF TRAINABLE PARAMETERS" title="Direct link to 4.5 SCALING THE NUMBER OF TRAINABLE PARAMETERS">​</a></h2><p>마지막으로, GLUE benchmark 의 RTE task 에서 RoBERTa large model 을 사용하여 LoRA 와 VeRA 의 parameter scalability trade-off 를 분석하였다. VeRA 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mn>1</mn><mo separator="true">,</mo><mn>4</mn><mo separator="true">,</mo><mn>16</mn><mo separator="true">,</mo><mn>64</mn><mo separator="true">,</mo><mn>256</mn><mo separator="true">,</mo><mn>1024</mn></mrow></mrow><annotation encoding="application/x-tex">r = {1, 4, 16, 64, 256, 1024}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">16</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">64</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">256</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1024</span></span></span></span></span></span>, LoRA 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mn>4</mn><mo separator="true">,</mo><mn>8</mn><mo separator="true">,</mo><mn>16</mn><mo separator="true">,</mo><mn>32</mn><mo separator="true">,</mo><mn>64</mn></mrow></mrow><annotation encoding="application/x-tex">r = {1, 2, 4, 8, 16, 32, 64}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">8</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">16</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">32</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">64</span></span></span></span></span></span> 를 설정하고, trainable parameter 수와 accuracy 간의 trade-off 를 관찰하였다. 각 설정은 5 개의 random seed 로 반복하여 median 결과를 보고하였다. LoRA 는 HuggingFace PEFT 구현을 사용하고 Hu et al. 의 hyperparameter 를 따랐으며, VeRA 는 이전 subsection 의 RTE 실험에서 사용한 동일한 hyperparameter 를 적용하였다.</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-407-adf20cc71adadd668744f7963fc01ca0.png" width="568" height="484" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-4">Results.<a href="#results-4" class="hash-link" aria-label="Direct link to Results." title="Direct link to Results.">​</a></h4><p>결과 (Fig. 2) 에 따르면, VeRA 는 LoRA 보다 훨씬 더 parameter-efficient 하다. 특히 동일한 parameter 수를 가진 고랭크 VeRA 는 LoRA 대비 accuracy 가 4%p 높았다.</p><h1>4.6 ABLATION STUDY</h1><p>이 절에서는 저자의 방법을 구성하는 개별 요소들의 영향을 살펴보기 위해 ablation study 를 수행한다. 모든 실험은 MRPC 와 RTE task 에 초점을 맞추고 RoBERTa large model 을 사용하였다. 각 실험은 이전 섹션의 hyperparameter 를 유지하며, 조사 대상 구성 요소만 수정하였다. 모든 설정은 5 개의 random seed 로 반복 실행되었고, 결과는 평균과 표준편차로 보고하였다.</p><p><img loading="lazy" alt="Table 6" src="/assets/images/image-409-03033c14ba634ea77ff2f26df448ad08.png" width="1174" height="342" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="single-scaling-vector">Single Scaling Vector.<a href="#single-scaling-vector" class="hash-link" aria-label="Direct link to Single Scaling Vector." title="Direct link to Single Scaling Vector.">​</a></h4><p>먼저 저자의 방법에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span> scaling vector 두 개 모두가 필요한지 조사하였다. 두 가지 ablation 설정을 만들었는데, 하나는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 를 제외한 &quot;only b&quot;, 다른 하나는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span> 를 제외한 &quot;only d&quot; 이다. &quot;only d&quot; 설정에서는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 를 0 으로 초기화하였다. Tab. 6 에 따르면 어느 scaling vector 를 제거하더라도 성능이 저하된다. 다만 &quot;only d&quot; 설정이 &quot;only b&quot; 설정보다 다소 더 나은 성능을 보였다. 이는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> scaling vector 가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span> vector 보다 더 높은 표현력을 가진다는 점을 시사한다. 구체적으로, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 는 두 low-rank matrix 의 행(row) 을 조정하여 최종적으로 구성되는 matrix 의 더 넓은 부분에 영향을 미친다. 반면 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span> 는 두 matrix 곱의 결과 matrix 의 행만 scale 한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="initialization-of-shared-matrices">Initialization of Shared Matrices.<a href="#initialization-of-shared-matrices" class="hash-link" aria-label="Direct link to Initialization of Shared Matrices." title="Direct link to Initialization of Shared Matrices.">​</a></h4><p>shared matrix 의 초기화 방식으로 Kaiming normal, Kaiming uniform, 그리고 <!-- -->[<!-- -->0, 0.1] 범위의 uniform initialization 세 가지를 비교하였다. Tab. 6 결과에 따르면 Kaiming 초기화 방식이 uniform 범위 초기화보다 성능이 우수했으며, uniform 변형이 normal 변형보다 약간 더 나은 결과를 보였다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="initialization-of-scaling-vector">Initialization of Scaling Vector.<a href="#initialization-of-scaling-vector" class="hash-link" aria-label="Direct link to Initialization of Scaling Vector." title="Direct link to Initialization of Scaling Vector.">​</a></h4><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> vector 초기값이 성능에 미치는 영향을 탐구하기 위해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext>init</mtext></msub><mo>∈</mo><mrow><mn>1.0</mn><mo separator="true">,</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo separator="true">,</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>7</mn></mrow></msup></mrow></mrow><annotation encoding="application/x-tex">d_{\text{init}} \in {1.0, 10^{-1}, 10^{-7}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">init</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord">1.0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">7</span></span></span></span></span></span></span></span></span></span></span></span></span></span> 을 실험하였다. Tab. 6 결과, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext>init</mtext></msub></mrow><annotation encoding="application/x-tex">d_{\text{init}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">init</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 의 선택이 성능에 크게 영향을 미쳤으며, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span> 과 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>7</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-7}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">7</span></span></span></span></span></span></span></span></span></span></span></span></span> 설정이 1.0 보다 더 나은 성능을 보였다. 이는 frozen matrix 의 특정 행에서 학습 초기에 sign 변화가 일어나 최적화 과정에서 더 큰 유연성을 제공했기 때문으로 보인다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="magnitude-of-adaptation">Magnitude of Adaptation.<a href="#magnitude-of-adaptation" class="hash-link" aria-label="Direct link to Magnitude of Adaptation." title="Direct link to Magnitude of Adaptation.">​</a></h4><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-408-0ae2f1b6ec9eb7626807046b8ad71872.png" width="544" height="456" class="img_ev3q"></p><p>Fig. 3 은 RTE task finetuning 후 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> vector 변화량의 크기를 시각화한 것이다. low-rank frozen matrix 는 layer 마다 동일하므로, 각 layer 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> vector 길이를 직접 비교하여 상대적 adaptation 정도를 평가할 수 있다. 전체적으로 query matrix 에서 value matrix 보다 더 큰 adaptation 이 발생하였으며, 이는 해당 부분에서 finetuning 필요성이 크거나 더 쉽게 적응된다는 것을 시사한다. 또한 이전 연구 결과와 유사하게, later layer 가 earlier layer 에 비해 더 큰 adaptation 을 보였다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="sharing-random-matrices">Sharing Random Matrices.<a href="#sharing-random-matrices" class="hash-link" aria-label="Direct link to Sharing Random Matrices." title="Direct link to Sharing Random Matrices.">​</a></h4><p>마지막으로, RTE, MRPC, CoLA, STS-B task 에 대해 random matrix 공유 여부가 성능에 미치는 영향을 평가하였다. 하나의 설정은 모든 adapted layer 가 동일한 random matrix 를 공유하고, 다른 하나는 layer 마다 고유한 matrix 를 생성하는 방식이다. </p><p><img loading="lazy" alt="Table " src="/assets/images/image-410-aeaa001dbc636954bd802211809bb404.png" width="1008" height="204" class="img_ev3q"></p><p>Tab. 7 결과, RTE 와 STS-B task 에서는 평균 성능이 동일하였으며, MRPC 와 CoLA 에서는 고유 matrix 를 사용할 때 약간의 개선이 있었다.</p><h1>5 CONCLUSION</h1><p>본 연구에서는 LoRA 대비 trainable parameter 수를 크게 줄이면서도 downstream task 에서 유사하거나 더 나은 성능을 달성하는 finetuning 방법을 제안하였다. 구체적으로, RoBERTa large 에 대해 GLUE benchmark 에서 동일 성능을 유지하며 parameter 수를 10 배 줄였고, image classification task 에서도 10 배 감소, E2E benchmark 에서 3 배 감소를 달성하였다. 이 방법은 사용자별로 수많은 finetuned model 의 빈번한 교체가 필요한 cloud 기반 AI 서비스와 같은 상황에서 특히 적합하다. scaling vector 크기가 매우 작기 때문에 단일 GPU 의 제한된 memory 내에 다수의 model 버전을 저장할 수 있어, 특정 model 을 memory 에 로드해야 하는 병목 현상을 제거하고 serving 효율성을 크게 향상시킨다.</p><p>현재 연구는 Transformer 구조를 가진 language 및 vision model 에 초점을 맞추었지만, 본 방법의 적용 가능성은 다른 구조 및 domain 으로 확장될 여지가 있다. 또한 dynamic parameter budget allocation, 다양한 initialization 및 regularization 기법과 같은 추가적인 개선을 통해 성능이 더욱 향상될 가능성이 있다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/ve-ra">VeRA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/lo-ra">LoRA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/low-rank">Low-Rank</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vector-based-random-matrix">Vector-based Random Matrix</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/scaling-vector">scaling vector</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/PEFT/Composition/2023-10-VeRA.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/PEFT/Composition/SaLoRA"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Structure-Aware Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/PEFT/Composition/SoRA"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Sparse Low-rank Adaptation of Pre-trained Language Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-method-formulation" class="table-of-contents__link toc-highlight">3.1 Method Formulation</a></li><li><a href="#32-parameter-count" class="table-of-contents__link toc-highlight">3.2 Parameter Count</a></li><li><a href="#33-initialization-strategies" class="table-of-contents__link toc-highlight">3.3 Initialization Strategies</a></li><li><a href="#41-glue-benchmark" class="table-of-contents__link toc-highlight">4.1 GLUE BENCHMARK</a></li><li><a href="#42-e2e-benchmark" class="table-of-contents__link toc-highlight">4.2 E2E BENCHMARK</a></li><li><a href="#43-instruction-tuning" class="table-of-contents__link toc-highlight">4.3 INSTRUCTION TUNING</a></li><li><a href="#44-image-classification" class="table-of-contents__link toc-highlight">4.4 IMAGE CLASSIFICATION</a></li><li><a href="#45-scaling-the-number-of-trainable-parameters" class="table-of-contents__link toc-highlight">4.5 SCALING THE NUMBER OF TRAINABLE PARAMETERS</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.3298527c.js"></script>
<script src="/assets/js/main.aca6605f.js"></script>
</body>
</html>