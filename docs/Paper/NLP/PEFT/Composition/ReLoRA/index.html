<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/PEFT/Composition/2023-07-ReLoRA">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">ReLoRA: High-Rank Training Through Low-Rank Updates | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/ReLoRA"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="ReLoRA: High-Rank Training Through Low-Rank Updates | My Site"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/ReLoRA"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/ReLoRA" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/PEFT/Composition/ReLoRA" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.d2ad26d0.css">
<link rel="preload" href="/assets/js/runtime~main.45816a06.js" as="script">
<link rel="preload" href="/assets/js/main.0f1c04dd.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Chain-of-Thought">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">Composition</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">LoRA: Low-Rank Adaptation of Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoHa">FedPara: Low-Rank Hadamard Product For Communication-Efficient Federated Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/BitFit">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/IA³">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/DyLoRA">DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoKr">KronA: Parameter Efficient Tuning with Kronecker Adapter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/AdaLoRA">Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ReLoRA">ReLoRA: High-Rank Training Through Low-Rank Updates</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/IncreLoRA">IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/Delta-LoRA">Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LongLoRA">LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SaLoRA">Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SoRA">Sparse Low-rank Adaptation of Pre-trained Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/COLA">Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ApiQ">ApiQ: Finetuning of 2-Bit Quantized Large Language Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/DoRA">DoRA: Weight-Decomposed Low-Rank Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/FLoRA">FLoRA: Low-Rank Adapters Are Secretly Gradient Compressors</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA+">LoRA+: Efficient Low Rank Adaptation of Large Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MeLoRA">MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ResLoRA">ResLoRA: Identity Residual Mapping in Low-Rank Adaption</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SIBO">SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/ALoRA">ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/AutoLoRA">AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/BiLoRA">BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/HiddenKey">LoRA Meets Dropout under a Unified Framework</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA-Dropout">LoRA Dropout as a Sparsity Regularizer for Overfitting Control</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/PISSA">PISSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/DoRA2">DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MoRA">MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MiLoRA">MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/MoSLoRA">Mixture-of-Subspaces in Low-Rank Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/OLoRA">OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/RoseLoRA">RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/SinkLoRA">SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Mixture/UniPELT">Mixture</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Module/Adapter">Module</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Pruning/SMP">Pruning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning">Quantization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning">Soft Prompt</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Composition</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">ReLoRA: High-Rank Training Through Low-Rank Updates</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>ReLoRA: High-Rank Training Through Low-Rank Updates</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2307.05695" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2307.05695</a></p><h1>Abstract</h1><p>scaling 의 지배적인 효과로 인해 billions parameters 를 가진 large networks 가 등장했지만, overparameterized models 를 훈련할 필요성은 여전히 명확히 이해되지 않았으며, training costs 는 기하급수적으로 증가하고 있다. </p><p>이 논문에서는 large networks 를 훈련하는 접근법으로 parameter-efficient training 기법을 탐구한다. </p><p>저자는 ReLoRA 라는 새로운 방법을 소개하는데, 이는 low-rank update 를 활용하여 high-rank networks 를 훈련한다. </p><ul><li>ReLoRA 를 최대 1.3B parameters 를 가진 transformer language models 에 적용하여, 일반적인 neural network training 과 비교 가능한 성능을 보여준다. </li><li>ReLoRA 는 per GPU 최대 5.5GB 의 RAM 을 절약하고, model size 및 hardware setting 에 따라 training speed 를 9-40% 까지 향상시킨다. </li><li>저자의 연구 결과는 large-scale pre-training 에서 parameter-efficient 기법의 잠재력을 보여준다.</li></ul><h1>1. Introduction</h1><p>최근 점점 더 overparameterized networks 를 훈련하거나 &quot;stack more layers&quot; 접근법을 채택하는 경향이 지배적이었다.</p><p>large networks 의 정의는 100M parameters 를 가진 모델에서 100B parameters 를 가진 모델로 진화했으며, 이러한 network training 에 관련된 computational costs 는 대부분의 연구 그룹에 부담스러운 수준이 되었다. </p><p>그럼에도 불구하고, training examples 보다 훨씬 더 많은 파라미터를 가진 모델을 훈련해야 하는 필요성은 이론적으로 잘 이해되지 않고 있다.</p><hr><p>대안적인 scaling 접근법으로는 more compute-efficient scaling optima, retrieval-augmented models , 그리고 smaller models 을 오래 training 하는 간단한 접근법 등이 있다. </p><p>그러나 이들 접근법은 overparameterized models 이 필요한 이유에 대한 이해를 높이지 않으며, 이러한 models 의 training 을 민주화하는 데도 기여하지 않는다. </p><ul><li>예로, RETRO 를 훈련하려면 복잡한 training settings 와 trillions tokens 를 빠르게 검색할 수 있는 인프라가 필요하며, LLaMA-7B 를 훈련하는 데는 여전히 수백 개의 GPU 가 필요하다.</li><li>반면, zero-redundancy optimizer, 16-bit training, 8-bit inference, 그리고 parameter-efficient fine-tuning (PEFT) 같은 접근법은 large models 를 보다 접근 가능하게 만드는 데 중요한 역할을 해왔다. </li><li>특히, PEFT 방법은 billion-scale language models 나 diffusion models 를 consumer hardware 에서 fine-tuning 할 수 있게 했다. </li><li>이는 이러한 접근법이 pre-training 에도 도움이 될 수 있는지에 대한 질문을 제기한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="contribution">Contribution<a href="#contribution" class="hash-link" aria-label="Direct link to Contribution" title="Direct link to Contribution">​</a></h4><p>본 연구에서는 ReLoRA 를 소개하며, 이는 training process 에서 집합적으로 high-rank network 를 훈련하기 위해 individially low-rank update 를 사용한다. </p><ul><li>저자는 ReLoRA 가 high-rank update 를 수행하며 일반적인 neural network training 과 유사한 성능을 달성함을 실증적으로 보여준다. </li><li>ReLoRA 의 구성 요소는 neural network 의 initial full-rank training, LoRA training, restarts, jagged learning rate schedule, partial optimizer resets 등이 있다. </li><li>저자는 ReLoRA 를 최대 1.3B parameters 를 가진 transformer language models 에서 평가한다. </li><li>마지막으로, ReLoRA 의 효율성은 model size 가 커질수록 증가하여, multi-billiton parameter networks 의 efficient training 을 위한 실용적인 옵션임을 확인했다.</li></ul><h1>2. Method</h1><p>저자는 two matrices sum 의 rank 에 관심이 있다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mo stretchy="false">(</mo><mi>A</mi><mo>+</mo><mi>B</mi><mo stretchy="false">)</mo><mo>≤</mo><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>+</mo><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">rank(A + B) \leq rank(A) + rank(B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal" style="margin-right:0.03148em">ank</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal" style="margin-right:0.03148em">ank</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal" style="margin-right:0.03148em">ank</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mclose">)</span></span></span></span></span>. </p><p>matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span> 에 대해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mi>d</mi><mi>i</mi><mi>m</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">rank(A) &lt; dim(A)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal" style="margin-right:0.03148em">ank</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">im</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span></span> 일 때, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span> 가 존재하며 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo><mo>&lt;</mo><mi>d</mi><mi>i</mi><mi>m</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">rank(B) &lt; dim(B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal" style="margin-right:0.03148em">ank</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">im</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mclose">)</span></span></span></span></span> 이면서 이들의 합이 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span> 나 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span> 보다 higher rank 를 가진다.</p><p>저자는 이 성질을 활용하여 flexible parameter-efficient training 방법을 만들고자 한다. </p><ul><li>먼저 LoRA 로 시작하는데, 이는 low-rank update 아이디어를 기반으로 한 parameter-efficient fine-tuning 방법이다. </li><li>LoRA 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W \in \mathbb{R}^{m \times n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7713em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span></span></span></span></span> 에 의한 reparameterized any linear 에 적용될 수 있다.</li><li>구체적으로, LoRA 는 weight upodate <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">\delta W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03785em">δ</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span></span> 를 Eq. (1) 같이 rank-<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span></span></span></span> 의 곱인 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>A</mi></msub><msub><mi>W</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">W_AW_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 로 분해한다. <ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">s \in \mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6889em"></span><span class="mord mathbb">R</span></span></span></span></span> 은 보통 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>r</mi></mrow><annotation encoding="application/x-tex">1/r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">1/</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span></span></span></span> 로 설정된 fixed scaling factor</li></ul></li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>δ</mi><mi>W</mi><mo>=</mo><mi>s</mi><msub><mi>W</mi><mi>A</mi></msub><msub><mi>W</mi><mi>B</mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><msub><mi>W</mi><mi>A</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>i</mi><mi>n</mi><mo>×</mo><mi>r</mi></mrow></msup><mo separator="true">,</mo><mtext> </mtext><msub><mi>W</mi><mi>B</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>r</mi><mo>×</mo><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msup></mrow></mstyle></mtd></mtr></mtable></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \begin{aligned} &amp;\delta W = sW_AW_B \\ &amp;W_A \in \mathbb{R}^{in \times r}, \, W_B \in \mathbb{R}^{r \times out} \end{aligned} \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.0347em;vertical-align:-1.2673em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7673em"><span style="top:-3.7673em"><span class="pstrut" style="height:3.7673em"></span><span class="mord"><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7673em"><span style="top:-3.802em"><span class="pstrut" style="height:2.8747em"></span><span class="mord"></span></span><span style="top:-2.2673em"><span class="pstrut" style="height:2.8747em"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2673em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7673em"><span style="top:-3.9273em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mord mathnormal" style="margin-right:0.03785em">δ</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">s</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.3927em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8747em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">in</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8436em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2673em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2673em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7673em"><span style="top:-3.7673em"><span class="pstrut" style="height:3.7673em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2673em"><span></span></span></span></span></span></span></span></span></div><ul><li>실제로, LoRA 는 training 후에 original parameter 에 병합될 수 있는 new trainable parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">W_A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">W_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 를 추가하는 방식으로 구현된다.</li><li>따라서 이러한 구현은 rank <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><msub><mi>W</mi><mi>A</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>B</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">r = \max_{W_A, W_B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 인 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mo stretchy="false">(</mo><msub><mi>W</mi><mi>A</mi></msub><msub><mi>W</mi><mi>B</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">rank(W_AW_B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal" style="margin-right:0.03148em">ank</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 에 의해 제한된다.</li></ul><p>만약 LoRA 를 training 도중에 restart 할 수 있다면, 즉 training 중에 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">W_A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">W_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 를 병합하고 이들의 값을 초기화할 수 있다면, update 의 total rank 를 증가시킬 수 있다. </p><p>이를 여러 번 수행하면 neural network update 는 다음과 같이 된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="normal">Δ</mi><mi>W</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>T</mi><mn>1</mn></msub></munderover><mi>δ</mi><msub><mi>W</mi><mi>t</mi></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><msub><mi>T</mi><mn>1</mn></msub></mrow><msub><mi>T</mi><mn>2</mn></msub></munderover><mi>δ</mi><msub><mi>W</mi><mi>t</mi></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><msub><mi>T</mi><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><msub><mi>T</mi><mi>N</mi></msub></munderover><mi>δ</mi><msub><mi>W</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><msubsup><mi>W</mi><mi>A</mi><mn>1</mn></msubsup><msubsup><mi>W</mi><mi>B</mi><mn>1</mn></msubsup><mo>+</mo><mi>s</mi><msubsup><mi>W</mi><mi>A</mi><mn>2</mn></msubsup><msubsup><mi>W</mi><mi>B</mi><mn>2</mn></msubsup><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>s</mi><msubsup><mi>W</mi><mi>A</mi><mi>N</mi></msubsup><msubsup><mi>W</mi><mi>B</mi><mi>N</mi></msubsup></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \Delta W = \sum_{t=0}^{T_1} \delta W_t + \sum_{t=T_1}^{T_2} \delta W_t + \cdots + \sum_{t=T_{N-1}}^{T_N} \delta W_t = sW_{A}^{1}W_{B}^{1} + sW_{A}^{2}W_{B}^{2} + \cdots + sW_{A}^{N}W_{B}^{N} \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.2759em;vertical-align:-1.388em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.888em"><span style="top:-3.888em"><span class="pstrut" style="height:3.8396em"></span><span class="mord"><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8394em"><span style="top:-1.8829em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3111em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03785em">δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8394em"><span style="top:-1.8557em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3111em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3944em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03785em">δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8396em"><span style="top:-1.8557em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2028em"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3113em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:-0.1389em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4363em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03785em">δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">s</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">s</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">s</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.388em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.888em"><span style="top:-3.888em"><span class="pstrut" style="height:3.8396em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.388em"><span></span></span></span></span></span></span></span></span></div><ul><li>restart 를 구현하는 것은 실제로 간단하지 않으며 optimization procedure 에 몇 가지 수정이 필요하다. </li><li>일반적인 SGD 과 달리, Adam update 는 주로 previous steps 에서 축적된 gradient 의 first 및 second moments 에 의해 guide 된다. </li><li>실제로, Adam 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\beta_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>과 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\beta_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 보통 very high <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.9</mn><mo>−</mo><mn>0.999</mn></mrow><annotation encoding="application/x-tex">0.9 - 0.999</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">0.9</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.999</span></span></span></span></span> 사이에 있다. <ul><li>이는 merge-and-reinit 후에도 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>A</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">W_A^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0894em;vertical-align:-0.2753em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-2.4247em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2753em"><span></span></span></span></span></span></span></span></span></span></span> 의 old gradient moments 를 계속 사용하면 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>A</mi><mn>1</mn></msubsup></mrow><annotation encoding="application/x-tex">W_A^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0894em;vertical-align:-0.2753em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-2.4247em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2753em"><span></span></span></span></span></span></span></span></span></span></span> 와 동일한 directions 로 이끌어져 동일한 subspace 을 optimizing 하게 됨을 의미한다.</li></ul></li></ul><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-20-c3524c835f025266b8d14116e91db907.png" width="1652" height="714" class="img_ev3q"></p><p>이를 해결하기 위해 ReLoRA 는 merge-and-reinit 중에서 optimizer state 를 partial reset 하기 위해 magnitude pruning 을 수행한다. </p><p>또한 optimizer reset 후 loss 가 diverging 되지 않도록 learning rate 를 0 으로 설정한 후 subsequent warm-up 을 진행한다 (Fig. 2). </p><p>저자의 ablation study (Tab. 6)는 이러한 수정이 LoRA 보다 성능을 향상시키는 데 필요함을 보여준다.</p><p>마지막으로, 실험에서 scratch (random initialization) training 으로 시작하는 경우에는 ReLoRA 를 &quot;warm start&quot; 하기 위해 short full-rank training 이 필요하다는 것을 발견했다. </p><p>이 모든 것을 통해 ReLoRA 는 한 번에 small parameters 만 훈련하면서도, 특히 large transformer network 에서 full-rank training 에 필적하는 성능을 달성할 수 있다. </p><p>이는 Algorithm 1 에 설명된다.</p><p><img loading="lazy" alt="Algorithm 1" src="/assets/images/image-21-58641944890058fb26f5d098d99a3d1e.png" width="1175" height="892" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="enhancing-computational-efficiency">Enhancing computational efficiency<a href="#enhancing-computational-efficiency" class="hash-link" aria-label="Direct link to Enhancing computational efficiency" title="Direct link to Enhancing computational efficiency">​</a></h4><p>다른 low-rank training 기법과 달리, ReLoRA 는 original network 의 frozen weights 를 유지하고 new trainable parameter 를 추가하는 LoRA 접근 방식을 따른다. </p><p>처음에는 이 방식이 계산적으로 비효율적이라고 생각할 수 있지만, frozen parameters 와 trainable parameter 의 차별화는 parameter-efficient fine-tuning 에서 중요한 역할을 한다.</p><ul><li>trainable parameter 의 수를 줄임으로써, ReLoRA 는 optimizer state 에 소비되는 메모리를 크게 줄이고 larger batch siezs 를 사용할 수 있게 하여 hardware efficiency 를 극대화한다. </li><li>또한, 이는 large-scale distributed training 종종 제한 요인이 되는 bandwidth 요구사항을 줄인다. </li><li>게다가, frozen parameters 는 restart 간에 업데이트되지 않으므로 low-precision 로 양자화된 형식으로 유지할 수 있어, 메모리와 계산 부담을 더욱 줄일 수 있다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="locally-low-rank-training-intuition">Locally Low-Rank Training: Intuition<a href="#locally-low-rank-training-intuition" class="hash-link" aria-label="Direct link to Locally Low-Rank Training: Intuition" title="Direct link to Locally Low-Rank Training: Intuition">​</a></h4><p>여러 연구에 따르면 neural network training 은 완전히 low-rank 이거나, 초기에는 high-rank 을 가지고 이후에 low-rank 으로 전환되는 여러 단계를 가진다고 한다. </p><ul><li>예로, Aghajanyan et al. <!-- -->[2021]<!-- --> 은 모델이 커지거나 더 오랜 시간 동안 pre-training 될수록 downstream task 를 학습하는 데 필요한 업데이트의 rank 가 감소함을 보여준다. </li><li>Arora et al. <!-- -->[2019]<!-- --> 는 SGD 가 low-rank 솔루션을 향한 bias 를 가진다고 발견했다. </li><li>training 초기의 Lottery Tickets 의 존재도 이 가설을 부분적으로 뒷받침하는데, Lottery Tickets network 를 훈련하는 것이 일반적인 training process 의 low-rank 근사로 볼 수 있기 때문이다.</li></ul><p>저자의 empirical analysis (Sec. 4)은 pre-trained neural network 이 long trajectory 에 걸쳐 high-rank update 를 나타냄을 보여준다 (Fig. 4). </p><p>그러나 충분히 small trajectory 에서는 training 이 low-rank update 로 효과적으로 근사될 수 있다. </p><p>위 결과를 바탕으로, 저자는 neural network training 이 locally low-rank 이라는 가설을 세우며, 이는 ReLoRA 의 직접적인 동기가 된다.</p><h1>3. Experiments</h1><p>ReLoRA 의 효과를 평가하기 위해 C4 dataset 에서 다양한 model size (60M, 130M, 250M, 350M, 1.3B)를 사용하여 transformer language models 를 훈련시켰다. </p><p>모든 실험에서 data repetition 없이 (single epoch) Chinchilla Scaling Laws 를 기반으로 compute-optimal data amount 를 사용하여 훈련을 진행했다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="architecture-and-training-hyperparameters">Architecture and training hyperparameters<a href="#architecture-and-training-hyperparameters" class="hash-link" aria-label="Direct link to Architecture and training hyperparameters" title="Direct link to Architecture and training hyperparameters">​</a></h4><p>저자의 architecture 는 transformer 에 기반하며 LLaMA 와 유사하다. </p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-23-303f338e9b116dc0861a7232e52c6bbb.png" width="1182" height="348" class="img_ev3q"></p><ul><li>저자는 pre-normalization, RMSNorm, SwiGLU activation function, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>8</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{8}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>h fully-connected hidden state 크기, 그리고 rotary embeddings 을 사용했다. </li><li>pre-training token 수는 Chinchilla scaling laws 을 기반으로 선택했다. </li><li>architecture 와 training hyper-parameters 는 Tab. 1에 제시되어 있다.</li><li>all LoRA 와 ReLoRA 실험에서 rank <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">r = 128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span></span></span></span></span> 을 사용했다. <ul><li>이는 초기 실험에서 이 값이 best perplexity/memory trade-off 을 제공했기 때문 </li></ul></li><li>ReLoRA hyper-parameters 선택에 대한 추가 권장 사항은 부록 A 에서 찾을 수 있다.</li><li>1.3B model 에 대한 rank choice 는 Sec 4.1 에 다룬다.</li><li>저자는 all floating point operations 에 대해 bfloat 16 를 사용하며, 효과적인 attention computation 을 위해 FlashAttention 을 사용한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="relora-and-baselines-setup">ReLoRA and baselines setup<a href="#relora-and-baselines-setup" class="hash-link" aria-label="Direct link to ReLoRA and baselines setup" title="Direct link to ReLoRA and baselines setup">​</a></h4><ul><li>ReLoRA 는 all attention 및 fully connected network parameters 를 대체하며, embeddings 및 normalization layers 은 full-rank 로 업데이트된다.</li><li>ReLoRA-wrapped models 는 full-rank training 보다 fewer trainable parameters 이기 때문에, ReLoRA 와 동일한 trainable parameters 를 가진 full-rank 인 Control baseline 을 포함한다.</li><li>ReLoRA 는 5,000 update steps 에서 full-rank training 의 checkpoint 로 초기화되며, 이후 5,000 steps 마다 3 번 reset 하여 총 20K steps 에 도달한다.<ul><li>각 reset 후, optimization state 의 99% 가 magnitude 에 기반하여 prune 되며, 다음 100 iterations 동안 loss 는 warm up 된다 </li></ul></li><li>ReLoRA parameters 는 LoRA best 를 따르며, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span>-matrix 에 Kaiming initialization 을 사용하고, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05017em">B</span></span></span></span></span>-matrix 는 0 으로 초기화된다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-up-to-13b">Scaling up to 1.3B<a href="#scaling-up-to-13b" class="hash-link" aria-label="Direct link to Scaling up to 1.3B" title="Direct link to Scaling up to 1.3B">​</a></h4><ul><li>130M 및 350M model size 에서 초기 결과를 바탕으로, 1.3B parameter language model 을 훈련하기 위해 ReLoRA 를 적용.</li><li>baseline 으로, 23B tokens 를 scratch learning 으로 1.3B model 을 pre-training. </li><li>2K, 5K 및 10K checkpoint 에서 시작하여 여러 ReLoRA 실행을 수행. </li><li>대부분의 실험에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">r = 128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span></span></span></span></span> 을 계속 사용했으며, 추가 실험에서는 rank 128 과 512(hidden size 2048) 간의 차이가 거의 없음을 보여준다. (Sec. 4.1)</li></ul><h1>4. Results</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="parameter-efficient-pre-training">Parameter-efficient pre-training<a href="#parameter-efficient-pre-training" class="hash-link" aria-label="Direct link to Parameter-efficient pre-training" title="Direct link to Parameter-efficient pre-training">​</a></h4><p>Tab.2 및 Fig. 1 에서 결과를 보여준다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-22-6bdff0c489c0673a666af023419b6992.png" width="1176" height="596" class="img_ev3q"></p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-24-212e9ddf18cd1b8af4c1c1ca3cd0e184.png" width="1172" height="390" class="img_ev3q"></p><ul><li>ReLoRA 는 LoRA training 을 초월하며, 제안된 수정의 효과를 입증한다.</li></ul><p><img loading="lazy" alt="Table 3" src="/assets/images/image-25-96ef643fa02837d8d1a571bd0ff48e82.png" width="1712" height="391" class="img_ev3q"></p><ul><li>더욱이, ReLoRA 는 upstream 및 downstream task 에서 full-rank training 과 유사한 성능을 달</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="high-rank-training-through-low-rank-updates">High-rank training through low-rank updates<a href="#high-rank-training-through-low-rank-updates" class="hash-link" aria-label="Direct link to High-rank training through low-rank updates" title="Direct link to High-rank training through low-rank updates">​</a></h4><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-26-bcb5322e216a6bfe84435b229b1c41f7.png" width="1623" height="671" class="img_ev3q"></p><ul><li>ReLoRA 가 LoRA 보다 higher rank update 를 수행하는지 여부를 결정하기 위해 learned update 의 singular value spectrum 을 warm-start weights 를 plot</li><li>구체적으로, warm-start weights 와 ReLoRA, LoRA, 그리고 full-rank trained models 의 final weights 간의 차이를 비교하는 추가 실험을 수행했다. </li><li>Fig. 3 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>W</mi><mi>Q</mi></msub></mrow><annotation encoding="application/x-tex">\Delta W_Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">\Delta W_K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">\Delta W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>W</mi><mrow><mi>d</mi><mi>o</mi><mi>w</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\Delta W_{down}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02691em">w</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 의 singular values 에 대한 LoRA 와 ReLoRA 의 중요한 질적 차이를 보여준다.</li><li>대부분의 LoRA singular values 은 0 이며 (Fig. 4), 눈에 띄는 high values (1.5 이상)을 가진 값들이 있는 반면, ReLoRA 는 0.1과 1.0 사이에 higher distribution 를 보이며, 이는 full-rank training 과 유사하다.</li></ul><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-27-1fa9d1afcbd882976052414411b37cbb.png" width="1330" height="403" class="img_ev3q"></p><ul><li>추가적으로, LoRA, ReLoRA, 그리고 full-rank training 에서 0.1 미만의 singular values 수를 계산했다. </li><li>저자의 결과 (Fig. 4)는 ReLoRA 가 LoRA 에 비해 0 에 가까운 singular values 의 수가 훨씬 적고, full-rank training 에 더 가깝다는 것을 보여준다.</li><li>이 관찰은 high-rank update 의 중요성을 강조하며, ReLoRA 가 multiple low-rank update 를 수행함으로써 실제로 high-rank update 를 달성한다는 것을 입증한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-scaling-up-to-13b">4.1 Scaling up to 1.3B<a href="#41-scaling-up-to-13b" class="hash-link" aria-label="Direct link to 4.1 Scaling up to 1.3B" title="Direct link to 4.1 Scaling up to 1.3B">​</a></h2><ul><li>이 model size 에서의 best run 은 10K step warm start (total update steps  33%) 후에 시작. </li><li>ReLoRA 는 rank <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">r = 128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span></span></span></span></span>, learning rate <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mi>e</mi><mo>−</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">5e-4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">5</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">4</span></span></span></span></span>, 100 steps 의 learning rate warmup, 50 steps 의 restart warmup 으로 훈련되었다.</li></ul><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-28-c26bb70fc69daf2fb071b162d64c9839.png" width="1579" height="737" class="img_ev3q"></p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-29-c52d46e05634706fb2851da58a0aa243.png" width="1456" height="413" class="img_ev3q"></p><ul><li>ReLoRA 는 training 내내 LoRA 를 명확히 능가하며, 두 방법 간의 격차는 15K steps 에서 0.56 에서 시작해 30K steps 에서 0.96 으로 증가했다.</li><li>training 이 끝날 때, ReLoRA 는 17.24 의 perplexity 에 도달했으며, 이는 full-rank training 보다 0.44 높다. </li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="varying-relora-rank">Varying ReLoRA rank<a href="#varying-relora-rank" class="hash-link" aria-label="Direct link to Varying ReLoRA rank" title="Direct link to Varying ReLoRA rank">​</a></h4><p>이 실험에서는 rank <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">r = 128</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">128</span></span></span></span></span> 이 이 크기의 model (hidden size 2048)에도 여전히 적용 가능한지, 또는 증가시킬 필요가 있는지 평가하고자 했다.</p><p>이를 위해, 30K steps 중 5K steps 의 early checkpoint 를 warm start  사용했다. </p><p>이 시점에서 loss changes 가 빠르기 때문에 training 역학에서의 차이가 더욱 분명하게 드러난다. </p><p>이 모델들은 추가로 10K iterations 로 훈련되었다. </p><p>예상치 못하게도, rank 128(perplexity 19.16) 과 rank 512(perplexity 19.00) 간의 차이가 거의 없음을 발견했다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="negative-results-online-relora">Negative results: Online ReLoRA<a href="#negative-results-online-relora" class="hash-link" aria-label="Direct link to Negative results: Online ReLoRA" title="Direct link to Negative results: Online ReLoRA">​</a></h4><p>직관적으로, 더 자주 ReLoRA 를 reset 하면 더 나은 성능을 보일 것으로 예상되는데, 이는 higher rank update 를 학습할 수 있기 때문이다. </p><p>일반적으로, 각 ReLoRA reset 시에 optimizer reset 과 learning rate scheduler 의 rewarmup 를 수행한다(Sec 1). </p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-30-3b1e4cef3000fa7230b745434036c397.png" width="779" height="320" class="img_ev3q"></p><ul><li>그러나 실험 결과, very high ReLoRA reset rate 는 오히려 성능을 저하시켰다. </li><li>Online ReLoRA 는 이 문제를 해결하는데, LoRA parameters 를 매우 자주(e.g., 매 100 iterations) 병합하면서도 optimizer reset rate 는 2-5K iterations 마다 유지한다. </li><li>예상치 못하게도, 250M 및 1.3B size 에서 Online ReLoRA 가 일반적인 ReLoRA 보다 성능이 더 나쁜 것으로 나타났다 (Tab. 5).</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="relora-training-speedup">ReLoRA Training Speedup<a href="#relora-training-speedup" class="hash-link" aria-label="Direct link to ReLoRA Training Speedup" title="Direct link to ReLoRA Training Speedup">​</a></h4><p>ReLoRA training 에는 440 A100-hours 이 소요되었으며, 이는 full-rank training 에 비해 56 A100-hours 을 절약한 것이다. </p><p>speedup 의 일부는 두 배 larger microbatch size 를 사용할 수 있었기 때문이다. </p><ul><li>동일한 microbatch size 로 훈련할 때, ReLoRA 는 RAM 소비를 27.8Gb 에서 22.3Gb 로 줄여 5.5Gb 의 GPU RAM 을 절약했다. </li><li>8xA100 setting 에서 warm start 및 ReLoRA training time 을 결합했을 때, 1.3B-ReLoRA training 에는 86시간(wall clock)이 걸렸으며, 동일한 데이터 양으로 1.3B full-rank model 을 훈련하는 데 걸린 93.5 hours 에 비해 상대적인 speedup 이 9% 였다.</li></ul><p><img loading="lazy" alt="Table 7" src="/assets/images/image-32-faaadb0b5e06c309011484c50823d7c3.png" width="1565" height="429" class="img_ev3q"></p><ul><li>추가적으로, ReLoRA speedup 은 hardware 에 크게 의존하는 것으로 나타났다(Tab. 7). </li><li>ealry 2xRTX3090 에서의 실험에서는 speedup 이 42% 로 추정되었다. </li><li>보다 실용적이지만 여전히 상대적으로 예산 친화적인 6xA6000 Ada setting 에서는 1B full-rank model training 에 152 hour wall-block training time 소요된 반면, ReLoRA model (33% warm start)에는 119 hours 가 소요되어 33 hours 를 절약하여 21% 의 상대적인 speedup 을 보였다. </li><li>이러한 차이는 GPU 메모리 속도에 기인한다고 생각되며, ReLoRA 는 less trainable parameters 를 가지고 있어 low-bandwidth memory 를 더 효율적으로 활용할 수 있다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-ablation-studies">4.2 Ablation studies<a href="#42-ablation-studies" class="hash-link" aria-label="Direct link to 4.2 Ablation studies" title="Direct link to 4.2 Ablation studies">​</a></h2><p>저자는 130M size model 을 사용하여 ReLoRA 의 4 crucial components (restart, jaged schedule, optimizer reset, warm start)에 대한 ablation studies 수행</p><p><img loading="lazy" alt="Table 6" src="/assets/images/image-31-f8f47a132700669cf316836e0d28edb8.png" width="1554" height="617" class="img_ev3q"></p><p>결과는 Tab. 6 에 제시되었다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="lora">LoRA<a href="#lora" class="hash-link" aria-label="Direct link to LoRA" title="Direct link to LoRA">​</a></h4><ul><li>components 없이 ReLoRA 는 LoRA 의 parameterized low-rank network 를 훈련하는 것과 사실상 동일하다. </li><li>이 접근법은 매우 높은 perplexity 를 나타내며, 이는 simple matrix decomposition 이 full-rank training 과는 상당히 다른 training 역학을 가지고 있음을 시사한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="adding-restarts-and-optimizer-resets">Adding restarts and optimizer resets<a href="#adding-restarts-and-optimizer-resets" class="hash-link" aria-label="Direct link to Adding restarts and optimizer resets" title="Direct link to Adding restarts and optimizer resets">​</a></h4><ul><li>ReLoRA 에서 jaged schedule 과 optimizer reset 없이 수행하면 LoRA 와 유사한 성능을 보이는데, 이는 prior weights 와 동일한 subspace 로 newly initialized parameters 를 강제로 이동시켜 모델의 용량을 제한하기 때문이다.</li><li>그러나 ReLoRA 에서 simple optimizer reset 을 수행하면 모델이 발산하게 된다. </li><li>jaged schedule 은 훈련을 안정화하는 데 도움이 되며 mixture 에 긍정적인 영향을 미친다.</li><li>초기 실험에서 partial optimizer reset 과 jaged scheduler 의 combination 이 learning rate 를 scratch initialization 에 수백 steps 이 필요한 경우보다 더 빠르게 warm-up 할 수 있게 해준다는 것을 관찰했다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="warm-start">Warm start<a href="#warm-start" class="hash-link" aria-label="Direct link to Warm start" title="Direct link to Warm start">​</a></h4><ul><li>warm start 는 가장 큰 개선을 보여주며 perplexity 를 거의 10 point 낮춘다. </li><li>warmup 이후의 training 이 loss 에 기여하는지를 조사하기 위해 warmed-up networks 의 perplexity 를 측정한 결과 27.03 으로 나타났다. </li><li>이는 final ReLoRA 레시피를 제외한 all low-rank 방법을 능가하지만, 여전히 final network 와는 상당한 차이를 보인다. </li><li>이는 early training 의 중요성을 보여주며, 이는 lottery ticket hypothesis 과 rewinding 의 개념과 유사하다. </li><li>저자의 실험에서는 별도로 명시되지 않는 한 total training updates 의 약 1/4 동안 warm start 를 수행했다.</li></ul><h1>5. Related work</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-versus-efficiency">Scaling versus Efficiency<a href="#scaling-versus-efficiency" class="hash-link" aria-label="Direct link to Scaling versus Efficiency" title="Direct link to Scaling versus Efficiency">​</a></h4><p>overparameterization 및 neural network 의 trainability 및 generalization 사이의 관계는 <!-- -->[Zhang et al., 2017, Belkin et al., 2018, Frankle and Carbin, 2019, Nakkiran et al., 2019, Singh et al., 2021]<!-- --> 에서 광범위하게 연구되었지만, 여전히 미스터리로 남아 있다. </p><p>또한, scaling lows 는 다양한 modality 에 걸쳐 network size 와 성능 사이에 단순하면서도 strong power-law dependence 가 있음을 보여준다. </p><p>이 발견은 overparameterization 을 지지할 뿐만 아니라, 엄청나게 resource-intensive neural network training 을 장려한다. </p><p>그럼에도 불구하고, Lottery Ticket Hypothesis 는 원칙적으로 overparameterization  최소화할 수 있음을 시사한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="parameter-efficient-fine-tuning">Parameter-efficient fine-tuning<a href="#parameter-efficient-fine-tuning" class="hash-link" aria-label="Direct link to Parameter-efficient fine-tuning" title="Direct link to Parameter-efficient fine-tuning">​</a></h4><p>Aghajanyan et al. <!-- -->[2021]<!-- --> 은 pre-training 이 fine-tuning 을 통해 new task 를 학습하는 데 필요한 network 변경 양을 줄인다는 사실을 발견했다. </p><p>즉, larger networks 나 more data 로 pre-trained network 는 new task 학습하는 데 필요한 범위의 rank 측면에서 더 적은 수정이 필요하다. </p><p>이는 parameter-efficient fine-tuning 방법의 성공을 설명하며 LoRA 와 Compacter 와 같은 low-rank fine-tuning 방법의 개발을 촉진했다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="low-rank-neural-network-training">Low-rank neural network training<a href="#low-rank-neural-network-training" class="hash-link" aria-label="Direct link to Low-rank neural network training" title="Direct link to Low-rank neural network training">​</a></h4><p>Low-rank representation training 은 CNN compression, regularization, efficient training 의 맥락에서 탐구되었다. </p><p>그러나 대부분의 방법들은 CNN 에만 특화되어 있거나, 확장성이 떨어지거나, millions parameters 의 large transformer 에선 평가되지 않았다. </p><p>large transformer 는 efficient training 에서 큰 이점을 얻을 수 있다. </p><p>transformer 는 low-rank internal dimensionality 와 representations 를 가지는 것으로 나타났지만, Bhojanapalli et al. <!-- -->[2020]<!-- --> 의 연구는 multi-head attention 에서 key 와 query projection 의 low-rank 가 transformer 의 성능을 저해한다는 것을 보여주었다. </p><p>저자의 실험(Sec. 6) 또한 low-rank transformer 가 full-rank baseline 및 ReLoRA 에 비해 성능이 상당히 떨어진다는 것을 보여준다.</p><p><strong>6 결론</strong></p><p>이 논문에서는 parameter-efficient fine-tuning 방법을 large language models 의 pre-training 에 적용할 수 있음을 입증했다. </p><p>먼저 low-rank matrix factorization (LoRA) 접근법의 한계를 검토하고, 이 방법이 high-performing transformer 모델을 효과적으로 훈련하는 데 어려움을 겪는다는 것을 관찰했다. </p><p>이러한 문제를 해결하기 위해 ReLoRA 를 제안했으며, 이 방법은 rank of sum property 를 활용하여 multiple low-rank update 를 통해 high-rank network 를 훈련한다. </p><p>ReLoRA 는 lottery ticket hypothesis 과 rewinding 개념과 유사하게, ReLoRA 로 전환하기 전에 full-rank training warm start 를 사용한다.</p><p>training 중 ReLoRA 는 주기적으로 parameter 를 network 의 main parameters 에 병합하고, optimizer reset 및 learning rate rewarmup 을 수행한다.</p><p>저자는 ReLoRA 가 large transformer model training 에서 LoRA 를 일관되게 능가한다는 것을 입증했다. </p><p>가장 큰 실험에서는 8xA100 setting 에서 wall-clock time 을 9% 줄였으며, 더 저렴한 hardware 에서 larger speedup(20~40%) 을 달성했다. </p><p>나아가, 저자의 결과는 ReLoRA 가 regular training 과 유사한 성능을 보여주어, large models training 의 효율성을 개선할 수 있는 유망한 후보임을 시사한다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/re-lo-ra">ReLoRA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/lo-ra">LoRA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/high-rank">High-Rank</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/low-rank">Low-Rank</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/PEFT/Composition/2023-07-ReLoRA.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/PEFT/Composition/AdaLoRA"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/PEFT/Composition/IncreLoRA"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#41-scaling-up-to-13b" class="table-of-contents__link toc-highlight">4.1 Scaling up to 1.3B</a></li><li><a href="#42-ablation-studies" class="table-of-contents__link toc-highlight">4.2 Ablation studies</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.45816a06.js"></script>
<script src="/assets/js/main.0f1c04dd.js"></script>
</body>
</html>