<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/Generalization/2023-10-NEFTune">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">NEFTune: Noisy Embeddings Improve Instruction FineTuning | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/Generalization/NEFTune"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="NEFTune: Noisy Embeddings Improve Instruction FineTuning | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/Generalization/NEFTune"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/Generalization/NEFTune" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/Generalization/NEFTune" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.5b088738.js" as="script">
<link rel="preload" href="/assets/js/main.7cbc98af.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Attacking/universal-adversarial-prompt">Attacking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Generalization/NoisyTune">Generalization</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/Generalization/NoisyTune">NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/Generalization/NEFTune">NEFTune: Noisy Embeddings Improve Instruction FineTuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/Generalization/SymNoise">SymNoise: Advancing Language Model Fine-tuning with Symmetric Noise</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Flan-T5">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Engineering/CoT/Chain-of-Thought">Prompt Engineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Generalization</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">NEFTune: Noisy Embeddings Improve Instruction FineTuning</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>NEFTune: Noisy Embeddings Improve Instruction FineTuning</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2310.05914" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2310.05914</a></p><h1>Abstract</h1><p>저자는 language model finetuning 이 단순한 augmentation 을 통해, 때로는 극적으로 개선될 수 있음을 보인다. </p><p><code>NEFTune</code> 은 training 동안 embedding vector 에 noise 를 추가한다. Alpaca 를 사용한 LLaMA-2-7B 의 standard finetuning 은 AlpacaEval 에서 29.79% 를 달성하지만, noisy embedding 을 사용하면 64.69% 로 상승한다. </p><p><code>NEFTune</code> 은 또한 최신 instruction dataset 에서 강력한 baseline 보다 성능을 향상시킨다. Evol-Instruct 로 학습된 model 은 10% 개선, ShareGPT 는 8% 개선, OpenPlatypus 는 8% 개선을 보인다. 심지어 RLHF 로 추가적으로 refinement 된 강력한 model 인 LLaMA-2-Chat 역시 <code>NEFTune</code> 을 사용한 추가 training 으로부터 이점을 얻는다.</p><h1>1 Introduction</h1><p>LLM 의 상세한 instruction 을 따르는 능력은 그 유용성에 필수적이다. Generative language model 은 일반적으로 raw web data 로 학습된 후, 상대적으로 작은 규모지만 신중하게 선별된 instruction data 로 finetuning 된다. Instruction finetuning 은 LLM 의 능력을 제어하는 데 핵심적이며, model 의 유용성은 작은 instruction dataset 에서 최대한 성능을 이끌어낼 수 있는 능력에 의해 크게 좌우된다.</p><p>이 논문에서는 finetuning 의 forward pass 동안 training data 의 embedding vector 에 random noise 를 추가하는 방법을 제안한다. 저자는 이 단순한 trick 이 instruction finetuning 의 결과를, 종종 큰 폭으로, 추가적인 compute 나 data overhead 없이 개선할 수 있음을 보인다. <strong>Noisy Embedding Instruction Fine Tuning (<code>NEFTune</code>)</strong> 은 단순하지만, downstream 대화 품질에 강력한 영향을 미친다. </p><p>LLaMA-2-7B 와 같은 raw LLM 이 noisy embedding 으로 finetuning 될 때, AlpacaEval 에서의 성능은 29.8% 에서 64.7% 로 향상되며 (Fig. 1), 이는 약 35 퍼센트 포인트의 인상적인 향상이다. <code>NEFTune</code> 은 대화 task 에서 놀랍고도 큰 성능 향상을 이끌어내며, factual question answering baseline 에서의 성능은 유지한다. 이 기법은 LLM finetuning 을 위한 &quot;free lunch&quot; 와도 같다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-dfa30a8737d7cb41007e7b8d1ae02ef1.png" width="3358" height="1362" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="11-related-work">1.1 Related Work<a href="#11-related-work" class="hash-link" aria-label="Direct link to 1.1 Related Work" title="Direct link to 1.1 Related Work">​</a></h2><p>가장 초기의 instruction finetuning 은 FLAN 과 T0 로, cross-task generalization 에 초점을 맞췄다. Encoder-decoder language model 은 약 100 개의 다양한 NLP task 로 finetuning 되었고, 이후 다른 task 에 대해 평가되었다. 이후 수천 개의 task 로 확장되며 original FLAN 보다 더 나은 성능을 보였다. 이러한 연구들은 LLM 이 단순하고 고전적인 NLP task 에 쉽게 적응할 수 있음을 보여주었지만, 실제 세계의 시나리오에서는 LLM 이 open-ended query 에 대해 자유 형식의 답변을 제공할 수 있어야 한다.</p><p>InstructGPT 는 처음으로 open-ended query 를 인상적으로 처리한 model 이었다. OpenAI 는 GPT-3 을 human feedback 기반의 reinforcement learning (RLHF) 으로 추가 학습하여 model 을 alignment 시켰다. 이 과정은 대중의 상상력을 사로잡고 InstructGPT 이전보다 더 길고 일관된 텍스트를 생성하는 매우 인기 있는 model 인 ChatGPT 를 탄생시켰다.</p><ul><li>이후 Wang et al. (Self-Instruct) 은 InstructGPT (Text-Davinci-003) 를 이용해 instruction-output pair 를 생성하고, 이를 LLaMA 와 같은 foundation model 을 instruction following variant (e.g., Alpaca) 로 finetuning 하는 방법을 제안했다. </li><li>Taori et al. 은 distilled model 의 인기에 힘입어, ChatGPT 등 다른 model 로부터 특정 방식으로 distillation 한 dataset 을 구축했고, Xu et al. 또한 이와 유사한 dataset 을 만들었다. </li><li>또 다른 접근으로, ShareGPT 는 ChatGPT 사용자들의 실제 대화를 crowdsourcing 으로 수집하여 구성되었다. </li><li>Lee et al. 은 STEM question answering 과 logical reasoning 같은 특정 능력을 향상시키기 위해 dataset 을 구축했다. AlpaGasus 는 GPT-4 로 평가된 data quality 를 기준으로 filtering 하여 성능을 개선했다.</li></ul><p>한편, noisy input 은 다양한 방식으로 model 을 개선하는 데 사용되어 왔다. </p><ul><li>Language model 개선을 위해 noise 가 사용된 최초의 사례는 Zhu et al. 의 FreeLB 방법으로, adversarial perturbation 이 MLM model 의 성능을 향상시킨다는 것을 관찰했다. </li><li>이 경우 noise 는 random 이 아니라, embedding 에 작은 Gaussian perturbation 을 추가한 후, model 성능을 최대한 변화시키는 방향으로 gradient step 을 통해 계산된다. </li><li>이러한 adversarial augmentation 접근법은 graph model 성능 향상에도 기여했다. </li></ul><p>저자의 방식은 non-adversarial 이지만, 이러한 연구의 noise scaling rule 을 채택한다. Noisy input 으로 학습하는 방법은 image captioning system 을 개선하거나, 초기 differential privacy mechanism 의 공통 요소로도 사용되었다.</p><h1>2 <code>NEFTune</code>: Noisy Embedding Instruction Finetuning</h1><p>Instruction model 은 instruction 과 response 의 pair 로 구성된 dataset 에 대해 학습된다. <code>NEFTune</code> 의 각 step 은 dataset 에서 하나의 instruction 을 sampling 하고, 이를 token 에서 embedding vector 로 변환하는 것에서 시작된다. <code>NEFTune</code> 은 이후 standard training 과 달리 embedding 에 random noise vector 를 추가한다. 이 noise 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span> 범위에서 iid uniform sampling 하여 생성되며, 전체 noise vector 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mi mathvariant="normal">/</mi><msqrt><mrow><mi>L</mi><mi>d</mi></mrow></msqrt></mrow><annotation encoding="application/x-tex">\alpha / \sqrt{Ld}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1822em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mord">/</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord mathnormal">L</span><span class="mord mathnormal">d</span></span></span><span style="top:-2.8922em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1078em"><span></span></span></span></span></span></span></span></span></span> 로 scaling 된다. 여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span></span> 은 sequence length, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 는 embedding dimension, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 는 tunable parameter 이다.</p><p>이 scaling rule 은 adversarial ML 연구에서 차용되었으며, 결과적으로 기대되는 Euclidean magnitude 가 대략 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mi mathvariant="normal">/</mi><msqrt><mn>3</mn></msqrt></mrow><annotation encoding="application/x-tex">\alpha / \sqrt{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1572em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mord">/</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9072em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord">3</span></span></span><span style="top:-2.8672em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1328em"><span></span></span></span></span></span></span></span></span></span> 인 random vector 를 생성한다. Algorithm 1 은 저자의 방법을 자세히 설명한다.</p><p><img loading="lazy" alt="Algorithm 1" src="/assets/images/image-1-b4108e1b419926b53287d18cda441287.png" width="3358" height="1597" class="img_ev3q"></p><h1>3 Experimental Set-up</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-models">3.1 Models<a href="#31-models" class="hash-link" aria-label="Direct link to 3.1 Models" title="Direct link to 3.1 Models">​</a></h2><p>저자는 주로 7B parameter LLM 을 사용하여 실험을 수행한다. 구체적으로 LLaMA-1, LLaMA-2, OPT-6.7B 를 사용한다. 이들 transformer 는 구조적으로 유사하나, training 중에 본 token 의 수에서 차이가 있다. OPT, LLaMA-1, LLaMA-2 는 각각 180B, 1T, 2T token 으로 학습되었다. 이 차이는 MMLU 와 같은 benchmark 에서 성능 차이로 나타나며, LLaMA-2 가 가장 우수하고 OPT 가 가장 낮은 성능을 보인다. 13B 및 70B parameter model 의 경우 LLaMA-2 를 학습시킨다. 추가적으로, 저자는 LLaMA-2-Chat (7B) 과 같이 RLHF 로 refinement 된 model 을 <code>NEFTune</code> 으로 finetuning 하여 개선한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-instruction-finetuning-datasets">3.2 Instruction Finetuning Datasets<a href="#32-instruction-finetuning-datasets" class="hash-link" aria-label="Direct link to 3.2 Instruction Finetuning Datasets" title="Direct link to 3.2 Instruction Finetuning Datasets">​</a></h2><p>저자는 널리 사용되었거나 최근 SOTA 결과를 낸 finetuning dataset 에 집중한다. 하드웨어 제약으로 인해 single-turn dataset 만 사용한다.</p><ul><li><strong>Alpaca</strong>: Self-Instruct 방법과 Text-Davinci-003 model 을 사용하여 구축되었다. Self-Instruct 는 작은 seed task set 을 사용해 새로운 instruction tuning task 를 생성하고, 부적절한 task 를 제거한다.</li><li><strong>Evol-Instruct</strong>: 70k 개의 single-turn instruction 으로 구성되며 Alpaca 보다 더 복잡하다. Alpaca dataset 을 기반으로 ChatGPT 를 사용하여 초기 instruction 을 진화시켜 생성되었다.</li><li><strong>Open-Platypus</strong>: 11 개의 open-source dataset 을 결합하여 만든 curated dataset 으로, STEM 및 논리적 reasoning domain 에서 LLM 성능 향상을 목적으로 한다. 약 25k 개의 질문을 포함하며, 이 중 약 10% 는 LLM 이 생성한 것이고 나머지는 사람이 작성했다.</li><li><strong>ShareGPT</strong>: 70k 개의 자발적으로 공유된 ChatGPT 대화로 구성되었다. ShareGPT 는 multiturn 이지만, Vicuna v1.1 dataset version 을 사용하여 multi-turn 대화를 single-turn 형식에 가깝게 분할하여 사용한다.</li></ul><p>모든 model 은 Alpaca system prompt 로 finetuning 하되, ShareGPT 의 경우 Vicuna system prompt 를 사용한다. 저자는 LLaMA-1 (7B) 를 Alpaca dataset 으로 학습하며 coarse sweep 을 통해 hyperparameter 를 설정했고, standard Alpaca model 대비 6% 의 성능 향상을 확인했다. 이 값을 모든 model 에 대해 default 로 사용한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-evaluation">3.3 Evaluation<a href="#33-evaluation" class="hash-link" aria-label="Direct link to 3.3 Evaluation" title="Direct link to 3.3 Evaluation">​</a></h2><p>저자는 주로 single-turn data 로 학습하므로, model 의 대화 능력을 AlpacaEval 로 평가한다. 또한 OpenLLM Leaderboard 의 task 를 평가하여 <code>NEFTune</code> augmentation 이 standard multiple choice task 성능을 저해하지 않는지 확인한다.</p><ul><li><strong>AlpacaEval</strong>: AlpacaEval dataset 은 generation 의 전반적 품질을 평가하기 위해 사용된다. <ul><li>AlpacaEval 은 automatic model-based evaluation 으로, 805 개 instruction 에 대해 Text-Davinci-003 과 model 의 출력을 비교하고 Win Rate 를 산출한다. </li><li>Win Rate 는 GPT-4 evaluator 가 판단했을 때 Text-Davinci-003 보다 선호된 비율이다. </li><li>805 개의 test prompt 는 Vicuna, Koala, Anthropic’s hh-rlhf 등에서 수집되어 포괄적이고 다양한 평가를 제공한다. </li><li>또한 AlpacaEval 은 사람 평가와 높은 일치도를 보인다 (20k annotation 검증). </li><li>저자는 7B 및 13B scale 에서 이 평가가 충분히 합리적이라고 본다. Evaluator 로 GPT-4 와 ChatGPT 를 모두 사용하며, 비용과 API 제한 문제로 사전 test 는 ChatGPT 로 수행하고, GPT-4 평가 여부를 결정한다.</li></ul></li><li><strong>Hugging Face OpenLLM Leaderboard</strong>: Leaderboard 에 사용되는 평가 dataset 은 ARC, HellaSwag, MMLU, TruthfulQA 로, 이는 LLM 이 factual question 및 reasoning challenge 에 응답할 수 있는 능력을 폭넓게 평가한다. <ul><li>저자는 이 dataset 으로 평가하여 <code>NEFTune</code> 이 model 의 능력을 저해하지 않는지 확인한다.</li></ul></li></ul><h1>4 Results</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="neftune-improves-text-quality">NEFTune` Improves Text Quality.<a href="#neftune-improves-text-quality" class="hash-link" aria-label="Direct link to NEFTune` Improves Text Quality." title="Direct link to NEFTune` Improves Text Quality.">​</a></h4><p><img loading="lazy" alt="Table 1" src="/assets/images/image-2-debec246f8cc9cf3ecb74884dea98cc5.png" width="3358" height="641" class="img_ev3q"></p><p>Tab. 1 에서 보듯, 7B scale 에서 모든 dataset 에 대해 평균 15.1% 의 향상이 나타났으며, 이는 AlpacaEval 로 측정한 대화 능력 및 답변 품질이 NEFT 학습을 통해 크게 개선됨을 보여준다. </p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-3-bdeaa219396bbb87ff9682b89214e35b.png" width="3358" height="2059" class="img_ev3q"></p><p>또한 Fig. 2 에서 보듯, LLaMA-1 및 OPT 와 같은 구형 model 에서도 성능 향상이 나타난다. 흥미롭게도, ChatGPT 평가 기준에서는 ShareGPT dataset 에서 다른 dataset 보다 향상이 덜한 것으로 나타난다. 그러나 GPT-4 평가에서는 이러한 차이가 반영되지 않는다.</p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-5-32cbcbeb47509319d023946fdfdf7dec.png" width="3358" height="743" class="img_ev3q"></p><p>Tab. 2 에 따르면, Evol-Instruct 로 학습된 70B parameter model 에 <code>NEFTune</code> 을 적용할 경우 Win Rate 가 75.03% 에서 88.81% 로 상승하며 (+13.78%), 세부 hyperparameter 는 Appendix A.1 에 기술되어 있다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="neftune-can-improve-chat-models">NEFTune` Can Improve Chat Models.<a href="#neftune-can-improve-chat-models" class="hash-link" aria-label="Direct link to NEFTune` Can Improve Chat Models." title="Direct link to NEFTune` Can Improve Chat Models.">​</a></h4><p>Tab. 2 에서 보듯, Evol-Instruct 로 LLaMA-2-Chat (7B) 을 추가 instruction finetuning 할 경우, 이미 다단계 RLHF 로 광범위하게 조정된 model 임에도 불구하고 성능이 3% 향상된다. 더 나아가, <code>NEFTune</code> 을 적용하면 추가로 10% 의 성능 향상이 나타난다. 다만, 해당 checkpoint model 의 특정 능력, 예를 들어 toxic behavior 출력을 회피하는 능력은 영향을 받을 수 있다. 그럼에도 불구하고, 이미 정교하게 조정된 chat model 의 대화 품질이 이처럼 극적으로 개선된다는 것은 놀라운 결과이다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="effect-on-capabilities">Effect on Capabilities.<a href="#effect-on-capabilities" class="hash-link" aria-label="Direct link to Effect on Capabilities." title="Direct link to Effect on Capabilities.">​</a></h4><p>한 가지 잠재적 우려는 <code>NEFTune</code> 이 대화 능력을 향상시키는 대신 기존의 classical skill 을 저해할 수 있다는 점이다. 이를 검증하기 위해, 저자는 OpenLLM Leaderboard task (MMLU, ARC, HellaSwag, TruthfulQA) 를 LMEval Harness implementation 으로 평가한다. 이 benchmark 는 model 의 knowledge, reasoning, truthfulness 를 평가한다. Fig. 3 은 score 가 안정적으로 유지되며, <code>NEFTune</code> 이 model 의 capability 를 보존함을 보여준다.</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-4-a0647bdae9dec6bdccde0fde17237b2c.png" width="3358" height="2512" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="neftune-works-with-qlora">NEFTune` Works with QLORA.<a href="#neftune-works-with-qlora" class="hash-link" aria-label="Direct link to NEFTune` Works with QLORA." title="Direct link to NEFTune` Works with QLORA.">​</a></h4><p>저자는 <code>NEFTune</code> 이 제한된 자원 환경에서도 성능을 개선함을 보인다. 이를 위해 Quantized Low Rank Adapters (QLORA) 로 학습을 진행한다. Dettmers et al. 의 implementation 을 사용하고, 모든 model weight 에 대해 기본 training hyperparameter 를 적용하며 단 1 epoch 학습한다. 30B 의 경우, effective batch size 를 2 배로 늘리고 learning rate 를 절반으로 줄인다. </p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-6-f5bfc9ff50f1ac2b1e140baac184d09a.png" width="3358" height="964" class="img_ev3q"></p><ul><li>Tab. 3 에 따르면 QLORA 로 학습할 때도 AlpacaEval 성능은 모든 model scale 및 dataset 에서 증가한다. </li><li>다만 full-scale finetuning 과 비교했을 때 성능 향상 폭은 크지 않은데, 이는 finetuning epoch 수와 같은 hyperparameter 가 달라야 하거나, 4-bit 로 강하게 quantization 했기 때문일 수 있다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="a-qualitative-example">A Qualitative Example.<a href="#a-qualitative-example" class="hash-link" aria-label="Direct link to A Qualitative Example." title="Direct link to A Qualitative Example.">​</a></h4><p>저자는 Alpaca dataset 으로 학습된 LLaMA-2 model 에서 NEFT 적용 전후의 예시를 제시한다. </p><ul><li>Vicuna Eval subset 의 AlpacaEval instruction 중 quantum computing prompt 를 선택했다. </li><li>Alpaca model 의 응답은 단순히 quantum computing 의 기본 정의를 간략히 제시하며, qubit, superposition, entanglement, 복잡한 계산 능력 증가 등을 짧게 언급하는 수준이다. </li><li>반면, Alpaca-NEFT model 의 응답은 보다 유창하고, superposition 과 quantum entanglement 에 대해 더 명확히 설명하며, 잠재적 응용 가능성까지 언급한다. </li></ul><p>저자는 이 예시가 NEFT 가 유도하는 변화의 전형적인 사례라고 본다.</p><p><img loading="lazy" alt="Prompt" src="/assets/images/image-7-e510253960d52f361a874ea05390d1a4.png" width="3358" height="2212" class="img_ev3q"></p><h1>5 Analysis</h1><p>저자는 training 시 embedding 에 noise 를 추가함으로써, model 이 instruction-tuning dataset 의 세부 요소들—e.g., formatting detail, exact wording, text length—에 과적합하는 현상이 줄어든다고 가정한다. 즉, model 이 특정 instruction 분포에 그대로 수렴하는 대신, pretrained base model 의 knowledge 와 behavior 를 더 잘 활용한 응답을 생성할 수 있게 된다.</p><p>즉각적으로 관찰되는 부수 효과는 model 이 더 일관되고 긴 completion 을 생성한다는 점이다. 대부분의 dataset 에서 human 과 machine evaluator 는 더 길고 verbose 한 completion 을 선호한다. 그러나 저자는 이러한 verbosity 증가는 instruction 분포에 대한 과적합 감소에서 나타나는 가장 눈에 띄는 부산물일 뿐이며, 성능 향상을 전적으로 설명할 수는 없다고 본다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-overfitting">5.1 Overfitting<a href="#51-overfitting" class="hash-link" aria-label="Direct link to 5.1 Overfitting" title="Direct link to 5.1 Overfitting">​</a></h2><p>이 분석에서는 Alpaca dataset 으로 학습된 LLaMA-2-7B model 을 <code>NEFTune</code> 적용 여부에 따라 비교한다. 두 model 모두 Alpaca dataset 에 대해 noise 없이 training loss 를 측정하고, Evol-Instruct dataset 에 대해 “testing” loss 를 측정한다. </p><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-8-7bac4d283988a1afda5d981eaddca3b6.png" width="3358" height="1542" class="img_ev3q"></p><ul><li>Fig. 4 에 따르면, <code>NEFTune</code> model 은 base model (<code>NEFTune</code> 미적용) 대비 training loss 는 현저히 높지만 testing loss 는 약간 더 낮다. </li><li>이는 <code>NEFTune</code> 을 사용할 때 과적합이 줄어들고 일반화가 더 잘 이루어짐을 시사한다.</li></ul><p>과적합 가설을 더 검증하기 위해, 저자는 greedy decoding 으로 training prompt 에 대한 응답을 생성한다. 생성된 응답을 dataset 의 ground truth response 와 비교하고, 그 결과를 Fig. 5 에 제시한다. </p><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-9-59da2ef66111ee1efe5d28758174162c.png" width="3358" height="1368" class="img_ev3q"></p><ul><li>응답 간 유사도 측정을 위해 ROUGE-L 과 BLEU (n-gram order 4 까지) 를 사용한다. </li><li>Fig. 5 에 따르면, <code>NEFTune</code> 으로 학습된 model 의 응답은 ROUGE-L 과 BLEU score 가 유의미하게 낮다. </li><li>ROUGE-L 은 단어의 longest common subsequence 기반이고, BLEU 는 응답 간 common n-gram 기반이므로, <code>NEFTune</code> 없이 학습된 model 이 더 높은 score 를 보인다는 것은 그 응답이 ground truth response 와 동일한 단어 순서를 훨씬 더 많이 포함한다는 것을 의미한다.</li></ul><p>이러한 관찰을 종합하면, standard finetuning recipe 는 최대 성능을 위해 조정되었지만 instruction dataset 에 과도하게 과적합하여 일부 응답을 거의 그대로 재현하게 된다. 반면 <code>NEFTune</code> model 은 test set 성능을 저해하지 않으면서 과적합을 줄이고, instruction data 의 exact wording 에 “lock-in” 되지 않음을 ROUGE-L metric 을 통해 확인할 수 있다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-length-versus-token-diversity">5.2 Length Versus Token Diversity<a href="#52-length-versus-token-diversity" class="hash-link" aria-label="Direct link to 5.2 Length Versus Token Diversity" title="Direct link to 5.2 Length Versus Token Diversity">​</a></h2><p>저자는 AlpacaEval task 에서 length 증가와 성능 향상 간의 강한 상관관계(저자의 실험 및 public leaderboard 제출 사례에서 모두 확인됨)를 고려하여, <code>NEFTune</code> 으로 관찰된 length 증가가 text 다양성 감소를 초래하는지 여부를 조사했다. 이를 위해, <code>NEFTune</code> 적용 여부에 따라 서로 다른 finetuning dataset 으로 학습된 LLaMA-2 에 대해 n-gram repetition rate 를 계산했다.</p><p>일반적으로 긴 passage 에서는 n-gram 이 더 자주 반복되므로, passage length 를 통제해야 한다. 저자는 각 sample 의 시작 부분에서 고정 길이 chunk 를 사용하여 repetition 과 diversity score 를 계산했다. cutoff 는 Alpaca 학습 model 의 경우 50, Evol-Instruct 는 100, ShareGPT 는 150, OpenPlatypus 는 150 으로 설정했다. 이 길이는 최소 절반 이상의 generation 이 cutoff 보다 길도록 선택했으며, 길이가 부족한 sequence 는 제거했다. Diversity score 는 2-, 3-, 4-gram repetition rate 의 요약 지표인 log-diversity 로 계산했으며, 이는 Kirchenbauer et al. 및 Li et al. 의 정의를 따른다.</p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-10-176ba44a8e38efea91fbede505a3d1d8.png" width="3358" height="1692" class="img_ev3q"></p><ul><li>Tab. 4 와 Tab. 6 에 따르면, NEFT model 은 대응 model 보다 더 긴 출력을 생성한다. </li><li>그러나 2-gram repetition rate 및 token log-diversity 는 NEFT 적용 여부와 거의 동일하여, 더 긴 응답이 단순한 반복 증가로 인한 것이 아니라 추가적인 세부 정보 제공 때문임을 보여준다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="53-length-is-not-all-you-need-length">5.3 Length is (Not) All You Need Length<a href="#53-length-is-not-all-you-need-length" class="hash-link" aria-label="Direct link to 5.3 Length is (Not) All You Need Length" title="Direct link to 5.3 Length is (Not) All You Need Length">​</a></h2><p>저자는 length–leaderboard 상관관계를 더 면밀히 분석하기 위해, 단순히 model 의 출력을 길게 유도하는 것만으로 NEFT 학습 model 의 성능 향상을 재현할 수 있는지 실험했다 (Tab. 5).</p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-11-56513564e48226f81895d88233eb25ef.png" width="3358" height="1075" class="img_ev3q"></p><ul><li>먼저, 명시적으로 긴 답변을 요청하는 prompt 를 주었다. 흥미롭게도, 이 경우 AlpacaEval score 가 16% 상승했다. </li><li>또 다른 방법으로, <!-- -->[EOS]<!-- --> token 을 250 token 길이에 도달할 때까지 차단하여, standard model 이 NEFT 와 동일한 길이의 답변을 강제로 생성하게 했다. 이 경우 standard finetuning 대비 소폭의 개선만 나타났다.</li></ul><p>마지막으로, NEFT algorithm 에서 uniform noise 대신 Gaussian noise 를 사용하는 ablation 을 수행했다. Gaussian noise 는 더 긴 출력을 유도했지만 성능 향상은 나타나지 않았다 (Tab. 6).</p><p><img loading="lazy" alt="Table 6" src="/assets/images/image-12-1383ddf28550a60215c1b68394c12002.png" width="3358" height="1164" class="img_ev3q"></p><p>즉, 더 긴 generation 이 성능 향상에 기여할 수는 있지만, 어떤 generation-time 전략도 <code>NEFTune</code> model 의 성능에는 미치지 못했다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="54-human-study">5.4 Human Study<a href="#54-human-study" class="hash-link" aria-label="Direct link to 5.4 Human Study" title="Direct link to 5.4 Human Study">​</a></h2><p>저자의 주요 결과는 LLM 기반 평가 지표인 AlpacaEval 에 기초하고 있으므로, 저자는 소규모 human study 도 수행했다. AlpacaEval 에서 140 개 instruction 을 추출하여, annotator 에게 Alpaca data 로 finetuning 된 LLaMA-2 model 의 두 응답(NEFT 적용/미적용)을 무작위 순서로 제시했다.</p><p>Human annotator 는 88 개 instance 에서 NEFT 응답을 선호했고, 22 개 instance 는 무승부였다. 이는 AlpacaEval 공식(88 / (140 − 22))을 적용하면 NEFT 가 74.6% win score 를 얻는다는 의미다. 이어서 저자는 AlpacaEval 을 변형하여, evaluator (GPT-4) 가 Text-Davinci-003 과의 비교 대신 standard finetuned model 과 동일 model 의 NEFT 버전 출력을 비교하도록 했다. 이 경우, win score 는 92.80% 로 나타났다.</p><h1>6 Conclusions and Limitations</h1><p><code>NEFTune</code> 의 성공은 LLM training 에서 종종 간과되는 algorithm 및 regularizer 의 중요성을 시사한다. Computer vision 분야에서는 regularization 과 overfitting 이 오랫동안 연구되어 왔지만, LLM 분야에서는 일반화보다는 optimizer stability 를 목표로 설계된 표준화된 training loop 가 주로 사용되어 왔다. 이런 환경에서 LLM 연구자들은 dataset 과 model scaling 에 지나치게 집중해왔다. 그러나 <code>NEFTune</code> 의 일관된 성능 향상과, 작은 instruction dataset 에서 나타나는 과적합 경향을 고려할 때, LLM setting 에서 regularization 연구가 다시 주목받을 필요가 있다.</p><p>저자의 연구에는 몇 가지 한계가 있다. 첫째, 저자는 LLM 의 instruction-following 능력을 측정하는 주요 지표로 AlpacaEval 을 사용했는데, 이는 단일 evaluator (GPT-4) 의 편향에 영향을 받을 수 있다. 둘째, 제한된 compute 자원으로 인해, 저자는 70B variant 를 여러 dataset 에서 검증하지 못했으며, 대부분의 <code>NEFTune</code> run 에서 sweeping 대신 고정된 hyperparameter 에 의존해야 했다. 마지막으로, 경험적 연구 결과에도 불구하고 <code>NEFTune</code> 이 작동하는 이유에 대해서는 여전히 확정적인 이해에 도달하지 못했다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/nef-tune">NEFTune</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/noisy-embedding">noisy embedding</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/l-la-ma">LLaMA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/uniform-distribution">Uniform Distribution</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/Generalization/2023-10-NEFTune.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/Generalization/NoisyTune"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/Generalization/SymNoise"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">SymNoise: Advancing Language Model Fine-tuning with Symmetric Noise</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#11-related-work" class="table-of-contents__link toc-highlight">1.1 Related Work</a></li><li><a href="#31-models" class="table-of-contents__link toc-highlight">3.1 Models</a></li><li><a href="#32-instruction-finetuning-datasets" class="table-of-contents__link toc-highlight">3.2 Instruction Finetuning Datasets</a></li><li><a href="#33-evaluation" class="table-of-contents__link toc-highlight">3.3 Evaluation</a></li><li><a href="#51-overfitting" class="table-of-contents__link toc-highlight">5.1 Overfitting</a></li><li><a href="#52-length-versus-token-diversity" class="table-of-contents__link toc-highlight">5.2 Length Versus Token Diversity</a></li><li><a href="#53-length-is-not-all-you-need-length" class="table-of-contents__link toc-highlight">5.3 Length is (Not) All You Need Length</a></li><li><a href="#54-human-study" class="table-of-contents__link toc-highlight">5.4 Human Study</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.5b088738.js"></script>
<script src="/assets/js/main.7cbc98af.js"></script>
</body>
</html>