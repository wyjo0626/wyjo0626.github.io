<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/Multi-Task/2023-05-CodeT5p">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">CodeT5+: Open Code Large Language Models for Code Understanding and Generation | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/Multi-Task/CodeT5+"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="CodeT5+: Open Code Large Language Models for Code Understanding and Generation | My Site"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/Multi-Task/CodeT5+"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/Multi-Task/CodeT5+" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/Multi-Task/CodeT5+" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.d2ad26d0.css">
<link rel="preload" href="/assets/js/runtime~main.845dd18d.js" as="script">
<link rel="preload" href="/assets/js/main.8a2584ef.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Chain-of-Thought">Multi-Task</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Chain-of-Thought">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Flan-T5">Scaling Instruction-Finetuned Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/Multi-Task/CodeT5+">CodeT5+: Open Code Large Language Models for Code Understanding and Generation</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Multi-Task</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">CodeT5+: Open Code Large Language Models for Code Understanding and Generation</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>CodeT5+: Open Code Large Language Models for Code Understanding and Generation</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2305.07922v2.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2305.07922v2.pdf</a></p><h1>Abstract</h1><p>현재 code LLMs 는 두 가지 주요 한계점 존재</p><ol><li>specific architecture (encoder-only, decoder-only) 또는 encoder-decoder network 에 의존<ul><li>전자는 응용하는데 있어 inflexibility</li><li>후자는 모든 task 에 대한 single system 을 다루어 subset 에 대해 suboptimal 성능을 보임</li></ul></li><li>관련없는 downstream task 로 pretraining limited set 을 사용</li></ol><p>이를 해결하기 위해, component modules 를 유연하게 결합하여 넓은 범위의 downstream code task 에 적합한 encoder-decoder LLMs 의 <strong>CodeT5+</strong> 를 제안</p><p>이러한 유연성은 pretrain-finetune 불일치성을 완화하기 위해 <strong>pretraining objectives 의 mixture</strong>을 제안</p><p>이 objectives 는 단일 또는 이중의 code 말뭉치에서 span denoising, contrasive learning, text-code matching 및 causal LM pretraining tasks 수행 가능</p><p>또한 모델을 효율적으로 scale up 하기 위해 CodeT5+ 를 처음부터 훈련하지 않고 frozen off-the-shelf LLMs 으로 초기화하는 방법</p><p>그리고 instruction-tuning 을 탐구</p><p>저자는 CodeT5+ 를 20개의 code-related benchmarks 에 광범위하게 평가를 했으며, zero-shot, finetuning, instruction-tuning 을 포함한다.</p><p>code generation/completion, math programming 및 text-to-code retrieval task 같은 다양한 code-related task 에서 SOTA 를 달성</p><h1>1. Introduction</h1><p>LLMs 는 대규모 코드 기반 데이터 (예; GitHub 공개 데이터)로 pretrain 하여 다양한 코드 관련 downstream task 로 transfer 할 수 있다.</p><p>하지만 기존의 많은 모델이 특정 downstream task 만 잘 수행되도록 설계가 되어 있다. 이는 주로 아키텍처와 pretraining task 수행 과제에 대한 두 제한으로 인한 것으로 주장한다.</p><hr><p><strong>Architecture</strong></p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-8e1ada16915a7783d6b61a8bf2815fb1.png" width="690" height="381" class="img_ev3q"></p><p>기존의 code LLM 은 understanding / generation task 에만 잘 수행되는 encoder-only / decoder-only 모델을 채택한다.</p><p>특히, text-to-code retrieval 같은 understading task 용이한 encoder model,
code generation 같은 generation task 에 대한 decoder model 이 강력한 성능을 보인다.</p><p>그러나 디코더 모델은 인코더 모델에 비해 검색 및 감지 작업같은 understading task 에 이상적이지 않으며, 최근 encoder-decoder architecture 를 많이 채택한다.</p><p>understanding 과 generation 모두를 지원하지만, 여전히 최적의 성능을 내지 못한다.</p><p>Unixcoder 는 encoder-decoder 모델이 검색 및 코드 완성 작업에서 SOTA 인 encoder 및 decoder-only 모델을 능가하지 못하는 것을 발견했다.</p><p>이 결함은 주로 모든 작업에 적응되는 단일 모듈 아키텍처의 한계다. 요약하자면, 기존 접근방식은 개별 구성 요소가 다른 downstream task 에 더 활성화될 수 있도록 설계되지 않았다.</p><hr><p>현재 제한된 훈련셋으로 사용하여, pretrain, transfer 사이의 불일치로 인한 downstream task 의 성능 하락을 야기함.</p><p>예로 T5 기반 모델은 종종 span denoising 목적으로 훈련된다. 그러나 코드 생성 같은 downstream task 의 대부분 SOTA 모델은 프로그램 토큰을 하나씩 auto-regressively predict 하여 다음 토큰을 예측하는 목적으로 pretrain 한다.</p><p>최근 시도는 위 문제 완화를 위해 contrastive learning 을 도입하지만, text 와 code representation 사이의 alignment 를 무시한다.</p><hr><p>저자는 CodeT5+ 의 모델 크기를 확장하기 위해 계산 효율적인 pretrain 전략을 사용하여 CodeT5+ 의 구성 요소를 초기화하는데 존재하는 code LLMs 를 활용한다.</p><p>pretrained checkpoint 로 encoder, decoder 를 초기화하고 cross attention layer 로 연결하는 &quot;shallow encoder, deep decoder&quot; architecture 를 채택한다.</p><p>deep decoder LLM 은 고정하며, shallow encoder 와 cross attention layer 만 훈련하여 효율적인 조정을 위해 훈련 가능한 매개 변수의 수를 크게 줄인다.</p><p>마지막으로, NLP 분야의 최근 연구인로 Instruction tuning 으로 탐구한다.</p><hr><p>20개 이상의 code 관련 벤치마크에서 CodeT5+ 를 광범위하게 평가했다.</p><p>zero-shot, finetuning 및 instruction tuning 을 포함한다.</p><p>결과는 CodeT+ 가 많은 downstream 작업에서 SOTA baseline 에 비해 상당한 성능 향상을 보여준다.</p><h1>2. Related Work</h1><h1>3. CodeT5+: Open Code Large Language Models</h1><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/05132274-f16b-4877-8d32-6083f1b509ea/image.png" class="img_ev3q"></p><p>code understanding 및 generation task 를 위한 새로운 open code LLMs 인 CodeT5+ 를 개발.</p><p>encoder-decoder 를 기반으로한 CodeT5+ 는 unimodal 및 bimodal 데이터에 대한 다양한 pretrain objectives 를 통해 다양한 모드에서 작동 가능한 유연성을 갖추고 있다.</p><ol><li>unimodal pretraining 첫 단계로, 계산 효율적인 목적으로 대규모 코드 데이터로 pretrain</li><li>bimodal pretraining 두 번째 단계로, cross-modal 학습 목적으로 code-text data 의 smaller set으로 모델을 계속 pretrain</li><li>각 단계에서 여러 pretrain objective 를 동일한 가중치로 공통 최적화</li></ol><p>위 접근 방식이 모델이 다양한 데이터에 노출되어 풍부한 context representation 을 학습하는 데 효율적이란 것을 발견</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/0855f084-7dc0-4ab4-8ec3-8069254a01fc/image.png" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-unimodal-pretraining-on-code-data">3.1. Unimodal Pretraining on Code Data<a href="#31-unimodal-pretraining-on-code-data" class="hash-link" aria-label="Direct link to 3.1. Unimodal Pretraining on Code Data" title="Direct link to 3.1. Unimodal Pretraining on Code Data">​</a></h2><p>먼저, CodeT+ 를 대규모 코드 unimodal data 로 사전학습. GitHub 의 오픈 소스로, 코드 및 주석이 포함되어 있음.</p><p>두 번째로, code-text 쌍의 데이터로, Span Denoising 및 CLM task 를 사전학습한다. 이 작업은 모델이 다양한 범위의 code context 를 복구하는 방법을 학습할 수 있도록 한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="span-denoising">Span Denoising<a href="#span-denoising" class="hash-link" aria-label="Direct link to Span Denoising" title="Direct link to Span Denoising">​</a></h3><p>encoder input 에 15% 토큰을 무작위 mask 로 대체하고, decoder 가 복구하도록 한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="causal-language-modeling-clm">Causal Language Modeling (CLM)<a href="#causal-language-modeling-clm" class="hash-link" aria-label="Direct link to Causal Language Modeling (CLM)" title="Direct link to Causal Language Modeling (CLM)">​</a></h3><p>두 가지 변형으로 auto-regressive generation 을 위해 최적화.</p><ol><li>임의로 피벗 위치를 선택하여, 그 이전의 context 를 source sequence 로, 이 후의 시퀀스를 target output 으로 간주.
이를 seq2seq 와 언어 모델링 목적으로 표기
피벗 위치는 전체 sequence 의 10% - 90% 사이에서 균등하게 샘플링되도록 제한하고 source sequence 에 특수 토큰 <!-- -->[CLM]<!-- --> 을 추가</li><li>decoder 전용 generation task 로, 첫 번째 변형의 극단적인 경우.
<!-- -->[CLM]<!-- --> 토큰을 encoder 에 입력으로 전달하고, decoder 에게 전체 코드 시퀀스를 생성하도록 요구.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-bimodal-pretraining-on-text-code-data">3.2. Bimodal Pretraining on Text-code Data<a href="#32-bimodal-pretraining-on-text-code-data" class="hash-link" aria-label="Direct link to 3.2. Bimodal Pretraining on Text-code Data" title="Direct link to 3.2. Bimodal Pretraining on Text-code Data">​</a></h2><p>두 번째 단계로, 저자는 text-code bimodal data 로 pretrain 하였다.</p><p>여기서 각 text-code 쌍은 code function 과 대응하는 docstring describing 을 포함한다.</p><p>이런 bimodal data 는 모델 훈련이 cross-modal understanding 과 generation 에 용이하도록 한다.</p><p>bimodal tasks 는 cross-modal contrastive learning, matching 및 causal LM task 를 포함한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="text-code-contrastive-learning">Text-Code Contrastive Learning<a href="#text-code-contrastive-learning" class="hash-link" aria-label="Direct link to Text-Code Contrastive Learning" title="Direct link to Text-Code Contrastive Learning">​</a></h3><p>이 tasks 는 text 및 code representations 의 feature space 를 정렬하기 위해 positive text-code paris 는 모으고 negative text-code pairs 를 분리하는 것을 목표로 한다.</p><p>Guo et al. <!-- -->[2022]<!-- --> 는 code understanding task 에서 이점을 입증했다. 이 task 는 encoder 에서만 활성화하며, text 나 code snippet 을 bidirectional self-attention 을 통해 continuous representation 으로 인코딩한다.</p><p>BERT 와 유사하게, 저자는 input 앞에 special token <!-- -->[CLS]<!-- --> 을 붙이고, 최종 Transformer layer 의 output embeddings 를 해당 input text 나 input code 의 representations 으로 간주한다.</p><p>또한, linear layer 를 추가하고 L2 normalization 을 사용하여 출력을 256-dimensional embeddings 로 매핑한다.</p><p>negative samples 를 보강하기 위해, momentum encoder 를 사용하여 이전 mini-batches 의 임베딩을 저장한다.</p><p>구체적으로, momentum encoder 는 현재 mini-batch 의 샘플을 enqueue 하고 가장 오래된 mini-batch 는 dequeue 하여 queuing 시스템을 유지한다.</p><p>기존의 encoder 와 momentum encoder 의 linear interpolation 을 통해 momentum encoder 를 업데이트하여 training step 간의 representation 일관성을 보장한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="text-code-matching">Text-Code Matching<a href="#text-code-matching" class="hash-link" aria-label="Direct link to Text-Code Matching" title="Direct link to Text-Code Matching">​</a></h3><p>이 task 는 decoder 를 활성화시키고 text 와 code snippet 이 동일안 의미를 가지는지 예측하는 것을 목표로 함.</p><p>이는 text 와 code modalities 간의 fine-grained alignment 를 포착하는 더 나은 bimodal representations 를 학습 하는데 도움이 된다.</p><p>code sample 이 주어지면 decoder 는 embedding layer 및 causal self-attention layer 를 통과시킨 후, self-attention representation 은 cross attention layer 로 전달되어 encoder 로부터 받은 text representation 와 관련한 signal 을 query 한다.</p><p>task-specific <!-- -->[Match]<!-- --> token 은 code input sequence 의 맨 앞에 추가되어 decoder 에 text-code matching functionality 를 제공해주며, <!-- -->[EOS]<!-- --> token 은 code input 끝에 추가된다.</p><p>decoder 는 causal self-attention mask 를 사용하며 last decoder token 만 전체 context 에 참여할 수 있어, <!-- -->[EOS]<!-- --> 의 output embedding 을 text-code cross-modal 의 alignment representation 으로 다룬다.</p><p>마지막으로, binary matching task 를 위해 decoder 의 output embedding 위에 linear layer 를 사용하여 text-code pair 가 positive (match) 인지 negative (unmatched) 인지를 예측한다.</p><hr><p>정보가 많은 negative 를 찾기 위해, 저자는 hard negative mining 전략을 사용한다.</p><p>특히, 현재 샘플과 momentum encoder 가 유지하는 queue 내의 이전 샘플 간의 contrastive-based similarity score 에 따라 hard negative 를 샘플링한다.</p><p>이렇게 하면 harder negative 가 선택될 가능성이 높아진다.</p><p>positive pairs 의 batch 의 경우, code/text query 를 사용하여 text/code queue 에서 negative 를 mining 하여 negative pairs 의 두 batch 를 구성한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="text-code-causal-lm">Text-Code Causal LM<a href="#text-code-causal-lm" class="hash-link" aria-label="Direct link to Text-Code Causal LM" title="Direct link to Text-Code Causal LM">​</a></h3><p>이 task 는 encoder 및 decoder 모두 활성화시키며, text-to-code 및 code-to-text 생성으로 dual multimodal conversion 을 통한 cross-modal generative 목표에 초점을 둔다.</p><p>구체적으로, text sample 이 input 일 경우, decoder 의 input sequence 에 <!-- -->[CDec]<!-- --> token 을 앞에 추가한다. </p><p>이 경우, decoder 는 code generation funtionality 로 작동한다. </p><p>반대로, input 이 code sample 인 경우엔 decoder 의 input sequence dp <!-- -->[TDec]<!-- --> token 맨 앞에 추가하여, deocder 는 text generation functionality 로 작동한다.</p><p>이런 유형의 Causal LM 은 code summarization 같은 multimodal generative downstream tasks 에서 pretrain-finetune gap 을 줄이기 위한 효과적인 learning objective 로 입증되었다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-compute-efficient-pretraining-with-frozen-off-the-shelf-llms">3.3. Compute-efficient Pretraining with Frozen Off-the-shelf LLMs<a href="#33-compute-efficient-pretraining-with-frozen-off-the-shelf-llms" class="hash-link" aria-label="Direct link to 3.3. Compute-efficient Pretraining with Frozen Off-the-shelf LLMs" title="Direct link to 3.3. Compute-efficient Pretraining with Frozen Off-the-shelf LLMs">​</a></h2><p>모델을 첨부터 pretraining 하지 않고 효율적으로 확장하기 위해, 저자는 CodeT5+ 의 component (encoder, decoder)를 off-the-shelf pretrained LLM 로 초기화하는 compute-efficient pretraining 전략을 제안한다. (Fig 2. 오른쪽)</p><p>이 확장을 위해, <!-- -->[Li et al., 2022b]<!-- --> 의 영감을 받아, 기존 T5 모델과 동일한 크기의 encoder 와 decoder 대신 &quot;shallow encoder and deep decoder&quot; architecture 를 사용한다.</p><p>[Li et al., 2022b]<!-- --> 에 따르면, T5-based model 의 decoder 는 종종 생성 작업에서 더 높은 복잡성을 처리해야 하므로, 더 많은 neural parameters 로 강화해야 한다.</p><p>분리되어 pretrain 된 encoder 와 decoder 를 연결하기 위해, 저자는 self-attention layer 이후 decoder block 에 무작위로 초기화된 cross-attention layer 를 삽입한다.</p><p>efficient tuning 을 위해, cross-attention layer 는 top-<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span></span> decoder layer (본 실험은<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span></span>=1)에만 삽입한다.</p><p>small encoder 와 cross-attention layer 만 trainable 하도록 유지하면서, 대부분의 decoder parameter 는 고정한다. 또한, training stability 향상을 위해 gating function 을 추가하거나 특정 frequency 로 multiple cross-attention layer 를 삽입하는 등의 설계도 탐구한다.</p><p>하지만, 상당한 성능 향상은 관찰되지 않았으며, 더 좋지 않은 결과로는 이런 설계 선택은 계산 비용이 너무 많이 들게 된다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-adaptation-to-downstream-understanding-and-generation-tasks">3.4. Adaptation to Downstream Understanding and Generation Tasks<a href="#34-adaptation-to-downstream-understanding-and-generation-tasks" class="hash-link" aria-label="Direct link to 3.4. Adaptation to Downstream Understanding and Generation Tasks" title="Direct link to 3.4. Adaptation to Downstream Understanding and Generation Tasks">​</a></h2><p>pretraining 의 두 단계 후, CodeT5+ 는 다양한 모드에 유연하게 작동하여 Seq2Seq generation task, decoder-only tasks 및 understanding-based tasks 를 포함한 다양한 task 를 지원할 수 있다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seq2seq-generation-tasks">Seq2Seq Generation Tasks<a href="#seq2seq-generation-tasks" class="hash-link" aria-label="Direct link to Seq2Seq Generation Tasks" title="Direct link to Seq2Seq Generation Tasks">​</a></h3><p>encoder-decoder model 인 CodeT5+ 는 code generation 및 summarization 같은 Seq2Seq generation task 에 자연스럽게 적응할 수 있다.</p><p>또한, encoder 를 사용하여 code snippets 를 검색하고, 이를 code generation 을 위해 encoder 및 decoder 모두 사용하는 retrieval-agumented generation model 로 적용할 수 있다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="decoder-only-tasks">Decoder-only Tasks<a href="#decoder-only-tasks" class="hash-link" aria-label="Direct link to Decoder-only Tasks" title="Direct link to Decoder-only Tasks">​</a></h3><p>이 설정에선, encoder input 에 항상 <!-- -->[CLM]<!-- --> 토큰을 주입하고, source sequence 를 prefix context 로 decoder 에 전달한다.</p><p>encoder 와 decoder 의 cross-attention layers 의 weight 를 고정한다.</p><p>이 전략은 decoder 파트만 활성화하며 기술적으로 전체 model parameter 의 약 절반을 줄인다.</p><p>저자는 next-line code completion task 를 사용하여 CodeT5+ 의 decoder 전용 generation 능력을 평가한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="understanding-tasks">Understanding Tasks<a href="#understanding-tasks" class="hash-link" aria-label="Direct link to Understanding Tasks" title="Direct link to Understanding Tasks">​</a></h3><p>CodeT5+ 는 understanding tasks 를 두 가지 방식으로 지원할 수 있다</p><ol><li>encoder 를 사용하여 text/code embedding 을 얻어 이를 detection task 나 retrieval task 를 위해 binary classifier 에 전달할 수 있다.</li><li>encoder 를 decoder 와 결합하여 text-to-code retrieval task 에 대한 text-code matching score 를 예측할 수 있다.</li></ol><h1>4. Pretraining and Instruction Tuning</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-pretraining-dataset">4.1. Pretraining Dataset<a href="#41-pretraining-dataset" class="hash-link" aria-label="Direct link to 4.1. Pretraining Dataset" title="Direct link to 4.1. Pretraining Dataset">​</a></h2><p>최근 출시된 GitHub Code 데이터셋을 사용하여 CodeSearchNet 의 pretraining dataset 을 확장했다.</p><p>9 개의 PLs (Python, Java, Ruby, JavaScript, GO, PHP, C, C++, C#)를 선택하였고, 허용 라이선스를 가진 코드와 50 ~ 2000 tokens 을 가진 파일들만 필터링 하였다.</p><p>또한, GitHub repository name 을 확인하여 CodeSearchNet 과 다른 downstream tasks 에서 중복되는 부분을 필터링했다.</p><p>중복 데이터는  exact match 를 기준으로 필터링되어 중복되는 부분이 있을 수 있지만, 이 중복은 모델 성능에 큰 영향을 미치지 않을 것으로 예상된다.</p><p>저자는 CodeT5 tokenizer 를 사용하여 다국어 데이터셋을 토큰화했으며, 이로 인해 5150억 개의 토큰이 생성되었다. 이는 CodeSearchNet 보다 약 50배 큰 크기이다.</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/0ae8a69d-13f6-417f-b2b0-fa999be692cc/image.png" class="img_ev3q"></p><p>Table 1 에는 unimodal code 및 bimodal text-code pretrained dataset 의 통계이다.</p><p>표에서 보이듯, GitHub 코드로부터 정리된 데이터셋은 CodeSearchNet bimodal data 의 function level 보다 훨씬 큰 데이터 크기를 가지고 있어, 모델이 pretrain 의 첫 단계에서 풍부한 representation 을 학습할 수 있게 해준다.</p><p>CodeT5 와 달리 저자는 CodeSearchNet 의 bimodal 데이터만을 CodeT5+ 의 두 번째 단계 pretrain 에 사용한다.</p><p>이 단계에서는 주로 text-code 관련 task 인 text-to-code retrieval 및 generation 에 모델을 적응시킨다.</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/02324785-4135-4a1f-84aa-b405f15475df/image.png" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-pretraining-setup">4.2. Pretraining Setup<a href="#42-pretraining-setup" class="hash-link" aria-label="Direct link to 4.2. Pretraining Setup" title="Direct link to 4.2. Pretraining Setup">​</a></h2><p>저자는 CodeT5+ models 을 두 그룹으로 pretrain 한다.</p><ol><li>CodeT5+ 220M 및 770M 은 T5 의 아키텍처에 따라 처음부터 train 된다.</li><li>CodeT5+ 2B, 6B, 16B 는 decoder 를 CodeGen-mono 2B, 6B, 16B model 에서 초기화하고 encoder 를 CodeGen-mono 350M 에서 초기화한다.</li></ol><p>scaing 전략을 주목하자, </p><p>원래의 CodeGen 모델과 비교하여 후자의 CodeT5+ 모델 그룹은 무시할 만한 학습 가능한 매개변수를 도입합니다(2B, 6B, 16B 모델에 대해 350M 인코더와 36M, 67M, 151M의 크로스 어텐션 레이어 하나). 이 두 그룹의 모델에는 각각 CodeT5 토크나이저와 CodeGen 토크나이저를 사용합니다. 사전학습에서는 16개의 A100-40G GPU가 장착된 Google Cloud Platform의 클러스터에서 CodeT5+를 대규모의 단모달 데이터셋과 이후 작은 이중모달 데이터셋에 대해 단계적인 전략으로 사전학습합니다.</p><p>첫 번째 단계에서는 10,000번의 학습 단계 동안 모델을 스팬 노이즈 제거 작업으로 예열한 후, 두 개의 CLM 작업과 같은 가중치로 합동 훈련을 100,000번의 단계 동안 진행합니다. 스팬 노이즈 제거 작업에 대해 선형 감쇠 학습률(LR) 스케줄러를 사용하며, 최대 학습률은 2e-4이고 배치 크기는 노이즈 제거에는 2048, CLM에는 512로 설정합니다. 입력 및 출력 데이터를 준비하기 위해 스팬 노이즈 제거 작업의 최대 길이를 512로 설정하고, 코드 완성 CLM의 소스 및 타겟 시퀀스의 최대 길이를 각각 768과 600으로, 디코더만을 사용한 생성 CLM의 최대 길이를 1과 1024로 설정합니다.</p><p>두 번째 단계에서는 대조 학습, 매칭 및 두 개의 CLM 손실을 동일한 가중치로 10 에폭 동안 합동으로 최적화합니다. 배치 크기는 256이고 최대 시퀀스 길이는 코드와 텍스트 시퀀스 각각 420과 128로 설정합니다.</p><p>모든 실험에서는 0.1 가중치 감쇠를 가진 AdamW 옵티마이저 <!-- -->[Loshchilov and Hutter, 2019]<!-- -->를 사용합니다. 또한 DeepSpeed의 ZeRO Stage 2 <!-- -->[Rasley et al., 2020]<!-- -->와 FP16의 혼합 정밀도 훈련을 사용하여 훈련 가속화를 수행합니다. CodeT5+ 2B, 6B 및 16B의 훈련에는 FP16로 고정된 디코더 가중치를 사용하고 다른 학습 가능한 가중치는 FP32로 유지합니다. CodeT5+ 6B 및 16B 모델에 대해서는 DeepSpeed ZeRO Stage 3의 매개변수 분할을 사용합니다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/code-t-5-p">CodeT5p</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/code-t-5">CodeT5+</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/nlp">NLP</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/code-understanding">Code Understanding</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/code-generation">Code Generation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multi-task">Multi-Task</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/paper">Paper</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/Multi-Task/2023-05-CodeT5p.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/Multi-Task/Flan-T5"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Scaling Instruction-Finetuned Language Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/PEFT/Composition/LoRA"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">LoRA: Low-Rank Adaptation of Large Language Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-unimodal-pretraining-on-code-data" class="table-of-contents__link toc-highlight">3.1. Unimodal Pretraining on Code Data</a><ul><li><a href="#span-denoising" class="table-of-contents__link toc-highlight">Span Denoising</a></li><li><a href="#causal-language-modeling-clm" class="table-of-contents__link toc-highlight">Causal Language Modeling (CLM)</a></li></ul></li><li><a href="#32-bimodal-pretraining-on-text-code-data" class="table-of-contents__link toc-highlight">3.2. Bimodal Pretraining on Text-code Data</a><ul><li><a href="#text-code-contrastive-learning" class="table-of-contents__link toc-highlight">Text-Code Contrastive Learning</a></li><li><a href="#text-code-matching" class="table-of-contents__link toc-highlight">Text-Code Matching</a></li><li><a href="#text-code-causal-lm" class="table-of-contents__link toc-highlight">Text-Code Causal LM</a></li></ul></li><li><a href="#33-compute-efficient-pretraining-with-frozen-off-the-shelf-llms" class="table-of-contents__link toc-highlight">3.3. Compute-efficient Pretraining with Frozen Off-the-shelf LLMs</a></li><li><a href="#34-adaptation-to-downstream-understanding-and-generation-tasks" class="table-of-contents__link toc-highlight">3.4. Adaptation to Downstream Understanding and Generation Tasks</a><ul><li><a href="#seq2seq-generation-tasks" class="table-of-contents__link toc-highlight">Seq2Seq Generation Tasks</a></li><li><a href="#decoder-only-tasks" class="table-of-contents__link toc-highlight">Decoder-only Tasks</a></li><li><a href="#understanding-tasks" class="table-of-contents__link toc-highlight">Understanding Tasks</a></li></ul></li><li><a href="#41-pretraining-dataset" class="table-of-contents__link toc-highlight">4.1. Pretraining Dataset</a></li><li><a href="#42-pretraining-setup" class="table-of-contents__link toc-highlight">4.2. Pretraining Setup</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.845dd18d.js"></script>
<script src="/assets/js/main.8a2584ef.js"></script>
</body>
</html>