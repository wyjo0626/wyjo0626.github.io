---
slug: PTR
title: "PTR: Prompt Tuning with Rules for Text Classification"
tags: [PEFT, prompt tuning, PTR]
---

ë…¼ë¬¸ ë° ì´ë¯¸ì§€ ì¶œì²˜: <https://www.sciencedirect.com/science/article/pii/S2666651022000183>

# Abstract

ìµœê·¼, pre-trained language models (PLMs) ì˜ rich knowledge ë¥¼ NLP task ì— í™œì„±í™”í•˜ê¸° ìœ„í•´ prompt tuning ì´ ë„ë¦¬ ì ìš©ëœë‹¤.

prompt tuning ì´ sentiment classification ë° natural language inference ê°™ì€ few-class classification tasks ì— ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ, prompt ìˆ˜ë™ ì„¤ê³„ëŠ” ë¬´ê±°ìš°ë©° í•œí¸, prompts ìë™ ìƒì„± ë˜í•œ ì–´ë µê³  ì‹œê°„ ì†Œìš”ê°€ í¬ë‹¤.

ê·¸ëŸ¬ë¯€ë¡œ ë³µì¡í•œ many-class classification ì— ëŒ€í•œ íš¨ìœ¨ì ì¸ prompts ë¥¼ ì–»ê¸°ë€ ì—¬ì „íˆ challenge ë¡œ ë‚¨ì•„ìˆë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œ ì €ìëŠ” classification task ì˜ prior knowledge ë¥¼ rules ì§€ì •í•˜ì—¬ encoding í•˜ë©°, ê·¸ í›„ rules ì— ë”°ë¼ sub-prompts ë¥¼ ì„¤ê³„í•˜ê³ , task ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ sub-prompts ë¥¼ ê²°í•©í•œë‹¤.

ì €ìëŠ” ì´ë¥¼ Prompt Tuning method with Rules **"PTR"" ë¡œ ëª…ëª…í•œë‹¤.

ì¡´ì¬í•˜ëŠ” prompt-based method ì™€ ë¹„êµí•˜ë©´, PTR ì€ prompts êµ¬ì¶•ì— íš¨ê³¼ì • ë° íš¨ìœ¨ì„± ê°„ì˜ tradeoff ë¥¼ ì˜ ë„ë‹¬í•œë‹¤.

relation classification, entity typing ë° intent classification ì„ í¬í•¨í•œ 3ê°œì˜ many-class classification tasks ì—ì„œ ì‹¤í—˜ì„ ì§„í–‰í•œë‹¤.

ê²°ê³¼ëŠ” PTR ì´ vanilla ë° prompt tuning baselines ë¥¼ ëŠ¥ê°€í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì–´, prompt tuning ì˜ rules í™œìš©ì˜ íš¨ê³¼ì„±ì„ ë‚˜íƒ€ëƒˆë‹¤.

# 1. Introduction

PLMs ëŠ” íš¨ê³¼ì ì¸ NLP ìˆ˜ë‹¨ìœ¼ë¡œ ë‚˜íƒ€ë‚˜, large-scale corpora ì˜ linguistic, semantic, syntactic ë° world knowledge ë¥¼ capture í•  ìˆ˜ ìˆë‹¤.

task-specific data ë¥¼ fine-tuning í•˜ì—¬, PLMs ì˜ rich knowledge ë¡œ ë‹¤ì–‘í•œ downstream NLP task ì— í™œìš©í•œë‹¤.

fine-tuning ì˜ ì„±ê³µì—ë„ ë¶ˆêµ¬í•˜ê³ , ìµœê·¼ ì—°êµ¬ë“¤ì—ì„œ critical challenge ê°€ ë‚˜íƒ€ë‚œë‹¤.

![Figure 1](image.png)

- pre-training ë° fine-tuning ê°„ì˜ object forms gaps
  - Fig. 1(a) ì—ì„œ ë³´ì´ë“¯, PLMs ëŠ” ë³´í†µ cloze-stype task ë¥¼ pre-training
  - í•˜ì§€ë§Œ Fig 1. 1(b) ì²˜ëŸ¼ fine-tuning ì€ all PLM's parameters ë° task-specific heads (e.g. linear layers for classification) with task-specific objectives tuning ì´ í•„ìš”.
  - transfer learning ê´€ì ì—ì„œ, ì´ objective gap ì€ PLMs's knowledge ì„ downstream task ì— adaptation í•˜ëŠ” ê²ƒì„ ì €í•´í•  ìˆ˜ ìˆë‹¤.

ìœ„ì™€ ê°™ì€ objective forms gap ì„ ì—°ê²°í•˜ê¸° ìœ„í•´, prompt tuning ì´ ë„ì…ë˜ì–´ ì™”ìœ¼ë©°, ì¼ë°˜ì ì¸ prompt ëŠ” template ë° label word set ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤.

- Fig. 1(c) ì—ì„œ ë³´ì´ë“¯, masked position $[ğ™¼]$ ì„ ì˜ˆì¸¡í•˜ê³  í•´ë‹¹í•˜ëŠ” class labels ì„ predicted word ë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•´ input ê³¼ template ì„ ê²°í•©í•˜ì—¬, classification task ë¥¼ cloze-style objective form ìœ¼ë¡œ ë³€í™˜ë  ìˆ˜ ìˆë‹¤.
  - label word set ì€ predicting $[ğ™¼]$ ì— ëŒ€í•œ candidate set ì„ ì œê³µ.
  - ì§ê´€ì ìœ¼ë¡œ, template "$<S_1>.$ It was $[ğ™¼]$." ë° setiment classification ì„ ìœ„í•œ label set {"great", "terrible"} ì„ ì‚¬ìš©
  - ê·¸ëŸ¼ $<S_1>$ ì´ $[ğ™¼]$ ì— ëŒ€í•œ PLMs ì˜ ì˜ˆì¸¡ "great" ë˜ëŠ” "terrible" ì„ ê¸°ë°˜ìœ¼ë¡œ positive ë˜ëŠ” negative ì¸ì§€ ê²°ì •í•  ìˆ˜ ìˆë‹¤.

---

prompt tuning ì€ natural language inference ê°™ì€ ë‹¤ë¥¸ few-class classification tasks ì—ì„œ ì¢‹ì€ ê²°ê³¼ì— ë„ë‹¬í•˜ì§€ë§Œ, many classes ê°€ ìˆëŠ” tasks ì—ì„œëŠ”, ë‹¤ì–‘í•œ classes ë¥¼ êµ¬ë³„í•˜ê¸° ìœ„í•´ ì ì ˆí•œ templates ë° ì•Œë§ëŠ” label words ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì°¾ê¸°ë€ ì–´ë µë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, many-class classification task ì¸ relation classification ì€ text ì—ì„œ ë‘ marked entities ë‹¨ì˜ semantic relations ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤.

relation "person:parent" ì™€ relation "organization:parent" ì´ ìˆì„ ë•Œ, template ë° ì´ë“¤ì„ êµ¬ë³„í•  label words ì„ ì„ íƒí•˜ê¸°ë€ ì–´ë µë‹¤.

- í•œ ì§ì„¤ì ì¸ ì†”ë£¨ì…˜ì€ Fig. 1(d) ì²˜ëŸ¼ prompts ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ auto-generated prompts ëŠ” ìƒì„±ê³¼ ê²€ì¦ì— ëŒ€í•œ ë§ì€ ì—°ì‚° ë¹„ìš©ì´ ë°œìƒí•œë‹¤.
- ë‹¤ë¥¸ ì†”ë£¨ì…˜ì€ Fig. 1(e) ì²˜ëŸ¼ discrete prompt tokens ëŒ€ì‹  soft prompt ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ soft prompts ëŠ” íš¨ê³¼ì ì´ê¸° ìœ„í•´ ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì¶©ë¶„íˆ ì»¤ì•¼í•œë‹¤.

---

ë³¸ ë…¼ë¬¸ì—ì„œëŠ”, many-class classification ì— ëŒ€í•´, rules ê°€ ìˆëŠ” prompt tuning (PTR) ì„ ì œì•ˆí•œë‹¤.

![Figure 2](image-1.png)

Fig. 2 ì²˜ëŸ¼ classification task ê°€ ì£¼ì–´ì§€ë©´, ë¨¼ì € rule ë¡œ prior task knowledge ë¥¼ encoding í•˜ê³  task ë¥¼ sub-tasks ë¡œ decomposing í•œë‹¤.

ê·¸í›„, task ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•„ìˆ˜ sub-prompt ë¥¼ ì„¤ê³„í•˜ê³ 

ë§ˆì§€ë§‰ìœ¼ë¡œ rules ì— ë”°ë¥¸ sub-prompts ë¥¼ êµ¬ì„±í•œë‹¤.

ë‹¤ë¥¸ prompt tuning ê³¼ ë¹„êµí•˜ë©´ PTR ì€ ë‘ ê°€ì§€ ì´ì ì´ ìˆë‹¤.

1. Prior Knowledge Encoding
   - prior task knowledge ëŠ” specific tasks í•´ê²°ì— ë„ì›€ì„ ì¤€ë‹¤.
   - ì˜ˆë¡œ, relation classification ì—ì„œ ì˜ˆì¸¡ ê²°ê³¼ê°€ sentence semantics ë° entity types ëª¨ë‘ì™€ ê´€ë ¨ë˜ì–´ ìˆë‹¤ ê°€ì •í•˜ë©´, two sub-prompts ì— ê¸°ë°˜í•˜ì—¬ prompts ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.
     - í•˜ë‚˜ëŠ” entity types ì„ ê°ì§€í•˜ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” entities ê°„ì˜ relational semantics ì„ ê°ì§€í•œë‹¤.
   - entity typing ë° intent classification ê°™ì€ ë‹¤ë¥¸ tasks ì—ì„œë„, í´ë˜ìŠ¤ ê³„ì¸µ êµ¬ì¡°ëŠ” prompt ì„¤ê³„ì— ì¢‹ì€ prior knowledge ë‹¤.
   - prior task knowledge ë¥¼ encoding í•˜ì—¬, ì¶©ë¶„í•œ í›ˆë ¨ ë°ì´í„° ì—†ì´ë„ íš¨ê³¼ì ì¸ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
2. Efficient and Effective Prompt Design
   - ê° class ì— ëŒ€í•œ ê°œë³„ template ë° label words ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ê³„í•˜ëŠ” ê²ƒë³´ë‹¤, few simple sub-prompts ë¥¼ ì„¤ê³„í•˜ê³  ë³µì¡í•œ specific tasks ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ rules ì— ë”°ë¥¸ ê²°í•©ì´ ë” ì‰½ë‹¤.
   - auto-generated ë° soft prompts ë³´ë‹¤, prompts ìƒì„±ì„ ìœ„í•œ rules ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì´ê³  íš¨ê³¼ì„±ì´ ìˆë‹¤.
   - ì§ê´€ì ì¸ ì¸¡ë©´ì„ ê³ ë ¤í•˜ë©´, ë³¸ ë…¼ë¬¸ì—ì„  PRT ì†Œê°œë¥¼ ìœ„í•´ human-picked sub-prompts ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, open framework ë° sub-prompts ëŠ” auto-generated ë° soft í•œ ê²ƒì¼ ìˆ˜ë„ ìˆë‹¤.

---

PTR íš¨ê³¼ì„± ê²€ì¦ì„ ìœ„í•´, relation classification, entity typing ë° intent classification ì„ í¬í•¨í•œ 3ê°€ì§€ì˜ many-class classification task ì—ì„œ ì‹¤í—˜ì„ ìˆ˜í•œë‹¤.

ì‹¤í—˜ ê²°ê³¼ëŠ” PTR ì´ vanilla fine-tuning ë° prompt tuning baselines ë¥¼ ëŠ¥ê°€í•˜ê³ , prior task knowledge ë° ë³µì¡í•œ classification tasks ì— ëŒ€í•œ PLMs ëª¨ë‘ì˜ ì´ì ì„ í™œìš©í•˜ëŠ” ì ‘ê·¼ë²•ì„ì„ ë‚˜íƒ€ë‚¸ë‹¤.

# 2. Preliminaries

# 3. Prompt tuning with rules (PTR)

many-class classification tasks ì— ëŒ€í•œ íš¨ê³¼ì ì¸ prompts ì„¤ê³„ì˜ ì–´ë ¤ì›€ì„ ê³ ë ¤í•˜ë ¤, ì €ìëŠ” ì–´ë ¤ìš´ task ë¥¼ ì—¬ëŸ¬ simple sub-tasks ë¡œ decomposing í•˜ê³  ê° sub-prompts ë¥¼ ì„¤ê³„í•˜ëŠ” PTR ì„ ì œì•ˆí•œê³ , ê¸°ì¡´ì˜ ë³µì¡í•œ task ì˜ ë” ë‚˜ì€ í•´ê²°ì„ ìœ„í•´ rules ì— ë”°ë¥¸ sub-prompts ë¥¼ í¬í•¨í•œë‹¤.

## 3.1 Overall framework of PTR

PTR ì€ basic human inferences ì— ê¸°ë°˜ì„ ë‘”ë‹¤.

relation classification ì„ ì˜ˆë¡œ, sentence ë‚´ì˜ two marked entities ê°€ "person:parent" ê´€ê³„ì¸ì§€ ê¶ê¸ˆí•˜ë‹¤ë©´, prior knowledge ë¥¼ í™œìš©í•˜ì—¬ sentence ë° two marked entities ê°€ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸í•´ì•¼ í•œë‹¤.

1. two marked entities ê°€ ì¸ê°„ì´ì–´ì•¼ í•˜ë©°;
2. sentence ì´ two marked entities ê°„ì˜ parental semantics ë¥¼ ë‚˜íƒ€ë‚´ì•¼ í•¨

ì´ì— ì˜ê°ì„ ë°›ì•„, ëª¨ë“  text classification task $\mathcal{T} = \{ \mathcal{X}, \mathcal{Y} \}$ ì— ëŒ€í•´, task ë¥¼ ì¼ë ¨ì˜ conditional function set $\mathcal{F}$ ìœ¼ë¡œ decomposing í•  ìˆ˜ ìˆë‹¤.

ê° function $f \in \mathcal{F}$ ëŠ” function input ì´ íŠ¹ì • ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•œë‹¤.

ì˜ˆë¡œ,

- $f(a,$ person$)$ ë¥¼ ì„¤ê³„í•˜ì—¬ $a$ ê°€ person ì¸ì§€ ê²°ì •í•˜ê³ , $f(a,'$s parent was $, b)$ ë¥¼ ì„¤ê³„í•˜ì—¬ $b$ ê°€ $a$ ì˜ parent ì¸ì§€ ê²°ì •í•  ìˆ˜ ìˆë‹¤.
- ì§ê´€ì ìœ¼ë¡œ, ì´ëŸ¬í•œ ì¡°ê±´ í•¨ìˆ˜ëŠ” ë³¸ì§ˆì ì¸ first-order rules ì„œìˆ ì´ë‹¤.

ê° ì¡°ê±´ í•¨ìˆ˜ $f \in \mathcal{F}$ ì— ëŒ€í•´, PRT ì€ template $T_f(\cdot)$ ë° label word set $\mathcal{V}_f$ ë¥¼ ì„¤ì •í•˜ì—¬ sub-prompt ë¥¼ êµ¬ì¶•í•œë‹¤.

$\mathcal{Y}$ ì˜ semantics ì— ë”°ë¼, rules ì„ ì‚¬ìš©í•˜ì—¬ task ë¥¼ ì¼ë ¨ì˜ ì¡°ê±´ í•¨ìˆ˜ ê³„ì‚°ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.

Fig. 2 ì—ì„œ ë³´ì´ë“¯, relation classification ì˜ ê²½ìš°, entities $a$ ë° $b$ ê°€ "person:parent" ê´€ê³„ë¥¼ ê°€ì§€ëŠ”ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì€ ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™”í•  ìˆ˜ ìˆë‹¤.

$$
\begin{align*}
& f_{e_s}(a, \text{person}) \wedge f_{e_s, e_o}(a,'\text{s parent was}, b) \wedge f_{e_o}(b, \text{person}) \\
& \rightarrow \text{"person:parent"}
\end{align*}
$$

- $f_{e_s}(\cdot, \cdot)$ : subject entity types ë¥¼ ê²°ì •í•˜ëŠ” ì¡°ê±´ í•¨ìˆ˜
- $f_{e_o}(\cdot, \cdot)$ : entities ê°„ì˜ ì˜ë¯¸ì  ì—°ê²°ì„ ê²°ì •í•˜ëŠ” ì¡°ê±´ í•¨ìˆ˜

ìœ„ì—ì„œ ì–¸ê¸‰í•œ rules ë° ì¡°ê±´ í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, task $\mathcal{T}$ ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ rule-related conditional functions ì˜ sub-prompts ë¥¼ êµ¬ì„±í•œë‹¤.

## 3.2 Task decomposition

tasks ë¥¼ sub-tasks ë¡œ ë¶„í•´í•˜ëŠ” ê¸°ë³¸ rule ì€ structural information ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.

1. structural label ì„ ê¸°ë°˜ìœ¼ë¡œ rules ì„¤ê³„
   - classification tasks ì˜ label ì€ internal structural information ì„ ê°€ì§ˆ ìˆ˜ë„ ìˆìœ¼ë©°, ì´ ì •ë³´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ label tag ì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œë¡œ í‘œí˜„ëœë‹¤.
     - intent classification ì„ ì˜ˆë¡œ, "card_activating" ê³¼ "card_linking" ì€ "card" ë¼ëŠ” ë™ì¼í•œ ë‹¨ì–´ë¥¼ ê³µìœ 
     - ì´ ë‘ label ì„ ë¶„ë¥˜í•˜ëŠ” ê²ƒì€ "card" ì™€ ê´€ë ¨ëœ ê²ƒì¸ì§€, "activating" ì´ë‚˜ "linking" ì´ ì˜ë„ëœ ê²ƒì¸ì§€ ë¶„í•´ë  ìˆ˜ë„ ìˆë‹¤.
2. intrinsic correlation ë¥¼ ê¸°ë°˜ìœ¼ë¡œ rules ì„¤ê³„
   - ì¼ë¶€ label tags ëŠ” ê³µí†µ í‚¤ì›Œë“œê°€ ì—†ì„ ìˆ˜ ìˆì§€ë§Œ, "uber" ì™€ "car_rental" ê°™ì´ ë™ì¼í•œ ë³¸ì§ˆì  ì˜ë¯¸ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.

ìœ„ rules ì— ë”°ë¼, many-class classification task ë¥¼ hierarchical classification í˜•íƒœë¡œ ë³€í™˜í•˜ê³  ê° ê³„ì¸µì— ëŒ€í•œ ì¡°ê±´ í•¨ìˆ˜ë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆë‹¤.

ì´ rules ë¥¼ ë”°ë¼ complex classification tasks ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ sub-prompts ë¥¼ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.

Table 3, 6, 10 ì—ì„œ ì˜ˆì‹œë¥¼ ë³´ì—¬ì£¼ë©°, relation classification, entity typing ë° intent classification ì„ ì–´ë–»ê²Œ ë¶„í•´í•˜ëŠ”ì§€ ë³´ì—¬ì¤€ë‹¤.

ì´ ì˜ˆì‹œì—ì„œ manual sub-prompts ë¥¼ ì‘ì„±í•˜ëŠ” ë° í•„ìš”í•œ ë…¸ë ¥ì´ ì–´ë µì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

---

tasks ë¥¼ ì–´ë–»ê²Œ ë¶„í•´í•˜ê³  rules ë¥¼ ì„¤ê³„í•˜ëŠ” ë²•ì€ open problem ì´ë©°, structural information ì™¸ì—ë„ ë” ë³µì¡í•œ heuristic rules ì„ ì‚¬ìš©í•˜ì—¬ ë¶„í•´ë‚˜ëŠ” ê²ƒë„ ìœ ë§í•  ìˆ˜ ìˆë‹¤.

ë³¸ ë…¼ë¬¸ì€ rule-based prompt tuning ì˜ framework ë¥¼ ì†Œê°œí•˜ëŠ” ë° ì¤‘ì ì„ ë‘”ë‹¤.

## 3.3 Sub-prompts for conditional functions

ê° ì¡°ê±´ í•¨ìˆ˜ $f \in \mathcal{F}$ ì— ëŒ€í•œ sub-prompt ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ê³„í•œë‹¤.

ì¼ë°˜ì ì¸ prompt ì„¤ì •ê³¼ ìœ ì‚¬í•˜ê²Œ, sub-prompt ë˜í•œ template ë° label word set ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

ê¸°ë³¸ì ì¸ ì¡°ê±´ í•¨ìˆ˜ëŠ” ë‹¨í•­(unary) í•¨ìˆ˜ì´ë‹¤. 

- binary sentiment classification ì„ ìœ„í•´, unary functions ëŠ” sentence ê°€ positive ì¸ì§€ negative ì¸ì§€ ê²°ì •í•  ìˆ˜ ìˆë‹¤.
- entity typing ì—ì„œëŠ”, unary functions ëŠ” entity types ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.
- topic labeling ì—ì„œëŠ”, unary functions ëŠ” article topics ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.

---

relation classification ì„ ì˜ˆë¡œ ë“¤ì. 

sentence $x = \{ \dots e_s \dots e_o \dots \}$ ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ìš°ë¦¬ëŠ” $f_{e_s} (\cdot,\{ \text{person | organization | } \dots \} )$ ì„ ì„¤ì •í•˜ì—¬ $e_s$ ì˜ type ì„ ê²°ì •í•  ìˆ˜ ìˆë‹¤.

- $e_s$ : subject entity
- $e_o$ : object entity

$f_{e_s}$ ì˜ sub-prompt template ë° label word set ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™”í•  ìˆ˜ ìˆë‹¤.

$$
\begin{equation}
  \begin{align*}
    & T_{f_{e_s}} = `` x\  \text{the} \ [ğ™¼] \ e_s", \\
    & \mathcal{V}_{f_{e_s}} = \{ ``\text{person}",``\text{organization}", \dots\},
  \end{align*}
\end{equation}
$$

object entity $e_o$ ì˜ ê²½ìš°, $f_{e_o}$ ì˜ sub-prompt template ë° label word set ì€ Eq. 1 ì™€ ìœ ì‚¬í•˜ë‹¤.

ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ ìœ í˜•ì˜ ì¡°ê±´ í•¨ìˆ˜ëŠ” ì´í•­(binary) í•¨ìˆ˜ì´ë‹¤.

natural language inference ì—ì„œ ì´í•­ í•¨ìˆ˜ëŠ” ë‘ ë¬¸ì¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ entailment(í¬í•¨), neutral(ì¤‘ë¦½), contradiction(ëª¨ìˆœ) ìœ¼ë¡œ ê²°ì •í•  ìˆ˜ ìˆë‹¤; relation classification ì—ì„œ ì´ëŸ¬í•œ í•¨ìˆ˜ê°€ ë‘ entities ê°„ì˜ ë³µì¡í•œ ì—°ê²°ì„ ê²°ì •í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.

ë§Œì•½ $f_{e_s, e_o}(\cdot , \{ \text{'s parent was|was born in| } \dots\}, \cdot)$, ìœ¼ë¡œ ì„¤ì •í•œë‹¤ë©´, $f_{e_s, e_o}$ ì˜ sub-prompt template ë° label word set ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³µì‹í™”í•  ìˆ˜ ìˆë‹¤.

$$
\begin{equation}
  \begin{align*}
    & T_{f_{e_s, e_o}} = `` x \ e_s \ [ğ™¼] [ğ™¼] [ğ™¼] \ e_o", \\
    & \mathcal{V}_{f_{e_s, e_o}} = \{ ``\text{'s person was}",``\text{was born in}", \dots\},
  \end{align*}
\end{equation}
$$

ë³´í†µ, unary ë° binary functions ëŠ” classfication tasks ì²˜ë¦¬ë¥¼ ìœ„í•œ rules ë¥¼ êµ¬ì„±í•˜ê¸°ì— ì¶©ë¶„í•˜ë‹¤.

complex semantics ì˜ classification tasks ì˜ ê²½ìš°, ì´ëŸ¬í•œ unary ë° binary functions ì„¤ê³„ ë°©ë²•ì„ multi-variable functions ë¡œ í™•ì¥í•˜ì—¬ ë³´ë‹¤ ê°•ë ¥í•œ sub-prompts ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.

## 3.4 Composing sub-prompts for tasks

ë‹¤ì–‘í•œ ì¡°ê±´ í•¨ìˆ˜ë“¤ì€ ì„œë¡œ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆë‹¤.

ì˜ˆë¡œ, probability $P(f_{e_s, e_o}(a,' \text{s parent was}, b) = \text{true})$ ëŠ” $P(f_{e_s}(a, \text{person}) = \text{true})$ ë° $P(f_{e_o}(b, \text{person}) = \text{false})$ ì˜ ë†’ì€ í™•ë¥ ì— ì¡°ê±´ì´ ê±¸ë ¤ ë‚®ì„ ìˆ˜ë„ ìˆë‹¤.

ê·¸ëŸ¬ë¯€ë¡œ, sub-prompts ë¥¼ ë”°ë¡œ í•™ìŠµí•˜ëŠ” ëŒ€ì‹ , ì—¬ëŸ¬ ì¡°ê±´ í•¨ìˆ˜ë“¤ì˜ sub-prompts ë¥¼ ê²°í•©í•˜ì—¬ ì™„ì „í•œ task-specific prompts ë¥¼ êµ¬ì„±í•´ì•¼ í•œë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„  ê°„ë‹¨í•œ ì „ëµì„ ì ìš©í•œë‹¤: conjunctive normal form(ë…¼ë¦¬ê³± í‘œì¤€í˜•)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  rul-related functions ì˜ sub-prompts ë¥¼ ì§ì ‘ ì—°ê²°í•œë‹¤.

ì˜ˆë¡œ, Fig. 2 ì—ì„œ Eq.1 ê³¼ Eq. 2 ì˜ sub-prompts ë¥¼ ì§‘ê³„í•œ, ì™„ì „í•œ prompt template ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\begin{equation}
  \begin{align*}
    & T(x) = [T_{f_{e_s}}(x); T_{f_{e_s,e_o}}(x); T_{f_{e_o}}(x)] = \\
    & `` x \ \text{the}[ğ™¼]_1 [ğ™¼]_2 [ğ™¼]_3 [ğ™¼]_4 \ \text{the} \ [ğ™¼]_5 \ e_o ",
  \end{align*}
\end{equation}
$$

- $[\cdot; \cdot; \cdot]$ : sub-prompts ì˜ aggregation function
- aggregated label word set ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\begin{equation}
  \begin{align*} 
    &\mathcal{V}[ğ™¼]_1 = \{â€˜â€˜\text{person}â€™â€™, â€˜â€˜\text{organization}â€™â€™, â€¦\}, \\ 
    &\mathcal{V}[ğ™¼]_2 = \{â€˜â€˜â€™\text{s}â€™â€™, â€˜â€˜\text{was}â€™â€™, â€¦\}, \\ 
    &\mathcal{V}[ğ™¼]_3 = \{â€˜â€˜\text{parent}â€™â€™, â€˜â€˜\text{born}â€™â€™,â€¦\}, \\ 
    &\mathcal{L}[ğ™¼]_4 = \{â€˜â€˜\text{was}â€™â€™, â€˜â€˜\text{in}â€™â€™, â€¦\}, \\ 
    &\mathcal{L}[ğ™¼]_5 = \{â€˜â€˜\text{person}â€™â€™, â€˜â€˜\text{organization}â€™â€™, â€¦\}. 
  \end{align*}
\end{equation}
$$

aggregated template ëŠ” multiple $[ğ™¼]$ tokens ë¥¼ ê°€ì§€ê³  ìˆì–´, inference time ì— ëª¨ë“  masked positions ë¥¼ ê³ ë ¤í•˜ì—¬ ì˜ˆì¸¡í•´ì•¼ í•œë‹¤. ì¦‰,

$$
\begin{equation}
  p(y|x) = \frac{\prod_{j=1}^n p([ğ™¼]_j = \phi(y)|T(x))}{\sum_{\tilde{y} \in \mathcal{Y}} \prod^n_{j=1}p([ğ™¼]_j = \phi(\tilde{y})|T(x))}
\end{equation}
$$

- $n$ : $T(x)$ ì˜ masked positions ìˆ˜
- $\phi_j(y)$ : class $y$ ë¥¼ $j$th masked position $[ğ™¼]_j$ ì˜ label word set $\mathcal{V}_{[ğ™¼]}$ ë¡œ ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜ë‹¤.
- Eq. 5 ëŠ” PLMs ë¥¼ tuning í•˜ê³  classes ë¥¼ ë¶„ë¥˜í•˜ëŠ”ë° ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.
- í•™ìŠµ ì¤‘ì—ëŠ”, ê° ì¡°ê±´ í•¨ìˆ˜ì˜ independent probability ì™€ ì „ì²´ task ì— ëŒ€í•œ joint probability ë¥¼ ìµœì í™”í•˜ëŠ” ë° ì¤‘ì ì„ ë‘”ë‹¤.

PTR ì˜ final training loss ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\begin{equation}
  \begin{align*} 
    \mathcal{L} &= \frac{1}{|\mathcal{X}|} \sum_{x \in \mathcal{X}} [L_{\text{indep}}(x) + \mathcal{L}_{\text{joint}}(x)], \\
    \mathcal{L}_{\text{indep}}(x) &= -\frac{1}{n} \sum_{j=1}^{n} \log p([ğ™¼]_j = \phi_j(y_{x})|T(x)), \\ \mathcal{L}_{\text{joint}}(x) &= -\log p(y_{x}|x), 
  \end{align*}
\end{equation}
$$

- $\mathcal{L}_{\text{joint}}$ : Eq. 5 ì— ê¸°ë°˜í•˜ì—¬ êµ¬ì„±

## 3.5 Sub-prompts with multiple label words

ì–´ë–¤ ì¼€ì´ìŠ¤ì—ì„ , injective mapping function $\phi$ ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° class ë¥¼ single label word ë¡œ ë§¤í•‘í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ ì¶©ë¶„í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.

entity typing ì„ ì˜ˆë¡œ, entity type class "location/city" ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í•´ë‹¹ class ë¥¼ label word "city"  ë˜ëŠ” "urban" ì´ë‚˜ "town" ì²˜ëŸ¼ "city" ì™€ í¬ê²Œ ê´€ë ¨ëœ ì—¬ëŸ¬ ë‹¨ì–´ë¡œ ë§¤í•‘í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.

ì‚¬ì‹¤ì€, "location/lake_sea_ocean" ê³¼ ê°™ì´ ì—°ê´€ëœ ì¼ë ¨ì˜ entity ë“¤ì„ ê°€ë¦¬í‚¤ëŠ” ì¼ë¶€ entity types ë„ ì¡´ì¬í•œë‹¤.

ì´ëŸ° types ì˜ ê²½ìš°, single label word ì— í• ë‹¹í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš¸ ìˆ˜ ìˆë‹¤.

í•˜ë‚˜ì˜ label word ì„ ì„ íƒí•˜ëŠ” ëŒ€ì‹ , ê° class ë¥¼ ì—¬ëŸ¬ ê´€ë ¨ëœ label words ë¡œ ë§¤í•‘í•˜ëŠ” ê²ƒì´ ê°„ë‹¨í•˜ë©° íš¨ê³¼ì ì´ë‹¤.

í˜•ì‹ì ìœ¼ë¡œ, input sentence $x$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $j$th masked position $[ğ™¼]_j$ ì—ì„œì˜ í•´ë‹¹ í´ë˜ìŠ¤ $y_x$ ì˜ label words ì„ $\phi_j(y_x) = w_j$ ì—ì„œ $\phi_j(y_x) = \{ w_{j,1}, w_{j,2}, â€¦, w_{j,m} \}$ ìœ¼ë¡œ ë³€ê²½í•œë‹¤.

template $T(\cdot)$ ê³¼ í•¨ê»˜, Eq. 6 ì˜ ì¡°ê±´ í™•ë¥  $p([ğ™¼]_j = \phi_j(y_x)|T(x))$ ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ëœë‹¤.

$$
\begin{equation}
  \frac{1}{m} \sum^m_{k=1} \lambda_k \ p([ğ™¼]_j = w_{j,k}|T(x))
\end{equation}
$$

- $\lambda_k$ : ê° label word ì˜ ì¤‘ìš”ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” weighted average coefficient 
- ì‹¤í—˜ì—ì„  ëª¨ë“  $\lambda_k$ ì„ 1 ë¡œ ì„¤ì •

# 4. Experiments

## 4.1 The results on relation classification

### Datasets

![Table 1](image-3.png)

ì´ ì„¹ì…˜ì—ì„  relation classification ì˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ.

ì €ìëŠ” ë‹¤ìŒ ì„¸ ê°€ì§€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©

- TACRED : ê°€ì¥ í¬ê³  ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” relation classification. 42 relation types ë¥¼ í¬í•¨
- TACREV : TACRED ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœ ë°ì´í„°ì…‹. training set ì€ ìœ ì§€í•˜ë©´ì„œ dev set ë° test set ì˜ ì¼ë¶€ ì˜¤ë¥˜ë¥¼ ê³ ì¹¨ 
- ReTACRED : TACRED ì˜ ë‹¤ë¥¸ ë²„ì „. ê¸°ì¡´ TACRED ì˜ ë‹¨ì  ë³´ì™„í•˜ì—¬ training, dev, test set ì¬êµ¬ì¶•

### Baselines and implementation details

relation classification ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ëª¨ë¸ë“¤ê³¼ PTR ë¹„êµ

![Table 2](image-2.png)

![Table 3](image-4.png)

- Learning Models from Scratch
  - relation classification ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì ‘ê·¼ë²•.
  - baselines ë¡œ PA-LSTM ë° C-GCN ì„ ì±„íƒ. ì´ ë‘ ëª¨ë¸ì€ recurrent neural networks ë° graph neural networks ì¤‘ íš¨ê³¼ì ì¸ ëª¨ë¸ ì¤‘ í•˜ë‚˜.
- Fine-Tuning PLMs
  - ì¼ë¶€ëŠ” relation classification ì— PMLs ë¥¼ fine-tuning í•˜ëŠ”ë° ì§‘ì¤‘ì„ í–ˆë‹¤. ì´ì— ëª‡ ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤.
   1. vanilia fine-tuning : íŠ¹ì • markers ë¥¼ input ì— ì¶”ê°€í•˜ì§€ ì•Šê³  RoBERTa$_{large}$ ë¥¼ ì§ì ‘ ì„ íƒ
   2. Entity ì •ë³´ê°€ ê´€ê³„ì  ì˜ë¯¸ë¥¼ ì´í•´í•˜ëŠ”ë° ì¤‘ìš”í•˜ë‹¤ ìƒê°ë˜ì–´, knowledge-enhanced PLMs ëŠ” knowledge bases ë¡œ PLMs ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì¶”ê°€ì ìœ¼ë¡œ ì—°êµ¬ë¨. SPANBERT, KNOWBERT, LUKE ë° MTB ë¥¼ baseline ìœ¼ë¡œ ì„ íƒ.
- Prompt Tuning
  1. Zhou and Chen (2021), Sentence-level relation extraction (RE) : entity marker ë¥¼ input ì— ì¶”ê°€í•˜ëŠ” several merker-based methods ì†Œê°œ. 
      - ì´ ì¤‘ ENT MARKER ëŠ” entity position ì„ ë‚˜íƒ€ë‚´ëŠ” ì¶”ê°€ì ì¸ ìˆœì°¨ ì •ë³´ë¥¼ ë„ì…í•˜ì—¬ Prompt ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•
      - TYP MARKER ì€ relation classification ì„ ìœ„í•œ template ì„ ë§Œë“œëŠ” ë° entity types information ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë„ì…
  2. Chen et al (2021a), AdaPrompt : label word ë¥¼ adaptive selection method ì„ ë„ì…í•˜ì—¬ ê° relation label ì„ ë‹¤ì–‘í•œ label words ë¡œ ë¶„ì‚°ì‹œì¼œ ë³µì¡í•œ many-class problem ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. Table 2 ì—ì„œ ì´ëŸ¬í•œ Prompt-based fine-tuning method ì˜ ëª‡ ê°€ì§€ template ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤€ë‹¤. PTR ì˜ ê²½ìš°ì—”, Table 3 ì—ì„œ relation-specific prompts ë¥¼ ë³´ì—¬ì¤€ë‹¤.

PTR ëŠ” Transformers ë° OpenNRE toolkit ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„

ëŒ€ë¶€ë¶„ì˜ hyperparameter ëŠ” ì´ì „ ì—°êµ¬ë“¤ì„ ë”°ë¥¸ë‹¤:

- RoBERTa$_{large}$ ì„ backbone ã…‡,ë¡œ ì‚¬ìš©í•˜ê³  learning rate $\{ 1e-5, 3e-5,5e-5 \}$ ì¤‘ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•˜ì—¬ 10% steps ì— ëŒ€í•´ linear warmup ì„ ì ìš©
- weight decay ì€ $1e - 2$
- ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ 64 batch size ë¡œ 5 epochs ë™ì•ˆ ëª¨ë¸ íŠœë‹
- dev set ì˜ ì„±ëŠ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ëª¨ë¸ checkpoint ì„ íƒ

ìœ„ ì„¤ì •ì„ ê¸°ì¤€ìœ¼ë¡œ ì—¬ëŸ¬ random seeds ë¥¼ ì‚¬ìš©í•˜ì—¬ 5ë²ˆ ìƒ˜í”Œë§í•˜ê³  í‰ê·  ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤.

### Comparison between PTR and fine-tuning

![Table 4](image-5.png)

PTR ê³¼ íŠœë‹ ê¸°ë²• ê°„ì˜ ë¹„êµëŠ” Table 4 ì—ì„œ í‘œì‹œí•œë‹¤.

1. ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ ë‹¤ì–‘í•œ recurrent neural layers, attention neural layers ë° graph neural layers ë¥¼ í¬í•¨í•œ ë³µì¡í•œ ëª¨ë¸ì„ ì„¤ê³„
   - ìœ„ ë³µì¡í•œ ëª¨ë¸ì€ scratch learning ì´ íš¨ê³¼ì ì´ì—ˆì§€ë§Œ, Vanilla PLM ì¸ RoBERTA$_{large}$ ëŠ” ì´ ëª¨ë¸ë“¤ì„ ì‰½ê²Œ ëŠ¥ê°€í•˜ë¯€ë¡œ large-scale unlabeled data ì—ì„œ ì–»ì€ rich knowledge ì´ downstream ì— ì¤‘ìš”í•¨ì„ ë³´ì—¬ì¤Œ
2. PLMs ë‚´ë¶€ì—ì„œ, knowledge-enhanced PLMs ëŠ” Vanilla PLM RoBERTA$_{large}$ ë¥¼ ì´ê¹€
   - ì´ëŠ” ëª¨ë¸ì„ ê°•í™”í•˜ê¸° ìœ„í•´ extra task-specific knowledge ë¥¼ ë„ì…í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤€ë‹¤.
   - í•˜ì§€ë§Œ, ë‹¨ìˆœíˆ PLM ì„ fine-tuning í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” donwstream tasks ì— í•„ìš”í•œ knowledge ì„ ì™„ì „íˆ ì´í•´í•˜ê¸´ ì–´ë µë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¸ë‹¤.
3. knowledge-enhanced PLMs ë‚´ë¶€ì—ì„œ, LUKE ì™€ KNOWBERT ê°€ ë‹¤ë¥¸ ëª¨ë¸ë“¤ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„
   - ë‘ ëª¨ë¸ì´ ê°ê° fine-tuning ë‹¨ê³„ì—ì„œ í–¥ìƒëœ ì•„í‚¤í…ì²˜ì™€ knowledge ë¥¼ data augmentation ì²˜ëŸ¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì´ë‹¤. 
   - ë°˜ë©´, SpanBERT ë° MTB ëŠ” pre-training ë‹¨ê³„ì—ì„œ task-specific knowledge ë¥¼ parameters ì— ì£¼ì…í•˜ê³  extra knowledge ì—†ì´ parameter ë¥¼ fine-tuning í•œë‹¤.
   - ìœ„ ê²°ê³¼ë“¤ì€ task-specific knowledge ê°€ ì´ë¯¸ PLM ì— í¬í•¨ë˜ì–´ ìˆë”ë¼ë„, ì´ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.
4. PTR ì€ ëª¨ë“  baseline ë³´ë‹¤ íšê¸°ì ì¸ ì„±ëŠ¥ í–¥ìƒ ì–»ìŒ
   - knowledge-enhanced models ê³¼ ë¹„êµí•´ë„ TACRED ì—ì„  ì—¬ì „íˆ ì´ëŸ° ëª¨ë¸ë“¤ì„ ëŠ¥ê°€
   - ì „ë°˜ì ì¸ ì‹¤í—˜ê²°ê³¼ëŠ” pre-training ë‹¨ê³„ê°€ PLM ì— ì¶©ë¶„í•œ task-specific knowledge ë¥¼ í¬ì°©í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ë©°, downstream task ì— ëŒ€í•œ knowledge ë¥¼ ì–´ë–»ê²Œ ìê·¹í•˜ëŠ” ì§€ê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.
   - ì¼ë°˜ì ì¸ fine-tuning ê³¼ ë¹„êµí•˜ì—¬ PTR ì€ task-specific knowledge ë¥¼ ë” ì˜ ìê·¹í•  ìˆ˜ ìˆë‹¤.

### Comparison between PTR and prompt tuning

Table 2 ë° 4 ì—ì„œ,  prompts ë¥¼ PLMs ì˜ vocabulary ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë‚˜ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤.

íŠ¹ì • sub-prompts ë¥¼ ì„¤ê³„í•˜ì—¬, PTR ì€ TYP Marker ì™€ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.

ê²Œë‹¤ê°€, PTR ì€ ì¶”ê°€ì ì¸ human annotations ì—†ì´ ì‹¤í—˜ì—ì„œ íš¨ê³¼ë¥¼ ì…ì¦í•œë‹¤.

- PTR ì€  ê¸°ì¡´ relation type annotations ì—ë§Œ ì˜ì¡´í•˜ì—¬ entity ì˜ sub-prompt label ì„ ë„ì¶œí•˜ê³ , inference time ì—ëŠ” entity type ì„ ì˜ˆì¸¡í•œë‹¤.
- í•œí¸, TYP Marker ëŠ” extra human annotations ë¥¼ ì‚¬ìš©í•˜ê³  ëª¨ë¸ì—ê²Œ training ë° inference ë‹¨ê³„ì—ì„œ ground-truth entity types ë¥¼ ì¤€ë‹¤.

---

Prompt ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ë“¤ê³¼ ë¹„êµí•˜ê¸° ìœ„í•´, few-shot learning ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë„ì…í•œë‹¤.

Gao et al. (Making pre-trained language models better few-shot learners) ì˜ few-shot setting ì„ ë”°ë¼, ê¸°ì¡´ì˜ training set ë° dev set ì—ì„œ ê° í´ë˜ìŠ¤ ë‹¹ $K$ ê°œì˜ training instances ì™€ $K$ ê°œì˜ validation instances ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ëª¨ë¸ì˜ ê¸°ì¡´ test set ì—ì„œ í‰ê°€í•œë‹¤.

$K$ ë¥¼ $\{ 8, 16, 32 \}$ ì¤‘ ì„¤ì •í•˜ê³ , ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒ˜í”Œë§í•˜ê¸° ìœ„í•´ ê³ ì •ëœ 5ê°œì˜ random seeds ë¥¼ ì‚¬ìš©í•œë‹¤.

----

![Table 5](image-6.png)

Table 5 ì—ì„œ, PTR ì´ full training dataset ì„ ì‚¬ìš©í•˜ëŠ” TYP Marker (PUNCT) ë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤.

ê·¸ëŸ¬ë‚˜ TYP Marker (PUNCT) ëŠ” training ë° testing ì— entity typing human-annotations ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ PTR ì€ few-shot ìƒí™©ì—ì„œë„ ìœ ì‚¬í•˜ê±°ë‚˜ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë‹¬ì„±í•œë‹¤.

íŠ¹íˆ ë°ì´í„°ì— annotation error ê°€ ì ì€ ReTACRED ì—ì„  PTR ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

TYP Marker (PUNCT) ì™€ AdaPrompt ì™€ ë¹„êµí•˜ì—¬, human knowledge ì™€ model knowledge ëª¨ë‘ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.

## 4.2 The results on entity typing

### Dataset

![Table 7](image-7.png)

Table 7 ì²˜ëŸ¼, ë‹¤ìŒ 3 ê°œì˜ dataset ì—ì„œ ì‹¤í—˜ì„ ìˆ˜í–‰

- FewNERT : êµ¬ì¡°í™”ê°€ ì˜ ë˜ì–´ ìˆëŠ” entity types ìœ¼ë¡œ large-scale manually annotated dataset ì´ë©°, 8ê°œì˜ entity types ë° 66ê°œ ì„¸ë¶€ entity types
- OntoNotes : OntoNotes 5.0 dataset ì—ì„œ ì„ íƒëœ ë°ì´í„°. PLET ì˜ ì„¤ì •ì„ ë”°ë¼ 86-classes ë²„ì „ì„ ì‚¬ìš©
- BBN : Wall Street Journal text ì˜ Penn Treebank corpus ì—ì„œ ì„ íƒëœ ë°ì´í„°ì…‹. PLET ì˜ ì„¤ì •ì„ ë”°ë¼ 46-classes ë²„ì „ì„ ì‚¬ìš©

ìœ„ ë°ì´í„°ì…‹ì—ì„œ, accuracy (ACC), loose micro F1 (MiF) ë° loose macro F1 (MaF) ì„ metric ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.

### Baselines and implementation details

![Table 6](image-8.png)

entity typing ì‹¤í—˜ì—ì„œ, vanila fine-tuning ë° PLET ì„ baseline ìœ¼ë¡œ ì‚¬ìš©

PLET ë„ prompt-based method ì§€ë§Œ, ì €ìì˜ ë°©ë²•ê³¼ ë‹¤ë¥´ê²Œ label words ì˜ˆì¸¡ì„ ìœ„í•´ template ì—ì„œ í•˜ë‚˜ì˜ masked position ë§Œ ì‚¬ìš©í•œë‹¤.

Table 6 ì—ì„œ PTR ê³¼ PLET ê°„ì˜ type-specific prompts ì„¤ê³„ì˜ ì°¨ì´ì— ëŒ€í•œ ë‚´ìš©ì„ ë³´ì—¬ì¤€ë‹¤.

PTR ì€ Transformers ë° OpenPrompt toolkit ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆë‹¤.

ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´,PLET ì™€ ë™ì¼í•œ ì„¤ì •ì„ ë”°ë¥¸ë‹¤:

- BERT_base backbone ì„ ì±„íƒí•˜ê³  learning rate $\{ 1e-5, 3e-5,5e-5 \}$ ì™€ ì²˜ìŒ 500 steps ì— linear warmup ìœ¼ë¡œ ëª¨ë¸ì„ ìµœì í™”í•œë‹¤.
- 16 batch size
- weight decay $1e-2$
- supervised ì„¤ì •ì—ì„œ, ê° ëª¨ë¸ì€ 10 epochs ë™ì•ˆ í›ˆë ¨
- few-shot ì—ì„œëŠ” 30 epochs ë™ì•ˆ í›ˆë ¨.
- 1-shot, 2-shot, 4-shot, 8-shot, 16-shot ë° fully-supervised ì„¤ì •ì—ì„œ ëª¨ë¸ì„ tuning í•˜ë©°, $K$-shot ì€ entity type ë‹¹ $K$ ê°œì˜ training examples ì„ ì˜ë¯¸
- baseline ì„¤ì •ì— ë”°ë¼ ë‹¤ë¥¸ random seed ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ 5ë²ˆ ìƒ˜í”Œë§í•˜ê³  í‰ê·  ê²°ê³¼ë¥¼ ì–»ìŒ

### Comparison between PTR and baselines

![Table 8](image-9.png)

Table 8 ì€ ì„¸ ë°ì´í„°ì…‹ì—ì„œ PTR ê³¼ baseline ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. 

í‘œì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤:

1. PLET ë° PTR ì€ ëª¨ë“  ì‹¤í—˜ì—ì„œ vanilla fine-tuning
   - ì´ëŠ” Table 4 ë° 5 ì˜ relation extraction ì‹¤í—˜ì˜ ê²°ë¡ ê³¼ ì¼ì¹˜í•œë‹¤.
2. ë‹¤ì–‘í•œ set ì„¤ì •ì—ì„œì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ë©´, training sample ìˆ˜ê°€ ì ì„ ë•Œ PLET ë° PTR ì´ vanilla fine-tuning ëŒ€ë¹„ ê°œì„ ë˜ì—ˆë‹¤.
   - ì´ëŠ” entity typing í•´ê²°ì„ ìœ„í•´ ë§ì€ knowledge ê°€ PLM ì— ì˜¤ë©°, prompt ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ PLM ì— ë¶„ì‚°ëœ entity information ì„ ë” ì˜ í™œì„±í™”í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ„
3. ê²°ê³¼ì—ì„œ entity types ì˜ structured hierarchy information ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ë‹¤ë¥¸ masked position ì„ ì œê³µí•¨ìœ¼ë¡œì¨, PTR ì´ PLET ë³´ë‹¤ ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ
   - ì¼ë°˜ì ìœ¼ë¡œ, entity type schema ì˜ prior knowledge ë¥¼ ê·œì¹™ì ìœ¼ë¡œ ì¸ì½”ë”©í•˜ì—¬ ê³ ì°¨ì› ë° ì„¸ë¶€ entity information ì„ ë”°ë¥´ë„ë¡ prompt ë¥¼ ì„¤ê³„í•¨ìœ¼ë¡œì¨ PLM ì—ê²Œ entity type ì„ ë” ì˜ ê°ì§€í•˜ ìˆ˜ ìˆë„ë¡ í•¨

## 4.3 The results on intent classification

ì´ ì„¹ì…˜ì—ì„œëŠ” task intent classification ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì œì‹œ

### Dataset

![Table 9](image-10.png)

Table 9 ì²˜ëŸ¼, ë‹¤ìŒ ì„¸ ê°œì˜ ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜ì„ ìˆ˜í–‰:

- CLINC150 : ì€í–‰, ì—…ë¬´, ì—¬í–‰, ìš”ë¦¬ ë° ì¼ë¶€ ë©”ë‹¤ ì •ë³´ì™€ ê°™ì€ 10ê°œ domain ì—ì„œ fine-grained 150 ê°€ì§€ intents ë¥¼ ë‹¤ë£¨ëŠ” ë°ì´í„°ì…‹
- BANKING77 : ì€í–‰ domain ì—ì„œ fine-grained intent detection ì„ ìœ„í•´ ìƒì„±ëœ ë°ì´í„°ì…‹
- HWU64 : 21 domain ì—ì„œ fine-grained 64 intents ë¥¼ ë‹¤ë£¨ëŠ” ë°ì´í„°ì…‹. ì´ ë„ë©”ì¸ì€ ì£¼ë¡œ human-home robot ìƒí˜¸ì‘ìš©ì„ ë°˜ì˜

ìœ„ ì„¸ ë°ì´í„°ì…‹ ëª¨ë‘ metric ìœ¼ë¡œëŠ” accuracy (%) ë¥¼ ì‚¬ìš©í•œë‹¤.

### Baselines and implementation details

intent classification ì‹¤í—˜ì—ì„œ, CPET ë…¼ë¬¸ (Zhang et al., 2021a)ì˜ ì„¤ì •ê³¼ ë™ì¼í•˜ê²Œ í•œë‹¤.

- RoBERTa_base ë¥¼ backbone ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  batch size 16 ìœ¼ë¡œ ì„¤ì •
- 5-shot ë° 10-shot (ê° ì˜ë„ë‹¹ Nê°œì˜ í›ˆë ¨ ì˜ˆì œ) ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ íŠœë‹
- learning rate $\{ 1e-5, 3e-5, 5e-5 \}$ ì¤‘ ëª¨ë¸ì„ ìµœì í™”
- ëª¨ë“  tuning ê³¼ì •ì—ëŠ” 30 epochs ì†Œìš”
- ë°ì´í„°ì…‹ 5ê°œì˜ ë‹¤ë¥¸ seeds ë¡œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§í•˜ì—¬ í‰ê·  ê²°ê³¼

---

PTR ì˜ íš¨ê³¼ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ 5ê°€ì§€ ìµœì‹  intent classification model ê³¼ ë¹„êµ:

- RoBERTa_base : vanilla fine-tuning
- USE : 16ê°œ ì–¸ì–´ì— ëŒ€í•´ pre-training ëœ large-scale multilingual model ë¡œ intent classification ì„ ìœ„í•´ ì‚¬ìš©
- ConveRT :  ëŒ€í™” ë°ì´í„°ì—ì„œ retrieval-based response selection task ë¥¼ í†µí•œ pre-trained model
- ConvBERT : BERT ë¥¼ í™•ì¥í•˜ì—¬ open-domain dialogue corpus ì— ëŒ€í•´ ì¶”ê°€ì ì¸ fine-tuning ë° task-adaptive self-supervised training ì„ ìˆ˜í–‰í•œ ëª¨ë¸
- CPFT : intent classification ì„ ìœ„í•´ contrastive pre-training ë° fine-tuning ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸
  - ì—¬ê¸°ì„  contrastive pre-training ì—†ëŠ” CPFT ì™€ PTR ì„ ë¹„êµ

prompt tuning ì´ intent classification ì— ë„ë¦¬ íƒêµ¬ë˜ì§€ ì•Šì•„, ì—¬ê¸°ì„  PTR ì˜ simplified one mask ë²„ì „ì¸ "PTINT" ì„ êµ¬í˜„

![Table 10](image-11.png)

### Comparison between PTR and baselines

![Table 11](image-12.png)

Table 11 ì€ 3 ê°€ì§€ ë°ì´í„°ì…‹ì—ì„œ PTR ê³¼ baselines ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤.

ì´ë¥¼ í†µí•´ ë‹¤ìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤:

1. PTINT ì™€ PTR ì€ ëª¨ë“  ìƒí™©ì—ì„œ vanilla fine-tuning ë³´ë‹¤ í° ì„±ê³¼
   - ì´ëŠ” PTR ì„ tuning í•˜ê¸° ìœ„í•´ prompt ì‚¬ìš©ì´ íš¨ê³¼ì ì„ì„ ì‹œì‚¬
   - prompt tuning ì™¸ì—ë„, large-scale intent-related data ë¥¼ ê°•í™”í•˜ëŠ” ê²ƒì€ intent classification ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.
   - ì§ê´€ì ìœ¼ë¡œ, prompt tuning ì€ PLMs ì˜ task-specific knowledge ë¥¼ ëª¨ë°©í•˜ëŠ” ê²ƒì„ ëª©í‘œí•˜ì§€ë§Œ, pre-training ì„ ê°•í™”í•˜ëŠ” ë°©ë²•ì€ PLM ì— ë” ë§ì€ task-specific knowledge ë¥¼ ì£¼ì…í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶˜ë‹¤.
2. ëŒ€ë¶€ë¶„ì˜ ìƒí™©ì—ì„œ, prompt-based methods ëŠ” pre-training ì„ ê°•í™”í•˜ëŠ” ëª¨ë¸ë³´ë‹¤ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ìŒ
   - ì´ëŠ” PLM ì— ë§ì€ intent-specific knowledge ì´ ì €ì¥ë˜ì–´ ìˆìœ¼ë©°, ê¸°ì¡´ì˜ ë‚´ì¬ëœ knowledge ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ì´ ì¤‘ìš”í•˜ê³  ìœ ë§í•¨ì„ ë‚˜íƒ€ëƒ„
   -  ì‚¬ì‹¤, pre-training ê°•í™” ë° prompt tuning ì„ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆë‹¤. (í–¥í›„ ì—°êµ¬)
3. intent types ëŠ” entity types ë° relation types ë³´ë‹¤ ëœ structure í•˜ê¸° ë•Œë¬¸ã…”, PTINT ëŠ” ì˜ ìˆ˜í–‰ë˜ë©°, PTR ëŒ€ë¹„ PTINT ê°œì„ ì€ ì•½í•˜ë‹¤.
   - ì „ë°˜ì ì¸ ê²°ê³¼ëŠ” relation classification ë° entity typing ì™€ ì¼ê´€ì„±ì´ ìˆìœ¼ë©°, rules ë¥¼ ì‚¬ìš©í•œ prompt ì‘ì„±ì´ íš¨ê³¼ì ì„ì„ ë³´ì—¬ì¤€ë‹¤.
   - ë³¸ ë…¼ë¬¸ì˜ rules ì€ heuristic í•˜ê²Œ êµ¬ì¶•ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” ì¶©ë¶„íˆ íš¨ê³¼ì ì´ì§€ë§Œ ìµœì ì´ ì•„ë‹ ìˆ˜ ìˆë‹¤. (í–¥í›„ ì—°êµ¬)

## 4.4 The convergence of PTR

ë‹¤ì–‘í•œ prompt tuning ì˜ í•™ìŠµ íš¨ìœ¨ì„±ì„ ë¹„êµí•˜ê¸° ìœ„í•´ ReTACRED ì™€ FewNERD ë‘ ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì—¬ ê° í›ˆë ¨ ê³¼ì •ì˜ ì„±ëŠ¥ ê³¡ì„ ì„ ë³´ì—¬ì¤Œ

![Figure 3](image-13.png)

Fig.3 ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯, PTR ì€ íš¨ê³¼ì ì´ë©´ì„œë„ ë¹ ë¥¸ ìˆ˜ë ´ì„ ì´ë¤˜ë‹¤.

ë‹¤ë¥¸ prompt tuning ê³¼ ë¹„êµí•˜ì—¬, ë¹„ë¡ PTR ì˜ template ì€ ìµœì¢…ì ìœ¼ë¡œ ë” ë³µì¡í•˜ê³  ëª¨ë¸ì— ë§ì€ ì •ê·œí™”ë¥¼ ê°€ì ¸ì˜¤ì§€ë§Œ, PTR ì˜ ìˆ˜ë ´ì€ ëŠë ¤ì§€ì§€ ì•Šì•˜ë‹¤.

í•œí¸, PTR ì˜ prompt ëŠ” ë³´ë‹¤ íš¨ê³¼ì ì´ê³  íš¨ìœ¨ì ìœ¼ë¡œ PLMs ì˜ knowledge ìê·¹ì„ ìœ„í•´ ë” ë§ì€ ì •ë³´ë¥¼ ë„ì…í•  ìˆ˜ë„ ìˆë‹¤. ë˜í•œ PLMs ì—ê²Œ prior task knowledge ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì€ curriculum learning ê³¼ ë§¤ìš° ìœ ì‚¬í•˜ë©°, ì´ëŠ” PTR ì˜ ë” ë‚˜ì€ ìˆ˜ë ´ì„±ì˜ ì´ìœ ë‹¤.

ì´ëŸ¬í•œ ê²°ê³¼ëŠ” prompt tuning ê³¼ rules ê°€ ìˆ˜ë ´ì— ëŒ€í•œ ì ì¬ì  ê¸ì •ì ì¸ ì˜í–¥ì„ ë‚˜íƒ€ë‚¸ë‹¤.

# 5. Related work

# 6. Conclusion

ë³¸ ë…¼ë¬¸ì—ì„œ, ì €ìëŠ” many-clas text classification task ë¥¼ ìœ„í•œ prompt tuning with rules (PTR) ì„ ì œì‹œ.

rule ì— ë”°ë¼ sub-prompts ë¥¼ task-specific prompts ë¡œ êµ¬ì„±í•˜ì—¬, prior task knowledge ë¥¼ prompt tuning ìœ¼ë¡œ ì¸ì½”ë”©í•  ìˆ˜ ìˆë‹¤.

sub-prompts ì˜ ë„ì…ì€ template ë° label word set ì„¤ê³„ì˜ ì–´ë ¤ì›€ì„ ì™„í™”í•  ìˆ˜ ìˆë‹¤.

relation classification, entity typing ë° intent classification ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼ëŠ” PTR ì´ ì¶”ê°€ model layer, manual annotations ë° augmented data ì—†ì´ ê¸°ì¡´ì˜ comparable vanilla fine-tuning ë° prompt tuning baselines ë¥¼ ëŠ¥ê°€í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.