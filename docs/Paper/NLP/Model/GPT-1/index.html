<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/Model/2018-06-GPT">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Improving Language Understanding by Generative Pre-Training | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/Model/GPT-1"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Improving Language Understanding by Generative Pre-Training | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/Model/GPT-1"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/Model/GPT-1" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/Model/GPT-1" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.64498265.js" as="script">
<link rel="preload" href="/assets/js/main.fd7722de.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Attacking/universal-adversarial-prompt">Attacking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Attention Is All You Need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/Model/GPT-1">Improving Language Understanding by Generative Pre-Training</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/Model/BERT">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Flan-T5">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Engineering/CoT/Chain-of-Thought">Prompt Engineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Model</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Improving Language Understanding by Generative Pre-Training</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Improving Language Understanding by Generative Pre-Training</h1></header><p>논문 및 이미지 출처 : <a href="https://www.mikecaptain.com/resources/pdf/GPT-1.pdf" target="_blank" rel="noopener noreferrer">https://www.mikecaptain.com/resources/pdf/GPT-1.pdf</a></p><h1>Abstract</h1><p>Natural language understanding (NLU) 은 textual entailment, question answering, semantic similarity assessment 및 document classification 등으로 구성</p><p>large unlabeled text corpora 는 풍부하지만, 학습하기 위한 labeled data 는 희소하다. 이는 개별 trained model 이 적절하게 수행하기 어렵게 한다.</p><p>저자는 unlabeled text 의 다양한 corpus 에 대한 <em>generative pre-training</em> 을 통해 큰 이득을 얻음을 보여주며, 각 specific task 에 대한 <em>discriminative fine-tuning</em> 을 수행한다.</p><ul><li>이전 방식과 달리, fine-tuning 중 task 에 대한 task-aware input transformation 을 활용하여 effective transfer 을 달성하며 model architecture 를 최소한으로 변경</li><li>NLU benchmarks 에 대한 저자의 접근법의 효과를 입증</li><li>저자의 general task-agnostic model 은 각 task 데 특화된 아키텍처를 사용하는 학습된 모델보다 우수한 성능을 보임</li><li>결과, 12 tasks 중 9 tasks 에서 SOTA 보다 크게 개선<ul><li>commonsense reasoning (Stories Cloze Test) 8.9%, question answering (RACE) 5.7%, textual entailment (MultiNLI) 1.5% 향상 달성</li></ul></li></ul><h1>1. Introduction</h1><p>raw text 에서 효과적인 학습은 NLP 에서 supervised learning 의존을 완화하는 데 중요하지만, 상당한 양의 manual labeled data 가 필요하며, 이는 부족한 domain 적용을 제한하게 된다.</p><p>이에 unlabeled data 에서 linguistic information 을 활용할 수 있는 모델은 추가 주석 수집이 대안이지만, 시간 소모와 비용이 발생한다. 게다가 상당한 supervision 이 제공되는 경우에도 unlabeled 방식으로 좋은 표현을 학습하는 것이 성능 향상에 도움이 될 수 있다. 이런 증거는 pre-trained word embedding 을 사용하여 다양한 NLP task 의 성능을 개선한 것이다.</p><p>하지만 unlabeled text 의 word-level information 을 활용하는 것은 두 이유로 어렵다.</p><ul><li>어떤 유형의 optimization objectives 가 transfer 에 효과적인지 명확하지 않음<ul><li>최근 language modeling, machine translation 및 discourse coherence 같은 다양한 objectives 를 고려하며, 각 방법은 서로 다른 task 에서 outperforming 함</li></ul></li><li>learned representations 를 target task 로 transfer 하는 효과적인 방법에 대한 일치된 의견이 없음<ul><li>기존 기술은 모델 아키텍처에 대한 task-specific changes, intricate learning schemes 및 adding auxiliary learning 을 결합한다. 이런 불확실성으로 언어 처리를 위한 효과적인 semi-supervised learning 개발이 어려워짐</li></ul></li></ul><p>본 논문은 unsupervised pre-training 및 supervised fine-tuning 의 조합을 사용하여 language understanding tasks 에 대한 semi-supervised approach 를 탐구</p><ul><li>목표는 넓은 범위의 task 에 대해 little adaptation 을 transfer 하는 universal representration 을 학습하는 것<ul><li>unlabeled text 의 large corpus 및 수동 주석이 있는 training examples 가 포함된 여러 데이터셋 (target tasks)에 접근</li><li>이 설정은 target task 가 unlabeled corpus 와 동일한 domain 에 있어야 하는 것을 요구하지 않음</li></ul></li><li>저자는 two-stage training procedure 사용<ol><li>unlabeled data 에 대한 language modeling objective 를 사용하여 neural network model 의 initial parameters 를 학습</li><li>이후 supervised objective 를 사용하여 target task 에 맞게 parameter adapting</li></ol></li></ul><p>저자는 <em>Transformer</em> 를 사용</p><ul><li>Transformer 는 machine translation, document generation 및 syntactic parsing 과 같은 다양한 task 에서 강력한 성능 발휘</li><li>이 모델은 text 의 long-term dependencies 를 처리하기 위해 structured memory 를 제공하여 recurrent 과 비교하여 다양한 task 에서 robust transfer 제공</li><li>transfer 중, traversal-style approches 에서 파생된 task-specific input adaptations 를 활용하며, 이러한 adaptations 는 structured text input 을 tokens 의 contiguous sequence 로 처리</li><li>실험에서 증명하며, 이러한 adaptations 는 pre-trained model 의 아키텍처를 minimal changes 로 효과적인 fine-tuning 을 가능케함</li></ul><p>language understanding tasks 의 네 가지 유형 - natural language inference, question answering, semantic similarity 및 text classification 에서 평가</p><ul><li>general task-agnostic model 은 각 task 의 특별히 설계된 아키텍처를 사용한 trained model 보다 우수한 성능 보임</li><li>연구 결과, 12 tasks 에서 9 tasks 에서 SOTA 를 크게 개선<ul><li>commonsense reasoning (Stories Cloze Test) 8.9%, question answering (RACE) 5.7%, textual entailment (MultiNLI) 1.5% 향상 달성</li><li>GLUE benchmark 5.5% 개선 달성</li><li>pre-trained model 의 zero-shot 동작을 분석하고, 다른 네 가지 설정에서 linguistic knowledge 를 획득함을 보여줌</li></ul></li></ul><h1>2. Related Work</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="semi-supervised-learning-for-nlp">Semi-supervised learning for NLP<a href="#semi-supervised-learning-for-nlp" class="hash-link" aria-label="Direct link to Semi-supervised learning for NLP" title="Direct link to Semi-supervised learning for NLP">​</a></h4><p>저자의 연구는 natural language 에 대한 semi-supervised 범주에 넓게 속함</p><ul><li>이는 sequence labeling 또는 text  classification 같은 task 에 응용</li><li>초기 접근법ㅇ은 unlabeled data 를 사용하여 word-level 또는 phrase-level statistics 를 계산한 다음 이를 supervised model 의 특성으로 사용</li><li>몇 년간 unlabeled corpora 에서 훈련된 word embedding 을 사용하여 다양한 task 의 성능을 개선하는 이점을 입증</li><li>이는 word-level 의 information 을 transfer 하지만, 저자는 higher-level emantics 의 의미를 포착하는 것이 목표</li></ul><p>최근 unlabeled data 에서 word-level semantics 이상의 의미를 학습하고 활용하는 것을 조사. Phrase-level 또는 sentence-level embeddings 는 다양한 target tasks 를 위해 text 를 suitable vector representations 로 인코딩하는 데 사용</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="unsupervised-pre-training">Unsupervised pre-training<a href="#unsupervised-pre-training" class="hash-link" aria-label="Direct link to Unsupervised pre-training" title="Direct link to Unsupervised pre-training">​</a></h4><p>good initialization point 를 찾는 것을 목표로하는 semi-supervised learning 의 경우.</p><p>이는 image classification 및 regression tasks 에 사용되었으며, 이후 pre-training 이 deep neural network 에서 더 나은 generalization 을 가능하게 하는 정규화 체계로 작용하는 것을 입증했다.</p><p>저자의 연구와 가까운 연구는 language modeling objective 를 사용하여 neural network 를 pre-training 한 후 supervision 이 있는 target task 에 fine-tuning. 하지만 pre-training 단계가 linguistic information 을 일부 포착하는 것을 도와주더라도, LSTM model 의 사용은 예측 능력의 범위를 짧게 제한된다.</p><p>반면, Transformer 는 longer-range linguistic structure 를 포착할 수 있도록 해준다. 나아가, natural language inference, paraphrase detection 및 story completion 을 포함한 넓은 범위의 task 에서 효과를 발휘</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="auxiliary-training-objectives">Auxiliary training objectives<a href="#auxiliary-training-objectives" class="hash-link" aria-label="Direct link to Auxiliary training objectives" title="Direct link to Auxiliary training objectives">​</a></h4><p>auxiliary unsupervised training objectives 를 추가하는 것은 alternative form 이다.</p><ul><li>초기 연구에 Collobert 및 Weston 은 semantic role labeling 개선을 위해 POS tagging, chunking, named entity recognition 및 language modeling 같은 다양한 auxiliary NLP task 를 사용 </li><li>최근, Rei 는 target task objective 에 auxiliary language modeling objective 를 추가했으며, sequence labeling task 에서 성능 향상을 보여줌</li></ul><p>저자의 실험도 auxiliary objective 를 사용하지만, unsupervised pre-training 은 target tasks 에 관련된 여러 linguistic 측면을 학습</p><h1>3. Framework</h1><p>저자의 training procedure 는 two stages</p><ol><li>large corpus 에서 high-capacity language model 을 학습</li><li>labeled data 로 discriminative task 에 adapting 시키는 fine-tuning stage</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-unsupervised-pre-training">3.1 Unsupervised pre-training<a href="#31-unsupervised-pre-training" class="hash-link" aria-label="Direct link to 3.1 Unsupervised pre-training" title="Direct link to 3.1 Unsupervised pre-training">​</a></h2><p>unsupervised corpus tokens <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>u</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>u</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{U} = \{u_1, \dots, u_n\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.09931em">U</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span> 가 주어졌을 때, standard language modeling objective 를 사용하여 다음과 같이 likelihood 를 maximize</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>L</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi mathvariant="script">U</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>u</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>u</mi><mrow><mi>i</mi><mo>−</mo><mi>k</mi></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>u</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">;</mo><mi mathvariant="normal">Θ</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} L_1(\mathcal{U}) = \sum_i \log P(u_i|u_{i-k},\dots, u_{i-1};\Theta) \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.3277em;vertical-align:-0.9138em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4138em"><span style="top:-3.4138em"><span class="pstrut" style="height:3.05em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathcal" style="margin-right:0.09931em">U</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">Θ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9138em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4138em"><span style="top:-3.4138em"><span class="pstrut" style="height:3.05em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9138em"><span></span></span></span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> : context window size</li><li>conditional probability <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span></span> : parameters <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Θ</mi></mrow><annotation encoding="application/x-tex">\Theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">Θ</span></span></span></span></span> 가 있는 neural network 를 사용하여 modeling<ul><li>이러한 parameters 는 stochastic gradient descent 를 사용하여 훈련</li></ul></li></ul><p>저자의 실험에서는 language modeling 에 대해 multi-layer <em>Transformer decoder</em> 사용</p><ul><li>이는 Transformer 의 변형</li><li>input context tokens 에 대해 mutli-headed self-attention 연산을 적용한 다음 position-wise feedforward layers 를 통해 target tokens 에 대한 output distribution 생성</li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>h</mi><mn>0</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>U</mi><msub><mi>W</mi><mi>e</mi></msub><mo>+</mo><msub><mi>W</mi><mi>p</mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>h</mi><mi>l</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext>transformer_block</mtext><mo stretchy="false">(</mo><msub><mi>h</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∀</mi><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">]</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><msub><mi>h</mi><mi>n</mi></msub><msubsup><mi>W</mi><mi>e</mi><mi>T</mi></msubsup><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \begin{align*} h_0 &amp;= UW_e + W_p \\ h_l &amp;= \text{transformer\_block}(h_{l-1}) \forall i \in [1, n] \\ P(u) &amp;= \text{softmax}(h_nW_e^T) \end{align*} \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.5513em;vertical-align:-2.0257em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5257em"><span style="top:-4.5257em"><span class="pstrut" style="height:4.5257em"></span><span class="mord"><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5257em"><span style="top:-4.6857em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.1857em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-1.6343em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0257em"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5257em"><span style="top:-4.6857em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span><span style="top:-3.1857em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord text"><span class="mord">transformer_block</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∀</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">n</span><span class="mclose">]</span></span></span><span style="top:-1.6343em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0257em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0257em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5257em"><span style="top:-4.5257em"><span class="pstrut" style="height:4.5257em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0257em"><span></span></span></span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>u</mi><mrow><mo>−</mo><mi>k</mi></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>u</mi><mrow><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">U = (u_{-k}, \dots, u_{-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">U</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> : tokens 의 context vector</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> : layers 수</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>e</mi></msub></mrow><annotation encoding="application/x-tex">W_e</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> : token embedding matrix</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">W_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> : position embedding matrix</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-supervised-fine-tuning">3.2 Supervised fine-tuning<a href="#32-supervised-fine-tuning" class="hash-link" aria-label="Direct link to 3.2 Supervised fine-tuning" title="Direct link to 3.2 Supervised fine-tuning">​</a></h2><p>Eq. 1 의 objective 로 model 을 training 한 후, supervised target task 에 parameters 를 adapting 한다.</p><p>각 instance 가 input tokens sequence <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mn>1</mn></msup><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>x</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">x^1, \dots, x^m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span></span></span></span></span></span></span></span> 와 label <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span> 로 구성된 labeled dataset <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">C</mi></mrow><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.05834em">C</span></span></span></span></span> 를 가정</p><p>input 은 pre-trained model 을 통해 final transformer block 의 activation <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>l</mi><mi>m</mi></msubsup></mrow><annotation encoding="application/x-tex">h_l^m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9775em;vertical-align:-0.2831em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em"><span></span></span></span></span></span></span></span></span></span></span> 을 얻고, 이는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 예측을 위해 추가된 linear output layer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">W_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 에 주입</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><msup><mi>x</mi><mn>1</mn></msup><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>x</mi><mi>m</mi></msup><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><msubsup><mi>h</mi><mi>l</mi><mi>m</mi></msubsup><msub><mi>W</mi><mi>y</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} P(y|x^1,\dots,x^m) = \text{softmax}(h_l^mW_y). \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2241em;vertical-align:-0.3621em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8621em"><span style="top:-2.9979em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7144em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3621em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8621em"><span style="top:-2.8621em"><span class="pstrut" style="height:2.8641em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3621em"><span></span></span></span></span></span></span></span></span></div><p>그리고 다음과 같은 objective 를 maximize</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>L</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi mathvariant="script">C</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></munder><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><msup><mi>x</mi><mn>1</mn></msup><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>x</mi><mi>m</mi></msup><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} L_2(\mathcal{C}) = \sum_{(x,y)}\log P(y|x^1,\dots,x^m). \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.566em;vertical-align:-1.033em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.533em"><span style="top:-3.533em"><span class="pstrut" style="height:3.05em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathcal" style="margin-right:0.05834em">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.809em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.033em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.533em"><span style="top:-3.533em"><span class="pstrut" style="height:3.05em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.033em"><span></span></span></span></span></span></span></span></span></div><p>저자는 fine-tuning 에 language modeling 에 auxiliary objective 을 포함하는 것이 학습에 도움이 되는 것을 발견. 이는 (a) supervised model 의 generalization 을 개선하고 (b) 수렴을 가속도화함으로써 학습을 돕는다.</p><p>구체적으로, 다음과 같은 objective 를 optimizing (with weight <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">λ</span></span></span></span></span>)</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>L</mi><mn>3</mn></msub><mo stretchy="false">(</mo><mi mathvariant="script">C</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>L</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi mathvariant="script">C</mi><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><mo>∗</mo><msub><mi>L</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi mathvariant="script">C</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda * L_1(\mathcal{C}) \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathcal" style="margin-right:0.05834em">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathcal" style="margin-right:0.05834em">C</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathcal" style="margin-right:0.05834em">C</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-2.85em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span></span></div><p>전체적으로, fine-tuning 중 필요한 extra parameters 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">W_y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 와 delimiter token 의 embeddings 뿐이다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-task-specific-input-transformations">3.3 Task-specific input transformations<a href="#33-task-specific-input-transformations" class="hash-link" aria-label="Direct link to 3.3 Task-specific input transformations" title="Direct link to 3.3 Task-specific input transformations">​</a></h2><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-9-a351d62224617ac8d1745245737c853d.png" width="1542" height="786" class="img_ev3q"></p><ul><li>text classification 같은 일부 task 의 경우 위에서 설명한 대로 directly fine-tuning 가능</li><li>question answering 또는 textual entailment 같은 특정 task 는 ordered sentence pairs 나 document, question 및 answer 의 triplets 같은 structured inputs 을 갖는다.</li><li>저자의 pre-trained model 이 text contiguous sequence 에서 훈련되어, 이러한 tasks 에 적용하려면 일부 수정이 필요<ul><li>이전 연구에서는 transferred representations 위에 task specific architecture 를 학습하는 것을 제안. 이러한 방식은 상당한 양의 task-specific customization 을 재도입하여 additional architectural components 에 대해 transfer learning 을 사용하지 않는다.</li></ul></li><li>저자는 대신에 traversal-style approach 를 사용<ul><li>structured inputs 를 pre-trained model 이 처리할 수 있는 ordered sequence 로 변환</li><li>이러한 입력 변환으로 task 간의 architecture 를 광범위하게 변경하지 않고도 task 에 적용할 수 있음 (Fig. 1)</li><li>all transformations 에는 randomly initialized start 및 end token <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo><mi>s</mi><mo>&gt;</mo><mo separator="true">,</mo><mo>&lt;</mo><mi>e</mi><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&lt;s&gt;, &lt;e&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span></span><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span></span></span></span></span> 추가</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="textual-entailment">Textual entailment<a href="#textual-entailment" class="hash-link" aria-label="Direct link to Textual entailment" title="Direct link to Textual entailment">​</a></h4><p>entailment tasks 의 경우, premise <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span></span> 와 hypothesis <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">h</span></span></span></span></span> token sequence 를 연결하고, 그 사이에 delimiter token ($) 를 넣는다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="similarity">Similarity<a href="#similarity" class="hash-link" aria-label="Direct link to Similarity" title="Direct link to Similarity">​</a></h4><p>similarity task 의 경우, 비교되는 두 sentences 의 순서가 별도로 표현되지 않음. 이를 반영하기 위해 input sequence 를 both possible sentence ordering (사이에 delimiter 삽입) 을 포함하도록 수정하고 각각 독립적으로 처리하여 linear output layer 에 주입되기 전에 element-wise 로 추가된 two sequence representations <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>l</mi><mi>m</mi></msubsup></mrow><annotation encoding="application/x-tex">h_l^m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9775em;vertical-align:-0.2831em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-2.4169em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831em"><span></span></span></span></span></span></span></span></span></span></span> 생성</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="question-answering-and-commonsense-reasoning">Question Answering and Commonsense Reasoning<a href="#question-answering-and-commonsense-reasoning" class="hash-link" aria-label="Direct link to Question Answering and Commonsense Reasoning" title="Direct link to Question Answering and Commonsense Reasoning">​</a></h4><p>이러한 tasks 의 경우, context document <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span>, question <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span></span> 및 possible answer set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{a_k\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span> 가 제공되는데, 각각 추가하여 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>z</mi><mo separator="true">;</mo><mi>q</mi><mo separator="true">;</mo><mi mathvariant="normal">$</mi><mo separator="true">;</mo><msub><mi>a</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[z;q;\$;a_k]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">$</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span> 를 얻는다.</p><p>각 sequence 는 저자의 모델에 독립적으로 처리되고 이후 possible answers 에 대한 output distribution 을 생성하기 위해 softmax layer 를 통해 normalize</p><h1>4. Experiments</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-setup">4.1 Setup<a href="#41-setup" class="hash-link" aria-label="Direct link to 4.1 Setup" title="Direct link to 4.1 Setup">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="unsupervised-pre-training-1">Unsupervised pre-training<a href="#unsupervised-pre-training-1" class="hash-link" aria-label="Direct link to Unsupervised pre-training" title="Direct link to Unsupervised pre-training">​</a></h4><ul><li>저자는 훈련을 위해 BooksCorpus dataset 사용<ul><li>어드벤처, 판타지, 로맨스 등 7,000 개의 고유한 미게시 도서 포함</li><li>이는 long contiguous text 를 포함하고 있어, generative model 이 long-range information 에 대한 condition 을 학습할 수 있다.</li></ul></li><li>비슷한 방식인 ELMo 는 거의 동일한 크기지만, sentence level 에서 섞여 있어 long-range structure 을 파괴한다.</li><li>저자의 모델은 이러한 corpus 에서 very low token level 의 perplexity 18.4 달성</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="model-specifications">Model specifications<a href="#model-specifications" class="hash-link" aria-label="Direct link to Model specifications" title="Direct link to Model specifications">​</a></h4><p>모델은 기존 transformer work 를 따른다.</p><ul><li>masked self-attention heads (768 dimensional states 및 12 attention heads) 가 있는 12-layer decoder-only transformer 를 훈련</li><li>position-wise feed-forward networks 의 경우, 3072 dimensional innter states 사용</li><li>Adam optimization 체계를 사용하며 max learning rate 2.5e-5<ul><li>learning rate 는 처음 2000 updates 동안 linearly increase, cosine schedule 을 사용하여 0 으로 annealing</li></ul></li><li>contiguous sequences 512 tokens 의 randomly sampled minibatches 64개에서 100 epochs training</li><li>layernorm 이 사용되므로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>0.02</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">N(0, 0.02)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">0.02</span><span class="mclose">)</span></span></span></span></span> 의 간단한 weight initialization 로도 충분</li><li>40,000 개의 병합이 있는 bytepair encoding (BPE) vocabulary 를 사용</li><li>regularization 은 residual, embedding 및 attention drops 0.1 rate</li><li>[Fixing weight decay regularization in adam]<!-- --> 에서 제안된 L2 regularization 의 수정된 버전을 사용하여 all non bias or gain weights 에 대해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">w=0.01</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.01</span></span></span></span></span> 설정</li><li>activation function 은 Gaussian Linear Unit (GELU) 사용</li><li>기존 연구에서 제안된 sinusoidal 버전 대신 learned position embedding 사용</li><li>BooksCorpus 에서 raw text 를 정리하고 일부 punctuation 과 whitespace 를 표준화하고 <em>spaCy</em> tokenizer 를 사용하기 위해 <em>ftfy</em> 라이브러리 사용</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="fine-tuning-details">Fine-tuning details<a href="#fine-tuning-details" class="hash-link" aria-label="Direct link to Fine-tuning details" title="Direct link to Fine-tuning details">​</a></h4><p>특정한 명시사항이 없ㅇ면 unsupervised pre-training 의 hyperparameter setting 을 재사용</p><ul><li>classifier 에는 dropout rate 0.1 추가</li><li>대부분 tasks 에선, learning rate 6.25e-5 및 batch size 32 사용</li><li>빠르게 fine-tuning 되어 대부분의 경우 3 epoch training 이 충분</li><li>0.2% training 을 위한 warmup 을 사용하는 linear learning rate decay schedule 사용</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">λ</span></span></span></span></span> 는 0.5 설정</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-supervised-fine-tuning">4.2 Supervised fine-tuning<a href="#42-supervised-fine-tuning" class="hash-link" aria-label="Direct link to 4.2 Supervised fine-tuning" title="Direct link to 4.2 Supervised fine-tuning">​</a></h2><p>저자는 natural language inference, question answering, semantic similarity 및 text classification 등 다양한 supervised tasks 에서 실험 수행</p><p>이러한 작업은 GLUE 를 사용할 수 있으며, Table 1 에 all tasks 및 dataset 제공</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-10-5efddeb9485dbaa41e50ffd587521b3d.png" width="1329" height="285" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="natural-language-inference">Natural Language Inference<a href="#natural-language-inference" class="hash-link" aria-label="Direct link to Natural Language Inference" title="Direct link to Natural Language Inference">​</a></h4><p>natural language inference (NLI) task 은 sentence pair 을 읽고 그 사이의 <em>entailment</em>, <em>contradiction</em> 및 <em>neutral</em> 관계를 추론하는 것</p><p>이는 여러가지 현상 (어휘적 함축, 공통 참조, 어휘 및 문법적 모호성 등)이 있어 어렵다.</p><p>저자는 image captions (SNLI), transcribed speech, popular fiction 및 government reports (MNLI), Wikipedia articles (QNLI), science exames (SciTail) 및 news articles (RTE) 에서 평가</p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-11-53b61f8a9195383b1f747747340f95e6.png" width="1649" height="690" class="img_ev3q"></p><ul><li>5 dataset 중 4 가지에서 baseline 능가<ul><li>MNLI 1.5%, SciTail 5%, QNLI 5.8% 및 SNLI 0.6%</li></ul></li><li>RTE (2490 example) 에서 59% 달성했으며</li><li>larger NLI dataset 에서 저자의 성능을 고려하면, multi-task training 에서도 이점이 될 것으로 보임</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="question-answering-and-commonsense-reasoning-1">Question answering and commonsense reasoning<a href="#question-answering-and-commonsense-reasoning-1" class="hash-link" aria-label="Direct link to Question answering and commonsense reasoning" title="Direct link to Question answering and commonsense reasoning">​</a></h4><p>single 및 multi-sentence reasoning 측면을 필요로 하는 또 다른 task 는 question answering</p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-12-a1bca4424534da33e9ce9a309c676a00.png" width="1656" height="614" class="img_ev3q"></p><ul><li>중고등학교 시험의 영어 단락과 관련 질문이 포함된 RACE dataset 을 사용<ul><li>이 corpus 는 CCN 및 SQuAD 같은 dataset 보다 추론 유형의 질문이 많아 훈련을 통해 long-range contexts 처리를 할 수 있게 한다.</li></ul></li><li>Story Cloze Test 에서도 평가<ul><li>이 task 는 두 옵션 중 multi-sentence stories 의 correct ending 을 선택</li></ul></li><li>위 작업에서 이전의 SOTA 보다 큰 폭으로 능가</li><li>Story Cloze 8.9%, RACE 5.7% 향상</li></ul><p>이로서 long-range contexts 를 효과적으로 처리하는 능력을 보여줌</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="semantic-similarity">Semantic Similarity<a href="#semantic-similarity" class="hash-link" aria-label="Direct link to Semantic Similarity" title="Direct link to Semantic Similarity">​</a></h4><p>Semaintic similarity (or paraphrase detection) task 는 two sentences 가 의미적으로 동등한지 여부를 예측</p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-13-0c1c6bf25b33598866396260ac7227ca.png" width="1637" height="755" class="img_ev3q"></p><p>이 task 의 어려움은 개념적 표현을 인식하고 부정의 이해, 그리고 구문적 모호성 처리에 있다.</p><ul><li>3 dataset 인 Microsoft Paraphrase corpus (MRPC), Quora Question Pairs (QQP) 및 Semantic Textual Similarity (STS-B)</li><li>STS-B 1% 향상</li><li>QQP 에서는 성능 차이가 상당</li><li>Single-task BiLSTM + ELMo + Attn 에 비해 4.2% 향상</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="classification">Classification<a href="#classification" class="hash-link" aria-label="Direct link to Classification" title="Direct link to Classification">​</a></h4><p>classification task 에 대한 두 가지에 평가 </p><ul><li>Corpus of Linguistic Acceptability (CoLA) : sentence 가 문법적인지 아닌지 평가하며 고유한 언어적 편견을 테스트</li><li>Stanford Sentiment Treebank (SST-2) : standard binary classification task</li><li>이전 SOTA 인 35.0 을 큰 폭으로 45.4 기록</li><li>SST-2 에서 91.3% 달성하여 SOTA 와 competitive</li><li>GLUE 에서 전체 score 72.8 달성하여 이전 SOTA 인 68.9 보다 훨씬 우수</li><li>전반적으로, 12 dataset 중 9개 에서 SOTA 달성하며 앙상블도 능가</li><li>small STS-B (약 5.7K training examples) 및 large SNLI (약 550K training examples) 까지 다양하게 잘 작동</li></ul><h1>5. Analysis</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="impact-of-number-of-layers-transferred">Impact of number of layers transferred<a href="#impact-of-number-of-layers-transferred" class="hash-link" aria-label="Direct link to Impact of number of layers transferred" title="Direct link to Impact of number of layers transferred">​</a></h4><p>저자는 unsupervised pre-training 에서 supervised task 로 다양한 수의 layer 를 transferring 하는 영향 관찰</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-14-b0789c9d5c0ca0e7880974dc68297b7b.png" width="1241" height="695" class="img_ev3q"></p><ul><li>Fig. 2 left : MultiNLI 및 RACE 에서 저자의 성능을 transferred layers function 으로 나타낸다.<ul><li>transferring embedding 이 성능을 향상시키고 각 transformer layer 가 MultiNLI 에서 full transfer 에 대해 9% 까지 추가 이점을 제공</li><li>이는 pre-trained model 의 각 layer 가 target tasks 해결에 유용하다는 것</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="zero-shot-behaviors">Zero-shot Behaviors<a href="#zero-shot-behaviors" class="hash-link" aria-label="Direct link to Zero-shot Behaviors" title="Direct link to Zero-shot Behaviors">​</a></h4><p>transformers 의 pre-training 효과를 이해해보자. 기본 generative model 이 language modeling 능력 향상을 위해 평가한 많은 task 를 수행하는 것을 배우며 이는 transformer 의 structured attention memory 가 LSTM 보다 더 transferring 을 지원한다는 가설을 세운다.</p><p>저자는 supervised fine-tuning 없이 task 를 수행하는 generative model 의 heuristic solutions 설계하고, generative pre-training 의 heuristic solutions 의 효과를 시각화한다. (Fig. 2 right)</p><ul><li>이러한 heuristic 의 성능은 안정적이며 꾸준히 향상되는 것이 관찰되며, generative pretraining 이 다양한 task 의 기능을 학습하는 데 도움이 됨을 시사</li><li>LSTM 의 zero-shot 성능이 더 높은 분산을 나타내며 transfer 에서 transformer architecture 의 inductive bias 가 도움이 되는 것을 관찰</li><li>COLA 의 경우 (linguistic acceptability), examples 는 generative model 이 할당하는 average token log-probability 로 점수를 매기며 predictions 는 임계값을 기준으로 함</li><li>SST-2 의 경우 (sentiment analysis),  각 examples 에 token <em>very</em> 를 추가하고 model 의 output distribution 에 <em>positive</em> 와 <em>negative</em> word 로 제한하고 higher probability 를 할당하는 token 을 예측</li><li>RACE (question answering) 의 경우, 문서와 질문을 조건으로 할 때 model 이 average token log-probability 가 가장 높은 답변을 선택</li><li>DPRD (winograd schemas) 의 경우, 정해진 대명사를 두 가지 positive referrents 로 대체하고 예측. 대체 후 sequence 의 나머지에 higher average token log-probability 할당</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ablation-studies">Ablation studies<a href="#ablation-studies" class="hash-link" aria-label="Direct link to Ablation studies" title="Direct link to Ablation studies">​</a></h4><p><img loading="lazy" alt="Table 5" src="/assets/images/image-15-e50d5e7bbca6c04a4fdba897acbea372.png" width="1643" height="444" class="img_ev3q"></p><p>세 가지 ablation study 수행 </p><ol><li>fine-tuning 중 auxiliary LM objective 없이 저자의 방법의 성능 조사<ul><li>auxiliary objective 가 NLI 및 QQP task 에 도움이 됨을 관찰</li><li>전반적으로 larger dataset 이 auxiliary objective 에서 이득을 얻지만 smaller dataset 에서는 그렇지 않음을 시사</li></ul></li><li>동일한 프레임워크를 사용하여 Transformer 와 single layer 2048 unit LSTM 과 비교하여 transformer 의 효과 분석<ul><li>LSTM 사용 시, 5.6 average score drop 을 관찰</li><li>LSTM 은 MRPC 에서만 transformer 능가</li></ul></li><li>pre-training 없이 supervised target tasks 에 직접 transformer 비교<ul><li>pre-training 부재가 all tasks 에 성능을 해치는 것을 관찰했으며, full model 과 비교하여 14.8% 감소</li></ul></li></ol><h1>6. Conclusion</h1><p>저자는 single task-agnostic model 을 generative pre-training 및 discriminative fine-tuning 을 통해 strong natural language 이해를 달성하기 위한 프레임워크 소개</p><ul><li>contiguous text 의 긴 텍스트로 pre-training 수행함으로써 모델은 world knowledge 와 long-range dependencies 처리 능력을 습득하고 이를 해결하기 위해 효과적으로 transfer</li><li>question answering, semantic similarity assessment, entailment determination 및 text classification 같은 discriminative task 를 해결하는 데 성공</li><li>12 dataset 중 9개에서 SOTA 달성</li><li>성능 향상이 가능했으며, Transformer 와 dataset (long range dependencies text)가 잘 작동하는지에 대한 힌트 제공</li><li>unsupervised learning 이 어떻게 작동하는지에 대한 이해를 더욱 개선</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/transformer">transformer</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/decoder-only">decoder-only</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/auto-regressive-model">auto-regressive model</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/causal-language-model">causal language model</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/Model/2018-06-GPT.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/Model/Transformer"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Attention Is All You Need</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/Model/BERT"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-unsupervised-pre-training" class="table-of-contents__link toc-highlight">3.1 Unsupervised pre-training</a></li><li><a href="#32-supervised-fine-tuning" class="table-of-contents__link toc-highlight">3.2 Supervised fine-tuning</a></li><li><a href="#33-task-specific-input-transformations" class="table-of-contents__link toc-highlight">3.3 Task-specific input transformations</a></li><li><a href="#41-setup" class="table-of-contents__link toc-highlight">4.1 Setup</a></li><li><a href="#42-supervised-fine-tuning" class="table-of-contents__link toc-highlight">4.2 Supervised fine-tuning</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.64498265.js"></script>
<script src="/assets/js/main.fd7722de.js"></script>
</body>
</html>