<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/NLP/Model/2018-10-BERT">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/NLP/Model/BERT"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | WYJLab"><meta data-rh="true" name="description" content="논문 이미지 및 출처 :"><meta data-rh="true" property="og:description" content="논문 이미지 및 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/NLP/Model/BERT"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/Model/BERT" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/NLP/Model/BERT" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.ec4b5b0a.js" as="script">
<link rel="preload" href="/assets/js/main.d9f0a85a.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">Analysis</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Attacking/universal-adversarial-prompt">Attacking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Augmentation/PromptDA">Augmentation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Generalization/NoisyTune">Generalization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Model</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/Model/Transformer">Attention Is All You Need</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/NLP/Model/GPT-1">Improving Language Understanding by Generative Pre-Training</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/NLP/Model/BERT">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Multi-Task/Flan-T5">Multi-Task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/PEFT/Composition/LoRA">PEFT</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Engineering/CoT/Chain-of-Thought">Prompt Engineering</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Prompt Tuning/PTR">Prompt Tuning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Reinforcement Learning/Reflexion">Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Text Generation/InstructGPT">Text Generation</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">NLP</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Model</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h1></header><p>논문 이미지 및 출처 : <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1810.04805.pdf</a></p><h1>Abstract</h1><p>저자는 <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers 의 약자 <strong>BERT</strong> 라는 new language representation model 을 소개</p><p>최근 language representation model 과 달리, BERT 는 all layer 에서 left 와 right context 를 모두 공동으로 conditioning 함으로써 unlabeled text 로부터 deep bidirectional representations 를 pre-training 하기 위해 설계</p><p>결과, pre-trained BERT 는 additional output layer 하나만으로도 question answering 및 language inference 같은 다양한 task 에 대해 SOTA model 을 만들기 위해 특정 아키텍처 수정 없이 fine-tuning 이 가능하다.</p><ul><li>BERT 는 개념적으로 간단하면서 경험적으로 강력</li><li>GLUE score 80.5% (7.7% point 향상)</li><li>MultiNLI accuracy 86.7% (4.6% point 향상)</li><li>SQuAD v1.1 question answering test F1 93.2% (1.5% point 향상)</li><li>SQuAD v2.0 test F1 83.1% (5.1% 향상)</li></ul><p>11 가지의 NLP task 에서 SOTA 달성</p><h1>1. Introduction</h1><p>Language model pre-training 은 다양한 NLP tasks 의 향상에 효과적임을 입증. 이에는 sentence-level tasks 인 natural language inference 및 paraphrasing 같은 task 뿐만 아니라, named entity recognition 및 question answering 같은 token-level task 도 포함</p><p>pre-trained language representations 를 적용하는 데는 두 전략이 있다: <em>feature-based</em> 및 <em>fine-tuning</em></p><ul><li>ELMo 같은 feature-based approach 는 pre-trained representations 를 additional features 로 포함하는 task-specific architectures 를 도입</li><li>Generative Pre-trained Transformer (OpenAI GPT) 같은 fine-tuning approach 는 minimal task-specific parameters 를 도입하고 all pre-trained parameter 를 fine-tuning 하여 downstream tasks 에 훈련</li></ul><p>위 두 방식은 pre-training 중 동일한 objective function 를 공유하는데, 여기서 unidirectional language mask 를 사용하여 general language representations 를 학습</p><hr><p>저자는 최근 기술이 fine-tuning 방식에 대한 pre-trained representations 의 능력을 제한한다고 주장.</p><ul><li>주요 한계는 standard language model 이 unidirectional 하다는 점이고, 이는 pre-training 중 사용 가능한 아키텍처 선택을 제한<ul><li>예로 GPT 는 모든 token 이 Transformer 의 self-attention layer 에서 previous token 에만 attend 하는 left-to-right architecture 를 사용한다.</li><li>이러한 제한은 sentence-level tasks 에 최적이 아니며, fine-tuning 을 token-level 에 적용할 때 해로울 수 있다. </li><li>이는 question answering 같은 task 에서 both directons 의 context 를 효과적으로 통합하는 것이 중요하기 때문</li></ul></li></ul><p>본 논문에서 BERT: <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers 을 제안하여 fine-tuning 방식을 개선한다.</p><ul><li>BERT 는 &quot;masked language model&quot; (MLM) pre-training objective 를 사용하여 이전의 unidirectionality constraint 를 완화 (Cloze task 에서 영감 받)<ul><li>masked language model 은 input 의 tokens 를 randomly masking 하고, objective 는 context 에만 기반하여 masked word 의 original vocabulary id 를 예측</li></ul></li><li>left-to-right language model pre-training 과 달리, MLM objective 는 representation 이 left 및 right context 를 융합할 수 있도록 하여, deep bidirectional Transformer 를 pre-training 할 수 있도록 함</li><li>masked language model 외에도, text-pair representations 를 공동으로 pre-training 하는 &quot;next sentence prediction&quot; task 도 사용</li></ul><p>논문의 기여는 다음과 같다.</p><ul><li>language representations 를 위한 bidirectional pre-training 의 중요성 입증<ul><li>Radford et al. (2018) 은 pre-training 에 unidirectional language models 를 사용하는 반면, BERT 는 deep bidirectional representations 를 가능하게 하는 masked language models 를 사용</li><li>이는 Peters et al. (2018a)의 독립적으로 훈련된 left-to-right 및 right-to-left LMs 의 얕은 결합을 사용하는 것과는 대조적</li></ul></li><li>pre-trained representation 이 heavily-engineered task-specific architectures 의 필요성을 줄인다는 것을 보여줌<ul><li>BERT 는 많은 task-specific architectures 를 능가하는 first fine-tuning based representation model 로, sentence-level 및 token-level tasks 에서 SOTA 달성</li></ul></li><li>BERT 는 11 가지 NLP task 에 SOTA 를 선도</li></ul><h1>2. Related Work</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-unsupervised-feature-based-approaches">2.1 Unsupervised Feature-based Approaches<a href="#21-unsupervised-feature-based-approaches" class="hash-link" aria-label="Direct link to 2.1 Unsupervised Feature-based Approaches" title="Direct link to 2.1 Unsupervised Feature-based Approaches">​</a></h2><p>words representation learning 은 수년 간 연구되어 왔으며, 이에 non-neural 및 neural 방법이 포함</p><ul><li>Pre-trained word embedding 은 NLP 에서 중요하며, scratch learned embedding 보다 개선을 제공</li><li>word embedding vector 를 pre-training 하기 위해 left-to-right language modeling objectives 와 left-to-right context 에서 incorrect words 를 구별하는 objective 가 사용</li></ul><p>위 방식은 sentence embedding 또는 paragraph embedding 같은 무거운 것들로 일반화. sentence representations 훈련을 위해, previous sentence representation 을 사용하여 next sentence words 의 left-to-right generation 또는 denoising auto-encoder derived objectives 를 사용</p><hr><p>ELMo 와 그 이전 model 은 word embedding 을 다른 차원으로 일반화</p><ul><li>left-to-right 및 right-to-left language model 로부터 <em>context-sensitive</em> feature 를 추출</li><li>각 token 의 contextual representation 은 left-to-right 및 right-to-left representation 의 연결</li><li>contextual word representation 을 기존 task-specific architectures 와 통합할 때, ELMo 는 여러 NLP 에서 SOTA 제공 (question answering, sentiment analysis 및 named entity recognition)</li></ul><p>Melamud et al. (2016)는 좌우 문맥에서 왼쪽과 오른쪽 문맥에서 single 단어를 예측하는 작업을 통해 문맥적 표현을 학습하는 것을 제안했습니다. 그들의 model 은 ELMo와 유사하게 특성 기반이며 깊게 bidirectional 적이지 않습니다. Fedus et al. (2018)은 클로즈 작업이 text generation model 의 견고성을 향상시키는 데 사용될 수 있다는 것을 보여줍니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-unsupervised-fine-tuning-approaches">2.2 Unsupervised Fine-tuning Approaches<a href="#22-unsupervised-fine-tuning-approaches" class="hash-link" aria-label="Direct link to 2.2 Unsupervised Fine-tuning Approaches" title="Direct link to 2.2 Unsupervised Fine-tuning Approaches">​</a></h2><p>feature-based approach 와 마찬가지로, 이 방향에서의 초기 연구들은 unlabeled text 의 word embedding parameters 만 pre-training 했다.</p><p>최근에는 sentence 또는 document encoder 들이 unlabeled text 에서 pre-training 후, supervision learning 을 위한 downstream task 에 fine-tuning 되어 사용되었다.</p><p>이러한 접근법의 장점은 parameters 를 scratch learning 할 필요가 없다는 점이다.</p><p>이러한 장점 덕분에, OpenAI GPT 는 GLUE benchmark 의 여러 sentence-level tasks 에서 SOTA 를 달성할 수 있었다.</p><p>이 model 들은 pre-training 을 위해 Left-to-right language modeling 및 auto-encoder objectives 를 사용했다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-transfer-learning-from-supervised-data">2.3 Transfer Learning from Supervised Data<a href="#23-transfer-learning-from-supervised-data" class="hash-link" aria-label="Direct link to 2.3 Transfer Learning from Supervised Data" title="Direct link to 2.3 Transfer Learning from Supervised Data">​</a></h2><p>또한, large-scale dataset 을 가진 supervision learning task 로부터의 효과적인 transfer learning 이 이루어졌다는 연구도 있다.</p><p>예로, natural language infernece 나 machine translation task 가 그렇다.</p><p>CV 연구에서도 large-scale pre-trained model 로부터의 transfer learning 의 중요성이 입증되었으며, 그 중에서 ImageNet pre-trained model 을 fine-tuning 하는 것이 효과적인 방법으로 알려져 있다.</p><h1>3. BERT</h1><p>이 섹션에선 BERT 와 그 구현 세부 사항을 소개한다.</p><p>저자의 framework 는 크게 two steps 로 구성: <em>pre-training</em> 및 <em>fine-tuning</em></p><ul><li>pre-training : model 이 different pre-training tasks 에 대해 unlabeled data 를 학습한다.</li><li>fine-tuning : BERT 가 먼저 pre-trained parameter 로 초기화된 후, downstream task 의 labeled data 를 사용하여 all parameters 를 fine-tuning 한다.</li></ul><p>각 downstream task 은 동일한 pre-trained parameters 로 초기화되지만, 각각 별도의 fine-tuned model 을 갖는다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-16-9929305cecb8c1ea3be6221a1244c9d9.png" width="1536" height="863" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="model-architecture">Model Architecture<a href="#model-architecture" class="hash-link" aria-label="Direct link to Model Architecture" title="Direct link to Model Architecture">​</a></h4><p>BERT 의 model architecture 는 multi-layer bidirectional Transformer encoder 로, `tensor2tensor`` library 에서 제공된다.</p><p>Transformer 의 사용이 일반화되었고, 저자의 구현도 원본과 거의 동일하기 때문에 model architecture 에 대한 상세한 설명은 생략하고, &quot;The Annotated Transformer&quot; 와 같은 훌륭한 가이드를 참고하도록 한다.</p><p>이 논문에서는 layer 수 (i.e., Transformer blocks)를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span></span>, hidden size 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span></span>, 그리고 self-attention head 수를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span> 로 표기한다.</p><p>저자는 주로 두 가지 model sizes 에 대한 결과를 보고하는데, 이는 BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>BASE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{BASE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">BASE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mn>12</mn></mrow><annotation encoding="application/x-tex">L=12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">12</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">H=768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">768</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mn>12</mn></mrow><annotation encoding="application/x-tex">A=12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">12</span></span></span></span></span>, Total parameters=110M) 와 BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>LARGE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{LARGE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">LARGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mn>24</mn></mrow><annotation encoding="application/x-tex">L=24</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">24</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">H=1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1024</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">A=16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">16</span></span></span></span></span>, Total parameters=340M) 이다.</p><p>BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>BASE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{BASE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">BASE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 OpenAI GPT 와의 비교를 위해 동일한 model size 로 선택되었다. 그러나 중요한 점은 BERT Transformer 가 bidirectional self-attention 을 사용하는 반면, GPT Transformer 는 각 token 이 left context 에만 집중할 수 있는 제한된 self-attention 을 사용한다는 점이다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="inputoutput-representations">Input/Output Representations<a href="#inputoutput-representations" class="hash-link" aria-label="Direct link to Input/Output Representations" title="Direct link to Input/Output Representations">​</a></h4><p>BERT 가 다양한 downstream task 을 처리할 수 있도록 하기 위해, 저자의 input representation 은 single sentence 와 sentence pair (e.g., <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo><mtext>Question, Answer</mtext><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&lt;\text{Question, Answer}&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord text"><span class="mord">Question, Answer</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span></span></span></span></span>) 를 하나의 token sequence 로 명확하게 표현할 수 있도록 설계되었다.</p><p>이 논문 전반에 걸쳐 &quot;sentence&quot; 은 actual linguistic sentence 가 아니라, continuous text 의 임의의 범위를 의미할 수 있다.</p><p>&quot;sequence&quot; 는 BERT 의 input token sequence 를 의미하며, 이는 single sentence 일 수도 있고, 두 개의 sentence 이 함께 포함된 것일 수도 있다.</p><hr><ul><li>저자는 30,000 token vocabulary 를 가진 WordPiece embedding 를 사용한다. </li><li>모든 sequence 의 first token 은 항상 special classification token (<!-- -->[<code>CLS</code>]<!-- -->) 이다.<ul><li>이 token 에 해당하는 final  hidden state 는 classification task 를 위한 aggregate sequence representation 으로 사용된다.</li></ul></li><li>sentence pairs 는 하나의 sequence 로 묶인다.<ul><li>저자는 두 가지 방법으로 sentences 를 구분한다.</li><li>(1) special token (<!-- -->[<code>SEP</code>]<!-- -->)으로 sentence 들을 분리하는 것이고,</li><li>(2) 각 token에 learned embedding 을 추가하여 해당 token 이 sentence <code>A</code> 에 속하는지, sentence <code>B</code> 에 속하는지를 나타내는 것이다.</li><li>Fig. 1 에서 볼 수 있듯이, input embedding 을 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span>, special <!-- -->[<code>CLS</code>]<!-- --> token 의 final hidden vector 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">C \in \mathbb{R}^H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span></span></span></span></span></span></span></span></span></span></span></span>, 그리고 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>-th input token 의 final hidden vector 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">T_i \in \mathbb{R}^H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span></span></span></span></span></span></span></span></span></span></span></span> 로 표기한다.</li></ul></li></ul><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-17-3f1b383ba8ee1a0f484eefc862035c6d.png" width="1667" height="570" class="img_ev3q"></p><p>주어진 token 의 input representation 은 해당 token embedding, segment embedding, position embedding 을 합산하여 구성된다.</p><p>이 구조에 대한 시각적 표현은 Fig. 2 에서 확인할 수 있다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-bert의-pre-training">3.1 BERT의 Pre-training<a href="#31-bert의-pre-training" class="hash-link" aria-label="Direct link to 3.1 BERT의 Pre-training" title="Direct link to 3.1 BERT의 Pre-training">​</a></h2><p>Peters et al. (2018a) 와 Radford et al. (2018) 와는 달리, 저자는 BERT 를 pre-training 할 때 전통적인 left-to-right 또는 right-to-left language modeling 을 사용하지 않는다.</p><p>대신, 이 섹션에서 설명하는 두 가지 unsupervised tasks 를 사용하여 BERT 를 pre-training 한다.</p><p>이 step 은 Fig. 1 의 왼쪽 부분에 표시되어 있다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="task-1-masked-lm">Task #1: Masked LM<a href="#task-1-masked-lm" class="hash-link" aria-label="Direct link to Task #1: Masked LM" title="Direct link to Task #1: Masked LM">​</a></h4><p>직관적으로, deep bidirectional model 이 left-to-right model 또는 left-to-right 와 right-to-left model 을 shallow concatenation 보다 더 강력하다고 믿는 것이 합리적이다.</p><p>불행히도, standard conditional language models 은 bidirectional conditioning 이 각 단어가 간접적으로 &#x27;자신을 볼 수&#x27; 있게 만들어 multi-layered context 에서 target word 를 쉽게 예측할 수 있기 때문에, left-to-right 또는 right-to-left 로만 학습될 수 있다.</p><p>deep bidirectional representation 을 학습하기 위해, 저자는 단순히 input token 의 일정 비율을 randomly masking 후, 이러한 masked token 을 예측한다.</p><p>저자는 이 절차를 &quot;masked LM&quot; (MLM)이라고 부르지만, 문헌에서는 종종 이를 Cloze task 라고 한다.</p><p>이 경우, mask token 에 해당하는 final hidden vector 는 standard LM 에서와 같이 vocabulary 에 대한 output softmax function 에 input된다.</p><p>모든 실험에서, 저자는 각 sequence 에서 all WordPiece token 의 15% 를 무작위로 masking 한다.</p><p>denoising auto-encoder 와는 달리, 저자는 entire input 을 재구성하는 것이 아니라 masked words 만 예측한다.</p><hr><p>이렇게 하면 bidirectional pre-training model 을 얻을 수 있지만, pre-training 과 fine-tuning 간에 불일치를 초래한다는 단점이 있다.</p><p>왜냐하면 <!-- -->[<code>MASK</code>]<!-- --> token 은 fine-tuning 동안 나타나지 않기 때문이다. 이를 완화하기 위해, 저자는 &#x27;masked&#x27; word 를 항상 실제 <!-- -->[<code>MASK</code>]<!-- --> token 으로 대체하지 않는다.</p><p>training data generator 는 예측을 위해 무작위로 token positions 의 15% 를 선택한다.</p><p>만약 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>-th token 이 선택되면, 저자는 다음과 같이 대체한다:</p><ol><li>80% 의 확률로 <!-- -->[<code>MASK</code>]<!-- --> token 으로 대체</li><li>10% 의 확률로 random token 으로 대체</li><li>10% 의 확률로 변경 없이 유지</li></ol><p>그런 다음, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">T_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 cross-entropy loss 로 original token 을 예측하는 데 사용된다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="task-2-next-sentence-prediction-nsp">Task #2: Next Sentence Prediction (NSP)<a href="#task-2-next-sentence-prediction-nsp" class="hash-link" aria-label="Direct link to Task #2: Next Sentence Prediction (NSP)" title="Direct link to Task #2: Next Sentence Prediction (NSP)">​</a></h4><p>Question Answering (QA)과 Natural Language Inference (NLI) 같은 중요한 downstream task 은 두 sentence 간의 관계를 이해하는 것에 기반을 두고 있으며, 이는 language modeling 으로는 직접적으로 캡처되지 않는다.</p><p>sentence relationships 를 이해하는 model 을 학습하기 위해, 저자는 monolingual corpus 에서 쉽게 생성할 수 있는 binarized <em>next sentence prediction</em> task 를 pre-training 한다.</p><p>구체적으로, </p><ul><li>각 pre-training example 에 대해 sentence <code>A</code> 와 <code>B</code> 를 선택할 때, 50% 의 확률로 <code>B</code> 는 <code>A</code> 다음에 실제로 따라오는 sentence 으로 선택되고(<code>IsNext</code>), </li><li>나머지 50% 의 경우 <code>B</code> 는 corpus 에서 무작위로 선택된 sentence 다(<code>NotNext</code>).</li><li>Fig. 1 에서 보여지듯이, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span></span> 는 next sentence prediction (NSP)에서 사용된다.</li><li>이 task 는 단순하지만, Sec. 5.1 에서 보여주듯이, 이 task 에 대한 pre-training 이 QA와 NLI 모두에 매우 유익하다는 것을 입증한다.</li><li>NSP task 는 Jernite et al. (2017)과 Logeswaran and Lee (2018) 에서 사용된 representation learning objectives 와 밀접한 관련이 있다.</li><li>그러나 이전 연구에서는 sentence embedding 만 downstream task 으로 전이되었지만, BERT 는 all parameters 를 전이하여 end-task model parameters 를 초기화한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="pre-training-data">Pre-training data<a href="#pre-training-data" class="hash-link" aria-label="Direct link to Pre-training data" title="Direct link to Pre-training data">​</a></h4><p>pre-training 절차는 language model pre-training 에 대한 기존 문헌을 주로 따른다.</p><p>pre-training corpus 로는 BooksCorpus (800M words) 와 English Wikipedia(2,500M words)를 사용한다.</p><p>Wikipedia 에서는 text 본문만 추출하고 목록, 표, 헤더는 무시한다.</p><p>long continuous sequence 를 추출하기 위해서는 shuffled sentence-level corpus (e.g., Billion Word Benchmark)보다는 document-level corpus 를 사용하는 것이 중요하다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-fine-tuning-bert">3.2 Fine-tuning BERT<a href="#32-fine-tuning-bert" class="hash-link" aria-label="Direct link to 3.2 Fine-tuning BERT" title="Direct link to 3.2 Fine-tuning BERT">​</a></h2><p>Fine-tuning 은 간단한데, Transformer 의 self-attention 메커니즘이 single text 또는 text pair 를 포함하는 많은 downstream task 을 적절한 input 과 output 을 교체함으로써 modeling 할 수 있게 해주기 때문이다.</p><p>text pair 을 포함하는 applications 의 경우, 일반적으로 Parikh et al. (2016) 및 Seo et al. (2017) 와 같이 text pair 을 independently encoding 후 bidirectional cross attention 을 적용하는 것이 일반적이다.</p><p>그러나 BERT 는 self-attention 메커니즘을 사용하여 이 두 단계를 통합하는데, self-attention 을 사용하여 연결된 text pair 을 encoding 하는 것은 두 sentence 간의 bidirectional cross attention 을 효과적으로 포함하기 때문이다.</p><p>각 task 에 대해, 저자는 단순히 task-specific inputs 와 outputs 을 BERT 에 연결하고 all parameters 를 end-to-end 로 fine-tuning 한다.</p><p>input 에서, pre-training 의 sentence <code>A</code> 와 sentence <code>B</code> 는 (1) paraphrasing 의 sentence pair, (2) entailment 의 hypothesis-premise pairs, (3) question answering 에서의 question-passage pair, (4) text classification 또는 sequence tagging 의 퇴화된(degenerate) text-∅ pair 에 해당한다.</p><p>output 에서, token representation 은 sequence tagging 또는 qeustion-answering 같은 token-level tasks 를 위해 output layer 에 전달되며, <!-- -->[<code>CLS</code>]<!-- --> representation 은 entailment 또는 sentiment analysis 같은 classification task 을 위해 output layer 에 전달된다.</p><p>pre-training 과 비교할 때, fine-tuning 은 상대적으로 비용이 적게 든다.</p><p>이 논문에 있는 모든 결과는 동일한 pre-trained model 을 시작으로 single Cloud TPU 에서 최대 1시간, 또는 GPU 에서 몇 시간 내에 복제할 수 있다.</p><h1>4. Experiments</h1><p>이 섹션에서는 11 NLP tasks 에 대한 BERT 의 fine-tuning 결과를 제시한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-glue">4.1 GLUE<a href="#41-glue" class="hash-link" aria-label="Direct link to 4.1 GLUE" title="Direct link to 4.1 GLUE">​</a></h2><p>General Language Understanding Evaluation (GLUE) benchmark 는 다양한 natural language task 들로 구성된 dataset 이다.</p><p>GLUE 에서 fine-tuning 하기 위해, Sec. 3 에서 설명한 대로 single sentence 또는 sentence pair 의 input sequence 를 표현하고, first input token (<!-- -->[<code>CLS</code>]<!-- -->)에 해당하는 final hidden vector <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">C \in \mathbb{R}^H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span></span></span></span></span></span></span></span></span></span></span></span> 를 aggregate representation 으로 사용한다.</p><p>Fine-tuning 동안 도입되는 유일한 new parameters 는 classification layer 의 weight <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>K</mi><mo>×</mo><mi>H</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W \in \mathbb{R}^{K \times H}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span></span></span></span></span></span></span></span></span></span></span></span></span> 이다.</p><p>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> 는 label 수를 나타낸다. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span></span> 를 사용해 standard classification loss 를 계산하는데, 이는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>log</mtext><mo stretchy="false">(</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>C</mi><msup><mi>W</mi><mi>T</mi></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{log}(\text{softmax}(C W^T))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">log</span></span><span class="mopen">(</span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span> 와 같다.</p><p>all GLUE tasks 에 대해</p><ul><li>batch size 32</li><li>data 를 3 epochs 에 걸쳐 fine-tuning</li><li>각 task 에 대해 Dev set 에서 best fine-tuning learning rate(5e-5, 4e-5, 3e-5, 2e-5 중)을 선택했다.</li><li>추가적으로, BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>LARGE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{LARGE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">LARGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 의 경우 small dataset 에서 fine-tuning 이 때때로 불안정하다는 것을 발견했기 때문에, 여러 번의 무작위 재시작을 수행하고 Dev set 에서 best model 을 선택했다.<ul><li>무작위 재시작 시, 동일한 pre-trained checkpoint 를 사용하지만, 다른 fine-tuning data shuffling 과 classification layer initialization 을 수행한다.</li></ul></li></ul><p>결과는 Tab. 1 에 제시되어 있다.</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-18-610769a3997422affcbd490e65a8380d.png" width="1966" height="724" class="img_ev3q"></p><ul><li>BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>BASE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{BASE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">BASE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 와 BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>LARGE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{LARGE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">LARGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 all tasks 에서 모든 시스템을 상당한 차이로 능가하며, 이전 SOTA 대비 각각 4.5% 와 7.0% 의 평균 정확도 향상을 달성했다.</li><li>MNLI 와 같은 가장 크고 널리 보고된 GLUE task 에서 BERT 는 절대적으로 4.6%의 정확도 향상을 달성했다.</li><li>공식 GLUE 리더보드에서 BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>LARGE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{LARGE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">LARGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 80.5점을 기록한 반면, OpenAI GPT 는 작성 당시 72.8점을 기록했다.</li><li>BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>LARGE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{LARGE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">LARGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 all tasks 에서 BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>BASE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{BASE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">BASE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 보다 상당히 더 나은 성능을 보이며, 특히 training data 가 매우 적은 task 에서 더 큰 차이를 보였다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-squad-v11">4.2 SQuAD v1.1<a href="#42-squad-v11" class="hash-link" aria-label="Direct link to 4.2 SQuAD v1.1" title="Direct link to 4.2 SQuAD v1.1">​</a></h2><p>Stanford Question Answering Dataset (SQuAD v1.1)은 100k crowd-sourced question/answer pair 으로 구성된 dataset 이다.</p><p>주어진 qeustion 과 answer 가 포함된 Wikipedia 의 문단을 기반으로 answer text span 을 문단 내에서 예측하는 것이 이 task 의 objective 이다.</p><p>Fig. 1 에서 볼 수 있듯이, question answering task 에서 input question 과 passage 을 single packed sequence 로 표현하며, question 에는 <code>A</code> embedding 을, passage 에는 <code>B</code> embedding 을 사용한다.</p><p>Fine-tuning 동안에는 start vector <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">S \in \mathbb{R}^H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span></span></span></span></span></span></span></span></span></span></span></span> 와 end vector <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">E \in \mathbb{R}^H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span></span></span></span></span></span></span></span></span></span></span></span> 만을 새로 도입한다.</p><p>word <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span> 가 answer span 의 start 일 확률은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">T_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span></span></span></span></span> 사이의 dot product 를 구한 후 paragraph 내 all words 에 대해 softmax 를 취해 계산된다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>=</mo><mfrac><msup><mi>e</mi><mrow><mi>S</mi><mo>⋅</mo><msub><mi>T</mi><mi>i</mi></msub></mrow></msup><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><mrow><mi>S</mi><mo>⋅</mo><msub><mi>T</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">P_i = \frac{e^{S \cdot T_i}}{\sum_j e^{S \cdot T_j}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.8413em;vertical-align:-0.8039em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0374em"><span style="top:-2.5183em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1496em"><span style="top:-2.1786em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4603em"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em"></span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9595em"><span style="top:-2.9714em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">S</span><span class="mbin mtight">⋅</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.1389em;margin-right:0.1em"><span class="pstrut" style="height:2.6595em"></span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5092em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9191em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">S</span><span class="mbin mtight">⋅</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3448em;margin-left:-0.1389em;margin-right:0.1em"><span class="pstrut" style="height:2.6595em"></span><span class="mord mathnormal mtight">i</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3147em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8039em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>. end answer span 의 확률은 유사한 방식으로 계산된다. </p><p>position <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span> 에서 position <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span> 까지의 candidate span 의 score 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>⋅</mo><msub><mi>T</mi><mi>i</mi></msub><mo>+</mo><mi>E</mi><mo>⋅</mo><msub><mi>T</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">S \cdot T_i + E \cdot T_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 로 정의되며, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>≥</mo><mi>i</mi></mrow><annotation encoding="application/x-tex">j \geq i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span> 일 때 maximum scoring 을 가진 span 이 prediction 으로 사용된다.</p><p>learning objective 는 correct start 및 end 의 log-likelihood 의 합으로 설정된다.</p><p>저자는 learning rate 5e-5 와 batch size 32 로 3 epochs 동안 fine-tuning 했다.</p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-19-bb019642f2725ac0e1cd6276d6b4d080.png" width="952" height="979" class="img_ev3q"></p><p>Tab. 2 는 SQuAD 리더보드 상위 항목 및 상위 출판 시스템들의 결과를 보여준다.</p><p>SQuAD 리더보드의 상위 결과는 최신 공개 시스템 설명을 포함하지 않으며, 시스템을 훈련할 때 공개된 데이터를 사용할 수 있다.</p><p>따라서 저자는 SQuAD 에서 fine-tuning 하기 전에 TriviaQA 에서 먼저 fine-tuning 을 수행함으로써 시스템에서 적당한 data augmentation 을 사용했다.</p><ul><li>저자의 최상의 성능을 보이는 시스템은 ensemble 에서 F1 을 +1.5, single 시스템에서 F1 을 +1.3 만큼 상위 리더보드 시스템보다 능가했다.</li><li>사실, 저자의 single BERT model 은 F1 score 면에서 상위 ensemble 시스템보다 더 나은 성능을 보였다.</li><li>TriviaQA fine-tuning 데이터를 사용하지 않은 경우, F1 score 가 0.1-0.4 만큼 감소했지만 여전히 모든 기존 시스템을 큰 차이로 능가했다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-squad-v20">4.3 SQuAD v2.0<a href="#43-squad-v20" class="hash-link" aria-label="Direct link to 4.3 SQuAD v2.0" title="Direct link to 4.3 SQuAD v2.0">​</a></h2><p>SQuAD 2.0 task 는 제공된 문단에 short answer 이 존재하지 않을 가능성을 허용함으로써 SQuAD 1.1 문제 정의를 확장하여 문제를 보다 현실적으로 만든다.</p><p>이 task 에 대해 SQuAD v1.1 BERT model 을 확장하기 위해 간단한 approach 를 사용한다.</p><p>asnwer 가 없는 question 은 <!-- -->[<code>CLS</code>]<!-- --> token 에서 start 와 end 를 갖는 answer span 을 가진 것으로 간주한다.</p><p>answer span 의 start 와 end position 에 대한 probability space 는 <!-- -->[<code>CLS</code>]<!-- --> token 의 position 을 포함하도록 확장된다.</p><p>prediction 시, no-answer span 의 score <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mrow><mi>n</mi><mi>u</mi><mi>l</mi><mi>l</mi></mrow></msub><mo>=</mo><mi>S</mi><mo>⋅</mo><mi>C</mi><mo>+</mo><mi>E</mi><mo>⋅</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">s_{null} = S \cdot C + E \cdot C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">ll</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span></span> 와 best non-null span 의 score <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>s</mi><mo>^</mo></mover><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><msub><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>j</mi><mo>≥</mo><mi>i</mi></mrow></msub><mo stretchy="false">(</mo><mi>S</mi><mo>⋅</mo><msub><mi>T</mi><mi>i</mi></msub><mo>+</mo><mi>E</mi><mo>⋅</mo><msub><mi>T</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{s}_{i,j} = \max_{j \geq i} (S \cdot T_i + E \cdot T_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">s</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span><span class="mrel mtight">≥</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 를 비교한다.</p><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>s</mi><mo>^</mo></mover><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>&gt;</mo><msub><mi>s</mi><mrow><mi>n</mi><mi>u</mi><mi>l</mi><mi>l</mi></mrow></msub><mo>+</mo><mi>τ</mi></mrow><annotation encoding="application/x-tex">\hat{s}_{i,j} &gt; s_{null} + \tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">s</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">ll</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span></span></span></span></span> 일 때, non-null answer 을 예측하며, 이때의 threshold <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span></span></span></span></span> 는 Dev set 에서 F1 을 maximizing 하기 위해 선택한다.</p><p>이 model 에는 TriviaQA data 를 사용하지 않았으며, learning rate 5e-5 와 batch size 48 로 2 epochs 동안 fine-tuning 했다.</p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-20-9a3795c6432a11ce01f9807aaef69444.png" width="944" height="759" class="img_ev3q"></p><p>Tab. 3 은 이전의 리더보드 항목들과 상위 출판된 연구들과의 비교 결과를 보여준다.</p><ul><li>BERT 를 구성 요소로 사용하는 시스템들은 제외하였으며, 이전 최고 시스템보다 F1 score 에서 +5.1 의 개선을 확인할 수 있었다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="44-swag">4.4 SWAG<a href="#44-swag" class="hash-link" aria-label="Direct link to 4.4 SWAG" title="Direct link to 4.4 SWAG">​</a></h2><p>Situations With Adversarial Generations (SWAG) dataset 은 113k sentence pair completion example 을 포함하며, 현실적 상식을 평가하는 task 이다. </p><p>주어진 sentence 을 바탕으로 4 choices 중 가장 그럴듯한 후속 sentence 을 선택하는 것이 이 작업의 목표이다.</p><p>SWAG dataset 에서 fine-tuning 할 때, 주어진 sentence(sentence <code>A</code>)와 possible continuation sentence(sentence <code>B</code>)의 concatenation 을 포함하는 4 input sequence 를 구성한다.</p><p>도입되는 task-specific parameters 는 <!-- -->[<code>CLS</code>]<!-- --> token representation <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span></span> 와의 dot product 를 통해 각 choices 의 score 를 나타내는 vector 이며, 이는 softmax layer 로 normalizing 된다.</p><p>model 은 learning rate 2e-5 와 batch size 16 으로 3 epochs 동안 fine-tuning 되었다.</p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-21-eef5a47d885d6e79a34b1cb0d1dcb60d.png" width="962" height="681" class="img_ev3q"></p><p>결과는 Tab. 4 에 제시되어 있다.</p><ul><li>BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>LARGE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{LARGE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">LARGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 저자들이 제시한 기준 ESIM+ELMo 시스템보다 27.1%, OpenAI GPT 보다 8.3% 높은 성능을 보여주었다.</li></ul><h1>5. Ablation Studies</h1><p>이 섹션에서는 BERT 의 다양한 측면의 상대적 중요성을 더 잘 이해하기 위해 ablation study 를 수행한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-effect-of-pre-training-tasks">5.1 Effect of Pre-training Tasks<a href="#51-effect-of-pre-training-tasks" class="hash-link" aria-label="Direct link to 5.1 Effect of Pre-training Tasks" title="Direct link to 5.1 Effect of Pre-training Tasks">​</a></h2><p>BERT 의 deep bidirectionality 의 중요성을 two pre-training objectives 를 평가하여 입증하는데, 동일한 pre-training data, fine-tuning scheme, hyper-parameters 를 BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>BASE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{BASE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">BASE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 와 동일하게 사용한다:</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="no-nsp">No NSP<a href="#no-nsp" class="hash-link" aria-label="Direct link to No NSP" title="Direct link to No NSP">​</a></h4><p>&quot;next sentence prediction&quot; (NSP) task 없이 &quot;masked LM&quot; (MLM) 을 사용해 훈련된 bidirectional model 이다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ltr--no-nsp">LTR &amp; No NSP<a href="#ltr--no-nsp" class="hash-link" aria-label="Direct link to LTR &amp; No NSP" title="Direct link to LTR &amp; No NSP">​</a></h4><p>MLM 대신 standard Left-to-Right (LTR) LM 을 사용해 훈련된 left-context-only model 이다.</p><p>이를 제거하면 downstream 성능이 저하되는 pre-train/fine-tune mismatch 가 발생하므로 fine-tuning 시에도 left-only constraint 를 적용한다. 추가로, 이 model 은 NLP tasks 도 없이 pre-training 된다.</p><p>이는 OpenAI GPT 와 직접적으로 비교 가능하지만, 저자는 larger training dataset, input representation, 그리고 fine-tuning 방식을 사용했다.</p><p>우선 NSP task 가 가져오는 영향을 살펴본다.</p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-22-d4677fb41351fc15f9ae3efa4e175e9a.png" width="965" height="780" class="img_ev3q"></p><ul><li>Tab. 5 에서 NSP 를 제거하면 QNLI, MNLI, 그리고 SQuAD 1.1 에서 성능이 크게 저하되는 것을 보여준다.</li><li>다음으로, &quot;No NSP&quot; 와 &quot;LTR &amp; No NSP&quot; 를 비교하여 bidirectional representation training 의 영향을 평가한다.<ul><li>LTR model 은 all tasks 에서 MLM model 보다 성능이 저조하며, 특히 MRPC와 SQuAD 에서 큰 성능 저하를 보인다.</li></ul></li><li>SQuAD의 경우, LTR model 이 token prediction 에서 성능이 저조할 것임은 직관적으로 명백하다. 왜냐하면 token-level hidden state 는 right-side context 가 없기 때문이다.<ul><li>LTR 시스템을 강화하려는 선의의 시도로, 상단에 무작위로 초기화된 BiLSTM 을 추가했으며, 이로 인해 SQuAD 성능이 상당히 개선되었지만, 여전히 pre-trained bidirectional model 보다 훨씬 낮은 성능을 보였다. </li><li>BiLSTM 은 GLUE task 에서 성능을 저하시켰다.</li></ul></li><li>LTR 및 RTL model 을 별도로 훈련시키고 각 token 을 두 model 의 concatenation 으로 표현하는 것이 가능하지만, 다음과 같은 이유로 비효율적이다:<ul><li>(a) single bidirectional model 보다 두 배의 비용이 든다; </li><li>(b) QA 같은 task 에는 직관적이지 않다, 왜냐하면 RTL model 은 question 에 따라 answer 을 conditioning 할 수 없기 때문이다; </li><li>(c) 이는 every layer 에서 left 및 right context 를 모두 사용할 수 있는 deep bidirectional model 보다 엄격히 덜 강력하다.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-effect-of-model-size">5.2 Effect of Model Size<a href="#52-effect-of-model-size" class="hash-link" aria-label="Direct link to 5.2 Effect of Model Size" title="Direct link to 5.2 Effect of Model Size">​</a></h2><p>이 섹션에서는 fine-tuning task accuracy 에 대한 model size 의 영향을 탐구한다.</p><p>저자는 layer 수, hidden unit 수, attention head 수가 다른 여러 BERT model 을 훈련시켰으며, 이외의 hyper-parameters 와 training procedure 는 이전에 설명한 것과 동일하게 사용했다.</p><p>선택된 GLUE task 에 대한 결과는 Tab. 6 에 제시되어 있다.</p><p><img loading="lazy" alt="Table 6" src="/assets/images/image-23-5bcf0024f1922c41a98038a57ce88e4e.png" width="965" height="699" class="img_ev3q"></p><ul><li>이 표에서는 fine-tuning 의 5 무작위 재시작에서 얻은 평균 Dev set accuracy 를 보고한다.</li><li>larger model 이 MRPC 같은 pre-training task 와 상당히 다른 작업에서도 모든 4 dataset 에서 엄격한 정확도 향상을 가져오는 것을 확인할 수 있다.</li><li>또한, 저자는 이미 기존 문헌에 비해 상당히 큰 model 에 대해 이러한 중요한 개선을 달성할 수 있었다는 점에서 놀라운 일이다.<ul><li>예로, Vaswani et al. (2017) 에서 탐구된 largest Transformer는 encoder 의 경우 100M parameters 를 가지며, (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">L=6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">6</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">H=1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1024</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">A=16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">16</span></span></span></span></span>) structure 를 가지며,</li><li>문헌에서 발견된 largest Transformer 는 (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">L=64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">64</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">H=512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">512</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">A=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2</span></span></span></span></span>)로 235M parameters 를 가진다.</li><li>이에 비해 BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>BASE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{BASE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">BASE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 110M parameters 를 가지고, BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>LARGE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{LARGE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">LARGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 340M parameters 를 가진다.</li><li>model size 를 증가시키면 machine translation 과 language modeling 같은 large-scale task 에서 지속적인 개선을 가져올 것이라는 것은 오래전부터 알려진 사실이다.</li><li>이는 Tab. 6 에서 보여지는 유지된 training data 의 language model perplexity 로 입증된다.</li><li>그러나 저자는 극단적인 model size 로 확장하는 것이 매우 작은 규모의 task 에서도 큰 개선을 가져온다는 것을 설득력 있게 입증한 최초의 연구라고 믿는다.</li></ul></li><li>Peters et al. (2018b)은 pre-trained bi-LM 크기를 두 개에서 네 개의 layer 로 늘리는 것이 downstream task 에 미치는 영향을 혼합된 결과로 제시했으며, Melamud et al. (2016) 은 hidden dimension 크기를 200 에서 600 으로 늘리는 것이 도움이 되지만, 1,000 으로 더 늘려도 추가적인 개선은 없다고 언급했다.<ul><li>이 두 가지 이전 연구 모두 feature-based 접근 방식을 사용했으며, 저자는 model 이 downstream task 에서 직접 fine-tuning 되고 소수의 무작위 초기화된 additional parameters 만 사용하는 경우, downstream task data 가 매우 적더라도 더 크고 표현력 있는 pre-trained representation 으로부터 이득을 얻을 수 있다고 가정한다.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="53-feature-based-approach-with-bert">5.3 Feature-based Approach with BERT<a href="#53-feature-based-approach-with-bert" class="hash-link" aria-label="Direct link to 5.3 Feature-based Approach with BERT" title="Direct link to 5.3 Feature-based Approach with BERT">​</a></h2><p>지금까지 제시된 모든 BERT 결과는 pre-trained model 에 간단한 classification layer 를 추가하고, all parameters 를 downstream task 에 맞춰 함께 fine-tuning 하는 방식을 사용했다.</p><p>그러나, 고정된 특징을 pre-trained model 에서 추출하는 feature-based approach 에는 몇 가지 장점이 있다.</p><ol><li>all tasks 가 Transformer encoder 아키텍처로 쉽게 표현될 수 있는 것은 아니며, 따라서 task-specific model architecture 를 추가해야 할 수 있다.</li><li>고비용의 representation 을 한 번 미리 계산하고 나서 이 representation 위에 저비용의 model 들을 여러 번 실험하는 것은 큰 계산 이점을 제공한다.</li></ol><p>이 섹션에서는 CoNLL-2003 Named Entity Recognition (NER) task 에 BERT를 적용하여 두 접근 방식을 비교한다.</p><p>BERT input 에는 case-preserving WordPiece model 을 사용하고, data 에서 제공하는 minimal document context 를 포함시킨다.</p><p>표준 관행에 따라 이 task 을 tagging task 로 공식화하지만, output 에 CRF layer 를 사용하지 않는다.</p><p>first sub-token 의 representation 을 NER label set 에 대한 token-level classifier 에 대한 input 으로 사용한다.</p><p>fine-tuning 접근 방식을 소거하기 위해, BERT 의 어떤 parameter 도 fine-tuning 하지 않고 한 layer 또는 여러 layers 에서 activation 을 추출하는 feature-based approach 를 적용한다. </p><p>이러한 contextual embedding 은 classification layer 이전에 무작위로 초기화된 768-dimensional BiLSTM 에 대한 input 으로 사용된다.</p><p><img loading="lazy" alt="Table 7" src="/assets/images/image-24-0a55d7b721873ae6135a9cc6900c2258.png" width="967" height="950" class="img_ev3q"></p><p>결과는 Tab. 7 에 제시되어 있다.</p><ul><li><strong>BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>LARGE</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{LARGE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">LARGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span></strong> 는 SOTA 와 비교해 경쟁력 있는 성능을 보인다.</li><li>가장 성능이 좋은 방법은 pre-trained Transformer 의 top-4 hidden layer 에서 token representation 을 연결하는 방식이며, 이는 model 전체를 fine-tuning 하는 것보다 F1 score 에서 단지 0.3점 뒤처진다.</li><li>이는 BERT 가 fine-tuning 및 feature-based approach 모두에서 효과적임을 보여준다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="6-결론">6 결론<a href="#6-결론" class="hash-link" aria-label="Direct link to 6 결론" title="Direct link to 6 결론">​</a></h3><p>language model 을 사용한 transfer learning 으로 인한 최근의 실증적 개선은 풍부한 unsupervised pre-training 이 많은 language understanding systems 의 필수적인 부분임을 입증했다.</p><p>특히, 이러한 결과는 low-resource tasks 조차 deep unidirectional architecture 의 혜택을 받을 수 있게 한다.</p><p>저자의 주요 기여는 이러한 발견을 deep bidirectional architecture 로 더욱 일반화하여, 동일한 pre-trained model 이 광범위한 NLP tasks 를 성공적으로 처리할 수 있도록 하는 것이다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/transformer">transformer</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/encoder-only">encoder-only</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/masked-language-model">masked language model</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/mlm">MLM</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/auto-encoding-model">auto-encoding model</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/NLP/Model/2018-10-BERT.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/NLP/Model/GPT-1"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Improving Language Understanding by Generative Pre-Training</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/NLP/Multi-Task/Flan-T5"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Scaling Instruction-Finetuned Language Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-unsupervised-feature-based-approaches" class="table-of-contents__link toc-highlight">2.1 Unsupervised Feature-based Approaches</a></li><li><a href="#22-unsupervised-fine-tuning-approaches" class="table-of-contents__link toc-highlight">2.2 Unsupervised Fine-tuning Approaches</a></li><li><a href="#23-transfer-learning-from-supervised-data" class="table-of-contents__link toc-highlight">2.3 Transfer Learning from Supervised Data</a></li><li><a href="#31-bert의-pre-training" class="table-of-contents__link toc-highlight">3.1 BERT의 Pre-training</a></li><li><a href="#32-fine-tuning-bert" class="table-of-contents__link toc-highlight">3.2 Fine-tuning BERT</a></li><li><a href="#41-glue" class="table-of-contents__link toc-highlight">4.1 GLUE</a></li><li><a href="#42-squad-v11" class="table-of-contents__link toc-highlight">4.2 SQuAD v1.1</a></li><li><a href="#43-squad-v20" class="table-of-contents__link toc-highlight">4.3 SQuAD v2.0</a></li><li><a href="#44-swag" class="table-of-contents__link toc-highlight">4.4 SWAG</a></li><li><a href="#51-effect-of-pre-training-tasks" class="table-of-contents__link toc-highlight">5.1 Effect of Pre-training Tasks</a></li><li><a href="#52-effect-of-model-size" class="table-of-contents__link toc-highlight">5.2 Effect of Model Size</a></li><li><a href="#53-feature-based-approach-with-bert" class="table-of-contents__link toc-highlight">5.3 Feature-based Approach with BERT</a><ul><li><a href="#6-결론" class="table-of-contents__link toc-highlight">6 결론</a></li></ul></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.ec4b5b0a.js"></script>
<script src="/assets/js/main.d9f0a85a.js"></script>
</body>
</html>