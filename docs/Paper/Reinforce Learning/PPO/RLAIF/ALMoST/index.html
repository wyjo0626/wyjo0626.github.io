<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Reinforce Learning/PPO/RLAIF/2023-05-ALMoST">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Aligning Large Language Models through Synthetic Feedback | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Reinforce Learning/PPO/RLAIF/ALMoST"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Aligning Large Language Models through Synthetic Feedback | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Reinforce Learning/PPO/RLAIF/ALMoST"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Reinforce Learning/PPO/RLAIF/ALMoST" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Reinforce Learning/PPO/RLAIF/ALMoST" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.965e64d4.js" as="script">
<link rel="preload" href="/assets/js/main.a90b69a7.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Reinforce Learning/DPO/RLHF/Self-RLM">Reinforce Learning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Reinforce Learning/DPO/RLHF/Self-RLM">DPO</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/Constitutional AI">PPO</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/Constitutional AI">RLAIF</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/Constitutional AI">Constitutional AI: Harmlessness from AI Feedback</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/ALMoST">Aligning Large Language Models through Synthetic Feedback</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/d-RLAIF">RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/UltraFeedback">UltraFeedback: Boosting Language Models with Scaled AI Feedback</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/SER">Self-Evolved Reward Learning for LLMs</a></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Reinforce Learning</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PPO</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">RLAIF</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Aligning Large Language Models through Synthetic Feedback</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Aligning Large Language Models through Synthetic Feedback</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2305.13735" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2305.13735</a></p><h1>Abstract</h1><p>large language model (LLM) 을 human values 에 맞게 alignment 하는 것은 LLM 의 정교한 steering 을 가능하게 하므로 점점 더 중요해지고 있다. 그러나 이는 상당한 양의 human demonstration 과 feedback, 혹은 ChatGPT 와 같은 proprietary LLM 으로부터의 distillation 을 필요로 한다.</p><p>이 연구에서는 방대한 human annotation 이나 proprietary LLM 에 의존하지 않는 <strong>synthetic feedback</strong> 기반의 새로운 alignment learning framework 를 제안한다. </p><ol><li>먼저, 다양한 크기와 prompt 를 가진 vanilla LLM 들의 response 를 비교하여 synthetic feedback 을 사용한 reward modeling (RM) 을 수행한다. </li><li>이후 RM 을 이용해 high-quality demonstration 을 시뮬레이션하고, 이를 통해 supervised policy 를 학습하며 reinforcement learning 으로 model 을 추가적으로 최적화한다.</li></ol><p>이 과정을 통해 얻은 model 인 <strong>Aligned Language Model with Synthetic Training dataset (ALMoST)</strong> 는 InstructGPT output 또는 human-annotated demonstration 으로 학습된 최근의 open-source model 들보다 alignment benchmark 에서 더 우수한 성능을 보인다. human evaluation 에서 저자의 model 은 Alpaca 와 Dolly-v2 대비 각각 55.0% 및 58.5% 의 비율로 선호되었다. 추가적인 분석 결과는 저자의 framework 내에서 synthetic feedback 의 효율성과 중요성을 입증한다.</p><h1>1 Introduction</h1><p>alignment learning 은 large language model (LLM) 의 행동을 safety 와 truthfulness 와 같은 human value 에 맞추면서도 사용자의 의도를 정확히 따르게 하는 데 필수적인 학습 방식이다. 아직 alignment 되지 않은 vanilla LLM 은 사용자 의도를 오해하거나 안전하지 않고 부정확한 response 를 생성할 수 있다. helpfulness, harmlessness, honesty 등의 바람직한 human value 가 정의될 수 있으며, 이러한 value 를 반영한 human demonstration 이 alignment learning 에 사용된다.</p><p>일반적으로 alignment learning 은 세 단계로 구성된다:</p><ol><li>supervised fine-tuning (SFT),</li><li>reward modeling (RM),</li><li>reinforcement learning from human feedback (RLHF)</li></ol><p>그러나 이 세 단계의 학습 과정은 특히 처음 두 단계에서 상당한 human effort 를 요구한다. 구체적으로, SFT 와 RM 단계는 RLHF 를 위한 model 을 얻기 위해 대량의 high-quality human demonstration 과 ranking dataset 이 필요하다. 예를 들어, Ouyang et al. 은 13k 개의 human demonstration 과 33k 개의 comparison 을 준비하여 사용하였다.</p><p>한편, Self-Instruct 는 few seed demonstration 과 in-context learning 을 이용하여 synthetic self-generated instruction dataset 을 생성하려고 시도하였다. LLaMA 의 공개 이후에는 proprietary LLM 의 output 이나 human-annotated instruction 에 기반하여 학습된 많은 open-source aligned LLM 들이 등장하였다. 그러나 이러한 시도들은 여전히 InstructGPT, ChatGPT 등의 proprietary LLM API 또는 large-scale human annotation 에 강하게 의존한다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-12-06da30eaa7a204f34cb3341e7817b569.png" width="1526" height="2230" class="img_ev3q"></p><p>이에 본 연구에서는 proprietary LLM 에 의존하지 않으며 human labor 가 최소화된 alignment learning framework 를 제안한다. </p><ul><li>기존의 alignment learning 절차가 demonstration 수집부터 시작하는 것과 달리, 저자는 먼저 다양한 설정의 vanilla LLM output 을 비교하여 구성한 synthetic comparison dataset 으로 reward model (RM) 을 개발한다 (Fig. 1 참고). </li><li>이러한 synthetic ranking data 생성 규칙은 “더 큰 규모의 model 이나 더 최적화된 prompt 로부터 생성된 response 가 더 작은 규모의 model 이나 부적절한 prompt 로부터 생성된 response 보다 우수하다”는 가설에 기반한다.</li><li>이후, RM 을 활용한 <strong>Reward-Model-guided Self-Play (RMSP)</strong> 방법을 도입하여 rejection sampling 을 통해 high-quality demonstration 을 시뮬레이션한다. </li><li>이렇게 얻은 synthetic demonstration 으로 LLaMA-7B 를 학습(SFT)하고, synthetic RM 으로부터의 reward 를 이용하여 model 을 추가적으로 최적화하는 Reinforcement Learning from Synthetic Feedback (RLSF) 을 수행한다.</li></ul><p>결과적으로, Aligned Language Model with Synthetic Training dataset (ALMoST) 는 InstructGPT 로부터 distillation 된 Alpaca, human-annotated demonstration 으로 학습된 Dolly-v2 및 OpenAssistant 보다 alignment 관련 benchmark 에서 더 우수한 성능을 보인다. 특히, proprietary LLM 으로부터의 distillation 이나 intensive human annotation 없이도 human evaluation 에서 Alpaca 와 Dolly-v2 대비 각각 55–58% 의 winning rate 를 기록하였다.</p><p>저자는 이러한 강력한 성능이 synthetic feedback 을 통해 well-aligned behavior 의 경험적 지표가 효과적으로 strong backbone model 에 통합되어, model 이 self-align 능력을 내재적으로 발휘하며 human feedback 의 필요성을 부분적으로 대체할 수 있었기 때문이라고 추측한다.</p><p>저자의 주요 기여점은 다음과 같다.</p><ul><li>synthetic feedback 을 도입한 새로운 alignment learning framework 를 제안하였다. 이 framework 는 human feedback 과 proprietary LLM 없이도 high-quality comparison 및 demonstration 을 자동으로 구성한다.</li><li>제안된 model 인 ALMoST 는 alignment benchmark 에서 human value 와 잘 맞는 behavior 를 보이며, human study 에서 Alpaca 및 Dolly-v2 대비 55–58% 의 winning rate 를 달성하였다.</li><li>RM 에 대한 분석을 통해 synthetic feedback 의 효율성을 입증하였으며, 제안된 filtering 방법 및 faithful prompt design 과 같은 경험적 prior 의 중요성을 강조하였다.</li></ul><h1>2 Method</h1><p>이 절에서는 Fig. 2 에 나타낸 바와 같이 저자의 framework 절차를 상세히 설명한다.</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-13-6abb129a7a44e7dab979dc30b552edb0.png" width="3847" height="2423" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-step-1-reward-modeling-with-synthetic-feedback">2.1 Step 1: Reward Modeling with Synthetic Feedback<a href="#21-step-1-reward-modeling-with-synthetic-feedback" class="hash-link" aria-label="Direct link to 2.1 Step 1: Reward Modeling with Synthetic Feedback" title="Direct link to 2.1 Step 1: Reward Modeling with Synthetic Feedback">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="prompted-baseline">Prompted Baseline<a href="#prompted-baseline" class="hash-link" aria-label="Direct link to Prompted Baseline" title="Direct link to Prompted Baseline">​</a></h4><p>아직 비교를 위한 aligned baseline 이 존재하지 않기 때문에, 저자는 Askell et al. 이 고안한 HHH (Helpful, Harmless, Honest) prompt 를 활용한다. 이는 LLM alignment 를 유도하기 위해 작성된 14 human-written conversations 으로 구성된다. 저자는 HHH prompt 를 적용한 LLaMA model 들을 사용하여 synthetic comparison 을 생성한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="generating-synthetic-comparison">Generating Synthetic Comparison<a href="#generating-synthetic-comparison" class="hash-link" aria-label="Direct link to Generating Synthetic Comparison" title="Direct link to Generating Synthetic Comparison">​</a></h4><p>human feedback 을 수집하는 대신, 저자는 경험적 관찰에 기반한 단순한 가정을 바탕으로 synthetic comparison 을 생성한다. Askell et al. 은 larger model 이 smaller model 보다, 그리고 longer prompt 가 shorter prompt 보다 human preference 측면에서 더 우수한 성능을 보인다고 보고하였다.</p><p>즉, 저자는 다음의 경험적 규칙(rule of thumb)에 따라 response 의 품질이 결정된다고 가정한다.</p><ul><li>Larger model &gt; Smaller model</li><li>More few-shots &gt; Less few-shots</li><li>Better demonstration &gt; Worse demonstration</li></ul><p>동일한 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 에 대해, 다양한 설정의 model 로부터 responses <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi mathvariant="normal">∣</mi><mi>Y</mi><mi mathvariant="normal">∣</mi></mrow></msub></mrow></mrow><annotation encoding="application/x-tex">Y = {y_1, y_2, ..., y_{|Y|}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7858em;vertical-align:-0.3552em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.22222em">Y</span><span class="mord mtight">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span></span></span></span></span></span> 를 sampling 한다. 이후 위 규칙에 따라 better response 를 선택한다. 구체적으로, {7, 13, 30}B 규모의 LLM 과 {1, 3, 5} shot 의 HHH demonstration 을 사용하여 비교를 수행한다.</p><p>예를 들어 Fig. 1 과 같이</p><p>(1) 30B / 5-shot,<br>
<!-- -->(2) 13B / 3-shot,<br>
<!-- -->(3) 7B / 1-shot  </p><p>model 로부터 response 를 sampling 하면, 경험적 규칙에 따라 ranking 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>&gt;</mo><msub><mi>y</mi><mn>2</mn></msub><mo>&gt;</mo><msub><mi>y</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">y_1 &gt; y_2 &gt; y_3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 가 된다. 이에 따라 binary comparison set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>3</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>3</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{(y_1, y_2), (y_2, y_3), (y_1, y_3)\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)}</span></span></span></span></span> 를 얻는다. 각 comparison pair <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(y_1, y_2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 에서 전자를 ‘chosen response’ (e.g., <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">y_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>), 후자를 ‘rejected response’ (e.g., <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">y_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>) 로 정의한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="post-validation">Post Validation<a href="#post-validation" class="hash-link" aria-label="Direct link to Post Validation" title="Direct link to Post Validation">​</a></h4><p>prompt-based generation 의 확률적 특성 때문에 이러한 가정이 항상 옳지는 않다. dataset 내의 이러한 noise 는 reward modeling 의 safety 를 해치고 학습을 발산시킬 수 있다. 따라서 저자는 이러한 noise 를 제거하기 위한 <strong>post validation method</strong> 를 제안한다.</p><p>먼저, <strong>Heuristic Filter (HF)</strong> 를 prior knowledge 에 기반하여 설계한다. 이 필터는 “I don’t know”, “well” 등의 keyword 를 포함하거나 해당 단어로 시작하는 부적절한 response 를 제거한다. 또한 경험적으로 더 나은 response 가 일반적으로 더 긴 경향이 있음을 관찰하였다. 특히, 매우 짧은 response 는 확률적 generation failure 인 경우가 많다. 그러나 오직 longer chosen response 만을 사용하는 것은 length bias 를 초래할 수 있다.</p><p>이를 방지하기 위해, HF 는 chosen response 의 길이가 rejected response 보다 길거나 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>−</mo><mi>S</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">M - S/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord">/2</span></span></span></span></span> 보다 긴 경우에만 comparison pair 를 유지한다. </p><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span></span> 은 response 길이의 평균, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">S</span></span></span></span></span> 는 표준편차이며, </li><li>길이는 character 단위로 측정된다. </li></ul><p>이 length 제약은 각 response 길이가 신뢰 구간 내에 존재하는지를 확인함으로써 short response 가 stochastic generation failure 일 확률을 줄인다. 동시에 length bias 도 방지할 수 있다. 구체적 예시는 Appendix B 에 제시되어 있으며, Sec. 4.2 에서 그 효과를 보여준다.</p><p>둘째로, 추가적인 filtering 을 위해 <strong>As-is RM</strong> 을 활용한다. 구체적으로, StackExchange 와 같은 community QA dataset 을 이용해 또 다른 RM 을 학습한다. Askell et al. 이 언급한 large-scale pre-training 의 효과는 저자의 실험에서 관찰되지 않았으므로, pre-processed StackExchange dataset 에서 20k pair 를 sampling 하여 학습하였다. 이후 As-is RM 의 판단과 일치하는 synthetic comparison 만을 최종적으로 유지한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="reward-modeling">Reward Modeling<a href="#reward-modeling" class="hash-link" aria-label="Direct link to Reward Modeling" title="Direct link to Reward Modeling">​</a></h4><p>마지막으로, 위에서 생성된 synthetic comparison 을 기반으로 reward model 을 학습한다. 저자는 기존 연구에서 사용된 ranked preference modeling objective 를 따른다.</p><p>reward model <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">r_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 주어진 query <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 에 대해 response <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">y_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 의 전체적인 품질을 scalar 값으로 산출하며, baseline response <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">y_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 와의 상대적 품질을 학습한다. loss function 은 다음과 같이 정의된다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>∼</mo><mi>D</mi></mrow></msub><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>σ</mi><mo stretchy="false">(</mo><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">J(\theta) = -\mathbb{E}_{(x, y_j, y_k) \sim D} \log (\sigma(r_\theta(x, y_j) - r_\theta(x, y_k)))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.09618em">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1275em;vertical-align:-0.3775em"></span><span class="mord">−</span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mrel mtight">∼</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3775em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)))</span></span></span></span></span></div><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span></span> 는 synthetic comparison 으로 구성된 training set 이며, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r_\theta(x, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mclose">)</span></span></span></span></span> 는 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 에 대한 response <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 의 전체 품질을 나타내는 reward model 의 scalar output 이다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="implementation-details">Implementation Details<a href="#implementation-details" class="hash-link" aria-label="Direct link to Implementation Details" title="Direct link to Implementation Details">​</a></h4><p>먼저, 저자는 다양한 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 로 구성된 initial query set 을 생성한다. 구체적으로, few-shot in-context learning 기반의 Self-Instruct 방식을 따라 10k 개의 initial query 를 생성하였다. query generation 에 대한 세부 사항은 Appendix C 에 기술되어 있다.</p><p>response generation 단계에서는 다음 다섯 가지 prompted model 설정을 사용한다.</p><ul><li>A. <code>LLaMA-30B-Faithful-3shot</code></li><li>B. <code>LLaMA-30B-HHH-5shot</code></li><li>C. <code>LLaMA-13B-HHH-3shot</code></li><li>D. <code>LLaMA-7B-HHH-3shot</code></li><li>E. <code>LLaMA-7B-HHH-1shot</code></li></ul><p>각 query 에 대해 다섯 개의 model 로부터 response 를 생성하고, 경험적 규칙에 따라 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>A</mi></msub><mo>&gt;</mo><msub><mi>y</mi><mi>B</mi></msub><mo>&gt;</mo><msub><mi>y</mi><mi>C</mi></msub><mo>&gt;</mo><msub><mi>y</mi><mi>D</mi></msub><mo>&gt;</mo><msub><mi>y</mi><mi>E</mi></msub></mrow><annotation encoding="application/x-tex">y_A &gt; y_B &gt; y_C &gt; y_D &gt; y_E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">E</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 로 ranking 한다. </p><ul><li>여기서 <code>Faithful</code> 은 저자가 직접 설계한 prompt 로, response 형식을 고려하면서 더 충실하고 긴 대답을 유도하기 위한 세 개의 대화 예시로 구성된다. </li><li>HHH 는 Askell et al. 이 작성한 prompt 를 의미한다. 구체적 예시는 Appendix A 에 제시되어 있다.</li></ul><p>post-validation (HF 및 As-is RM) 을 거친 후, 최종적으로 13k binarized synthetic comparisons 을 생성하여 reward model 학습에 사용한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-step-2-supervised-fine-tuning">2.2 Step 2: Supervised Fine-Tuning<a href="#22-step-2-supervised-fine-tuning" class="hash-link" aria-label="Direct link to 2.2 Step 2: Supervised Fine-Tuning" title="Direct link to 2.2 Step 2: Supervised Fine-Tuning">​</a></h2><p>두 번째 단계에서는 <strong>Reward-Model-guided Self-Play (RMSP)</strong> 를 제안하여 high-quality demonstration, 즉 user 와 AI assistant 간의 대화를 시뮬레이션한다. 생성된 demonstration 은 initial alignment 된 policy model 의 supervised fine-tuning (SFT) 에 사용된다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="self-play">Self-Play<a href="#self-play" class="hash-link" aria-label="Direct link to Self-Play" title="Direct link to Self-Play">​</a></h4><p>기본적인 시뮬레이션은 user 와 assistant 역할의 model 이 번갈아 응답하는 turn-taking 방식으로 이루어진다. assistant 역할에는 <code>LLaMA-30B-Faithful-3shot</code> model 을 사용한다. user 역할에는 기존 HHH prompt 를 일부 수정하여 LLaMA-30B-User-3shot 을 설계하였다.</p><p>첫 단계에서 생성된 initial query 를 시작점으로, <code>LLaMA-30B-Faithful-3shot</code> 이 query 에 대한 response 를 생성하고, 이후 LLaMA-30B-User-3shot 이 그 응답에 이어 대화를 이어간다. 이러한 turn-taking 은 maximum turn <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span></span></span></span></span> 에 도달할 때까지 반복된다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="rm-guided-self-play-rmsp">RM-guided Self-Play (RMSP)<a href="#rm-guided-self-play-rmsp" class="hash-link" aria-label="Direct link to RM-guided Self-Play (RMSP)" title="Direct link to RM-guided Self-Play (RMSP)">​</a></h4><p>assistant 의 response 가 보다 잘 aligned 되도록, 저자는 첫 단계에서 학습된 synthetic RM 을 대화 루프에 포함시킨다. 이 과정을 <strong>Reward-Model-guided Self-Play (RMSP)</strong> 라 부른다.</p><p>이 과정에서 assistant model (<code>LLaMA-30B-Faithful-3shot</code>) 은 주어진 conversational context 에 대해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> 개의 response 를 sampling 한다. 이후 RM 이 각 response 에 점수를 부여하고, 가장 높은 점수를 받은 response 를 final response 로 선택한다. 즉, RM 이 rejection sampling (best-of-N sampling) 을 수행한다.</p><p>Self-Play 와 마찬가지로, LLaMA-30B-User-3shot 과의 turn-taking 은 maximum turn 까지 계속된다. 예시는 Fig. 10 에 제시되어 있다.</p><p><img loading="lazy" alt="Figure 10" src="/assets/images/image-20-49ddec90e72128546b91fdea0373a61f.png" width="2079" height="2457" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="implementation-details-1">Implementation Details<a href="#implementation-details-1" class="hash-link" aria-label="Direct link to Implementation Details" title="Direct link to Implementation Details">​</a></h4><p>저자는 RMSP 를 통해 약 20k high-quality demonstrations 을 생성하였다. 단순화를 위해 maximum turn 을 2 로 설정하여 single-turn 시나리오에 초점을 맞추었다. rejection sampling 횟수 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> 은 자원 제약을 고려하여 4 로 설정하였다. 이후, 이렇게 생성된 demonstration 으로 LLaMA-7B 를 supervised fine-tuning 하였다. (세부 학습 설정은 Appendix D 참고.)</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-step-3-reinforcement-learning-from-synthetic-feedback-rlsf">2.3 Step 3: Reinforcement Learning from Synthetic Feedback (RLSF)<a href="#23-step-3-reinforcement-learning-from-synthetic-feedback-rlsf" class="hash-link" aria-label="Direct link to 2.3 Step 3: Reinforcement Learning from Synthetic Feedback (RLSF)" title="Direct link to 2.3 Step 3: Reinforcement Learning from Synthetic Feedback (RLSF)">​</a></h2><p>마지막 단계에서는 synthetic RM 으로부터의 reward signal 을 활용하여 SFT model 을 추가적으로 alignment 하는 <strong>Reinforcement Learning from Synthetic Feedback (RLSF)</strong> 을 수행한다. 기존 연구를 따라, reinforcement learning 알고리즘으로 <strong>Proximal Policy Optimization (PPO)</strong> 를 사용하였다.</p><ul><li>이 단계에서 policy <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>ϕ</mi></msub></mrow><annotation encoding="application/x-tex">\pi_\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 는 prompt <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 가 주어졌을 때 response <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 를 autoregressive 하게 생성한다. </li><li>이후 reward model <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">r_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r_\theta(x, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mclose">)</span></span></span></span></span> 를 계산하여 reward score 를 부여한다. </li><li>학습 목표는 다음과 같이 expected reward 를 최대화하는 것이다.</li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="double-struck">E</mi><mrow><mi>x</mi><mo>∼</mo><mi>D</mi><mo separator="true">,</mo><mo separator="true">,</mo><mi>y</mi><mo>∼</mo><msub><mi>π</mi><mi>ϕ</mi></msub><mo stretchy="false">(</mo><mo>⋅</mo><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathbb{E}_{x \sim D,, y \sim \pi_\phi(\cdot|x)} [r_\theta(x, y)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1333em;vertical-align:-0.3833em"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∼</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="mpunct mtight">,,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2901em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight">⋅</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3833em"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mclose">)]</span></span></span></span></span></div><p>Stiennon et al. 은 initial policy <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding="application/x-tex">\rho</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ρ</span></span></span></span></span> 와 현재 policy <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>ϕ</mi></msub></mrow><annotation encoding="application/x-tex">\pi_\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 간의 KL penalty term 을 추가하면 성능이 향상된다고 제안하였다. 이를 반영한 final objective 는 다음과 같다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="double-struck">E</mi><mrow><mi>x</mi><mo>∼</mo><mi>D</mi><mo separator="true">,</mo><mi>y</mi><mo>∼</mo><msub><mi>π</mi><mi>ϕ</mi></msub><mo stretchy="false">(</mo><mo>⋅</mo><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mrow><mo fence="true">[</mo><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>−</mo><mi>λ</mi><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mrow><msub><mi>π</mi><mi>ϕ</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>ρ</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{E}_{x \sim D, y \sim \pi_\phi(\cdot|x)} \left[ r_\theta(x, y) - \lambda \log \left(\frac{\pi_\phi(y|x)}{\rho(y|x)}\right) \right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4em;vertical-align:-0.95em"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∼</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2901em"><span></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mtight">⋅</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3833em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">ρ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">]</span></span></span></span></span></span></span></div><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">λ</span></span></span></span></span> 는 KL coefficient</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="implementation-details-2">Implementation Details<a href="#implementation-details-2" class="hash-link" aria-label="Direct link to Implementation Details" title="Direct link to Implementation Details">​</a></h4><p>policy <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi></mrow><annotation encoding="application/x-tex">\rho</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ρ</span></span></span></span></span> 는 Step 2 에서 SFT 로 학습된 LLaMA-7B 로 초기화하였다. </p><p>PPO 학습에 사용되는 prompt 는 RMSP 로 생성된 demonstration dataset 에서 input (initial query) 만 추출하여 구성하였다. PPO 학습의 추가적인 세부 사항은 Appendix D 에 제시되어 있다.</p><h1>3 Evaluating Alignment of ALMoST</h1><p>저자는 Aligned Language Model with Synthetic Training dataset (ALMoST) model 을 세 가지 alignment benchmark 에서 검증하였다.</p><ul><li>Static HHH evaluation (Askell et al.)</li><li>TruthfulQA (Lin et al.)</li><li>Vicuna Questions (Chiang et al.)</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-dataset">3.1 Dataset<a href="#31-dataset" class="hash-link" aria-label="Direct link to 3.1 Dataset" title="Direct link to 3.1 Dataset">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="static-hhh-alignment-and-truthfulqa">Static HHH Alignment and TruthfulQA<a href="#static-hhh-alignment-and-truthfulqa" class="hash-link" aria-label="Direct link to Static HHH Alignment and TruthfulQA" title="Direct link to Static HHH Alignment and TruthfulQA">​</a></h4><ul><li>Askell et al. 은 model 이 human value 와 얼마나 잘 align 되었는지를 측정하기 위한 Static HHH alignment benchmark 를 제안하였다. 이 benchmark 는 두 개의 response 중 human value 에 더 부합하는 쪽을 선택하는 방식으로 구성된다. <ul><li>dataset 은 세 가지 human value category — helpful, harmless, honest — 와 기타 항목을 포함하는 misc (others) category 로 이루어져 있다. 전체 sample 수는 221 로 적지만, 서로 다른 human value 간의 tension 관계를 평가하기 위해 포함하였다.</li></ul></li><li>Lin et al. 은 TruthfulQA benchmark 를 제안하여, LLM 이 주어진 question 에 대해 얼마나 truthful 한 answer 를 생성하는지를 측정한다. <ul><li>이 dataset 은 특히 LLM 이 거짓된 모방형 답변(imitative falsehood)을 생성하도록 유도하는 817 개의 adversarial question 을 포함한다.</li></ul></li></ul><p>저자는 generative setup 대신 multiple-choice setup (MC1) 으로 평가를 수행하였다. 모든 평가는 zero-shot 방식으로 진행되며, target dataset 에 대해 별도의 fine-tuning 은 수행하지 않는다. 평가에 사용된 prompt 에 대한 세부 내용은 Appendix I 에 제시되어 있다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="vicuna-questions">Vicuna Questions<a href="#vicuna-questions" class="hash-link" aria-label="Direct link to Vicuna Questions" title="Direct link to Vicuna Questions">​</a></h4><p>저자는 Vicuna evaluation questions 를 이용하여 model 을 추가로 평가하였다. 이 dataset 은 일반 QA, writing, reasoning 등 다양한 주제를 포괄하는 80 questions 로 구성되어 있으며, user preference 를 파악하기 위한 목적으로 설계되었다.</p><p>두 개의 서로 다른 model 이 동일한 question 에 대해 각각 answer 를 생성하고, human A/B test 를 통해 어느 answer 가 더 선호되는지 평가한다. 각 test 마다 세 명의 human evaluator 를 모집하여, 각 answer 가 helpful, harmless, honest 한지를 종합적으로 고려하여 더 나은 쪽을 선택하게 한다. 구체적인 human evaluation 절차는 Appendix E.1 에 설명되어 있다.</p><p>추가적으로, 자동 평가를 위해 GPT-4 를 활용하였다. GPT-4 는 두 answer 에 대해 각각 1–10 scalar score 를 부여하고, 판단 근거를 함께 설명한다. 이는 완전한 정량 평가 방식은 아니지만, 상대적으로 합리적인 비용으로 model 간 overall response quality 를 비교할 수 있다.</p><p>GPT-4 평가의 positional bias 를 고려하여, 동일한 instance 에 대해 두 answer 의 순서를 바꿔 두 번 평가하였다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-baselines">3.2 Baselines<a href="#32-baselines" class="hash-link" aria-label="Direct link to 3.2 Baselines" title="Direct link to 3.2 Baselines">​</a></h2><p>저자는 backbone model 과 training dataset 의 차이에 따른 alignment behavior 를 비교하기 위해 최근의 open-source model 들을 baseline 으로 포함하였다.</p><ul><li>Alpaca: LLaMA 를 기반으로 한 최초의 open-source instruction-following model 로, proprietary LLM 인 InstructGPT 가 생성한 52k synthetic instruction dataset 으로 학습되었다.</li><li>Vicuna: ChatGPT 와 사용자 간의 대화 로그를 공유한 ShareGPT dataset (70k) 으로 학습된 model 로, ChatGPT 와 같이 강하게 aligned 된 model 중 하나이다.</li><li>Dolly-v2: 15k 개의 human-annotated instruction dataset 으로 학습된 open-source model 로, backbone 은 Pythia 이다.</li><li>OpenAssistant (Oasst): 웹 커뮤니티 참여자들의 데이터를 기반으로 aligned LLM 을 구축한 open-source 프로젝트로, human-annotated dataset 으로 학습된 Oasst (SFT) model 도 함께 공개하였다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-evaluation-results">3.3 Evaluation Results<a href="#33-evaluation-results" class="hash-link" aria-label="Direct link to 3.3 Evaluation Results" title="Direct link to 3.3 Evaluation Results">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="static-hhh-alignment-and-truthfulqa-1">Static HHH Alignment and TruthfulQA<a href="#static-hhh-alignment-and-truthfulqa-1" class="hash-link" aria-label="Direct link to Static HHH Alignment and TruthfulQA" title="Direct link to Static HHH Alignment and TruthfulQA">​</a></h4><p><img loading="lazy" alt="Table 1" src="/assets/images/image-14-c1148aeffe7ccb822d4903bbd1e8378d.png" width="3847" height="2103" class="img_ev3q"></p><ul><li>Tab. 1 에 제시된 결과에 따르면, 저자의 model 들은 proprietary LLM 으로부터의 distillation 이나 intensive human annotation 없이도 Alpaca, Dolly-v2, OpenAssistant 보다 우수한 성능을 보인다.</li><li>모든 model 규모에서 ALMoST 는 HHH 의 각 분할(Helpful, Harmless, Honest)과 TruthfulQA 에서 일관되게 높은 accuracy 를 보였다. 단, ChatGPT 의 output 으로 학습된 Vicuna 만이 일부 항목에서 우위를 보였다.</li><li>특히, 저자의 reward model (RM) 은 human value 기준에 따라 적절한 response 를 선택하는 능력에서 Vicuna-7B 보다도 뛰어난 성능을 보여주었다. 이는 Askell et al. 이 보고한 관찰 결과와 일관된다.</li><li>또한 ALMoST-PPO 는 HHH 평가 중 Helpful 항목에서 13B 규모의 model 을 포함한 모든 model 중 가장 높은 accuracy 를 달성하였다.</li><li>SFT 와 PPO 학습 model 을 비교하면, PPO model 은 helpfulness, harmlessness, truthfulness 는 향상시키는 반면 honesty 는 다소 감소시킨다.</li><li>honesty 와 truthfulness 는 유사하지만 미묘한 차이가 있다. honesty 는 불확실성을 표현하는 능력과 관련되고, truthfulness 는 adversarial falsehood 에 대한 model 의 견고함을 측정한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="human-evaluation-on-vicuna-questions">Human Evaluation on Vicuna Questions<a href="#human-evaluation-on-vicuna-questions" class="hash-link" aria-label="Direct link to Human Evaluation on Vicuna Questions" title="Direct link to Human Evaluation on Vicuna Questions">​</a></h4><p>저자는 human evaluation 을 통해 실제 human preference 에서도 ALMoST 의 강점을 확인하였다.</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-15-5601aa68b02bb18ecbe33323e1d8d80a.png" width="3847" height="1476" class="img_ev3q"></p><ul><li>Fig. 3a 에 따르면, human 평가에서 ALMoST 는 Dolly-v2 (58.8%), Alpaca (55.0%) 대비 더 높은 winning rate 를 보였다. </li><li>또한 ALMoST-PPO 는 ALMoST-SFT 대비 winning rate (37.5%) 가 향상되었으며, tie rate (36.3%) 또한 가장 높게 나타났다. <ul><li>이는 RLSF training 의 효과를 입증한다.</li></ul></li><li>더 나아가, ALMoST 는 ChatGPT 에 의존하지 않았음에도 불구하고 Vicuna 와 비교 시 25% 의 winning rate 와 25% 의 tie rate 를 기록하였다. <ul><li>이는 경쟁력 있는 성능을 의미하지만, 여전히 ALMoST 와 Vicuna 간에는 상당한 성능 차이가 존재함을 보여준다.</li></ul></li></ul><p>human 평가의 구체적 예시는 Appendix H 에 제시되어 있다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="gpt-4-evaluation-on-vicuna-questions">GPT-4 Evaluation on Vicuna Questions<a href="#gpt-4-evaluation-on-vicuna-questions" class="hash-link" aria-label="Direct link to GPT-4 Evaluation on Vicuna Questions" title="Direct link to GPT-4 Evaluation on Vicuna Questions">​</a></h4><ul><li>Fig. 3b 에 따르면, GPT-4 평가 결과 역시 human 평가와 유사한 경향을 보였다. ALMoST-PPO 는 Dolly-v2, Alpaca, ALMoST-SFT 대비 일관되게 높은 winning rate 를 기록하였다.</li><li>다만 GPT-4 는 human 평가보다 tie rate 가 낮게 나타났으며, 동일한 answer 에 대해 같은 점수를 주지 않는 경향이 있었다. 또한 GPT-4 는 human 평가보다 Vicuna 의 response 를 더 높게 평가하는 경향을 보였다.</li><li>그럼에도 불구하고, GPT-4 평가를 통해 model 간의 전반적 성능 차이 를 합리적인 비용으로 효과적으로 비교할 수 있었다.</li><li>추가적으로 model 규모를 확장한 평가에서, ALMoST-7B 는 Alpaca-13B, Oasst-12B, Dolly-v2-12B 등의 12–13B baseline model 보다 더 나은 성능을 보였다 (Fig. 8 참고).</li></ul><h1>4 Analysis</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-probing-main-hypothesis">4.1 Probing Main Hypothesis<a href="#41-probing-main-hypothesis" class="hash-link" aria-label="Direct link to 4.1 Probing Main Hypothesis" title="Direct link to 4.1 Probing Main Hypothesis">​</a></h2><p>저자는 synthetic feedback 의 가정들을 더 면밀히 검증하였다. 구체적으로, (1) model size, (2) demonstration 의 개수, (3) demonstration 의 품질 이 최종적으로 sampling 된 response 의 품질에 어떻게 기여하는지를 분석하였다.</p><p>이를 위해 Sec. 2.1 에서 synthetic feedback 생성을 위해 사용된 prompted response generator 들을 대상으로 Vicuna Questions 에 대해 GPT-4 evaluation 을 수행하였다. baseline model 로는 Alpaca-7B 를 사용하였으며, 결과는 Tab. 2 에 제시되어 있다.</p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-16-087c585938bd2a23c5341d8aaf09ee15.png" width="1934" height="1961" class="img_ev3q"></p><ul><li>예상대로 model size 는 response quality 에 크게 기여하였다. HHH prompt 와 Faithful prompt 모두에서 model 규모가 커질수록 Alpaca-7B 대비 winning rate 가 단조 증가하였다. 특히 13B 와 30B 사이의 성능 차이는 매우 컸다.</li><li>LLaMA-7B-HHH-{1,3,5}shot model 을 비교한 결과, demonstration 의 개수가 늘어날수록 winning rate 가 향상되었으나, 그 향상 폭은 상대적으로 작았다.</li><li>demonstration 의 품질 이 가장 중요한 요인으로 나타났다. 흥미롭게도, 잘 설계된 Faithful prompt 를 사용한 소형 model (LLaMA-7B-Faithful-3shot) 이 일반 prompt 를 사용한 대형 model (<code>LLaMA-30B-HHH-5shot</code>) 보다 더 높은 성능을 보였다.</li></ul><p>이러한 intrinsic evaluation 결과를 통해, 저자의 synthetic feedback dataset 이 다양한 품질의 response 를 효과적으로 포괄하고 있음을 확인할 수 있다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-rm-evaluation">4.2 RM Evaluation<a href="#42-rm-evaluation" class="hash-link" aria-label="Direct link to 4.2 RM Evaluation" title="Direct link to 4.2 RM Evaluation">​</a></h2><p>synthetic feedback 의 타당성을 검증하기 위해, 저자는 reward model (RM) 을 또 다른 comparison dataset 인 HH-RLHF (Bai et al.) 에서 평가하였다. HH-RLHF 는 학습 단계별로 다양한 split 을 포함하며, 여기서는 deploy 가능한 시스템이 없다고 가정하고 ‘Helpful-base’ split 에 초점을 맞추었다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="reward-modeling-1">Reward Modeling<a href="#reward-modeling-1" class="hash-link" aria-label="Direct link to Reward Modeling" title="Direct link to Reward Modeling">​</a></h4><p><img loading="lazy" alt="Table 3" src="/assets/images/image-17-0fbf950a8ddebc4047d83abd1a09fe8b.png" width="1934" height="1909" class="img_ev3q"></p><ul><li>Tab. 3 에 따르면, synthetic feedback 으로 학습된 저자의 RM 은 full training dataset 으로 학습된 상한선(upper bound) 대비 약 90% 의 성능을 달성하였다. </li><li>또한, single-turn subset (Helpful-base*) 으로 fine-tuning 한 결과와 동일한 accuracy 를 보였다.</li><li>참고로 HH-RLHF 는 multi-turn context 를 포함하지만, 저자의 synthetic dataset 은 single-turn 시나리오에 초점을 두고 있다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="effect-of-post-validation">Effect of Post Validation<a href="#effect-of-post-validation" class="hash-link" aria-label="Direct link to Effect of Post Validation" title="Direct link to Effect of Post Validation">​</a></h4><p>Sec. 2.1 에서 설명된 두 가지 post-validation 기법이 synthetic comparison 내의 noise 를 얼마나 효과적으로 제거하는지 평가하였다.</p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-18-f28f8d73068a721b149aa9704b2c8a5a.png" width="1907" height="1328" class="img_ev3q"></p><ul><li>Tab. 4 에 따르면, 각각의 filtering 방법이 RM 품질 향상에 기여하였다. 특히 length distribution 을 고려한 heuristic filter (HF) 가 synthetic data generation 에서 결정적인 역할을 한다는 점이 드러났다.</li><li>HF 를 제거하면 RM 성능이 약 10%p 감소하였다. 또한 HF 는 Sec. 2.1 에서 논의된 length bias 문제를 방지하였다. </li><li>HF 를 적용하여 학습된 RM 은 단순히 longer response 를 항상 좋은 답변으로 간주하는 lengthy baseline 보다 높은 성능을 보였다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="rmsp-vs-self-play">RMSP vs Self-Play<a href="#rmsp-vs-self-play" class="hash-link" aria-label="Direct link to RMSP vs Self-Play" title="Direct link to RMSP vs Self-Play">​</a></h4><p>RM-guided Self-Play (RMSP) 가 단순 Self-Play 대비 얼마나 이점을 가지는지 평가하였다. 이를 위해, 각각 RMSP 와 Self-Play 로 생성된 demonstration 으로 학습된 두 개의 SFT model 을 비교하였다.</p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-19-96d9d9879fc1ff94f82915d43316cf7e.png" width="1907" height="1270" class="img_ev3q"></p><ul><li>Tab. 5 에 따르면, RMSP 기반으로 학습된 SFT model 이 여러 benchmark 에서 Self-Play 기반 model 보다 우수한 성능을 보였다. 특히 GPT-4 평가에서 Alpaca-7B 와 비교 시, RMSP model 만이 winning rate 50% 이상을 달성하였다.</li><li>또한, prompt 설계의 중요성 역시 확인되었다. Faithful prompt 대신 HHH prompt 를 사용하면 alignment 관련 성능이 크게 하락하였다.</li><li>두 방법 간의 qualitative 비교 예시는 Tab. 10 에 제시되어 있다.</li></ul><h1>5 Related Work</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="aligning-llms-with-human-values">Aligning LLMs with Human Values<a href="#aligning-llms-with-human-values" class="hash-link" aria-label="Direct link to Aligning LLMs with Human Values" title="Direct link to Aligning LLMs with Human Values">​</a></h4><p>language model 을 human value 에 맞게 conditioning 하는 것은 model 이 human-aligned text 를 생성하는 능력을 향상시킨다는 것이 알려져 있다. reward model 을 도입하여 생성된 text 가 human value 를 얼마나 잘 반영하는지를 평가하는 접근은, 더 잘 aligned 된 language model 을 학습할 수 있게 했으며, reinforcement learning from human feedback (RLHF) 의 핵심 구성 요소로 작용하였다.</p><p>RLHF 는 최근 LLM 을 human value 에 맞추기 위한 방법으로 폭넓게 연구되었다.</p><p>최근 Zhou et al. 은 Superficial Alignment Hypothesis 를 제시하며, LLM 의 대부분의 능력은 pre-training 단계에서 이미 학습되며, 소수의 curated dataset 에 대한 fine-tuning 만으로도 well-aligned behavior 를 유도할 수 있다고 주장하였다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="distillation-from-proprietary-llms">Distillation from Proprietary LLMs<a href="#distillation-from-proprietary-llms" class="hash-link" aria-label="Direct link to Distillation from Proprietary LLMs" title="Direct link to Distillation from Proprietary LLMs">​</a></h4><p>최근의 open-source model 들인 Alpaca 등은 human demonstration 수집의 부담을 줄이기 위해 Self-Instruct 방식을 따른다. 그러나 이들은 vanilla LLM 이 아닌 InstructGPT 나 ChatGPT 와 같은 proprietary LLM 을 사용하여 synthetic instruction dataset 을 생성한다.</p><p>유사하게 Peng et al. 은 GPT-4 output 을 distillation 하여 alignment 학습을 시도하였다. Vicuna 또한 ChatGPT output 을 사용자들이 공유한 ShareGPT dataset (70k) 으로 학습된 open-source model 이다.</p><p>반면, Gudibande et al. 은 이러한 distillation 기반 접근의 한계를 지적하였다. synthetic dataset 의 규모를 확장해도 knowledge 관련 task 나 human preference 향상에는 도움이 되지 않으며, model size 확장이 주요한 요인임을 보였다. 즉, synthetic dataset 은 teacher knowledge 가 아니라 style 을 distillation 한다는 점을 경고하였다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="self-alignment-learning">Self-Alignment Learning<a href="#self-alignment-learning" class="hash-link" aria-label="Direct link to Self-Alignment Learning" title="Direct link to Self-Alignment Learning">​</a></h4><ul><li>Askell et al. 은 저자들이 직접 설계한 few-shot demonstration 을 이용하여 initial policy 를 얻는 context distillation 방법을 제안하였다. prompt 없이 학습된 student model 은 few-shot demonstration 을 포함한 prompt 로 학습된 teacher model 로부터 distillation 된다.</li><li>Self-Instruct 는 LLM 이 스스로 생성한 instruction dataset 을 통해 alignment 하는 접근이다. Wang et al. 은 175 개의 seed task 를 수동으로 설계하고, in-context learning 및 filtering 을 통해 자동으로 instruction dataset 을 생성하였다.</li></ul><p>저자의 연구는 여기에 synthetic feedback 기반의 reward modeling 을 통합하여 발전시킨 것이다.</p><ul><li>또한, Dromedary 는 저자와 유사한 목표를 가진 동시대 연구로, 최소한의 human effort 로 alignment learning 을 수행하고자 한다. 이들은 LLM 이 따를 수 있는 소수의 human-written “principle” 을 설계하고, in-context learning 을 통해 이러한 principle 에 기반한 aligned response 를 생성하도록 한다. 이는 Constitutional AI 와 유사한 접근이다.</li><li>구체적으로 Dromedary 는 약 200 개의 human annotation, 195 개의 seed prompt, 16 개의 principle, 5 개의 exemplar 를 필요로 하는 반면, 저자의 framework 는 단 18 개의 human annotation, 10 개의 seed prompt (query mining 용), 8 개의 demonstration 만을 필요로 한다.</li></ul><h1>6 Conclusion</h1><p>이 연구에서는 synthetic feedback 을 도입하여 human value 에 맞게 LLM 을 alignment 하는 새로운 framework 를 제안하였다.</p><p>저자는 다양한 크기와 prompt 를 가진 vanilla LLM 들의 response 중 더 나은 응답을 경험적 prior knowledge 에 기반해 식별하였다. 먼저, synthetically 생성된 comparison 으로 reward model 을 학습하고, 이어 reward model 을 활용하여 aligned policy 학습을 위한 또 다른 synthetic dataset 을 생성하였다.</p><p>실험 결과, 제안된 framework 는 alignment benchmark 에서 탁월한 성능을 보여주며 synthetic feedback 의 효율성을 입증하였다.</p><p>저자는 이러한 강력한 성능이 synthetic feedback 을 통해 well-aligned behavior 의 경험적 지표를 효과적으로 통합한 결과라고 판단한다. 또한, 본 방법은 방대한 human demonstration 이나 proprietary LLM 에 의존하지 않기 때문에 비용 효율적 (cost-effective) 인 장점이 있다.</p><h1>Limitations</h1><p>저자의 framework 는 여러 alignment 관련 benchmark (Askell et al., Lin et al., Chiang et al.) 에서 우수한 성능을 보였으나, 결과 model 의 다른 능력 측면을 충분히 평가하지는 못했다.  예를 들어, Gudibande et al. 은 pre-aligned LLM 으로부터 생성된 synthetic imitation dataset 의 한계를 지적하며, MMLU, HumanEval, Natural Questions 와 같은 knowledge-related benchmark 를 통해 이를 분석하였다.</p><p>또한, Askell et al. 및 Bai et al. 은 alignment 후 model 이 다른 NLP task 에서 성능이 저하되는 현상인 alignment tax 를 보고하였다. 저자 역시 Appendix F 에서 제시된 바와 같이, zero-shot MMLU 및 LAMBADA task 에서 PPO model 의 성능이 저하되는 유사한 결과를 관찰하였다. 이는 alignment tax 가 존재함을 시사한다.</p><p>일부 연구에서는 parameter 수가 10B 미만인 model 이 alignment tax 에 더 취약하며, model scale 을 확장하면 이러한 trade-off 가 완화된다고 보고하였다 (Bai et al.). 따라서 저자의 접근법은 helpfulness 와 같은 특정 목표 value 에 LLM 을 alignment 하는 데 초점을 두기 때문에, 이러한 trade-off 를 완전히 해소하지는 못한다는 한계가 있다.</p><p>향후 연구에서는 저자의 framework 에 대한 보다 총체적인 평가 (holistic evaluation) 와 alignment tax 완화 방안 을 탐구할 예정이다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/reinforce-learning">Reinforce Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/rl">RL</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/rlaif">RLAIF</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/ai-feedback">AI Feedback</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/al-mo-st">ALMoST</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/synthetic-feedback">synthetic feedback</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/alignment-learning">Alignment Learning</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Reinforce Learning/PPO/RLAIF/2023-05-ALMoST.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/Constitutional AI"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Constitutional AI: Harmlessness from AI Feedback</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/d-RLAIF"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-step-1-reward-modeling-with-synthetic-feedback" class="table-of-contents__link toc-highlight">2.1 Step 1: Reward Modeling with Synthetic Feedback</a></li><li><a href="#22-step-2-supervised-fine-tuning" class="table-of-contents__link toc-highlight">2.2 Step 2: Supervised Fine-Tuning</a></li><li><a href="#23-step-3-reinforcement-learning-from-synthetic-feedback-rlsf" class="table-of-contents__link toc-highlight">2.3 Step 3: Reinforcement Learning from Synthetic Feedback (RLSF)</a></li><li><a href="#31-dataset" class="table-of-contents__link toc-highlight">3.1 Dataset</a></li><li><a href="#32-baselines" class="table-of-contents__link toc-highlight">3.2 Baselines</a></li><li><a href="#33-evaluation-results" class="table-of-contents__link toc-highlight">3.3 Evaluation Results</a></li><li><a href="#41-probing-main-hypothesis" class="table-of-contents__link toc-highlight">4.1 Probing Main Hypothesis</a></li><li><a href="#42-rm-evaluation" class="table-of-contents__link toc-highlight">4.2 RM Evaluation</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.965e64d4.js"></script>
<script src="/assets/js/main.a90b69a7.js"></script>
</body>
</html>