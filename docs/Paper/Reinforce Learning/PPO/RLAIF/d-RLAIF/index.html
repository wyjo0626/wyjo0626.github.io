<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Reinforce Learning/PPO/RLAIF/2023-09-d-RLAIF">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Reinforce Learning/PPO/RLAIF/d-RLAIF"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Reinforce Learning/PPO/RLAIF/d-RLAIF"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Reinforce Learning/PPO/RLAIF/d-RLAIF" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Reinforce Learning/PPO/RLAIF/d-RLAIF" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.3298527c.js" as="script">
<link rel="preload" href="/assets/js/main.aca6605f.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Reinforce Learning/DPO/RLHF/Self-RLM">Reinforce Learning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Reinforce Learning/DPO/RLHF/Self-RLM">DPO</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/Constitutional AI">PPO</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/Constitutional AI">RLAIF</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/Constitutional AI">Constitutional AI: Harmlessness from AI Feedback</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/ALMoST">Aligning Large Language Models through Synthetic Feedback</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/d-RLAIF">RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/UltraFeedback">UltraFeedback: Boosting Language Models with Scaled AI Feedback</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/SER">Self-Evolved Reward Learning for LLMs</a></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Reinforce Learning</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PPO</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">RLAIF</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2309.00267" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2309.00267</a></p><h1>Abstract</h1><p>Human feedback 에 기반한 Reinforcement learning (RLHF) 은 large language model (LLM) 을 인간의 선호에 맞추는 데 효과적이라는 것이 입증되었지만, 고품질의 preference label 을 수집하는 것은 비용이 많이 든다. Bai et al. 이 제안한 <strong>RL from AI Feedback (RLAIF)</strong> 은 off-the-shelf LLM 이 생성한 preference 를 사용하여 reward model (RM) 을 학습하는 유망한 대안을 제시한다.</p><ul><li>Summarization, helpful dialogue generation, 그리고 harmless dialogue generation task 에서 RLAIF 는 RLHF 와 유사한 성능을 보인다. </li><li>또한, 저자는 <strong>self-improvement</strong> 로의 한 걸음을 내딛으며, AI labeler 가 policy 와 동일한 크기이거나 심지어 initial policy 와 동일한 checkpoint 일 때조차 RLAIF 가 supervised fine-tuned baseline 을 능가할 수 있음을 보인다.</li></ul><p>마지막으로, 저자는 RM 학습 과정을 생략하고 RL 중 off-the-shelf LLM 으로부터 directly reward 를 얻는 기법인 <strong>direct-RLAIF</strong> (d-RLAIF) 를 제안하며, 이는 기존 RLAIF 보다 우수한 성능을 달성한다. 이러한 결과는 RLAIF 가 human feedback 을 사용하는 경우에 필적하는 성능을 달성할 수 있으며, RLHF 의 확장성 한계를 해결할 수 있는 잠재적 해법임을 시사한다.</p><h1>1. Introduction</h1><p>Human feedback 에 기반한 <strong>Reinforcement Learning (RLHF)</strong> 은 language model 을 인간의 선호에 맞게 조정하는 효과적인 기법이다. RLHF 는 ChatGPT 및 Bard 와 같은 현대적 conversational language model 의 성공을 이끄는 핵심 요인 중 하나로 꼽힌다. Reinforcement Learning (RL) 을 사용하여 language model 을 학습하는 주요 이점은, 기존의 supervised fine-tuning (SFT) 방식으로는 미분 불가능한 복잡한 sequence-level objective 들을 최적화할 수 있다는 점이다.</p><p>그러나 RLHF 를 large-scale 로 적용하는 데에는 고품질의 human preference label 에 대한 의존도가 높은 것이 큰 장애물이다. 최근 large language model (LLM) 들은 인간의 판단과 높은 수준의 일치도를 보이고 있으며, 이는 LLM 이 생성한 preference label 이 human label 의 대체물로 활용될 가능성을 시사한다. Bai et al. 은 Reinforcement Learning from AI Feedback (RLAIF) 을 처음으로 탐구하였는데, 여기서 RL 은 human 과 AI preference 의 혼합 데이터로 학습된 reward model (RM) 을 사용하여 수행되었다. Constitutional AI self-revision 기법과 결합된 이 접근법은 conversational assistant 학습에서 supervised fine-tuning 을 능가하는 성능을 보였다. 그러나 human feedback 과 AI feedback 간의 효율성을 직접적으로 비교하지 않아, RLAIF 가 RLHF 의 적절한 대안이 될 수 있는지는 여전히 불분명했다.</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-22-035c51f14e402c148808695d6fd7d635.png" width="3139" height="1515" class="img_ev3q"></p><p>본 연구에서는 summarization, helpful dialogue generation, harmless dialogue generation 의 세 가지 task 에 대해 RLAIF 와 RLHF 의 효과를 비교하였다 (Fig. 2). </p><ul><li>저자의 실험 결과, summarization 에서 human 평가자들은 RLAIF 와 RLHF 를 각각 71%, 73% 의 비율로 SFT baseline 보다 선호하였고, helpful dialogue generation 에서는 각각 63%, 64% 의 비율로 SFT 보다 선호하였다. </li><li>두 경우 모두 RLAIF 와 RLHF 간의 차이는 통계적으로 유의하지 않았다. 또한 RLAIF 와 RLHF 를 직접 비교한 head-to-head 실험에서도 두 policy 간 선호도는 동등하게 나타났다.</li><li>Harmless dialogue generation 에서는 human 평가자가 각 응답의 harmlessness 를 독립적으로 평가하였다. RLAIF 는 RLHF 보다 높은 harmless rate 을 기록하였으며, 두 방법 모두 SFT baseline 을 능가하였다 (각각 88%, 76%, 64%). <ul><li>이러한 결과는 RLAIF 가 human annotation 에 의존하지 않으면서도 확장성 측면에서 매력적인 RLHF 의 대안이 될 수 있음을 시사한다.</li></ul></li></ul><p>또한 두 가지 추가 연구를 수행하였다.</p><ol><li>LLM self-improvement 로의 진전으로서, AI labeler 가 policy model 과 동일한 크기일 때조차 RLAIF 가 SFT baseline 을 유의미하게 향상시킴을 보였다.</li><li>direct-RLAIF (d-RLAIF) 라는 새로운 기법을 제안하였다. 이는 reward model 학습 과정을 생략하고 RL 과정 중 off-the-shelf LLM 으로부터 직접 reward 를 획득하는 방법이다. 실험 결과, d-RLAIF 는 canonical RLAIF 와 동등하거나 더 나은 성능을 보였다. 특히 helpful dialogue generation task 에서, initial policy 와 reward 를 제공하는 LLM 이 동일한 checkpoint 일 때조차 성능 향상이 나타나, strict LLM self-improvement 의 사례를 보여주었다.</li></ol><p>마지막으로, AI 가 생성한 preference 와 human preference 간의 alignment 를 극대화하기 위한 기법을 연구하였다. Chain-of-thought reasoning 을 유도하는 것이 alignment 를 지속적으로 향상시킴을 발견하였고, detailed preamble 과 few-shot prompting 은 특정 task 에서만 유의미한 개선을 보였다. 또한 LLM labeler 의 크기와 human preference alignment 간의 trade-off 를 분석하기 위해 scaling experiment 를 수행하였다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-21-1bcab046bf924b6dcd6c0bee124d776f.png" width="1642" height="2307" class="img_ev3q"></p><p>본 연구의 주요 기여는 다음과 같다.</p><ol><li>Summarization, helpful dialogue generation, harmless dialogue generation task 에서 RLAIF 가 RLHF 와 유사한 성능을 달성함을 보인다.</li><li>LLM labeler 가 policy 와 동일한 크기이거나 동일한 checkpoint 일 때에도 RLAIF 가 SFT policy 를 개선할 수 있음을 보인다.</li><li>RL 중 off-the-shelf LLM 으로부터 직접 reward 를 획득하는 direct-RLAIF (d-RLAIF) 를 제안하고, canonical RLAIF 와 동등하거나 더 우수한 성능을 달성함을 보인다.</li><li>AI 가 생성한 preference 와 human preference 간의 alignment 를 극대화하기 위한 기법을 분석한다.</li></ol><h1>2. Methodology</h1><p>이 절에서는 LLM 으로 preference 를 생성하는 기법, reinforcement learning setup, 그리고 evaluation metric 을 설명한다. RLHF 의 기초는 Appendix A 에 제공한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-preference-labeling-with-llms">2.1. Preference Labeling with LLMs<a href="#21-preference-labeling-with-llms" class="hash-link" aria-label="Direct link to 2.1. Preference Labeling with LLMs" title="Direct link to 2.1. Preference Labeling with LLMs">​</a></h2><p>저자는 “off-the-shelf” LLM — 일반적 용도로 pre-trained 또는 instruction-tuned 되었지만 특정 downstream task 에 대해 fine-tuned 되지 않은 model — 을 사용하여 preference 를 주석화한다. 주어진 text 와 두 개의 candidate response 가 있을 때, LLM 에게 어떤 response 가 더 선호되는지 평가하도록 요청한다. Prompt 의 구조는 다음과 같다 (예시는 Tab. 15 와 Tab. 21 참조).</p><ul><li><strong>Preamble</strong>: 현재 task 를 설명하는 소개 및 지시문</li><li><strong>Few-shot exemplars (optional)</strong>: 예시 input context, 두 개의 response, chain-of-thought rationale (optional), 그리고 preference label</li><li><strong>Sample to annotate</strong>: 라벨링할 input context 와 두 개의 response</li><li><strong>Ending</strong>: LLM 을 유도하는 마무리 text (e.g., “Preferred Response=”)</li></ul><p>Prompt 를 LLM 에 제공한 뒤, tokens “1” 과 “2” 를 생성할 log-probability 를 추출하고, softmax 를 계산하여 preference distribution 을 얻는다.</p><p>LLM 으로부터 preference label 을 얻는 방법에는 다양한 대안이 있다. 예를 들어 자유 형식의 생성 응답에서 preference 를 추출하는 방법 (e.g., “The first response is better”), 혹은 preference distribution 을 one-hot encoding 으로 표현하는 방법이 있다. 그러나 저자는 구현이 간단하고, 분포적 표현을 통해 one-hot encoding 보다 더 많은 정보를 전달한다는 이유로 위의 방법을 선택한다.</p><p>저자는 두 가지 스타일의 preamble 을 실험한다. “<em>Base</em>” 는 본질적으로 어느 response 가 더 나은지 묻는 형태이고, “<em>Detailed</em>” 는 일반적으로 human annotator 에게 제공되는 상세한 rating instructions 를 닮았다 (summarization task 에서 사용한 preamble 은 Tab. 16 참조). 또한 in-context learning 을 실험하며, 다양한 주제를 포괄하도록 hand-crafted high-quality exemplar 를 사용한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="211-addressing-position-bias">2.1.1. Addressing Position Bias<a href="#211-addressing-position-bias" class="hash-link" aria-label="Direct link to 2.1.1. Addressing Position Bias" title="Direct link to 2.1.1. Addressing Position Bias">​</a></h3><p>LLM 에 candidate 를 제시하는 순서는 어떤 candidate 가 선호되는지에 bias 를 유발할 수 있다. 저자는 position bias 의 증거를 발견하며, 이는 특히 더 작은 LLM labeler 에서 두드러진다 (Appendix B 참조).</p><ul><li>Position bias 의 영향을 완화하기 위해, 각 candidate 쌍에 대해 두 번의 inference 를 수행한다. </li><li>두 번째 inference 에서는 LLM 에 제시하는 candidate 의 순서를 뒤바꾼다. </li><li>그런 다음 두 inference 의 결과를 평균하여 finaL preference distribution 을 얻는다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="212-eliciting-chain-of-thought-reasoning">2.1.2. Eliciting Chain-of-Thought Reasoning<a href="#212-eliciting-chain-of-thought-reasoning" class="hash-link" aria-label="Direct link to 2.1.2. Eliciting Chain-of-Thought Reasoning" title="Direct link to 2.1.2. Eliciting Chain-of-Thought Reasoning">​</a></h3><p>저자는 AI labeler 로부터 chain-of-thought (CoT) reasoning 을 유도하기 위해 two-step inference 절차를 실험한다. </p><ul><li>먼저, 표준 prompt 의 Ending 을 생각과 설명을 요구하는 문장으로 대체한다 (e.g., “각 summary 의 coherence, accuracy, coverage, overall quality 를 고려하고, 어느 것이 더 나은지 설명하라. Rationale:”). </li><li>그리고 LLM 으로부터 response 를 decode 한다. </li><li>다음으로, original prompt, 그 response, 그리고 standard Ending 문자열을 연결하여 Sec. 2.1 의 scoring 절차를 따라 preference distribution 을 얻는다. </li></ul><p>이 과정은 Fig. 3 이 설명한다.</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-23-6c827024e065bd796d16c200ab4826e7.png" width="4062" height="1655" class="img_ev3q"></p><ul><li>Zero-shot prompt 에서는 LLM 에 reasoning 이 어떤 형태여야 하는지의 예시를 제공하지 않는다. </li><li>Few-shot prompt 에서는 model 이 따를 수 있도록 CoT reasoning 의 예시를 제공한다. </li></ul><p>예시는 Tab. 17 과 Tab. 18 을 참조한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-reinforcement-learning-from-ai-feedback">2.2. Reinforcement Learning from AI Feedback<a href="#22-reinforcement-learning-from-ai-feedback" class="hash-link" aria-label="Direct link to 2.2. Reinforcement Learning from AI Feedback" title="Direct link to 2.2. Reinforcement Learning from AI Feedback">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="221-canonical-rlaif">2.2.1. Canonical RLAIF<a href="#221-canonical-rlaif" class="hash-link" aria-label="Direct link to 2.2.1. Canonical RLAIF" title="Direct link to 2.2.1. Canonical RLAIF">​</a></h3><p>이 절에서는 canonical RLAIF setup 에 대한 저자의 적용 방식을 설명한다. 별도로 언급되지 않는 한, RLAIF 는 이 방법으로 수행된다.</p><p>Reward model (RM) 은 Appendix A.2 의 방법론을 따르며 LLM 이 생성한 preference label 로 학습된다. </p><ul><li>저자의 접근법은 soft label (e.g., <!-- -->[0.6, 0.4]<!-- -->) 을 생성하므로, RM 은 RM 이 생성한 score 의 softmax 에 대해 cross-entropy loss 로 학습된다. </li><li>Softmax 는 RM score 을 확률 분포로 변환한다. </li><li>AI label 로 구성된 dataset 에 RM 을 학습하는 것은 model distillation 의 한 형태로 볼 수 있다.</li><li>마지막으로, Appendix A.3 에 기술된 바와 같이, RM 이 model response 에 reward 를 부여하도록 하여 reinforcement learning 을 수행함으로써 RLAIF policy model 을 학습한다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="222-direct-rlaif-d-rlaif">2.2.2. Direct-RLAIF (D-RLAIF)<a href="#222-direct-rlaif-d-rlaif" class="hash-link" aria-label="Direct link to 2.2.2. Direct-RLAIF (D-RLAIF)" title="Direct link to 2.2.2. Direct-RLAIF (D-RLAIF)">​</a></h3><p>RLAIF 의 한 가지 문제는 policy 가 학습됨에 따라 reward model (RM) 이 “<em>stale</em>” 해질 수 있다는 점이다. </p><p>일반적인 setup 에서 RM 은 initial policy 로부터 sampling 된 generation 으로 학습된다. 그러나 policy 가 점차 학습되면서 생성되는 trajectory 가 RM 학습 시의 dataset 분포에서 벗어나게 되고, 그 결과 성능이 저하된다. (Bai et al. 은 iterative RLAIF 를 제안하였는데, 이는 최신 policy 에 대해 주기적으로 새로운 RM 을 학습하는 방식이지만, 시간 소모가 크다.)</p><p>저자는 canonical RLAIF 의 단순한 대안으로 <strong>direct-RLAIF (d-RLAIF)</strong> 를 제안한다. </p><ul><li>이 방법은 LLM feedback 을 reinforcement learning 중 직접 reward signal 로 사용하는 것이다. </li><li>D-RLAIF 는 off-the-shelf LLM 이 학습 과정 없이 직접 생성된 response 에 점수를 부여하므로 RM staleness 문제를 해결한다. </li><li>또한, AI preference labeling 과 RM 학습에 필요한 시간 소모적 과정을 제거한다. </li></ul><p>이 과정은 Fig. 4 에 설명되어 있다.</p><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-24-2f22ac0f72f320aca8d4f48cb9d9a1e2.png" width="2068" height="1251" class="img_ev3q"></p><ul><li>D-RLAIF 에서 LLM 은 1 에서 10 사이의 점수로 generation 의 품질을 평가하도록 prompt 된다. </li><li>Sec. 2.1 과 유사하게, prompt 는 generation 을 평가하는 기준을 LLM 에 지시한다. </li><li>이후 각 score token (1~10) 의 likelihood 를 계산하고, 이를 probability distribution 으로 정규화한다.</li><li>Weighted score 는 다음과 같이 계산된다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>10</mn></msubsup><mi>i</mi><mi>P</mi><mo stretchy="false">(</mo><mi>i</mi><mi mathvariant="normal">∣</mi><mi>y</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s(y|x) = \sum_{i=1}^{10} i P(i|y, x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">s</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2537em;vertical-align:-0.2997em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.954em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">10</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></li><li>마지막으로, score 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span> 범위로 다시 정규화된다. </li></ul><p>Prompting 기법의 추가 세부 내용은 Appendix D 에 제시되어 있다.</p><p>이후 RL 은 canonical RLAIF 와 유사한 방식으로 수행되며, 단 RM score 대신 direct score 가 reward 로 사용된다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-evaluation">2.3. Evaluation<a href="#23-evaluation" class="hash-link" aria-label="Direct link to 2.3. Evaluation" title="Direct link to 2.3. Evaluation">​</a></h2><p>저자는 세 가지 metric — <strong>AI Labeler Alignment</strong>, <strong>Win Rate</strong>, <strong>Harmless Rate</strong> — 으로 결과를 평가한다.</p><p><strong>AI Labeler Alignment</strong> 는 AI-labeled preference 가 human preference 와 일치하는 정확도를 측정한다. single example 의 soft AI-labeled preference (e.g., <!-- -->[0.6, 0.4]<!-- -->) 는 binary representation (<!-- -->[1, 0]<!-- -->) 으로 변환된다. 이후 human preference 와 일치하면 1, 불일치하면 0 의 score 를 부여한다. </p><p>Alignment 정확도 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mrow><mi>a</mi><mi>c</mi><mi>c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">z_{acc}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">cc</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 다음과 같이 정의된다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>z</mi><mrow><mi>a</mi><mi>c</mi><mi>c</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>D</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mn mathvariant="bold">1</mn><mo stretchy="false">[</mo><munder><mrow><mi mathvariant="normal">arg max</mi><mo>⁡</mo></mrow><mi>j</mi></munder><msubsup><mi>P</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mrow><mi>A</mi><mi>I</mi></mrow></msubsup><mo>=</mo><msubsup><mi>p</mi><mi>i</mi><mi>H</mi></msubsup><mo stretchy="false">]</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">z_{acc} = \frac{1}{D} \sum_{i=1}^{D} \mathbf{1}[\argmax_j P^{AI}_{i,j} = p^H_i],</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">cc</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathbf">1</span><span class="mopen">[</span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.1779em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">arg</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathrm">max</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0582em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.07847em">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em"><span></span></span></span></span></span></span><span class="mclose">]</span><span class="mpunct">,</span></span></span></span></span></div><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span></span> 는 preference dataset size, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>P</mi><mrow><mi>A</mi><mi>I</mi></mrow></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>D</mi><mo>×</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">P^{AI} \in \mathbb{R}^{D \times 2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8804em;vertical-align:-0.0391em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.07847em">I</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="mbin mtight">×</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span> 는 soft AI preference matrix, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mi>H</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>D</mi></msup></mrow><annotation encoding="application/x-tex">p^H \in \mathbb{R}^D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span></span></span></span></span></span></span></span></span></span></span></span> 는 human preference vector 로, 각각 첫 번째 또는 두 번째 response 가 선호되는지를 0 또는 1 로 나타낸다.</li></ul><hr><p><strong>Win Rate</strong> 는 두 policy 의 전반적인 품질을 평가하기 위한 지표로, human annotator 가 한 policy 의 output 을 다른 policy 보다 더 자주 선호하는 비율을 측정한다. 입력과 두 개의 generation 이 주어졌을 때, human annotator 는 더 선호하는 출력을 선택한다. Policy A 가 Policy B 보다 선호된 비율을 “A vs. B 의 win rate” 라 부르며, 50% 의 win rate 은 두 policy 가 동일하게 선호됨을 의미한다.</p><hr><p><strong>Harmless Rate</strong> 는 human evaluator 가 “무해하다고 판단한” 응답의 비율을 측정한다. 이 metric 은 harmless dialogue generation task 평가에 사용된다. 많은 응답이 동일하게 안전하다고 평가되는 경우가 많아 상대적 순위를 매기기 어렵기 때문이다.</p><h1>3. Experimental Details</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-datasets">3.1. Datasets<a href="#31-datasets" class="hash-link" aria-label="Direct link to 3.1. Datasets" title="Direct link to 3.1. Datasets">​</a></h2><p>저자는 다음 dataset 을 사용하여 실험을 수행한다.</p><ul><li>Reddit TL;DR — Reddit 게시물과 그 게시물에 대한 summary 로 구성된 dataset.</li><li>OpenAI’s Human Preferences — Reddit TL;DR 의 일부 subset 으로부터 생성된 dataset. 각 예시는 게시물, 두 개의 candidate summary, 그리고 human annotator 가 어느 summary 를 선호하는지 표시한 rating 으로 구성된다.</li><li>Anthropic Helpful and Harmless Human Preferences — human 과 AI assistant 간의 대화로 구성되며, 각 대화에는 human annotator 가 평가한 두 개의 가능한 AI 응답이 포함된다. <ul><li>Helpful task 의 경우 더 informative 하고 honest 한 응답, Harmless task 의 경우 더 안전한 응답이 선호된다.</li></ul></li></ul><p>Dataset 의 추가적인 세부 사항은 Appendix C 에 제시되어 있다.</p><p>또한 Stanford Human Preferences dataset 도 탐색하였으나, Appendix J 에서 설명한 length bias 수정 이후 RLHF 와 RLAIF policy 모두 SFT baseline 대비 의미 있는 향상을 보이지 않았다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-llm-labeling">3.2. LLM Labeling<a href="#32-llm-labeling" class="hash-link" aria-label="Direct link to 3.2. LLM Labeling" title="Direct link to 3.2. LLM Labeling">​</a></h2><p>AI labeling 기법을 평가할 때 빠른 실험 반복을 가능하게 하기 위해, 각 preference dataset 의 training split 을 무작위로 downsample 하였다. Summarization task 에 대해서는 human annotator 가 높은 확신으로 한 summary 를 다른 것보다 선호한 예시만 포함하도록 추가적인 filtering 을 수행하였다. Downsampling 및 filtering 이후, 각 task 에 대해 약 3–4k 개의 예시가 남았다. AI labeler alignment 는 이 downsampled dataset 을 기반으로 계산되었다.</p><ul><li>Preference labeling 에는 PaLM 2 model family 를 사용하였다. </li><li>모든 버전은 instruction-tuning 은 되어 있지만 RL 로 학습된 적은 없다. </li><li>별도로 명시되지 않는 한, AI label 은 각 task 에 대해 Sec. 4.4 에 기술된 최적의 prompt 를 사용하여 PaLM 2 Large (L) 으로 생성하였다. </li></ul><p>LLM labeling 에 대한 자세한 내용은 Appendix D 에 제시되어 있다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-model-training">3.3. Model Training<a href="#33-model-training" class="hash-link" aria-label="Direct link to 3.3. Model Training" title="Direct link to 3.3. Model Training">​</a></h2><p>모든 SFT model 은 PaLM 2 Extra-Small (XS) 로부터 초기화된다. Summarization task 에서는 PaLM 2 XS 를 Reddit TL;DR dataset 에 대해 fine-tuning 하여 SFT model 을 생성한다. 다른 모든 task 에 대해서는 task-specific fine-tuning 대신 instruction-tuned variant 의 PaLM 2 를 사용한다.</p><p>모든 RM 은 PaLM 2 XS checkpoint 로부터 학습된다. 각 RM 은 preference dataset 의 full training split 에 대해 fine-tuning 되며, AI feedback RM 의 경우 label 은 AI preference, human feedback RM 의 경우 label 은 human preference 이다. RM 의 정확도는 Appendix G 에 제시되어 있다.</p><p>RL 단계에서는, language modeling domain 에 맞게 수정된 <em>REINFORCE</em> 알고리즘을 사용하여 policy 를 학습한다 (Appendix E 참조). 최근 연구에서는 <em>Proximal Policy Optimization (PPO)</em> 을 주로 사용하지만, 본 연구에서는 단순하면서도 충분히 효과적인 REINFORCE with baseline 을 사용한다.</p><p>Policy 와 value model 모두 SFT model 로부터 초기화된다. Summarization task 에서는 Reddit TL;DR dataset 의 training split 을 사용하여 policy rollout 을 수행하며, 각 trajectory 의 initial state 는 Reddit post 자체이다. Helpful 및 harmless task 의 경우, initial state 는 해당 preference dataset 의 training split 에서 샘플링된다.</p><p>Summarization task 에 대해서는 RL 로 학습된 policy 가 생성한 response 에 간단한 post-processing 을 적용하며, 이는 Appendix H 에 설명되어 있다.</p><p>Model training 의 추가 세부 사항은 Appendix F 를 참조한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-human-evaluation">3.4. Human Evaluation<a href="#34-human-evaluation" class="hash-link" aria-label="Direct link to 3.4. Human Evaluation" title="Direct link to 3.4. Human Evaluation">​</a></h2><p>Win rate 을 측정하기 위해, 평가자에게 input context 와 서로 다른 policy (e.g., RLAIF, RLHF, SFT) 가 생성한 여러 response 가 제시된다. 평가자는 Fig. 5 와 같이, 동률 없이 응답의 품질 순서를 매기도록 요청받는다. Input context 는 각 dataset 의 test split 에서 추출되며, 이는 training 또는 다른 평가 단계에서 사용되지 않는다. 이렇게 얻은 ranking 을 바탕으로 policy 간의 win rate 을 계산한다.</p><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-28-86e9ef8edf0bb63dfcce9f1206d6322d.png" width="3833" height="1696" class="img_ev3q"></p><p>Harmless dialogue generation task 의 경우, 평가자들은 각 response 를 독립적으로 “harmless” 또는 “harmful” 로 평가한다.</p><p>Human evaluation 에 대한 세부 사항은 Appendix I 에 제시되어 있다.</p><h1>4. Results</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-rlaif-vs-rlhf">4.1. RLAIF vs. RLHF<a href="#41-rlaif-vs-rlhf" class="hash-link" aria-label="Direct link to 4.1. RLAIF vs. RLHF" title="Direct link to 4.1. RLAIF vs. RLHF">​</a></h2><p>RLAIF 는 세 가지 task 모두에서 RLHF 와 동등하거나 더 나은 성능 향상을 달성한다 (Fig. 1 및 Tab. 1 참조). 구체적으로, summarization task 에서 human evaluator 들은 baseline SFT policy 보다 RLAIF 와 RLHF 를 각각 71%, 73% 의 비율로 선호하였다. Helpful dialogue generation task 에서는 각각 63%, 64% 의 비율로 선호되었다. RLAIF vs. SFT 와 RLHF vs. SFT 간의 win rate 차이는 통계적으로 유의하지 않았다. RLAIF 와 RLHF 를 직접 비교했을 때, 두 policy 간 선호도는 동일하게 나타났으며, 즉 win rate 은 50% 와 통계적으로 유의미한 차이를 보이지 않았다.</p><p>Harmless dialogue generation task 에서 RLAIF 는 88% 의 harmless rate 을 달성하며, RLHF (76%) 와 SFT (64%) 모두를 능가했다.</p><p>Fig. 6 은 SFT, RLAIF, RLHF 로 생성된 summary 예시를 보여준다. RLAIF 와 RLHF 의 차이를 더 깊이 이해하기 위해, 저자는 Sec. 5 에서 summarization task 에 대해 두 policy 가 생성한 응답을 질적으로 비교하였다.</p><p><img loading="lazy" alt="Figure 6" src="/assets/images/image-29-023c4e1bced3fe662e2dfc4e0622f852.png" width="3833" height="1919" class="img_ev3q"></p><ul><li>Stiennon et al. 의 연구와 유사하게, RLAIF 와 RLHF policy 는 SFT policy 보다 더 긴 응답을 생성하는 경향이 있음을 관찰하였다. <ul><li>이는 human evaluation 에 bias 를 유발할 가능성이 있다. Length 를 통제하기 위한 사후 분석을 수행한 결과, 두 policy 는 여전히 SFT policy 보다 우수한 성능을 보였다 (Appendix J 참조).</li></ul></li><li>또 하나의 자연스러운 질문은 human feedback 과 AI feedback 을 결합하는 것이 추가적인 가치를 가지는가이다. <ul><li>저자는 두 종류의 feedback 을 결합한 실험을 수행했으나, human feedback 단독 사용을 넘어서는 개선은 관찰되지 않았다. </li><li>다만, 두 feedback 형태를 결합하여 더 나은 성과를 낼 수 있는 대안적 학습 구조가 존재할 가능성이 있으며, 이에 대한 논의는 Appendix K 에 제시되어 있다.</li></ul></li><li>이러한 결과는 RLAIF 가 human annotation 에 의존하지 않는 RLHF 의 실질적 대안이 될 수 있음을 보여준다. <ul><li>Label 수집 시간을 단축할 뿐 아니라, AI labeling 의 또 다른 장점은 <em>비용 절감</em>이다. </li><li>저자는 LLM labeling 비용이 human annotation 대비 <em>10 배 이상 저렴</em>하다고 추정한다 (상세 분석은 Appendix L 참조).</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-towards-self-improvement">4.2. Towards Self-Improvement<a href="#42-towards-self-improvement" class="hash-link" aria-label="Direct link to 4.2. Towards Self-Improvement" title="Direct link to 4.2. Towards Self-Improvement">​</a></h2><p>Sec. 4.1 의 실험에서는 preference labeling 에 사용된 LLM (PaLM 2 L) 이 학습되는 policy (PaLM 2 XS) 보다 훨씬 크다. 여기서 한 단계 더 나아가, 저자는 AI labeler 가 policy 와 동일한 크기일 때도 RLAIF 가 성능 향상을 달성할 수 있는지를 탐구한다.</p><p>Summarization task 에 대해, 저자는 PaLM 2 L 대신 PaLM 2 XS 를 AI labeler 로 사용하여 RLAIF 를 수행하였다. 나머지 setup 은 Sec. 4.1 의 실험과 동일하게 유지되었다. 이를 “same-size RLAIF” 라고 부른다.</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-25-89f0c35b75d6a7d8a81b591195d726fd.png" width="4084" height="1251" class="img_ev3q"></p><ul><li>Same-size RLAIF 는 여전히 SFT baseline 대비 상당한 개선을 보였으며, human evaluator 들은 same-size RLAIF 를 SFT 보다 68% 의 비율로 선호하였다 (Tab. 1 참조). <ul><li>참고로, policy 보다 큰 AI labeler 를 사용하는 RLAIF 는 71% 의 비율로 SFT 보다 선호되었다. </li><li>이 결과는 AI labeler 가 policy LLM 과 동일한 크기일 때에도 RLAIF 가 성능 향상을 이끌 수 있음을 보여준다.</li></ul></li><li>다만, 이 실험은 “strict self-improvement” 의 예시는 아니다. AI labeler 는 instruction-tuned PaLM 2 XS 인 반면, initial policy 는 Reddit TL;DR summarization dataset 에 대해 fine-tuning 된 PaLM 2 XS 이기 때문이다. <ul><li>그러나 저자는 다음 절에서 helpfulness task 에 대해 strict self-improvement 의 사례를 제시한다.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-d-rlaif">4.3. D-RLAIF<a href="#43-d-rlaif" class="hash-link" aria-label="Direct link to 4.3. D-RLAIF" title="Direct link to 4.3. D-RLAIF">​</a></h2><p>Sec. 4.1 및 4.2 에서는 AI feedback 을 reward model (RM) 에 distillation 하였다. 본 절에서는 summarization 과 helpfulness task 에 대해 d-RLAIF (Sec. 2.2.2 참조) 를 실험한다. 연산 비용을 줄이기 위해, 저자는 AI labeler 로 instruction-tuned PaLM 2 XS 를 사용한다.</p><ul><li>Summarization task 에서 human annotator 는 d-RLAIF 를 SFT 보다 74% 의 비율로 선호하였다 (Tab. 1 참조). </li><li>LLM feedback 을 직접 활용하는 것과 RM 으로 distillation 하는 것의 차이를 이해하기 위해, 저자는 Sec. 4.2 의 same-size RLAIF 와 결과를 비교한다. </li><li>두 실험은 reward function 을 제외한 모든 설정이 동일하다. d-RLAIF 는 same-size RLAIF (68%) 보다 통계적으로 유의하게 높은 win rate 을 보였다. </li><li>또한 두 결과를 나란히 제시했을 때, annotator 들은 d-RLAIF 를 same-size RLAIF 보다 60% 의 비율로 선호하였다.</li></ul><p>저자는 이러한 개선이 RM 으로 preference 를 distillation 하지 않고 AI labeler 로부터 직접 preference 를 질의함으로써, 그리고 Sec. 2.2.2 에서 설명한 “staleness” 문제를 회피함으로써 이루어진 것으로 가설을 세운다.</p><ul><li>Helpful dialogue generation task 에서 d-RLAIF 는 SFT baseline 대비 66% 의 win rate 을 기록하였다. </li><li>특히, feedback 을 제공하는 LLM 과 initial policy 가 동일한 model checkpoint 이므로, 이는 strict LLM self-improvement 의 명확한 사례를 구성한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="44-prompting-techniques">4.4. Prompting Techniques<a href="#44-prompting-techniques" class="hash-link" aria-label="Direct link to 4.4. Prompting Techniques" title="Direct link to 4.4. Prompting Techniques">​</a></h2><p><img loading="lazy" alt="Table 2" src="/assets/images/image-26-419974547dfc83e5f4907a0559a924db.png" width="2037" height="1863" class="img_ev3q"></p><ul><li>저자는 preamble specificity, chain-of-thought (CoT) reasoning, in-context learning 의 세 가지 prompting variation 을 실험하였다 (Tab. 2 참조). </li><li>Base prompt (“Base 0-shot”) 대비 최적의 prompt 는 summarization, helpfulness, harmlessness task 에서 각각 +1.9%, +1.3%, +1.7% 의 AI labeler alignment 향상을 보였다.</li><li>Detailed preamble 은 summarization 에서는 alignment 를 개선하지만, helpful 및 harmless dialogue generation 에서는 혼합된 결과를 보였다. <ul><li>저자는 summarization task 가 상대적으로 복잡하기 때문에 detailed preamble 의 효과가 크다고 가정한다. </li><li>반면 helpfulness 와 harmlessness 평가는 상대적으로 명확한 기준을 가지므로, 세부 지시문의 이점이 적을 수 있다.</li></ul></li><li>Chain-of-thought reasoning 은 전반적으로 alignment 를 향상시켰다. <ul><li>Summarization task 에서는 일관된 개선을 보였으며, helpful 및 harmless dialogue generation task 에서는 “Base” preamble 과 결합될 때만 alignment 향상을 보였다.</li></ul></li><li>흥미롭게도, in-context learning 은 harmless dialogue generation 에서만 alignment 를 향상시켰다. <ul><li>Summarization 과 helpfulness task 에서는 exemplar 수가 증가할수록 alignment 가 점진적으로 감소하였다. </li><li>Exemplar 선택이 부적절했을 가능성을 배제하기 위해, summarization task 에서 “Base 1-shot” 설정으로 10 회의 실험을 수행하였으며, </li><li>각 실험마다 서로 다른 exemplar 를 무작위로 선택하였다. 모든 실험 중 최대 AI labeler alignment 는 76.1% 로, 여전히 “Base 0-shot” 보다 낮았다.</li></ul></li><li>저자는 summarization 과 helpful dialogue generation task 가 이미 off-the-shelf AI labeler 에 의해 충분히 이해되고 있기 때문에 exemplar 가 도움이 되지 않거나 오히려 혼란을 줄 수 있다고 가정한다. <ul><li>또한 in-context learning 자체는 여전히 완전히 이해되지 않은 중요한 연구 영역이라는 점을 지적한다.</li></ul></li></ul><p>Summarization task 에 대해, 저자는 human inter-annotator agreement 와 비교하여 LLM labeler 의 절대적 성능을 평가하였다. Stiennon et al. 은 OpenAI human preference dataset 의 annotator 간 일치율을 73–77% 로 추정했으며, 저자의 off-the-shelf LLM 은 78% alignment 를 달성하여 절대적 기준에서도 우수한 성능을 보였다.</p><p>또한 저자는 self-consistency 를 실험하였다. 여기서는 temperature <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">T &gt; 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span></span> 조건에서 여러 개의 CoT rationale 을 sampling 하고, LLM 이 생성한 preference distribution 들을 평균하여 최종 label 을 산출한다. 그러나 self-consistency 는 오히려 AI labeler alignment 를 악화시키는 것으로 나타났다 (Appendix M 참조).</p><p>저자는 AI labeler alignment 가 RLAIF policy 의 성능 향상에 직접적인 영향을 미친다고 가정한다. 이를 검증하기 위해, alignment score 가 서로 다른 두 AI label set 을 사용하여 두 개의 RLAIF policy 를 학습하였다. 결과적으로, alignment 가 높은 AI label 로 학습된 policy 가 유의미하게 높은 win rate 을 기록하였으며, 이는 저자의 가설과 일치한다. 다만, 본 연구에서는 두 policy 만 비교하였으므로, 확정적 결론을 도출하기 위해서는 더 체계적인 실험이 필요하다 (Appendix N 참조).</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="45-size-of-llm-labeler">4.5. Size of LLM Labeler<a href="#45-size-of-llm-labeler" class="hash-link" aria-label="Direct link to 4.5. Size of LLM Labeler" title="Direct link to 4.5. Size of LLM Labeler">​</a></h2><p><img loading="lazy" alt="Table 3" src="/assets/images/image-27-fe5f291c7c535f45f082bef6fdbed9e5.png" width="2016" height="720" class="img_ev3q"></p><ul><li>대형 model 은 접근성이 낮고 실행 비용이 높다. Summarization task 에서 저자는 LLM 의 크기를 달리하여 preference labeling 을 수행하였으며, model size 와 alignment 간의 강한 양의 상관관계를 관찰하였다 (Tab. 3 참조).</li><li>PaLM 2 L 을 PaLM 2 S 로 대체할 경우 alignment 가 4% 감소하였고, PaLM 2 XS 를 사용할 경우 추가로 11% 감소하였다. 이러한 추세는 다른 연구에서 관찰된 scaling behavior 와 일치한다 (Kaplan et al., 2020).<ul><li>이러한 경향의 한 요인은 단순한 model capability 의 감소 외에도, smaller LLM 이 larger position bias 를 보이는 경향이 있기 때문일 수 있다 (Appendix B 참조).</li></ul></li><li>또 다른 관점에서 보면, AI labeler 의 크기를 확장함으로써 더 높은 품질의 preference label 을 생성할 수 있음을 시사한다. <ul><li>Canonical RLAIF 의 경우, AI labeler 는 한 번만 preference 예시 생성을 위해 사용되고 RL 학습 중에는 호출되지 않기 때문에, larger labeler 를 사용하는 것이 반드시 과도하게 비효율적이지는 않다.</li></ul></li></ul><h1>5. Qualitative Observations</h1><p>RLAIF 와 RLHF 의 차이를 보다 깊이 이해하기 위해, 저자는 summarization task 에서 두 policy 가 생성한 응답을 시각적으로 검토하였다. 두 policy 가 생성한 summary 는 대부분의 경우 유사하였으며, 이는 두 방법의 유사한 win rate 에 반영된다. 그러나 몇 가지 차이가 나타나는 패턴이 관찰되었다.</p><ol><li>일부 사례에서 RLHF 는 hallucination 을 일으키지만 RLAIF 는 그렇지 않았다. RLHF summary 의 hallucination 은 그럴듯하게 들리지만 원문과 일치하지 않았다. <ul><li>예를 들어, Tab. 23 의 Example #1 에서 RLHF summary 는 작성자가 20세라고 언급하지만, 원문에는 해당 내용이 명시되거나 암시되지 않는다.</li></ul></li><li>RLAIF summary 가 RLHF 보다 덜 유창한 경우도 관찰되었다. <ul><li>Tab. 24 의 예시에서, 세 개의 RLAIF summary 모두 run-on sentence 를 포함하고 있었다. </li><li>또한 RLAIF 응답이 문구를 반복하거나 원문의 의도를 정확히 전달하지 못하는 경우도 있었다. 예를 들어, 일부 summary 는 원문에 그런 질문이 포함되지 않았음에도 불구하고 “How do I get over this?” 라는 문장으로 끝났다.</li></ul></li></ol><p>저자는 70 개의 예시를 대상으로 소규모 평가를 수행하였다. Human annotator 들은 RLHF 와 RLAIF summary 를 accuracy, coverage, coherence 측면에서 블라인드 평가하여 순위를 매겼다. 그러나 두 방법 간의 점수 차이는 통계적으로 유의하지 않았다. 이러한 패턴이 large-scale 에서도 나타나는지를 규명하기 위해서는 보다 체계적인 분석이 필요하며, 이는 향후 연구로 남긴다.</p><h1>6. Related Work</h1><p>LLM 은 다양한 NLP task 에서 인상적인 성능을 보여왔다. 여러 task 에서 RL 은 효과적인 최적화 기법으로 부상하였으며, initial translation 및 summarization 연구에서는 자동 평가 metric 을 reward 로 사용하였다. 그러나 이러한 단순화된 reward formulation 은 인간의 품질 판단과 완전히 일치하지 않았다.</p><p>Human feedback-based RL 은 자연어 응답의 쌍 비교를 통해 reward model 을 학습하여 LLM 을 인간 선호에 직접 정렬시키는 기법으로 발전하였다. 이 접근법은 summarization, instruction following, dialogue, question answering 등 다양한 분야에서 성공적으로 적용되었다.</p><p>RL 의 안정성과 효율성 문제를 완화하기 위해, DPO (Rafailov et al., 2024) 는 학습 objective 를 classification loss 형태로 재정의하였고, RaFT 는 reward model 을 rejection-sampling fine-tuning 에 활용하였다.</p><p>LLM 은 data generation, augmentation, self-training 등 다양한 응용에서도 널리 사용되고 있다. </p><ul><li>Bai et al. (2022b) 은 LLM 과 human label 을 함께 사용하여 helpfulness 와 harmlessness 두 objective 를 공동 최적화하는 RLAIF 개념을 제안하였다. </li><li>이후 연구들은 LLM 으로부터 직접 reward signal 을 생성하는 관련 기법을 탐구하였다. 이러한 연구들은 LLM 이 RL fine-tuning 에 유용한 signal 을 생성할 수 있음을 보여주며, 본 연구의 동기 — 즉, LLM 이 human 대신 preference label 수집에 실질적인 대안이 될 수 있는가 — 에 영감을 주었다.</li></ul><h1>7. Conclusion</h1><p>본 연구는 RLAIF 가 세 가지 text generation task 에서 RLHF 와 비슷한 수준의 성능 향상을 달성함을 보였다. Head-to-head 비교에서도, human evaluator 들은 RLAIF 와 RLHF 를 유사한 비율로 선호하였다.</p><p>또한, LLM self-improvement 의 증거를 제시하였다. 즉, AI labeler 가 policy 와 동일한 크기이거나, 심지어 initial policy 와 동일한 checkpoint 일 때조차 RLAIF 가 효과적으로 작동함을 보였다.</p><p>아울러, LLM 의 reward 를 RL 중 직접 활용하는 direct-RLAIF (d-RLAIF) 를 제안하였으며, 이는 LLM preference 를 별도의 RM 으로 distillation 하는 canonical RLAIF 설정보다 더 나은 성능을 보였다.</p><p>마지막으로, 다양한 AI labeling 기법이 human preference 와의 alignment 에 미치는 영향을 분석하였다.</p><p>RLAIF 는 human feedback 의 비용 및 확장성 한계를 극복할 수 있는 잠재적 대안으로서 큰 가능성을 보여준다. 향후 연구에서는 RLAIF 를 model-based RL 환경으로 확장하여 human 과 assistant 모두를 LLM 으로 모델링하는 방법, 또는 AI feedback 을 세밀한 credit assignment 에 활용하는 방법 등을 탐구할 예정이다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/reinforce-learning">Reinforce Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/rl">RL</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/rlaif">RLAIF</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/ai-feedback">AI Feedback</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/d-rlaif">d-RLAIF</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/direct-rlaif">Direct-RLAIF</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Reinforce Learning/PPO/RLAIF/2023-09-d-RLAIF.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/ALMoST"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Aligning Large Language Models through Synthetic Feedback</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/UltraFeedback"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">UltraFeedback: Boosting Language Models with Scaled AI Feedback</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-preference-labeling-with-llms" class="table-of-contents__link toc-highlight">2.1. Preference Labeling with LLMs</a><ul><li><a href="#211-addressing-position-bias" class="table-of-contents__link toc-highlight">2.1.1. Addressing Position Bias</a></li><li><a href="#212-eliciting-chain-of-thought-reasoning" class="table-of-contents__link toc-highlight">2.1.2. Eliciting Chain-of-Thought Reasoning</a></li></ul></li><li><a href="#22-reinforcement-learning-from-ai-feedback" class="table-of-contents__link toc-highlight">2.2. Reinforcement Learning from AI Feedback</a><ul><li><a href="#221-canonical-rlaif" class="table-of-contents__link toc-highlight">2.2.1. Canonical RLAIF</a></li><li><a href="#222-direct-rlaif-d-rlaif" class="table-of-contents__link toc-highlight">2.2.2. Direct-RLAIF (D-RLAIF)</a></li></ul></li><li><a href="#23-evaluation" class="table-of-contents__link toc-highlight">2.3. Evaluation</a></li><li><a href="#31-datasets" class="table-of-contents__link toc-highlight">3.1. Datasets</a></li><li><a href="#32-llm-labeling" class="table-of-contents__link toc-highlight">3.2. LLM Labeling</a></li><li><a href="#33-model-training" class="table-of-contents__link toc-highlight">3.3. Model Training</a></li><li><a href="#34-human-evaluation" class="table-of-contents__link toc-highlight">3.4. Human Evaluation</a></li><li><a href="#41-rlaif-vs-rlhf" class="table-of-contents__link toc-highlight">4.1. RLAIF vs. RLHF</a></li><li><a href="#42-towards-self-improvement" class="table-of-contents__link toc-highlight">4.2. Towards Self-Improvement</a></li><li><a href="#43-d-rlaif" class="table-of-contents__link toc-highlight">4.3. D-RLAIF</a></li><li><a href="#44-prompting-techniques" class="table-of-contents__link toc-highlight">4.4. Prompting Techniques</a></li><li><a href="#45-size-of-llm-labeler" class="table-of-contents__link toc-highlight">4.5. Size of LLM Labeler</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.3298527c.js"></script>
<script src="/assets/js/main.aca6605f.js"></script>
</body>
</html>