<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Computer Vision/PEFT/Feature-Shift/2022-10-SSF">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Scaling &amp; Shifting Your Features: A New Baseline for Efficient Model Tuning | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Computer Vision/PEFT/Feature-Shift/SSF"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Scaling &amp; Shifting Your Features: A New Baseline for Efficient Model Tuning | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Computer Vision/PEFT/Feature-Shift/SSF"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Computer Vision/PEFT/Feature-Shift/SSF" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Computer Vision/PEFT/Feature-Shift/SSF" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.dc82af9c.js" as="script">
<link rel="preload" href="/assets/js/main.07c26eea.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Adversarial Attack</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Few-shot/AMT">Few-shot</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Generation/ Concept Editing/SLD">Generation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Image Classification/ViT">Image Classification</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Multi-task/Unified Interface">Multi-task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Computer Vision/PEFT/Composition/RLRR">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/PEFT/Composition/RLRR">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Computer Vision/PEFT/Feature-Shift/SSF">Feature-Shift</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Computer Vision/PEFT/Feature-Shift/SSF">Scaling &amp; Shifting Your Features: A New Baseline for Efficient Model Tuning</a></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Computer Vision</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Feature-Shift</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Scaling &amp; Shifting Your Features: A New Baseline for Efficient Model Tuning</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Scaling &amp; Shifting Your Features: A New Baseline for Efficient Model Tuning</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2210.08823" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2210.08823</a></p><h1>Abstract</h1><p>기존 fine-tuning method 는 pre-trained model 의 모든 parameter 를 tuning 하거나 (full fine-tuning), 마지막 linear layer 만 tuning 하는 (linear probing) 방식이 있다. Full fine-tuning 은 효율적이지 않고, linear probing 은 full fine-tuning 에 비해 accuracy 가 크게 떨어진다. </p><p>이 논문에서 저자는 <strong>SSF</strong> 라는 새로운 parameter-efficient fine-tuning method 를 제안한다. </p><ul><li>SSF 는 pre-trained model 에서 추출한 deep feature 를 Scale 하고 Shift 하기만 하면 full fine-tuning 의 성능을 따라잡을 수 있다. 이런 방식으로 SSF 는 tunable parameter 수가 더 적음에도 불구하고 다른 parameter-efficient fine-tuning approach 를 놀랍게도 능가한다. </li><li>게다가 Adapter 나 VPT 같은 기존 parameter-efficient fine-tuning method 가 training 과 inference step 에서 추가 parameter 와 computational cost 를 도입하는 것과 달리, SSF 는 training step 에서만 learnable parameter 를 추가하고, inference step 에서는 re-parameterization 을 통해 이 parameter 를 original pre-trained model weight 에 합칠 수 있다. </li><li>제안된 SSF 를 사용해 저자의 model 은 FGVC 에서 2.46% (90.72% vs. 88.54%), VTAB-1k 에서 11.48% (73.10% vs. 65.57%) 의 Top-1 accuracy 성능 향상을 얻었지만, 약 0.3M parameter 만 fine-tuning 했다. </li><li>저자는 다양한 model family (CNNs, Transformers, MLPs) 와 dataset 에서 많은 실험을 했다. 총 26 개 image classification dataset 과 3 개 robustness &amp; out-of-distribution dataset 에 대한 결과는 SSF 의 효과를 보여준다.</li></ul><h1>1 Introduction</h1><p>Deep learning community 에서 data-driven method 가 인기를 끌면서 dataset 규모와 model 크기가 엄청나게 커졌다. Large model 을 탐구한 뒤 이 pre-trained model 을 downstream task 에 적용해 더 나은 성능과 빠른 convergence 를 얻는 게 점점 일반적인 방식이 되고 있다.</p><p>하지만 현재 절차는 full fine-tuning 에 크게 의존한다. 여기서 model 의 모든 parameter 가 업데이트된다. 이는 필연적으로 model 이 작은 target dataset 에 over-fit 되게 만들어 fine-tuning 후 다른 task 에 사용할 수 없게 한다. 결과적으로 device 는 각 task 마다 dedicated model parameter set 을 저장해야 하고, 특히 오늘날 large model (e.g., ViT-G/14 1.8G, CoAtNet 2.4G) 에서는 엄청난 저장 공간을 차지한다.</p><p>위 문제에 대한 간단한 해결책은 linear probing 이다. 여기선 last head layer 만 fine-tuning 한다. 하지만 이 방식은 full fine-tuning proxy 에 비해 성능이 많이 떨어진다. NLP 분야에서 prompt 를 사용한 parameter-efficient fine-tuning strategy 의 성공에 영감을 받아 최근 연구는 vision task 에 비슷한 proxy 를 구현했다. 이를 Visual Prompt Tuning (VPT) 라고 한다. 구체적으로 VPT 는 learnable prompt 를 input 으로 삽입하고 이를 original image token 에 추가한다. 이 prompt 는 self-attention 을 수행하며 image token 과 상호작용하고, fine-tuning 과정에서 업데이트된다. 이런 방식으로 linear probing proxy 에 비해 downstream task 에서 큰 성능 향상을 얻을 수 있다. 하지만 full fine-tuning 과 linear probing 에 비해 추가로 두 가지 문제가 생긴다:</p><ul><li>VPT 는 task 마다 prompt 수를 tuning 해야 해서 task-dependent learnable parameter space 를 도입한다. Fine-tuning 성능은 각 task 의 prompt 수에 민감하고 신중히 설계해야 한다. Prompt 가 너무 적거나 많으면 fine-tuning accuracy 가 떨어지거나 computation redundancy 가 늘어난다 (e.g., Clevr/count 에 200 prompt vs. Flowers 102 에 1 prompt).</li><li>VPT 와 Adapter-based method 는 original pre-trained model 에 비해 inference step 에서 additional parameter 와 computational cost 를 도입한다. <ul><li>예로, VPT 는 image token 과 함께 self-attention 을 위한 추가 input 을 도입한다. </li><li>Adapter-based method 는 pre-trained model 에 추가 module 을 삽입한다. </li><li>이 methods 는 backbone architecture 나 network 의 input 을 바꾸고, 특히 edge device (e.g., mobile phone) 에 이미 배포된 model 에서 빈번한 구조 수정과 큰 workload 를 초래할 수 있다.</li></ul></li></ul><p>위 문제를 해결하기 위해 저자는 parameter-efficient fine-tuning 을 위한 general proxy 를 찾으려고 한다. 여기서 learnable parameter space 는 unified (task-independent) 하고 additional inference parameter 를 도입하지 않는다. Feature modulation method 에서 영감을 받아 저자는 SSF 라는 새로운 parameter-efficient fine-tuning method 를 제안한다. </p><ul><li>pre-trained model 에서 추출한 deep feature 를 Scale 하고 Shift 하기만 하면 fine-tuning 할 수 있다. </li><li>저자의 직관은 upstream dataset 과 downstream dataset 이 다른 data distribution 을 가진다는 사실에서 온다. 그래서 upstream dataset 에서 training 된 model weight 를 downstream dataset 에 바로 적용하기 어렵다. <ul><li>예로, backbone weight 를 frozen 시킨 naive linear probing strategy 는 성능 저하를 일으킨다. </li></ul></li><li>이 문제를 완화하기 위해 SSF 는 scale parameter 와 shift parameter 를 도입한다. <ul><li>이는 variance 와 mean 으로 간주할 수 있고, upstream dataset 에서 pre-trained model 로 추출한 downstream dataset 의 feature 를 modulate 해서 discriminative space 에 들어가게 한다. </li><li>이 scale parameter 와 shift parameter 는 input 에 의존하지 않고, 다른 task 에 대해 unified learnable parameter space 를 가진다. </li></ul></li><li>SSF 의 또 다른 장점은 추출된 feature 를 scale 하고 shift 하므로 linear transformation 만 도입한다는 점이다. <ul><li>이 linear transformation 은 inference step 에서 model re-parameterization 을 통해 original pre-trained weight 에 합쳐질 수 있어 downstream task 에 additional parameter 와 FLOPs 를 피할 수 있다. </li><li>Edge device 에 배포된 model 에서는 fine-tuning 후 업데이트된 weight 만 업로드하면 되고 backbone architecture 를 바꿀 필요가 없다. </li></ul></li></ul><p>Tab. 1 은 SSF 와 다른 fine-tuning method 간의 구체적인 특성 비교를 보여준다. </p><p>SSF 는 간단하고 효과적이며 효율적이고, Occam&#x27;s Razor 원칙에도 부합한다. 그래서 저자는 이 새로운 baseline 을 탐구했고, SSF 가 다른 parameter-efficient fine-tuning method 를 놀랍게도 능가한다는 걸 발견했다.</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-288e0f13b791e2384ecc3a834cb38ed0.png" width="795" height="636" class="img_ev3q"></p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-1-eb4801c7e02b21734b358fd3c06091db.png" width="861" height="662" class="img_ev3q"></p><p>저자는 26 classification datasets 와 3 robustness &amp; out-of-distribution datasets 에서 저자의 method 를 평가했다. </p><ul><li>SSF 는 trainable parameter 와 accuracy trade-off 에서 다른 parameter-efficient fine-tuning method 에 비해 state-of-the-art 성능을 얻는다 (Tab. 1, Fig. 1). </li><li>Full fine-tuning 에 비해 저자의 method 는 FGVC 에서 2.46% (90.72% vs. 88.54%), VTAB-1k 에서 11.48% (73.10% vs. 65.57%) 의 Top-1 accuracy 성능 향상을 얻었지만 약 0.3M trainable parameter 만 사용했다. 게다가 SSF 는 inference step 에서 additional parameter 를 필요로 하지 않는다. </li><li>Plug-and-play 방식이고 CNNs, Transformers, MLPs 같은 다양한 model family 에 쉽게 확장할 수 있다.</li></ul><h1>2 Related Work</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-model-families">2.1 Model Families<a href="#21-model-families" class="hash-link" aria-label="Direct link to 2.1 Model Families" title="Direct link to 2.1 Model Families">​</a></h2><p>Convolution 은 오랫동안 computer vision task 에서 image feature 를 추출하는 주요 module 로 사용됐고, CNN-based architecture 도 연구됐다. 최근 Transformer 라는 또 다른 architecture family 가 NLP 에서 큰 성공을 거두며 주목받고 있다. 이 방향을 따라 Dosovitskiy 등은 computer vision 영역에서 transformer 를 처음 적용하고 ViT 라는 새로운 architecture paradigm 을 소개해 유망한 결과를 얻었다. </p><p>이후 DeiT, Swin Transformer 같은 다양한 transformer-based model 이 소개됐고, object detection, semantic segmentation, action recognition 같은 다양한 task 에서 효과적임이 입증됐다. 또 다른 방향에서는 Tolstikhin 등이 pure MLP-based architecture 를 제안했고, 후속 논문들은 MLP-based architecture 가 transformer 를 따라잡을 수 있음을 흥미롭게 보여줬다. 하지만 잘 설계된 module 외에도 이들의 뛰어난 성능은 large-scale model 의 deployment 에도 기인한다. </p><p>Large dataset 에서 pre-trained 된 large-scale model 이 주어졌을 때, downstream task 에서 parameter-efficient fine-tuning 을 어떻게 수행할지는 필수적이지만 현재 덜 탐구되고 있다. 이 논문에서 저자는 SSF 를 새로운 baseline 으로 제안하고, 다양한 task 에서 종합적인 validation 을 통해 유망한 성능을 보여준다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-pre-training-and-fine-tuning">2.2 Pre-training and Fine-tuning<a href="#22-pre-training-and-fine-tuning" class="hash-link" aria-label="Direct link to 2.2 Pre-training and Fine-tuning" title="Direct link to 2.2 Pre-training and Fine-tuning">​</a></h2><p>Early model 은 보통 ImageNet-1K dataset 에서 pre-training 된 후 downstream task 에서 fine-tuning 해서 빠른 convergence 나 더 나은 성능을 얻었다. 이런 절차를 pre-training and fine-tuning, 즉 transfer learning 이라고 한다. 최근 연구는 더 나은 성능을 위해 ViT, Swin Transformer V2 같은 larger model 을 사용하고, ImageNet-21K, JFT-300M 같은 larger dataset 에서 training 한다. </p><p>NLP 와 computer vision 영역 모두에서 이 large model 은 small-scale model 에 비해 엄청난 성능 향상을 얻고 downstream task 에 pre-trained weight 를 제공한다. 일부 연구는 pre-trained model 을 target task 에 효율적으로 fine-tuning 하는 방법을 탐구한다. </p><ul><li>SpotTune 은 어떤 layer 를 fine-tuning 해야 하는지 조사한다. </li><li>Touvron et al 은 attention layer 의 weight 를 fine-tuning 하고 다른 부분의 weight 를 frozen 시키는 것만으로 vision transformer 를 downstream task 에 적응시키기에 충분하다고 발견했다. </li></ul><p>일부 연구는 network 에 adapter 를 삽입해 parameter-efficient 방식으로 fine-tuning 을 제안한다. </p><ul><li>이 adapter 는 small non-linear network, model weight 를 생성하는 hyper-network, 또는 parameter 를 줄이기 위해 low-rank decomposition 을 수행하는 compactor 일 수 있다. </li><li>일부 연구는 bias term 만 업데이트하려고 시도했다. </li><li>최근 VPT 는 소수의 learnable parameter (prompt) 를 삽입하고 backbone 을 frozen 시킨 채 이를 최적화해 full fine-tuning 에 비해 큰 성능 향상을 얻었다. </li></ul><p>이 논문 제출 중에 adapter module 삽입이나 neural prompt search 같은 parameter-efficient fine-tuning method 도 제안됐다. 위 모든 연구와 달리 저자는 pre-trained model 에서 추출한 deep feature 를 scale 하고 shift 하는 간단하지만 효과적인 방법을 제안하고, 다른 parameter-efficient fine-tuning method 를 능가한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-feature-modulation">2.3 Feature Modulation<a href="#23-feature-modulation" class="hash-link" aria-label="Direct link to 2.3 Feature Modulation" title="Direct link to 2.3 Feature Modulation">​</a></h2><p>많은 연구가 더 나은 성능을 얻기 위해 feature 를 modulate 하려고 시도했다. </p><p>저자의 연구와 가장 관련 있는 건 다양한 normalization method 다. Batch Normalization (BN), Layer Normalization (LN), Group Normalization (GN) 은 보통 feature 를 normalize 한 뒤 scale factor 와 shift factor 로 linear transformation 을 수행해 feature distribution 을 modulate 한다. 이는 많은 task 에서 효과적임이 입증됐다. </p><p>Spatial Transformer Network (STN) 은 feature map 을 spatially transform 하는 learnable module 을 도입한다. Image generation 분야에서 Adaptive Instance Normalization (AdaIN) 은 특정 image style 을 characterize 하기 위해 scale factor 와 shift factor 를 생성한다. Self-modulation 은 GAN 이 generator 의 self-modulation layer 에서 이익을 얻는다는 걸 보여준다. </p><p>Vision-language task 에서 Conditional Batch Normalization (Conditional BN) 과 Feature-wise Linear Modulation (FiLM) 은 두 modality 의 feature 를 modulate 하는 데 자주 사용된다. BN 같은 일부 algorithm 과 달리 저자의 SSF 는 normalization layer 의 modulation 에 국한되지 않고, upstream task 와 downstream task 간 distribution mismatch 를 완화하려는 다른 motivation 을 가진다. </p><p>비교를 위해 Sec. 4.3 에서 실험을 했고, SSF 가 normalization layer 만 tuning 하는 것보다 더 효과적임을 보여줬다. STN, AdaIN, FiLM 등과 비교해 저자의 method 는 input-independent 하고, scale parameter 와 shift parameter 는 전체 dataset 의 distribution 을 model 하므로 inference step 에서 original pre-trained model weight 에 흡수될 수 있다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="24-model-re-parameterization">2.4 Model Re-parameterization<a href="#24-model-re-parameterization" class="hash-link" aria-label="Direct link to 2.4 Model Re-parameterization" title="Direct link to 2.4 Model Re-parameterization">​</a></h2><p>Model re-parameterization 은 inference efficiency 를 높이기 위해 흔히 사용되는 방식이다. </p><p>대표적인 기술 중 하나는 model compression algorithm 에서 사용되는 batch normalization folding 이다. Batch normalization layer 에서 도입된 parameter 는 보통 그 앞에 쌓인 convolutional layer 에 합쳐진다. 이 기술은 network 의 다른 branch 를 새로운 branch 로 합치는 데도 사용된다. 마찬가지로 저자의 SSF 는 linear transformation 을 완전히 채택해 training step 의 scale parameter 와 shift parameter 를 inference step 에서 original pre-trained model weight 에 합칠 수 있어 additional parameter 와 computational cost 도입을 피한다.</p><h1>3 Approach</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-preliminaries">3.1 Preliminaries<a href="#31-preliminaries" class="hash-link" aria-label="Direct link to 3.1 Preliminaries" title="Direct link to 3.1 Preliminaries">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="transformers">Transformers.<a href="#transformers" class="hash-link" aria-label="Direct link to Transformers." title="Direct link to Transformers.">​</a></h4><p>Vision transformer (ViT) 에서 RGB image <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>3</mn><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding="application/x-tex">I \in \mathbb{R}^{3 \times H \times W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em">W</span></span></span></span></span></span></span></span></span></span></span></span></span> 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N \times N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> non-overlapping patch 로 나뉜다. 그리고 이 image patch 들에 class token 을 붙인 뒤 embedding layer 를 거쳐 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span></span>-layer vision transformer block 에 넣는다. 여기서 self-attention 이 핵심 operation 이다. Input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mrow><mo fence="true">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mn>1</mn><mo fence="true">)</mo></mrow><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^{\left(N^2+1\right) \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1078em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1078em"><span style="top:-3.4103em;margin-right:0.05em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">)</span></span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span> (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 는 embedding dimension) 은 먼저 key <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mrow><mo fence="true">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mn>1</mn><mo fence="true">)</mo></mrow><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">K \in \mathbb{R}^{\left(N^2+1\right) \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1078em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1078em"><span style="top:-3.4103em;margin-right:0.05em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">)</span></span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span>, value <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mrow><mo fence="true">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mn>1</mn><mo fence="true">)</mo></mrow><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">V \in \mathbb{R}^{\left(N^2+1\right) \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1078em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1078em"><span style="top:-3.4103em;margin-right:0.05em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">)</span></span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span>, query <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mrow><mo fence="true">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mn>1</mn><mo fence="true">)</mo></mrow><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Q \in \mathbb{R}^{\left(N^2+1\right) \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1078em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1078em"><span style="top:-3.4103em;margin-right:0.05em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">)</span></span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span> 로 변환된다. 그 후 global self-attention 은 다음 식으로 계산된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="normal">Softmax</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><mi>d</mi></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \text{Attention}(Q, K, V) = \operatorname{Softmax}\left(\frac{Q K^T}{\sqrt{d}}\right) V \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4684em;vertical-align:-0.9842em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4842em"><span style="top:-3.4842em"><span class="pstrut" style="height:3.5183em"></span><span class="mord"><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mop"><span class="mord mathrm">Softmax</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em"><span style="top:-2.1778em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord mathnormal">d</span></span></span><span style="top:-2.8922em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1078em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.22222em">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9842em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4842em"><span style="top:-3.4842em"><span class="pstrut" style="height:3.5183em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9842em"><span></span></span></span></span></span></span></span></span></div><p>Attention layer 의 output 은 channel dimension 에서 정보 추출을 위해 two-layer MLP 에 들어간다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="adapter">Adapter.<a href="#adapter" class="hash-link" aria-label="Direct link to Adapter." title="Direct link to Adapter.">​</a></h4><p>Adapter 는 efficient fine-tuning 을 위해 transformer layer 에 삽입된다. 이는 trainable parameter 수가 적은 bottleneck module 로, feature dimension 을 줄이는 down-projection, non-linear activation function, 원래 dimension 으로 되돌리는 up-projection 을 포함한다. 그래서 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mrow><mo fence="true">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mn>1</mn><mo fence="true">)</mo></mrow><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^{\left(N^2+1\right) \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1078em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1078em"><span style="top:-3.4103em;margin-right:0.05em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">)</span></span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span> 가 주어지면 output 은 다음과 같이 계산된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>out</mtext><mo>=</mo><msup><mrow><mo fence="true">[</mo><msup><mi>W</mi><mtext>up</mtext></msup><mi>ϕ</mi><mrow><mo fence="true">(</mo><msup><mi>W</mi><mtext>down</mtext></msup><msup><mi>x</mi><mi>T</mi></msup><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow><mi>T</mi></msup></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \text{out} = \left[W^{\text{up}} \phi\left(W^{\text{down}} x^T\right)\right]^T \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4903em;vertical-align:-0.4952em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9952em"><span style="top:-2.9952em"><span class="pstrut" style="height:3.1303em"></span><span class="mord"><span class="mord text"><span class="mord">out</span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size1">[</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">up</span></span></span></span></span></span></span></span></span></span><span class="mord mathnormal">ϕ</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">down</span></span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size1">)</span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size1">]</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1303em"><span style="top:-3.352em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4952em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9952em"><span style="top:-2.9952em"><span class="pstrut" style="height:3.1303em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4952em"><span></span></span></span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mtext>down</mtext></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msup><mi>d</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W^{\text{down}} \in \mathbb{R}^{d&#x27; \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8882em;vertical-align:-0.0391em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">down</span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.9425em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9425em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span> (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>≪</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">d&#x27; \ll d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.791em;vertical-align:-0.0391em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span>), <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ϕ</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mtext>up</mtext></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><msup><mi>d</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow></msup></mrow><annotation encoding="application/x-tex">W^{\text{up}} \in \mathbb{R}^{d \times d&#x27;}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">up</span></span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.9425em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9425em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 는 각각 down-projection matrix, non-linear function, up-projection matrix</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="vpt">VPT.<a href="#vpt" class="hash-link" aria-label="Direct link to VPT." title="Direct link to VPT.">​</a></h4><p>VPT 는 embedding layer 뒤 input space 에 learnable parameter (prompt) 를 삽입한다. 이 prompt 는 self-attention 을 수행하며 original image token 과 상호작용한다. Fine-tuning 중에는 backbone network 의 weight 를 frozen 시키고 prompt 의 parameter 만 업데이트한다. </p><p>VPT-Shallow 는 first layer 에 prompt 를 삽입하고, VPT-Deep 은 transformer 의 all layers 에 삽입한다. Input 이 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mrow><mo fence="true">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mn>1</mn><mo fence="true">)</mo></mrow><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^{\left(N^2+1\right) \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1078em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1078em"><span style="top:-3.4103em;margin-right:0.05em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">)</span></span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span> 라고 가정하고, 삽입된 prompt 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">p \in \mathbb{R}^{n \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span> (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> 은 prompt 수) 라고 하면, 합쳐진 token <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">x&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> 는 다음과 같다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mo stretchy="false">[</mo><mi>x</mi><mo separator="true">;</mo><mi>p</mi><mo stretchy="false">]</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} x&#x27; = [x; p] \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mopen">[</span><span class="mord mathnormal">x</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">p</span><span class="mclose">]</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-2.85em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mrow><mo fence="true">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo fence="true">)</mo></mrow><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">x&#x27; \in \mathbb{R}^{\left(N^2+n+1\right) \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.791em;vertical-align:-0.0391em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1078em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1078em"><span style="top:-3.4103em;margin-right:0.05em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">n</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">)</span></span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span> 는 self-attention (Eq. (1)) 을 위해 transformer block 에 들어간다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-scaling-and-shifting-your-features-for-fine-tuning">3.2 Scaling and Shifting Your Features for Fine-tuning<a href="#32-scaling-and-shifting-your-features-for-fine-tuning" class="hash-link" aria-label="Direct link to 3.2 Scaling and Shifting Your Features for Fine-tuning" title="Direct link to 3.2 Scaling and Shifting Your Features for Fine-tuning">​</a></h2><p>위 방법들과 달리 저자는 scale factor 와 shift factor 를 도입해 pre-trained model 이 추출한 deep feature 를 linear transformation 으로 modulate 해서 target dataset 의 distribution 에 맞춘다. 저자의 method 는 다음 다섯 가지 주요 특성을 가진다:</p><ul><li>SSF 는 full fine-tuning strategy 와 비슷한 성능을 낸다.</li><li>모든 downstream task 는 다른 task 에 의존하지 않고 독립적으로 model 에 입력될 수 있다.</li><li>Model 은 아주 적은 parameter 만 fine-tuning 하면 된다.</li><li>VPT 와 달리, SSF 에서 fine-tuning 하는 parameter set 은 task 가 바뀌어도 변하지 않아 multi-task learning 이나 continuous learning 을 위해 나중에 parameter 를 추가로 fine-tuning 하는 게 가능하다.</li><li>Linear transformation 덕분에 SSF 는 inference step 에서 추가 parameter 와 computational cost 를 도입하지 않아 zero overhead 를 만든다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-design-of-ssf">The design of SSF.<a href="#the-design-of-ssf" class="hash-link" aria-label="Direct link to The design of SSF." title="Direct link to The design of SSF.">​</a></h4><p>SSF 는 parameter-efficient fine-tuning 을 위해 linear transformation 을 수행해 feature 를 modulate 한다 (Fig. 2).</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-2-0a99e668353c17dcac604bb97b1ec510.png" width="758" height="767" class="img_ev3q"></p><ul><li>Fig. 2(a) 에서 upstream task 에서 pre-trained 된 model 이 주어지면, 저자는 network 의 각 operation (OP) 뒤에 SSF-ADA 를 삽입해 feature 를 modulate 한다. <ul><li>총 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> OP 가 있고, 이 operation 은 multi-head self-attention (MSA), MLP, layer normalization (LN) 등을 포함할 수 있다. </li><li>Fine-tuning 중에는 이 operation 의 pre-trained weight 를 frozen 시키고 SSF-ADA parameter 만 업데이트한다. </li></ul></li><li>SSF-ADA 의 구체적인 구조는 Fig. 2(c) 에 나와 있다. <ul><li>이전 operation 에서 나온 feature 는 scale factor 와 dot product 를 수행한 뒤 shift factor 를 더한다. 이는 input-independent 하다. </li><li>공식적으로, input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mrow><mo fence="true">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mn>1</mn><mo fence="true">)</mo></mrow><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^{\left(N^2+1\right) \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1078em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1078em"><span style="top:-3.4103em;margin-right:0.05em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">)</span></span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span> 가 주어지면 output <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mrow><mo fence="true">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mn>1</mn><mo fence="true">)</mo></mrow><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">y \in \mathbb{R}^{\left(N^2+1\right) \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1078em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.1078em"><span style="top:-3.4103em;margin-right:0.05em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">(</span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose sizing reset-size3 size6 mtight delimcenter" style="top:0.075em"><span class="mtight">)</span></span></span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span> (다음 operation 의 input) 은 다음과 같이 계산된다:</li></ul></li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>y</mi><mo>=</mo><mi>γ</mi><mo>⊙</mo><mi>x</mi><mo>+</mo><mi>β</mi></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} y = \gamma \odot x + \beta \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-2.85em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span></span></div><p>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\gamma \in \mathbb{R}^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\beta \in \mathbb{R}^d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span> 는 각각 scale factor 와 shift factor 이다. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊙</mo></mrow><annotation encoding="application/x-tex">\odot</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord">⊙</span></span></span></span></span> 는 dot product 이다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="re-parameterization">Re-parameterization.<a href="#re-parameterization" class="hash-link" aria-label="Direct link to Re-parameterization." title="Direct link to Re-parameterization.">​</a></h4><p>SSF-ADA 는 완전한 linear transformation 이므로 scale term 과 shift term 을 이전 linear layer 에 흡수해 re-parameterization 할 수 있다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>y</mi><mo>=</mo><mi>γ</mi><mo>⊙</mo><mi>x</mi><mo>+</mo><mi>β</mi><mo>=</mo><mi>γ</mi><mo>⊙</mo><mo stretchy="false">(</mo><mi>w</mi><mo>∗</mo><mi>t</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>+</mo><mi>β</mi><mo>=</mo><mo stretchy="false">(</mo><mi>γ</mi><mo>⊙</mo><mi>w</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>t</mi><mo>+</mo><mi>γ</mi><mo>⊙</mo><mi>b</mi><mo>+</mo><mi>β</mi></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} y = \gamma \odot x + \beta = \gamma \odot (w \ast t + b) + \beta = (\gamma \odot w) \ast t + \gamma \odot b + \beta \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-2.85em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span> 는 각각 weight 와 bias term 이다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo></mrow><annotation encoding="application/x-tex">\ast</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4653em"></span><span class="mord">∗</span></span></span></span></span> 는 convolutional layer 에서 &#x27;convolution&#x27; operation 이거나 MLP layer 에서 &#x27;multiplication&#x27; operation 이다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span> 는 이전 linear layer 의 input 이다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span> 는 frozen 상태이고 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> 는 fine-tuning 중 업데이트되므로, 위 식을 통해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> 는 inference step 에서 original parameter space (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span>) 에 합쳐질 수 있다. </li></ul><p>이런 관점에서 SSF-ADA 는 추가 parameter 와 computational cost 없이 downstream task 를 수행할 수 있게 한다 (Fig. 2(b)).</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="discussion">Discussion.<a href="#discussion" class="hash-link" aria-label="Direct link to Discussion." title="Direct link to Discussion.">​</a></h4><p>첫 번째 질문은 왜 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> 를 input-independent 하게 만들었냐는 거다. FiLM 과 AdaIN 이 보여주듯, image sample 에 따라 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> 를 얻을 수 있지만, 이는 두 가지 단점을 낳는다:</p><ul><li>저자는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> 가 input-independent 해서 전체 downstream dataset 의 distribution 을 나타내길 원한다. 그래야 feature 를 modulate 해서 downstream dataset 에 맞게 이전 weight distribution 을 수정할 수 있다.</li><li>Conditional input 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> 를 생성하기 위해 추가 network (e.g., MLP) 를 도입해야 해서 trainable parameter 를 늘린다. 더 중요한 건, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> 를 더 잘 생성하기 위해 non-linear activation function 이 필요할 수 있는데, 이는 re-parameterization 을 어렵게 만든다.</li></ul><p>그래서 저자는 완전한 linear transformation 을 수행해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05556em">γ</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> factor 를 original pre-trained weight 에 합쳐서 edge device 에 backbone architecture 를 수정하지 않고 weight 만 쉽게 업로드할 수 있게 한다.</p><hr><p>두 번째 질문은 어떤 operation 뒤에 SSF-ADA 를 삽입해야 하냐는 거다. 저자의 경험으로는 ViT 에서 linear coefficient 가 있는 각 operation 뒤에 SSF-ADA 를 삽입하면 된다. Neural Architecture Search (NAS) 를 통해 최적의 layer 나 operation 을 찾을 수도 있지만, trainable parameter 수를 줄이기 위해 저자의 method 는 너무 많은 trainable parameter 를 도입하지 않고도 (NAS 보다 나쁘지 않은) 더 나은 결과를 낼 거라고 믿는다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-complexity-analysis">3.3 Complexity Analysis<a href="#33-complexity-analysis" class="hash-link" aria-label="Direct link to 3.3 Complexity Analysis" title="Direct link to 3.3 Complexity Analysis">​</a></h2><p>Adapter, VPT, SSF 의 complexity 를 비교해보자. </p><ul><li>ViT 를 예로 들면, token 의 dimension 과 수는 각각 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>N</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">N^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> 이다. Adapter 가 feature 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span>-dim 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">d&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span>-dim (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>≪</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">d&#x27; \ll d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.791em;vertical-align:-0.0391em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">d</span></span></span></span></span>) 으로 projection 한다고 가정하면, 각 layer 에서 추가 trainable parameter 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>d</mi><msup><mi>d</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">2dd&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord">2</span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> 이다. <ul><li>VPT 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> 개 prompt 를 삽입해 각 layer 에 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">nd</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">n</span><span class="mord mathnormal">d</span></span></span></span></span> 개 추가 parameter 를 얻는다. </li></ul></li><li>SSF 는 linear coefficient 가 있는 각 operation 뒤에 SSF-ADA 를 삽입해 각 layer 에 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">md</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">d</span></span></span></span></span> 개 추가 parameter 를 얻는다. <ul><li>총 layer 수가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span></span> 일 때, Adapter, VPT, SSF 의 complexity 는 Tab. 2 에 나와 있다. </li></ul></li><li>Adapter, VPT, SSF 가 사용하는 추가 parameter 의 구체적인 수는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">d&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span> 값에 따라 다르다. <ul><li>하지만 실제로 SSF 는 training step 에서 Adapter 와 VPT-Deep 보다 약간 적은 parameter 로도 더 나은 성능을 낸다 (Sec. 4 에서 확인). </li><li>게다가 inference step 에서 model re-parameterization strategy 를 빌리면 SSF 의 추가 parameter 와 FLOPs 는 0 이 된다. 하지만 Adapter 와 VPT 의 complexity 는 training 때와 동일하게 유지돼 저자의 approach 의 강점을 보여준다.</li></ul></li></ul><h1>4 Experiments</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-experimental-settings">4.1 Experimental Settings<a href="#41-experimental-settings" class="hash-link" aria-label="Direct link to 4.1 Experimental Settings" title="Direct link to 4.1 Experimental Settings">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="datasets">Datasets.<a href="#datasets" class="hash-link" aria-label="Direct link to Datasets." title="Direct link to Datasets.">​</a></h4><p>저자는 주로 세 가지 유형으로 나눌 수 있는 dataset 에서 실험을 한다:</p><ul><li><em>FGVC</em> : VPT 를 따라 다섯 개 Fine-Grained Visual Classification (FGVC) dataset 을 사용해 SSF 의 효과를 평가한다. 이는 CUB-200-2011, NABirds, Oxford Flowers, Stanford Dogs, Stanford Cars 로 구성된다.</li><li><em>VTAB-1k</em> : VTAB-1k benchmark 는 다양한 도메인에서 19 tasks 를 포함한다:<ul><li>Natural image: 표준 camera 로 찍은 이미지</li><li>Specialized image: 비표준 camera (e.g., remote sensing, medical camera) 로 찍은 이미지</li><li>Structured image: 시뮬레이션 환경에서 합성된 이미지 이 benchmark 는 object counting, depth estimation 같은 다양한 task 를 포함하며, 각 task 는 1,000 개 training sample 만 있어 매우 어렵다.</li></ul></li><li>General Image Classification Datasets : SSF 의 효과를 general image classification task 에서도 검증한다. <ul><li>CIFAR-100 과 ImageNet-1K dataset 을 평가 dataset 으로 선택한다. CIFAR-100 은 100 category 로 60,000 images, ImageNet-1K 는 1,000 category 로 1.28M training 이미지와 50K validation 이미지로, object recognition 에 아주 큰 dataset 이다.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models">Models.<a href="#models" class="hash-link" aria-label="Direct link to Models." title="Direct link to Models.">​</a></h4><p>공정한 비교를 위해 VPT 를 따라 주로 ImageNet-21K 에 pre-trained ViT-B/16 model 을 fine-tuning 초기화로 선택한다. 또 저자의 method 를 다른 model family 의 backbone 에도 일반화한다. </p><p>최근 Swin Transformer (Swin-B), ConvNeXt-B, AS-MLP-B 를 포함한다. Swin-B 는 hierarchical transformer-based architecture 를 구축하고, ConvNeXt-B 와 AS-MLP-B 는 각각 CNN-based architecture 와 MLP-based architecture 에 속한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="baselines">Baselines.<a href="#baselines" class="hash-link" aria-label="Direct link to Baselines." title="Direct link to Baselines.">​</a></h4><p>먼저 두 가지 기본 fine-tuning method 와 비교한다:</p><ul><li>Full fine-tuning: model 의 모든 parameter 를 fine-tuning 때 업데이트</li><li>Linear probing: classification head (MLP layer) 의 parameter 만 업데이트 최근 parameter-efficient fine-tuning method 와도 비교한다:</li><li>Adapter: transformer 에 up-projection, non-linear function, down-projection 으로 새 adapter 구조를 삽입하고 이 module 의 parameter 만 업데이트</li><li>Bias: 모든 parameter 의 bias term 을 업데이트</li><li>VPT: transformer 에 prompt 를 input token 으로 삽입하고 fine-tuning 때 이를 업데이트</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="implementation-details">Implementation Details.<a href="#implementation-details" class="hash-link" aria-label="Direct link to Implementation Details." title="Direct link to Implementation Details.">​</a></h4><ul><li>FGVC dataset 에서는 image 를 224×224 로 randomly resize crop 하고 random horizontal flip 으로 data augmentation 을 한다. </li><li>VTAB-1k 에서는 VTAB 의 기본 설정을 따라 image 를 224×224 로 직접 resize 한다. </li><li>CIFAR-100 과 ImageNet-1K 에서는 ViT-B/16 의 fine-tuning 설정을 따라 더 강한 data augmentation strategy 를 사용한다.</li><li>AdamW optimizer 를 사용해 CIFAR-100 은 100 epoch, ImageNet-1K 는 30 epoch 동안 model 을 fine-tuning 한다.</li><li>Learning rate schedule 에는 cosine decay strategy 를 사용하고, CIFAR-100 은 처음 10 epoch, ImageNet-1K 는 처음 5 epoch 동안 linear warm-up 을 적용한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-performance-comparisons-on-image-classification">4.2 Performance Comparisons on Image Classification<a href="#42-performance-comparisons-on-image-classification" class="hash-link" aria-label="Direct link to 4.2 Performance Comparisons on Image Classification" title="Direct link to 4.2 Performance Comparisons on Image Classification">​</a></h2><p>저자는 SSF 와 다른 baseline method 의 성능을 26 image classification tasks 에서 비교했다. FGVC 와 VTAB-1k 에 대한 결과는 각각 Tab. 3 과 Tab. 4 에 나와 있고 (Fig. 1 도 참고), CIFAR-100 과 ImageNet-1K 에 대한 결과는 Tab. 5 에 나와 있다. 이는 Top-1 accuracy (%) 로 평가했다. 이 세 표에서 bold font 는 모든 method 중 최고 accuracy 를, underline font 는 두 번째로 높은 accuracy 를 보여준다.</p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-3-4dbf180d6a4213d6ce9386f617208981.png" width="1718" height="617" class="img_ev3q"></p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-4-7555a96ab32b2f4c36ec8605cc1798b1.png" width="1698" height="660" class="img_ev3q"></p><p>이 표들을 보고 다음을 알 수 있다:</p><ul><li>Tab. 3 과 Tab. 4 에서 마지막 열은 각 method 가 해당 dataset 에서 fine-tuning 한 parameter 의 평균이다. </li><li>SSF 는 VPT 와 다른 parameter-efficient fine-tuning method 를 능가하고, 심지어 full fine-tuning 보다 더 나은 성능을 낸다. 이는 주로 feature 에 적용한 linear transformation 덕분이다. </li><li>구체적으로 SSF 는 5 FGVC datasets 에서 1.81% (90.72% vs. 89.11%) 와 2.46% (90.72% vs. 88.54%), VTAB-1k benchmark 에서 5.29% (73.10% vs. 69.43%) 와 11.48% (73.10% vs. 65.57%) 의 accuracy 향상을 VPT 와 full fine-tuning 에 비해 얻었다. </li><li>동시에 SSF 는 두 dataset 에서 VPT-Deep 보다 적은 trainable parameter 를 사용한다 (0.39M vs. 0.85M, 0.24M vs. 0.60M). </li><li>SSF 는 적은 parameter 로 다른 task 에 대해 unified learnable parameter space 를 유지하지만, VPT 는 각 task 마다 다른 prompt 수를 설계해야 해서 우리 approach 의 간결함을 보여준다.</li></ul><p><img loading="lazy" alt="Table 5" src="/assets/images/image-5-215d4e50dd070adcdf95d75615a19cb4.png" width="1714" height="765" class="img_ev3q"></p><ul><li>Tab. 5, 즉 CIFAR-100 과 ImageNet-1K 에서 SSF 와 다른 parameter-efficient fine-tuning method 는 full fine-tuning 과 비슷한 성능을 내기 어렵다. 아마 이 dataset 들은 model 의 over-fitting 을 막을 만큼 충분한 data 를 가지고 있어서, 특히 ImageNet-1K 에서 그렇다. </li><li>반면 VTAB-1k benchmark 에서는 data 양이 많지 않아 (e.g., training image 1,000), full fine-tuning 에서 model 이 over-fitting 할 수 있다. 그럼에도 CIFAR-100 과 ImageNet-1K 에서 SSF 는 이전 parameter-efficient fine-tuning method (Adapter, Bias, VPT) 를 능가해서 SSF 의 효과를 보여준다.</li><li>Tab. 5 에서 Swin Transformer, ConvNeXt, AS-MLP model 에서 SSF 의 결과는 다른 parameter-efficient fine-tuning method 를 일관되게 능가한다. 이는 SSF 가 다양한 model 에서 효과적임을 검증한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="computational-cost">Computational cost.<a href="#computational-cost" class="hash-link" aria-label="Direct link to Computational cost." title="Direct link to Computational cost.">​</a></h4><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-6-5729a80c93880b595f52fc7b36802cec.png" width="1707" height="603" class="img_ev3q"></p><p>SSF 의 efficiency 를 검증하기 위해 Fig. 3 에 SSF 의 computational cost 를 보여준다. </p><ul><li>Training stage 와 inference stage 에서 batch size 16 을 사용하고 mixed precision training 을 했다. </li><li>Fig. 3 의 모든 실행 결과는 single GeForce RTX 2080Ti GPU 에서 측정했다. </li><li>SSF 는 VPT 와 비슷한 training time 과 training memory 를 가지지만 inference time 과 inference memory 는 더 적다. </li><li>여기서 VPT 의 computational cost 는 VPT-Shallow 와 VPT-Deep 에 각각 200/50 prompt (Tab. 5 의 성능을 얻은 prompt 수) 를 사용한 결과다. Prompt 수를 늘리면 time cost 와 memory 가 커지지만, SSF 는 zero-overhead inference 를 달성해서 더 유리하다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-the-impacts-of-different-designs">4.3 The Impacts of Different Designs<a href="#43-the-impacts-of-different-designs" class="hash-link" aria-label="Direct link to 4.3 The Impacts of Different Designs" title="Direct link to 4.3 The Impacts of Different Designs">​</a></h2><p>SSF 의 핵심 operation 으로서 SSF-ADA 가 결과에 어떤 영향을 미치는지, 예를 들어 삽입 위치, SSF-ADA 와 그 component 의 initialization 을 철저히 평가했다. Fine-tuning 에서 다른 design 의 영향을 분석하기 위해 실험을 했다. 모든 실험은 CIFAR-100 에서 pre-trained ViT-B/16 model 로 했고, 결과는 Tab. 6 에 나와 있다.</p><p><img loading="lazy" alt="Table 6" src="/assets/images/image-7-c364053d330e7da17d76cec24ea5b252.png" width="1589" height="467" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-impact-of-the-number-of-layers">The impact of the number of layers.<a href="#the-impact-of-the-number-of-layers" class="hash-link" aria-label="Direct link to The impact of the number of layers." title="Direct link to The impact of the number of layers.">​</a></h4><p>SSF-ADA 를 다른 layer 에 직접 삽입해 삽입 layer 의 효과를 평가했다. 결과는 Tab. 6a 에 나와 있다. </p><ul><li>#layers 열의 값은 SSF-ADA 가 있는 layer 수를 나타내고, #layers-0 은 linear probing 을 뜻한다. </li><li>첫 번째와 두 번째 행에서 처음 두 layer 에만 SSF-ADA 를 삽입하면 결과가 88.70% 에서 92.69% 로 향상되고 trainable parameter 도 적게 늘어난다 (0.08M vs. 0.11M). 이후 layer 에 SSF-ADA 를 계속 추가하면 결과가 더 좋아진다. </li><li>결과의 성장은 삽입된 SSF-ADA layer 수와 거의 선형적이다. 그래서 저자는 vision transformer 의 모든 (12) layer 에 SSF-ADA 를 삽입해 최고 결과 (93.99%) 를 얻었다. 이는 0.28M trainable parameter 를 사용했다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-impact-of-the-different-insertion-locations">The impact of the different insertion locations.<a href="#the-impact-of-the-different-insertion-locations" class="hash-link" aria-label="Direct link to The impact of the different insertion locations." title="Direct link to The impact of the different insertion locations.">​</a></h4><p>ViT 의 다른 operation 을 기반으로 SSF-ADA 의 삽입 위치 영향을 평가했다. 이 operation 뒤에서 SSF-ADA 를 따로 제거한 결과는 Tab. 6b 에 나와 있다. </p><ul><li>MLP operation 에서 SSF-ADA 를 제거하면 Attention operation 에서 제거한 것보다 결과가 더 나쁘다 (93.46% vs. 93.69%), trainable parameter 는 비슷하다 (0.19M vs. 0.21M). 이는 MLP operation 에서 feature modulation 이 더 중요할 수 있음을 시사한다. </li><li>NAS 를 사용해 다른 operation 의 중요도를 찾아 특정 위치에 SSF-ADA 를 삽입할 수도 있지만, 모든 operation 에 삽입하는 것보다 결과가 나아지지 않을 수 있다. 그래서 뛰어난 성능을 얻기 위해 NAS 를 하지 않고 모든 operation 에 SSF-ADA 를 삽입한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-impact-of-initialization">The impact of initialization.<a href="#the-impact-of-initialization" class="hash-link" aria-label="Direct link to The impact of initialization." title="Direct link to The impact of initialization.">​</a></h4><p>Scale factor 와 shift factor 를 다르게 초기화하는 방식이 성능에 어떤 영향을 미치는지 Tab. 6c 에서 조사했다. </p><ul><li>실험에서 먼저 scale 과 shift parameter 를 모두 평균 0 으로 random initialization 했지만, 성능이 낮았다 (90.11%). 일부 실험에서는 수렴하지 않았다. </li><li>그 후 scale factor 를 평균 1 로 random initialization 하니 더 나은 성능을 얻었다. 이는 pre-trained model 의 weight 를 fine-tuning 에서 완전히 망가뜨리지 말고, 이 pre-trained model 에서 시작해 model 을 최적화해야 함을 의미한다. </li><li>실험은 normal initialization 이 최고 성능을 낸다는 걸 보여준다. 여기서 scale factor 와 shift factor 의 평균은 각각 1 과 0 이다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-impact-of-different-components">The impact of different components.<a href="#the-impact-of-different-components" class="hash-link" aria-label="Direct link to The impact of different components." title="Direct link to The impact of different components.">​</a></h4><p>SSF-ADA 의 다른 component 의 영향을 평가한 결과는 Tab. 6d 에 나와 있다. </p><ul><li>Scale term 을 제거하면 shift term 을 제거한 것보다 성능이 더 나쁘다. Trainable parameter 는 같다. 이는 scale term 이 shift term 보다 더 중요할 수 있음을 보여준다. </li><li>또한 ‘w/o. scale’ 과 Tab. 5 의 ‘Bias’ method 의 차이는 ‘w/o. scale’ 에서 추가 shift term 으로 model 을 fine-tuning 한 반면, ‘Bias’ 는 original bias 를 기반으로 fine-tuning 했다는 점이다. 이는 res-like 방식으로 model 을 fine-tuning 하면 약간 더 나은 성능을 얻을 수 있음을 시사한다 (93.49% vs. 93.39%). </li><li>저자는 normalization layer (LN) 에서 모든 scale factor 와 shift factor 만 fine-tuning 하거나, SSF 로 model 을 fine-tuning 하되 scale term 을 scalar 로 설정해보기도 했다. </li><li>이 실험들은 SSF 보다 낮은 성능을 냈다 (93.26% vs. 93.99%, 93.59% vs. 93.99%). 하지만 SSF 의 trainable parameter 의 절반 정도만 사용하므로 대안으로 고려될 수 있다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="44-performance-comparisons-on-robustness-and-ood-datasets">4.4 Performance Comparisons on Robustness and OOD Datasets<a href="#44-performance-comparisons-on-robustness-and-ood-datasets" class="hash-link" aria-label="Direct link to 4.4 Performance Comparisons on Robustness and OOD Datasets" title="Direct link to 4.4 Performance Comparisons on Robustness and OOD Datasets">​</a></h2><p>저자는 SSF method 의 robustness 와 Out-Of-Distribution (OOD) 능력을 분석하기 위해 ImageNet-A, ImageNet-R, ImageNet-C dataset 에서 실험을 했다. 자세한 내용은 Appendix A.2 를 참고해라. ImageNet-1K 에서 fine-tuning 한 model 로 이 세 dataset 에서 robustness 와 OOD 평가를 했다. 모든 실험 결과는 Tab. 7 에 나와 있다.</p><p><img loading="lazy" alt="Table 7" src="/assets/images/image-8-dc0459c2d35734e973fae17a2d092421.png" width="916" height="640" class="img_ev3q"></p><ul><li>이 표에서 SSF 는 세 dataset 에서 VPT 와 다른 parameter-efficient fine-tuning method 보다 더 나은 성능을 낸다. 이는 SSF fine-tuning method 가 더 강한 robustness 와 out-of-distribution generalization 을 가짐을 보여준다. </li><li>게다가 SSF 는 ImageNet-1K 에서 full fine-tuning 보다 accuracy 가 낮지만, ImageNet-A, ImageNet-R, ImageNet-C 에서의 성능은 더 낫다. 이는 ImageNet-1K 와 ImageNet-A/R/C 간 성능이 절대적으로 양의 상관관계가 아님을 보여준다. </li><li>Robustness 와 OOD dataset 에서의 이런 향상은 SSF 가 대부분 pre-trained parameter 를 frozen 시켜 large-scale dataset 에서 배운 지식을 최대한 보존해 더 나은 generalization 능력을 유지하기 때문일 수 있다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="45-visualization-and-analysis">4.5 Visualization and Analysis<a href="#45-visualization-and-analysis" class="hash-link" aria-label="Direct link to 4.5 Visualization and Analysis" title="Direct link to 4.5 Visualization and Analysis">​</a></h2><p>저자의 목표는 pre-trained model 이 추출한 feature 를 modulate 하는 거지만, scale parameter 와 shift parameter 는 실제로 input-independent 하다. 그래서 이 parameter 는 전체 downstream dataset 의 정보를 encoding 한다고 볼 수 있다. </p><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-9-83e07e2dca8b1a2d93533c955c9cdbbe.png" width="1598" height="650" class="img_ev3q"></p><p>Re-parameterization 후 이 scale parameter 와 shift parameter 는 original model weight 에 흡수된다. SSF 가 배운 정보를 더 잘 이해하기 위해 Fig. 4a 에서 SSF 로 fine-tuning 전후의 weight 와 bias distribution 을 시각화했다. </p><ul><li>Scale parameter 와 shift parameter 가 original weight 와 bias 를 조정하고, downstream task 에 맞게 weight 와 bias 의 distribution 을 바꾼다. </li><li>비교로 Fig. 4b 에서 original weight distribution 과 full fine-tuning 후 weight distribution 을 시각화했다. 여기서 흥미로운 현상을 발견했다. </li><li>Full fine-tuning 은 weight 와 bias 의 distribution 을 많이 바꾸지 않고, 아마 값의 작은 부분만 바뀐다. SSF 는 full fine-tuning 의 weight distribution 과 맞지 않지만, CIFAR-100 에서 더 나은 성능을 낸다 (Tab. 5 에서 93.99% vs. 93.82%).</li></ul><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-10-a39e9aaad4212d448045fda5950e83f6.png" width="595" height="729" class="img_ev3q"></p><ul><li>SSF 가 왜 뛰어난 성능을 낼 수 있는지 더 알아보려고 weight distribution 외에 full fine-tuning 과 linear probing, full fine-tuning 과 VPT-Deep, full fine-tuning 과 SSF 간 feature similarity 를 Fig. 5 에서 시각화했다. </li><li>last layer 에서 SSF 는 full fine-tuning 과 가장 비슷한 feature 를 가지고 accuracy 도 가장 가깝다. 이는 SSF 가 배운 weight distribution 이 full fine-tuning 과 달라도 downstream task 의 이미지 feature 를 아주 잘 추출할 수 있음을 보여준다.</li></ul><h1>5 Conclusion</h1><p>이 논문에서 저자는 parameter-efficient fine-tuning 에 초점을 맞추고, pre-trained model 이 추출한 feature 를 scale 하고 shift 하는 SSF method 를 제안했다. </p><p>SSF 의 직관은 upstream task 와 downstream task 간 distribution mismatch 를 feature modulation 으로 완화하는 데서 온다. SSF 는 적은 learnable parameter 로 다른 parameter-efficient fine-tuning approach 를 놀랍게도 능가한다. 게다가 fine-tuning 중 도입된 scale parameter 와 shift parameter 는 inference step 에서 re-parameterization 을 통해 original pre-trained model weight 에 합쳐져 additional parameter 와 FLOPs 를 피한다. </p><p>제안된 SSF method 은 FGVC 에서 2.46% (90.72% vs. 88.54%), VTAB-1k 에서 11.48% (73.10% vs. 65.57%) 의 Top-1 accuracy 성능 향상을 full fine-tuning 에 비해 얻었지만, 약 0.3M parameter 만 fine-tuning 했다. 총 26 개 image classification dataset 과 3 개 robustness &amp; out-of-distribution dataset 에서 다양한 model family (CNNs, Transformers, MLPs) 로 한 실험은 SSF 의 효과를 보여주며, 이는 새로운 baseline 을 세운다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/ssf">SSF</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Computer Vision/PEFT/Feature-Shift/2022-10-SSF.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Computer Vision/PEFT/Composition/InfLoRA"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-model-families" class="table-of-contents__link toc-highlight">2.1 Model Families</a></li><li><a href="#22-pre-training-and-fine-tuning" class="table-of-contents__link toc-highlight">2.2 Pre-training and Fine-tuning</a></li><li><a href="#23-feature-modulation" class="table-of-contents__link toc-highlight">2.3 Feature Modulation</a></li><li><a href="#24-model-re-parameterization" class="table-of-contents__link toc-highlight">2.4 Model Re-parameterization</a></li><li><a href="#31-preliminaries" class="table-of-contents__link toc-highlight">3.1 Preliminaries</a></li><li><a href="#32-scaling-and-shifting-your-features-for-fine-tuning" class="table-of-contents__link toc-highlight">3.2 Scaling and Shifting Your Features for Fine-tuning</a></li><li><a href="#33-complexity-analysis" class="table-of-contents__link toc-highlight">3.3 Complexity Analysis</a></li><li><a href="#41-experimental-settings" class="table-of-contents__link toc-highlight">4.1 Experimental Settings</a></li><li><a href="#42-performance-comparisons-on-image-classification" class="table-of-contents__link toc-highlight">4.2 Performance Comparisons on Image Classification</a></li><li><a href="#43-the-impacts-of-different-designs" class="table-of-contents__link toc-highlight">4.3 The Impacts of Different Designs</a></li><li><a href="#44-performance-comparisons-on-robustness-and-ood-datasets" class="table-of-contents__link toc-highlight">4.4 Performance Comparisons on Robustness and OOD Datasets</a></li><li><a href="#45-visualization-and-analysis" class="table-of-contents__link toc-highlight">4.5 Visualization and Analysis</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.dc82af9c.js"></script>
<script src="/assets/js/main.07c26eea.js"></script>
</body>
</html>