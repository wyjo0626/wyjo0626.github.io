---
slug: SLD
title: "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models"
tags: [Text-to-Image, Diffusion, Concept Editing, SLD, training-free, weight modification-free]
---

논문 및 이미지 출처 : <https://arxiv.org/pdf/2211.05105>

# Abstract

text-conditioned image generation model 은 최근 image quality 와 text alignment 에서 놀라운 결과를 보여주며, 빠르게 늘어나는 application 에 사용되고 있다. 이 model 들은 데이터에 크게 의존하며, 인터넷에서 무작위로 수집된 billion-sized dataset 에 기반을 둔다. 그래서 저자가 보여주듯이, 인간의 부적절하고 편향된 행동에서 비롯된 degeneration 을 겪는다. 결과적으로 이런 bias 를 강화할 수도 있다. 

이런 원치 않는 부작용을 줄이기 위해 저자는 **safe latent diffusion (SLD)** 를 제안한다. 

- 구체적으로, 부적절한 degeneration 을 측정하기 위해, nudity 와 violence 같은 concept 를 포함하는 전용 real-world image-to-text prompt 로 구성된 새로운 image generation test bed 인 inappropriate image prompts (I2P) 를 만들었다. 
- 저자의 철저한 실험적 평가를 통해 SLD 는 추가 training 없이 diffusion 과정에서 부적절한 image 부분을 제거하고 억제하며, 전체 image quality 나 text alignment 에 부정적인 영향을 주지 않는다는 것을 보여준다.

# 1. Introduction

text-conditioned generative diffusion model (DM) 의 최근 breakthrough 는 pre-trained backbone 의 representation 품질과 multimodal training data 덕분이다. 이 model 들은 심지어 underlying syntax 와 semantics 를 배우고 반영하는 것으로 나타났다. 그래서 데이터에 암묵적으로 존재하는 general knowledge 를 유지한다. 

하지만 large-scale unfiltered data 로 training 된 system 은 degenerated 와 biased behavior 를 겪는다. 이런 심각한 문제는 많은 bias 가 human-like 하기 때문에 놀랍지 않지만, data collection 과정에서 자체 bias 를 보고하지 않는 데 많은 우려가 있다. 결과적으로 DM 을 포함한 model 은 이런 bias 를 반영하며 원치 않는 행동을 복제할 가능성이 있다. 연구자들은 인터넷에서 수집된 dataset, 특히 LAION-400M 과 그 후속인 LAION-5B 의 수많은 문제와 trained model 의 downstream harm 을 지적했다.

저자는 open-source latent diffusion model 인 Stable Diffusion (SD) 를 분석했으며, 이 model 은 LAION-5B 의 subset 으로 training 되었고, 직접 보면 불쾌하거나 모욕적, 위협적이거나 불안감을 줄 수 있는 상당한 양의 부적절한 content 를 생성한다. pre-trained text-to-image model 의 부적절한 degeneration 위험을 체계적으로 측정하기 위해, DM 의 부적절한 generation 을 평가하는 test bed 를 제공하고, pre-training 을 위한 더 나은 safety intervention 과 data selection 과정의 필요성을 강조한다. 

![Figure 1](image-26.png)

- 저자는 4703 text-to-image prompt 로 구성된 I2P (Sec. 5) 를 공개하며, 이는 real-world user prompt 에서 추출되었고 세 가지 detector 로부터 inappropriateness score 를 받았다 (Fig. 1 참조). 
- 최근 소개된 open-source DM, 여기서는 Stable Diffusion (SD) 가 저자의 prompt 로 조건화될 때, 심지어 해롭지 않아 보이는 prompt 에서도 부적절한 content 를 생성한다는 것을 보여준다 (Sec. 6 참조). 
- 그래서 저자는 safe latent diffusion (SLD, Sec. 3) 라는 mitigation strategy 를 소개하고 I2P (Sec. 6) 를 사용해 부적절한 content 생성을 억제하는 능력을 정량화한다. 
- SLD 는 external classifier 가 필요 없으며, model 이 이미 학습한 inappropriateness knowledge 에 의존하고 DM 의 추가 tuning 도 필요 없다.

일반적으로 SLD 는 generative diffusion model 의 latent space 를 조작하는 새로운 technique 을 도입하고 latent vector 의 arithmetic 에 대한 추가 insight 를 제공한다.

# 2. Risks and Promises of Unfiltered Data

#### Risks

현대 large-scale model, 예로 GPT-3, 은 general information 을 encoding 하고 반영하지만, large-scale unfiltered data 로 training 된 system 은 degenerated 와 biased behavior 를 겪는다. computational system 은 human bias 와 structural inequality 를 상쇄할 가능성이 있다고 약속되었지만, data-driven AI system 은 종종 이런 bias 를 반영하며, 결과적으로 이를 강화할 가능성이 있다. 

이런 risk 는 large-scale model 의 context 에서 널리 논의되고 입증되었다. 예로, model 은 stereotypical, derogatory content 를 생성하며 gender 와 racial bias 를 보인다. 이후 이런 model 의 bias 를 줄이기 위한 approach 가 개발되었다.

#### Promises

performance gain 외에도, large-scale model 은 training data 에서 factual knowledge 를 recall 하는 놀라운 능력을 보여준다. 예로, 연구자들은 large-scale pre-trained language model 의 knowledge 저장과 retrieval 능력이 model size 에 따라 증가한다고 밝혔다. 이를 바탕으로, 연구자들은 language model 이 toxic output 에 대해 self-debias 할 수 있음을 보여주었다. 

또한 연구자들은 이런 model 의 retained knowledge 가 언어로 표현된 human sense of "right" 와 "wrong" 에 맞는 moral norm 정보를 포함한다고 밝혔다. 비슷하게, 다른 연구는 autoregressive language model 의 text generation 을 toxic degeneration 을 막도록 guide 하는 방법을 보여주었다. 이에 따라 저자는 DM 이 pre-training 중 학습된 representation 과 natural language 로 정의된 concept 를 사용해 inappropriateness 를 피하도록 image generation 을 guide 할 수 있음을 보여준다.

이는 Text2LIVE, Imagic, UniTune 같은 diffusion model 의 text-based image editing technique 과 관련이 있다. 하지만 저자의 SLD approach 는 text-encoder 나 DM 의 fine-tuning 이 필요 없으며, 새로운 downstream component 도 도입하지 않는다. 대신 model 자체의 learned representation 을 사용해 computational efficiency 를 크게 개선한다. 이전에 Prompt-to-Prompt 는 text prompt 변경과 model 의 cross-attention layer 제어를 사용한 text-controlled editing technique 을 제안했다. 반면 SLD 는 classifier-free guidance 를 기반으로 하며 더 복잡한 image 변경을 가능하게 한다.

#### LAION-400M and LAION-5B

LAION-400M dataset 은 proof-of-concept 으로 공개되었지만, creator 들은 제기된 우려를 받아들여 후속 dataset 인 LAION-5B 에서 잠재적 부적절한 content 를 annotation 했다. 

safety, fairness, biased data 에 대한 연구를 촉진하기 위해 이 sample 들은 dataset 에서 제외되지 않았다. 사용자는 자신의 use case 에 따라 이 image 를 포함할지 결정할 수 있다.

#### Training Stable Diffusion

많은 DM 은 large-scale training data 에 대한 우려에 대해 model 을 공개하지 않거나, controlled environment 에서 dedicated guardrail 과 함께 배포하거나, published model 의 training data 를 엄격히 filtering 하며 대응했다. 반면 SD 는 LAION-5B 에 포함된 annotated content 를 제외하지 않고 model 을 공개적으로 release 했다. LAION 과 마찬가지로 Stable Diffusion 은 harmful content 를 생성할 가능성이 있는 model 의 safe deployment 에 대한 연구를 장려한다.

구체적으로 SD 는 LAION-5B 의 subset, 즉 2.32 billion 이상의 English image-text pair 를 포함하는 LAION-2B-en 으로 training 된다. SD training 은 여러 단계로 진행된다: 먼저 model 은 전체 LAION-2B-en 으로 training 된다. 그런 다음 "LAION High Resolution" 과 "LAION-Aesthetics v2 5+" 같은 다양한 subset 으로 fine-tuning 된다. 

모든 training sample 이 LAION-5B 또는 그 subset 에서 가져왔기 때문에, trained model 은 gender occupation correlation 같은 human-like bias 뿐만 아니라 reporting bias 도 반영할 것으로 예상된다. 또한 SD 는 huggingface 와 최근 lexica.art 같은 여러 platform 에 배포되어 일반 사용자가 쉽게 사용할 수 있으며, 현존하는 issue 를 모르는 사용자도 포함된다.

#### Ethnic Bias

이제 첫 번째 experiment 로 넘어간다. unfiltered multimodal dataset 에 대한 연구를 따라, 저자는 downstream generative model 에서 같은 issue 가 발생하는지 조사한다. 구체적으로 Asian women 의 biased representation 을 보여주기로 했다. 이 social phenomenon 은 western country 에서 잘 문서화된 trend 이며, English-speaking internet 과 이후 web-crawled LAION-2B-en 에도 반영된다. 'japanese body' term 에 대해 dataset 에서 CLIP space 에서 가장 가까운 100 개 image 를 검색한 결과, 90% 가 explicit nudity 를 보여주었다. 비교로, 50 개 선택된 country 의 global average 는 22% 로 4 배 이상 낮다.

![Figure 2](image-27.png)

이제 SD 에 bias transfer 를 조사하기 위해, '<country> body' (e.g, 'japanese body', 'nigerian body') prompt 를 사용해 같은 50 개 country 에 대해 explicit nude 의 percentage 를 비교했다. generated image 는 NudeNet image classifier 를 사용해 exposed genitalia 를 detection 하여 자동으로 평가했다. 이 모호한 prompt template 을 선택해 model 의 bias 를 드러냈다. image web search 결과에 따르면, 예로 lightly dressed people 을 보여주는 average body comparison image 나 country flag 같은 culturally related image 를 기대할 수 있다. 각 country 에 대해 동일한 seed 와 hyperparameter 를 사용해 100 개 image 를 생성했다. 

실제로 u.s. american 의 경우 average body comparison 같은 image 를 관찰할 수 있었다. 하지만 예상대로, Stable Diffusion 에서 일부 ethnic term 과 nudity 의 밀접한 연관성이 뚜렷하다. 전체적으로 European, Asian, Oceanic country 가 African 이나 American country 보다 nudity 와 훨씬 더 연관될 가능성이 높다. Japan 에 대해 가장 많은 nude image 가 생성되었으며, 75% 이상이다. 반면 global average 는 35% 다. 특히 'Asian' 과 'Japanese' term 은 다른 ethnic 이나 geographic term 보다 훨씬 높은 nudity 를 생성했다. 저자는 이 context 에서 'Japanese' 와 'Asian' 의 synonym usage 를 앞서 언급한 trend 와 LAION-5B 의 overwhelming content 에 기인한다고 본다. 불행히도 SD generation 의 이런 bias 는 problematic social phenomenon 을 더 강화할 수 있다.

#### SD's post-hoc safety measures

부적절한 image 를 detection 하고 filtering 하는 다양한 method 가 제안되었다. 비슷하게 SD implementation 은 generation 후 부적절한 image 를 detection 하고 withhold 하는 "NSFW" safety checker 를 포함한다. 하지만 이 safety measure 를 비활성화하려는 관심이 있는 것 같다. 저자는 lexica.art 에 최근 추가된 image generation feature 를 사용해 safety checker 가 withhold 하는 content 를 생성하는 example 을 확인했다. 현재 연구 시점에서 lexica.art 에서 이런 부적절한 image generation 이 제한 없이 가능한 것으로 보인다.

# 3. Safe Latent Diffusion (SLD)

latent diffusion model 에 대한 _safety guidance_ 를 도입해 DM 의 부적절한 degeneration 을 줄인다. 저자의 method 는 classifier-free guidance 를 통해 text conditioning 을 결합하고 output image 에서 부적절한 concept 를 제거하거나 억제함으로써 generative process 를 확장한다. 결과적으로 SLD 는 추가 fine-tuning 없이 inference 에서 image editing 을 수행한다.

Diffusion model 은 Gaussian distributed variable 을 반복적으로 denoising 하여 learned data distribution 의 sample 을 생성한다. 

- 직관적으로 image generation 은 random noise $\epsilon$ 에서 시작하며, model 은 이 noise 의 estimate $\bar{\epsilon}_0$ 를 예측해 초기 값에서 뺀다. 
- 이는 noise 없는 high-fidelity image $x$ 를 생성한다. 이건 매우 어려운 문제라 여러 step 이 적용되며, 각 step 은 predictive noise 의 작은 양 $\epsilon_t$ 를 빼서 $\epsilon$ 를 근사한다. 
- text-to-image generation 의 경우, model 의 $\epsilon$-prediction 은 text prompt $p$ 에 조건화되며 prompt 에 충실한 image 를 생성한다. diffusion model $\hat{x}_\theta$ 의 training objective 는 다음과 같이 쓸 수 있다:

$$
\begin{equation}
  \mathbb{E}_{\mathbf{x}, \mathbf{c}_p, \epsilon, t}\left[w_t\left\|\hat{\mathbf{x}}_\theta\left(\alpha_t \mathbf{x}+\omega_t \epsilon, \mathbf{c}_p\right)-\mathbf{x}\right\|_2^2\right]
\end{equation}
$$

- 여기서 $(\mathbf{x}, \mathbf{c}_p)$ 는 text prompt $p$ 에 조건화되고, 
- $t$ 는 uniform distribution $t \sim \mathcal{U}([0,1])$ 에서 추출되며, 
- $\epsilon$ 은 Gaussian $\epsilon \sim \mathcal{N}(0, \mathbf{I})$ 에서 sampling 된다. 
- 그리고 $w_t, \omega_t, \alpha_t$ 는 $t$ 에 따라 image fidelity 에 영향을 준다. 
- 결과적으로 DM 은 $\mathbf{z}_t := \mathbf{x} + \epsilon$ 를 denoising 하여 squared error 를 loss 로 사용해 $\mathbf{x}$ 를 생성하도록 training 된다. 
- inference 에서 DM 은 model 의 $\mathbf{x} = \left(\mathbf{z}_t - \epsilon_g\right)$ 예측을 사용해 sampling 되며, $\epsilon_0$ 는 아래에 설명된다.

Classifier-free guidance 는 추가 pre-trained classifier 필요 없이 purely generational diffusion model 을 사용하는 conditioning method 다. 이 approach 는 training 중 text conditioning $\mathbf{c}_p$ 를 fixed probability 로 무작위로 drop 하여 unconditional 과 conditional objective 를 위한 joint model 을 만든다. inference 중 $\mathbf{x}$-prediction 의 score estimate 는 다음과 같이 조정된다:

$$
\begin{equation}
  \hat{\epsilon}_\theta\left(\mathbf{z}_t, \mathbf{c}_p\right) := \epsilon_\theta\left(\mathbf{z}_t\right) + s_g\left(\epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_p\right) - \epsilon_\theta\left(\mathbf{z}_t\right)\right)
\end{equation}
$$

- 여기서 guidance scale $s_g$ 는 보통 $s_g \in (0, 20]$ 로 선택되며, $\epsilon_\theta$ 는 parameter $\theta$ 를 가진 noise estimate 를 정의한다. 
- 직관적으로 unconditioned $\epsilon$-prediction $\epsilon_\theta\left(\mathbf{z}_t\right)$ 는 conditioned $\epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_p\right)$ 방향으로 밀려 prompt $p$ 에 충실한 image 를 생성한다. 
- 마지막으로 $s_g$ 는 text $p$ 의 influence magnitude 를 결정한다.

![Figure 3](image-28.png)

diffusion process 를 영향 주기 위해 SLD 는 classifier-free guidance 와 같은 principle 을 사용한다 (Fig. 3 의 simplified illustration 참조). text prompt $p$ (blue arrow) 외에, 저자는 textual description $S$ 를 통해 부적절한 concept (red arrow) 를 정의한다. 그래서 세 가지 $\epsilon$-prediction 을 사용해 unconditioned score estimate $\epsilon_\theta\left(\mathbf{z}_t\right)$ 를 prompt conditioned estimate $\epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_p\right)$ 방향으로 이동시키고 동시에 concept conditioned estimate $\epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_S\right)$ 에서 멀어지게 한다. 이는 다음과 같다:

$$
\bar{\epsilon}_\theta\left(\mathbf{z}_t, \mathbf{c}_p, \mathbf{c}_S\right) = \epsilon_\theta\left(\mathbf{z}_t\right) + s_g\left(\epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_p\right) - \epsilon_\theta\left(\mathbf{z}_t\right) - \gamma\left(\mathbf{z}_t, \mathbf{c}_p, \mathbf{c}_S\right)\right)
$$

여기서 safety guidance term $\gamma$ 는

$$
\begin{equation}
  \gamma\left(\mathbf{z}_t, \mathbf{c}_p, \mathbf{c}_S\right) = \mu\left(\mathbf{c}_p, \mathbf{c}_S; s_S, \lambda\right)\left(\epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_S\right) - \epsilon_\theta\left(\mathbf{z}_t\right)\right),
\end{equation}
$$

이며, $\mu$ 는 guidance scale $s_S$ 를 element-wise 로 적용한다. 이를 위해 $\mu$ 는 generation process 를 부적절한 concept 방향으로 guide 하는 prompt conditioned estimate 의 dimension 을 고려한다. 따라서 $\mu$ 는 prompt conditioned estimate 와 safety conditioned estimate 간 element-wise difference 를 $s_S$ 로 scaling 하며, 이 difference 가 threshold $\lambda$ 아래일 때만 적용되고, 그렇지 않으면 0 이다:

$$
\begin{align}
    &\mu\left(\mathbf{c}_p, \mathbf{c}_S; s_S, \lambda\right) = 
    \begin{cases} \max(1, |\phi|), & \text{where } \epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_p\right) \ominus \epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_S\right) < \lambda \\ 0, & \text{otherwise} 
    \end{cases} \\
    & \text{with} \quad \phi = s_S\left(\epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_p\right) - \epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_S\right)\right)
\end{align}
$$

- larger $\lambda$ 와 $s_S$ 는 prompt text 에서 더 크게 벗어나 정의된 concept 의 반대 방향으로 이동한다. 
- image artifact 를 피하기 위해 $\mu$ 의 scaling factor 를 clip 한다. 
- 이전 연구에서처럼, 각 $\mathbf{x}$-prediction 의 value 는 low fidelity image 를 방지하기 위해 training bound $[-1, 1]$ 를 따라야 한다.

SLD 는 generated image 에서 부적절한 content 를 모두 제거하면서 변경을 최소화하는 balancing act 다. 이를 위해 위에 제시된 methodology 에 두 가지 adjustment 를 추가한다. warm-up parameter $\delta$ 를 추가해 diffusion process 의 initial warm-up period 이후에만 safety guidance $\gamma$ 를 적용한다. 즉, $t < \delta$ 이면 $\gamma\left(\mathbf{z}_t, \mathbf{c}_p, \mathbf{c}_S\right) := \mathbf{0}$ 이다. 당연히 higher values $\delta$ 는 generated image 의 덜 중요한 adjustment 를 초래한다. 

전체 image composition 을 변경하지 않으려는 목표에 따라, 충분한 higher $\delta$ 를 선택하면 output 의 fine-grained detail 만 변경된다. 또한 safety guidance $\gamma$ 에 momentum term $\nu_t$ 를 추가해 지속적으로 같은 방향으로 guide 되는 dimension 에 대해 time step 동안 guidance 를 가속한다. 그래서 $\gamma_t$ 는 다음과 같이 정의된다:

$$
\begin{equation}
  \gamma_t\left(\mathbf{z}_t, \mathbf{c}_p, \mathbf{c}_S\right) = \mu\left(\mathbf{c}_p, \mathbf{c}_S; s_S, \lambda\right)\left(\epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_S\right) - \epsilon_\theta\left(\mathbf{z}_t\right)\right) + s_m \nu_t
\end{equation}
$$

여기서 momentum scale $s_m \in [0, 1]$ 이고, $\nu$ 는 다음과 같이 update 된다:

$$
\begin{equation}
  \nu_{t+1} = \beta_m \nu_t + (1 - \beta_m) \gamma_t
\end{equation}
$$

- 여기서 $\nu_0 = \mathbf{0}$ 이고 $\beta_m \in [0, 1)$ 이며, 
- larger $\beta_m$ 은 momentum 의 less volatile 한 변화를 초래한다. 
- Momentum 은 warm-up period 동안 $\gamma_t$ 가 적용되지 않더라도 이미 build up 된다.

전체적으로 SLD progress 는 Fig. 3 에서 다양한 diffusion step 을 통해 예시적으로 visualize 된다. safety-guidance 는 초기 step 에서 이미 적용되지만, 부적절한 image 부분이 latent space 에서 구성되자마자 이를 제거하고 억제한다.

# 4. Configuring Safe Latent Diffusion

Inappropriateness 은 개인의 의견과 context 에 따라 subjective 할 수 있다. 그래서 저자는 diffusion process 의 safety adaptation 강도를 달리하는 네 가지 configuration 을 제안한다. 이 configuration 은 앞서 언급한 hyper-parameter 와 concept 를 포함한다. 제안된 value 는 alternation 정도와 trade-off 를 잘 맞춘다고 생각하지만, 쉽게 조정할 수 있다.

#### (Un)safe Concept

DALL-E 같은 deployed text-to-image DM 의 기존 content policy 에서 영감을 받아, 다음 text prompt 를 사용해 general inappropriate safety concept 를 정의한다: 

> "hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, cruelty".

Text-based definition 을 통해 추가 concept 를 쉽게 추가하거나 기존 concept 를 제거할 수 있다.

#### Threshold

SLD 의 가장 중요한 parameter 는 safety threshold $\lambda$ 다. 이는 latent space 를 appropriate 와 inappropriateness 로 나누는 hyperplane 의 위치를 결정한다 (Eq. (5) 참조). 이론적으로 $\lambda$ 는 training bound $[-1, 1]$ 에 제한되며, 직관적으로 최소 0 이상이어야 한다. 하지만 저자의 approach 는 model 의 "right" 와 "wrong" 에 대한 이해에 의존하므로, conservative 한, 즉 small positive values 를 선택하는 걸 추천한다. $\lambda \in [0.0, 0.03]$ 정도가 적당하다.

#### Safety guidance scale

Safety guidance scale $s_S$ 는 이론적으로 임의로 높게 선택할 수 있다. 어차피 scaling factor $\mu$ 는 clip 되기 때문이다. large $s_S$ value 는 latent representation 에서 더 많은 value 를 1 로 설정할 뿐이다. 그래서 high guidance scale $s_g$ 에서 관찰되는 image artifact 같은 adverse effect 는 없다. $s_S \in [100, 3000]$ 을 추천한다.

#### Warm-up

Warm-up period $\delta$ 는 image composition 변경이 적용되는 level 에 큰 영향을 준다. Diffusion process 초기에 large safe-guidance scale 을 적용하면 image 의 주요 부분이 구성되기 전에 큰 변경이 생길 수 있다. 그래서 최소 몇 개의 warm-up step, $\delta \in [5, 20]$ 를 사용해 initial image 를 구성하고, 최악의 경우 SLD 가 그 부분을 수정하게 하는 걸 추천한다. 어떤 경우든 $\delta$ 는 전체 diffusion step 수의 절반을 넘지 않아야 한다.

#### Momentum

Guidance momentum 은 image 의 큰 부분을 차지하는 부적절한 concept 를 제거하는 데 특히 유용하며, 이는 warm-up 중 생성된 경우 더 substantial editing 이 필요하다. 그래서 momentum 은 warm-up phase 동안 build up 되며, 이런 image 는 close editing distance 를 가진 image 보다 더 강하게 변경된다. 

Higher momentum parameter 는 보통 longer warm-up period 를 허용한다. 대부분의 diffusion process 가 약 50 개 generation step 을 사용하므로 momentum build-up window 는 제한적이다. 그래서 $s_m \in [0, 0.5]$ 와 $\beta_m \in [0.3, 0.7]$ 을 추천한다.

#### Configuration sets

![Figure 4](image-29.png)

이 추천은 다음과 같은 네 가지 hyper-parameter set 를 만든다. 이는 resulting image 에 대한 변경의 aggressiveness 를 점진적으로 증가시킨다 (Fig. 4). 어떤 설정을 사용할지는 use case 와 개인 preference 에 따라 크게 달라진다:

| Config | $\delta$ | $s_S$ | $\lambda$ | $s_m$ | $\beta_m$ |
|--------|---------|-------|----------|-------|---------|
| Hyp-Weak | 15 | 200 | 0.0 | 0.0 | - |
| Hyp-Medium | 10 | 1000 | 0.01 | 0.3 | 0.4 |
| Hyp-Strong | 7 | 2000 | 0.025 | 0.5 | 0.7 |
| Hyp-Max | 0 | 5000 | 1.0 | 0.5 | 0.7 |

- Weak configuration 은 보통 superficial blood splatter 를 제거하는 데 충분하지만, 더 심각한 injury 를 억제하려면 stronger parameter 가 필요하다. 
  - 비슷하게 weak set 은 clearly pornographic image 에서 nude content 를 억제할 수 있지만, oil painting 같은 artistic imagery 의 nudity 는 줄이지 않을 수 있다. 
  - 이는 성인 artist 에겐 괜찮을 수 있지만, 예로 model 을 사용하는 아이에겐 문제가 된다. Nudity 예시에서 medium hyper-parameter set 은 bikini 같은 generation 을 만들어내는 반면, strong 과 maximum set 은 점진적으로 dress 같은 더 많은 cloth 를 생성한다.
- Hyp-Max configuration 을 사용하면 부적절한 content generation 을 0 으로 만들 수 있다. 하지만 이렇게 하면 변경을 최소화하려는 목표에서 벗어날 가능성이 크다. 그래도 어린이를 포함하는 sensitive application 에선 이런 설정이 필요할 수 있다. 이런 경우 SD 의 safety checker 같은 post-hoc intervention 사용을 추가로 추천한다.

변경 정도와 관련해 Hyp-Max configuration 은 종종 classifier-free guidance 중 unconditioned estimate 를 negative prompt 기반 conditioned estimate 로 대체하는 것과 비슷하게 작동한다 (Fig. 4 의 Neg.). 즉, $\epsilon_\theta\left(\mathbf{z}_t\right)$ 를 $\epsilon_\theta\left(\mathbf{z}_t, \mathbf{c}_S\right)$ 로 대체한다 (Eq. (2)). 하지만 실험적 평가 (Tab. 1) 는 negative prompting 이 SLD 보다 mitigation 효과가 낮다는 걸 보여준다. 

게다가 이 approach 의 주요 단점은 generation process 에 대한 fine-grained control 이 부족해, 특히 high guidance scale $s_S$ 에서 original image 와 크게 다른 image 를 생성한다. 또한 negative prompt 는 text-to-image generation 에서 중요한 tool 이며, safety guidance 에 사용되면 사용자에게 더 이상 제공되지 않는다.

# 5. Inappropriate Image Prompts (I2P)

Pre-trained text-to-image model 의 부적절한 degeneration 위험을 체계적으로 측정하기 위해, 부적절한 content 를 생성할 가능성이 높은 4.5k 이상의 real-world text prompt 로 구성된 새로운 benchmarking dataset, inappropriate image prompts (I2P) 를 소개한다 (Fig. 1 참조). 이 dataset 은 nudity 를 넘어 다양한 부적절한 content 를 다루며, 다른 연구자가 사용할 수 있도록 공개된다.

#### Inappropriate content

부적절한 imagery 는 context, setting, cultural, social predisposition, 개인적 요인에 따라 다르며 전체적으로 매우 subjective 하다. 이 연구에서 저자는 부적절한 content 를 "직접 보면 불쾌하거나 모욕적, 위협적이거나 불안감을 줄 수 있는 data" 로 정의하며, 이는 DALL-E 의 OpenAI content policy 에도 반영된다. 구체적으로 다음 중 하나를 포함하는 image 를 고려한다:  

> hate, harassment, violence, self-harm, sexual content, shocking image, illegal activity  

Inappropriateness 는 이 일곱 가지 concept 에 국한되지 않으며, 문화에 따라 다르고 계속 진화한다. 여기서는 tangible 한 부적절한 행동을 보여주는 image 로 제한했다.

#### Prompt collection

위의 일곱 가지 concept 에 대해 26 keywords 와 phrase 를 사용해 각 concept 에 대해 최대 250 real-world text prompts 를 수집했다. 주어진 keyword 에 대해 https://lexica.art 에서 반환된 상위 250 개 image 의 prompt 를 crawl 했다. 

Lexica 는 SD 의 official discord server 에서 sourced 된 real-world, user-generated prompt collection 이다. Prompt, seed, guidance scale, image dimension 을 저장해 reproducibility 를 돕는다. Lexica 의 image retrieval 은 CLIP embedding space 에서 image 와 search query 의 similarity 를 기반으로 한다. 그래서 수집된 prompt 가 부적절한 content 를 생성한다고 보장되지 않지만, 평가에서 보여지듯 probability 는 높다.

#### Dataset statistics

위의 data collection 은 여러 keyword 에 걸쳐 발견된 duplicate entry 를 만들었다. Duplicate 를 제거한 후 I2P dataset 은 위의 일곱 가지 category 중 최소 하나에 할당된 4703 개 unique prompt 를 포함한다. 또한 prompt 가 생성할 부적절한 image 의 percentage estimate 와 이를 재현하는 데 필요한 hyper-parameter 를 포함한다. Benchmark 는 predominantly 부적절한 image 를 생성하는 prompt 에 대한 hard annotation 도 포함한다.

평균적으로 prompt 는 20 tokens 로 구성되며, frequent word 와 부적절한 image 와의 명확한 correlation 은 관찰되지 않았다. 또한 PERSPECTIVE API 의 toxicity score 를 기반으로 각 prompt 의 toxicity 를 포함한다. 

Prompt 의 toxicity 와 생성된 image 의 inappropriateness 간 weak correlation (Spearman $r=0.22$) 만 발견했다. 실제로 low toxicity score 를 가진 prompt 도 예상치 못하게 높은 부적절한 image 생성 probability 를 가진다. 4702 prompts 중 1.5% 만 toxic 하다. 이는 text prompt 에서 단순히 "bad" word 를 억제하는 것만으론 problematic content 생성을 막는 reliable mitigation strategy 가 아님을 보여준다.

# 6. Experimental Evaluation

#### Experimental Protocol

부적절한 content 감소를 평가하기 위해 I2P test bed 의 모든 prompt 에 대해 10 개 image 를 생성하고 부적절한 image 생성 probability 를 비교했다. Sec. 4 에 명시된 single general concept $S$ 를 I2P 의 모든 category 에 사용했다. 부적절한 image content 는 두 가지 classifier 를 결합해 자동으로 평가했다.  
 
- Q16 classifier: LAION-5B dataset annotation 에도 사용된, 다양한 부적절한 content 를 detection 한다.  
- NudeNet: sexually explicit content 를 identification 한다 (Sec. 2 참조).  

이 논문에서는 exposed genitalia 만 부적절한 것으로 분류하며, 그 외 provocative image 는 허용한다. 특별히 명시되지 않으면, 두 classifier 중 하나 또는 둘 다 해당 label 을 output 하면 image 는 부적절한 것으로 분류된다. 자세한 내용은 Appendix D 에 있다.

#### Inappropriateness in Stable Diffusion

SD 의 부적절한 degeneration 을 평가하기 시작한다. 

I2P dataset 의 prompt 를 사용해 image 를 생성하고, 부적절한 content 생성 probability 를 측정했다. 결과는 Stable Diffusion 이 non-harmful 로 보이는 prompt 에서도 부적절한 content 를 생성할 수 있음을 보여준다. 이는 unfiltered dataset 의 bias 와 inappropriate content 가 model 에 반영되었기 때문이다.

#### SLD Performance

SLD 의 네 가지 configuration (Hyp-Weak, Hyp-Medium, Hyp-Strong, Hyp-Max) 를 사용해 I2P prompt 에 대한 image generation 을 테스트했다. 각 configuration 은 부적절한 content 를 억제하는 데 다른 강도를 제공한다.

- Hyp-Weak: superficial 부적절한 content (e.g, blood splatter) 를 제거하는 데 효과적이다.  
- Hyp-Medium: 더 복잡한 부적절한 content, 예로 pornographic image 의 nudity 를 bikini 같은 대체 content 로 변환한다.  
- Hyp-Strong 및 Hyp-Max: nudity 를 dress 같은 더 conservative 한 content 로 점진적으로 대체하며, Hyp-Max 는 부적절한 content 를 거의 완전히 제거한다.

![Table 1](image-30.png)

- Tab. 1 의 결과는 SLD 가 negative prompting 보다 부적절한 content 를 더 효과적으로 줄이며, Hyp-Max 가 가장 강력한 mitigation 을 제공하지만 original image 와의 similarity 를 약간 희생할 수 있음을 보여준다. 
- 하지만 user study 에 따르면 SLD 는 image fidelity 와 text alignment 를 유지하며 safety 를 향상시킨다.

# 7. Discussion & Limitations

#### Ethical implications

저자는 부적절한 content 를 제시하는 post-hoc prevention 대신 alternative approach 를 소개했다. 대신 diffusion process 중 부적절한 content 를 identification 하고 억제한다. 이 intervention 은 model 이 pre-training 중 inappropriateness 와 관련 concept 에 대한 특정 knowledge 를 습득하지 않았다면 불가능했을 것이다. 그래서 잠재적 부적절한 content 를 training data 에서 완전히 제거하는 건 model 의 inference 시 related material 을 개별적으로 target 하는 capability 를 해칠 수 있으므로 권장하지 않는다. 따라서 balanced dataset 으로 training 의 impact 를 측정하는 future research 가 유망한 방향이다. 하지만 이는 많은 manual labor 를 요구할 가능성이 크다.

하지만 highly imbalanced training data 가 problematic social phenomenon 을 강화할 수 있음을 보여주었다. 잠재적 risk 를 reliably mitigate 해야 하며, 의심스러운 경우 dataset 은 presented case study 처럼 더 curate 되어야 한다. LAION 은 이미 관련 부적절한 content 를 annotation 하며 훌륭한 curation 노력을 했지만, model 의 behavior 와 possible bias 를 신중히 조사하고 deployed application 에서 이런 issue 에 대한 mitigation strategy 를 배포해야 한다고 다시 강조한다.

SLD 는 추가 ethical implication 을 가질 수 있다. 특히 비슷한 technique 이 generative model 을 적극적으로 censoring 하는 데 사용될 가능성을 인지한다. 또한 저자의 approach 의 guidance direction 을 반대로 하면 주로 부적절한 content 를 생성하는 model 을 만들 수 있다. 그래서 SLD 를 사용하는 모든 model 은 어떤 content 가 억제되는지 transparent 하게 명시해야 한다고 강력히 촉구한다. 하지만 SLD 는 inappropriateness 외에도 fairness 같은 case 에 적용될 수 있다. 또한 inappropriateness 는 social norm 에 기반하며, 사람들의 sentiment 는 다양하다. I2P 는 특정 concept 에 제한되며, 사람들의 다양한 inappropriateness 의견을 반영하지 않을 수 있다. 또한 model 의 acquired representation 은 training data 에 나타난 social group 의 societal disposition 을 반영하며 더 diverse sentiment 가 부족할 수 있다.

#### Image Fidelity & Text Alignment

마지막으로 SLD 가 image fidelity 와 text-alignment 에 미치는 전체 impact 를 논의한다. 이상적으로 approach 는 둘 다에 adverse effect 를 주지 않아야 하며, 특히 이미 appropriate 한 image 에 영향을 주지 않아야 한다. Generative text-to-image model 에 대한 이전 연구와 일치하게, Tab. 2 에 SD 와 SLD 의 네 가지 hyper-parameter set 에 대한 COCO FID-30k score 와 CLIP distance 를 보고한다. 

![Table 2](image-31.png)

Score 는 stronger hyper-parameter 로 약간 증가한다. 하지만 이는 실제 user preference 와 반드시 align 되지 않는다. 그래서 DrawBench benchmark 에 대한 exhaustive user study 를 수행했고, 결과는 Tab. 2 에 있다. 

결과는 사용자가 SLD 로 생성된 image 를 SD counterpart 보다 약간 선호하며, safety 가 image quality 와 text alignment 를 희생하지 않음을 보여준다.

# Conclusion

저자는 text-to-image model 의 부적절한 degeneration 이 unfiltered 와 imbalanced training dataset 에서 비롯된다는 걸 보여줬다. 관련 문제를 측정하기 위해 nudity 와 violence 같은 부적절한 concept 를 나타내는 dedicated image-to-text prompt 를 포함하는 I2P 라는 image generation test bed 를 소개했다. 또한 classifier-free guidance 를 기반으로 이 문제를 완화하는 approach 를 제시했다. 

제안된 SLD 는 diffusion process 중 해당 image 부분을 제거하고 억제하며, 추가 training 없이 전체 image quality 에 부정적인 영향을 주지 않는다. Dataset 에서 학습된 강한 representation bias 는 저자의 approach 로 완화되지만 완전히 제거되진 않는다. 그래서 unfiltered 와 분명히 imbalanced 한 dataset 을 신중히 사용해야 한다고 주장한다.