<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Computer Vision/Image Classification/2020-10-ViT">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">An Image Is Worth 16X16 Words: Transformers for image recognition at sacle | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Computer Vision/Image Classification/ViT"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="An Image Is Worth 16X16 Words: Transformers for image recognition at sacle | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Computer Vision/Image Classification/ViT"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Computer Vision/Image Classification/ViT" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Computer Vision/Image Classification/ViT" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.f5f9b19d.js" as="script">
<link rel="preload" href="/assets/js/main.e0ea5c8f.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Adversarial Attack</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Generation/ Concept Editing/TIME">Generation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Computer Vision/Image Classification/ViT">Image Classification</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Computer Vision/Image Classification/ViT">An Image Is Worth 16X16 Words: Transformers for image recognition at sacle</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Computer Vision/Image Classification/EfficientNetV2">EfficientNetV2: Smaller Models and Faster Training</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Multi-task/Unified Interface">Multi-task</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/PEFT/Composition/RLRR">PEFT</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Vision-Language</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Computer Vision</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Image Classification</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">An Image Is Worth 16X16 Words: Transformers for image recognition at sacle</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>An Image Is Worth 16X16 Words: Transformers for image recognition at sacle</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/abs/2010.11929v2" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2010.11929v2</a></p><h1>Abstract</h1><p>NLP task 에서 Transformer Architecture 가 사용되어 오고 있다.</p><p>본 논문에서는 vision 의 CNN 구조를 유지하면서 Transformer 를 적용해 훌륭한 결과로 SOTA 를 달성하였다.</p><p>이를 Vision Transformer (ViT) 로 명명한다.</p><h1>1. Introduction</h1><ul><li>비전 분야에서, CNN-like 아키텍처 와 self-attention 을 결합하는 여러 시도가 있었고, 일부에서는 CNN 을 완전 교체</li><li>후자 모델은 이론적으로는 효율적이지만 GPU 에서 효과적으로 확장되지 않았음</li></ul><p>위 사항들로 여전히 ResNet 과 같은 아키텍처가 여전히 SOTA 를 달성</p><p>본 논문은, 이에 가능한 한 적은 수정으로 표준 Transformer 를 직접 이미지에 적용하기 위해 실험을 진행</p><ul><li>이미지를 패치 단위로 분할</li><li>분할된 패치의 선형 임베딩 시퀀스를 NLP 의 토큰처럼 처리하여 Transformer 에 입력으로 제공</li></ul><p>중간 규모 데이터셋(ImageNet)으로 학습했을 때는 ResNet 보다 정확도가 낮은데, Transformer 는 equivariance 와 locality 같은 inductive bias 가 부족하여 일반화가 잘 되지 않기 때문으로 보인다.</p><p>큰 규모의 데이터셋(14M-300M 이미지)에서 훈련되면, 위 단점이 보완되어 다른 데이터셋에 transfer learning 을 한 결과 뛰어난 정확도를 보였다.</p><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary><b><u>inductive bias</u></b></summary><div><div class="collapsibleContent_i85q"><p>  inductive bias 란 머신 러닝이 학습할 때, 어떤 가정이나 제한을 통하여 문제 공간을 탐색하는 방식
모델이 데이터로부터 일반화를 하는 데 중요한 역할</p><p>  딥 러닝에서 inductive bias 의 예로 Layer 의 수, 각 Layer 의 뉴런 수, activation function 등의 구조적 제한이 있다.
적절한 inductive bias 로 일반화 성능을 올릴 수 있지만, 부적절하면 과적합을 발생한다.</p><p>  Trnasformer 에서 이러한 inductive bias 가 부족한 이유는 논문의 방법론 설명에 나온다.</p></div></div></details><h1>2. Related Work</h1><p>Transformer 를 이미지 처리에 적용하려면 각 픽셀이 다른 모든 픽셀에 대한 self-attention 을 적용해야 했기 때문에 실질적으론 힘들다.</p><ol><li>self-attention</li></ol><p>따라서 각 쿼리 픽셀의 지역 이웃에 대해서 self-attention 을 적용하거나, Sparse Transformer, block 크기 가변적 조정 등 다양한 방법으로 self-attention 을 적용하였지만 GPU 에서 효율적이지 못하였다.</p><ol start="2"><li>patch</li></ol><p>이어, Cordonnier 모델은 2x2 크기 패치로 self-attention 을 적용하였는데, ViT 와 유사하지만, 패치가 작아 작은 해상도에서만 적용이 가능하나 본 연구는 중간 해상도에도 적용이 가능하다.</p><ol start="3"><li>combination CNN with self-attention</li></ol><p>Bello 등은 feature map 에 self-attention 을 추가, Hu 등은 CNN 의 출력 결과를 self-attention 으로 추가 처리하며 다양하게 CNN 과 self-attention 을 결합한 연구가 이루어지고 있다.</p><p>본 연구에서는 ImageNet 보다 큰 ImageNet-21k 와 JFT-300M 과 같은 대규모 데이터셋에서, 데이터셋 크기에 따른 CNN 성능이 어떻게 변화하는 지 연구하며 Transformer 를 훈련시킨다.</p><h1>3. Method</h1><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/b1f17f21-4a77-4fbd-8f61-6351b8a0989e/image.png" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-vision-transformer-vit">3.1 Vision Transformer (ViT)<a href="#31-vision-transformer-vit" class="hash-link" aria-label="Direct link to 3.1 Vision Transformer (ViT)" title="Direct link to 3.1 Vision Transformer (ViT)">​</a></h2><p>표준 Transformer (NLP) 에서는 token embedding 의 1D sequence 를 받는다.</p><p>2D 인 이미지를 다루기 위해, 이미지를 다음과 같이 처리한다.</p><ul><li><strong>이미지 입력 Embedding</strong><ul><li>이미지 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^{H \times W \times C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em">W</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span></span></span></span></span></span></span></span></span></span></span></span></span> 를 flattened 2D patch 의 sequence 인 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>p</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><mo stretchy="false">(</mo><msup><mi>P</mi><mn>2</mn></msup><mo>⋅</mo><mi>C</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">x_p \in \mathbb{R}^{N \times ( P^2 \cdot C )}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8252em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.9869em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9869em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="mbin mtight">×</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">⋅</span><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span> 로 재배치한다.</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(H, W)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mclose">)</span></span></span></span></span> : 원본 이미지 해상도</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span></span> : 채널 수</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>P</mi><mo separator="true">,</mo><mi>P</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(P, P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mclose">)</span></span></span></span></span> : 각 이미지 패치의 해상도</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mi>H</mi><mi>W</mi><mi mathvariant="normal">/</mi><msup><mi>P</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">N = HW/P^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mord">/</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> : 생성된 패치의 수, Transformer 의 효과적인 input sequence 길이</li></ul></li></ul><p>Transformer 는 모든 레이어에서 일정한 latent vector 사이즈 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span></span> 를 사용하므로, 패치를 flatten 하고 linear projection (아래의 식 1) 으로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span></span> 차원으로 매핑한다.</p><p>이 projection 의 출력을 <strong>patch embedding</strong> 이라 칭한다.</p><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary><b><u>latent vector</u></b></summary><div><div class="collapsibleContent_i85q">$$<p>  latent vector 는 데이터를 압축하고 잠재적인 특징을 추출하는 데 사용되는 벡터이다.
일반적으로 입력 데이터를 저차원 벡터 공간으로 매핑하여 유용한 정보를 추출하고, 해당 정보를 바탕으로 다양한 테스크를 수행하는 데 사용된다.</p></div></div></details><ul><li><strong>Token</strong><ul><li>BERT 의 <code>[ class ]</code> token 과 유사하게,  embedding patch 에 학습 가능한 embedding ( <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mn>0</mn><mn>0</mn></msubsup><mo>=</mo><msub><mi>x</mi><mtext>class</mtext></msub></mrow><annotation encoding="application/x-tex">z_{0}^{0} = x_{\textup{class}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0622em;vertical-align:-0.2481em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-2.4519em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord textup mtight">class</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> ) 을 추가</li><li>Transformer encoder 의 출력 상태 ( <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mi>L</mi><mn>0</mn></msubsup></mrow><annotation encoding="application/x-tex">z_{L}^{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0894em;vertical-align:-0.2753em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-2.4247em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2753em"><span></span></span></span></span></span></span></span></span></span></span> ) 는 이미지 표현 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 로 사용 (아래의 식 4)</li><li>pre-training 및 fine-tuning 을 수행할 때 Classification Head 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mi>L</mi><mn>0</mn></msubsup></mrow><annotation encoding="application/x-tex">z_{L}^{0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0894em;vertical-align:-0.2753em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-2.4247em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2753em"><span></span></span></span></span></span></span></span></span></span></span> 에 부착</li></ul></li><li><strong>Classification Head</strong><ul><li>pre-training : 1-hidden layer 인 MLP</li><li>fine-tuning : 1-linear layer</li></ul></li><li><strong>Position embedding</strong><ul><li>position 정보 유지를 위해, Position embedding 을 patch embedding 에 추가</li><li>2D-aware position embedding 의 성능 향상은 관찰되지 않음
→ 1D position embedding 사용</li><li>embedding vector 의 sequence 를 encoder 에 입력</li></ul></li><li><strong>Transformer encoder</strong><ul><li>Multi-Head self-attention 와 MLP Block 을 교차하는 Layer 를 포함</li><li>Layernorm (LN) 을 모든 Block 이전에 적용</li><li>Residual Connection 을 모든 블록 끝에 적용</li><li>MLP 는 GELU non-linearity 가 있는 두 개의 레이어를 포함</li></ul></li></ul><p>(1) <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>o</mi></msub><mo>=</mo><mo stretchy="false">[</mo><msub><mi>x</mi><mtext>class</mtext></msub><mo separator="true">;</mo><mtext> </mtext><msubsup><mi>x</mi><mi>p</mi><mn>1</mn></msubsup><mtext>E</mtext><mo separator="true">;</mo><mtext> </mtext><msup><mi>x</mi><mn>1</mn></msup><mo>=</mo><msub><mn>2</mn><mi>p</mi></msub><mtext>E</mtext><mo separator="true">;</mo><mtext> </mtext><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">;</mo><mtext> </mtext><msubsup><mi>x</mi><mi>p</mi><mi>N</mi></msubsup><mtext>E</mtext><mo stretchy="false">]</mo><mo>+</mo><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub><mo separator="true">,</mo><mtext> </mtext><mi>E</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><mo stretchy="false">(</mo><msup><mi>P</mi><mn>2</mn></msup><mo>⋅</mo><mi>C</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><mtext> </mtext><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">z_o = [x_\textup{class};\ x^1_p\textup{E};\ x^1=2_p\textup{E};\ ...;\ x^N_p\textup{E}] + E_{pos},\ E \in \mathbb{R}^{N \times ( P^2 \cdot C )},\ E_{pos} \in \mathbb{R}^{(N + 1) \times D}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1972em;vertical-align:-0.3831em"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord textup mtight">class</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mord text"><span class="mord textup">E</span></span><span class="mpunct">;</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2244em;vertical-align:-0.3831em"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord text"><span class="mord textup">E</span></span><span class="mpunct">;</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mpunct">;</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mord text"><span class="mord textup">E</span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.273em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9869em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="mbin mtight">×</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em"><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">⋅</span><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.888em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span></span></span></span></span></span></span></span></span></span></span></span></span></p><ul><li>입력에 해당하는 수식으로, patch 로 나눈 각 이미지 시퀀스다.</li><li>마지막 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">E_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 는 각 시퀀스의 순서를 나타내는 Positional Encoding 이다.</li></ul><p>(2) <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mi>ϱ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo>=</mo><mtext>MSA</mtext><mo stretchy="false">(</mo><mtext>LN</mtext><mo stretchy="false">(</mo><msub><mi>z</mi><mrow><mi>ϱ</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><msub><mi>z</mi><mrow><mi>ϱ</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mtext> </mtext><mi>ϱ</mi><mo>=</mo><mn>1</mn><mtext> </mtext><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mtext> </mtext><mi>L</mi></mrow><annotation encoding="application/x-tex">z&#x27;_\varrho = \textup{MSA}(\textup{LN}(z_{\varrho-1})) + z_{\varrho-1},\ \varrho = 1 \ ...\ L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.135em;vertical-align:-0.3831em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-2.453em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϱ</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord text"><span class="mord textup">MSA</span></span><span class="mopen">(</span><span class="mord text"><span class="mord textup">LN</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ϱ</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ϱ</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">ϱ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">1</span><span class="mspace"> </span><span class="mord">...</span><span class="mspace"> </span><span class="mord mathnormal">L</span></span></span></span></span></p><ul><li>Transformer Encoder 의 수식으로, Layer Normalization (LN) 후, Multi-head Attention (MSA) 을 적용</li><li>이후, Residual 로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mrow><mi>ϱ</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">z_{\varrho - 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ϱ</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 을 더한다</li></ul><p>(3) <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>ϱ</mi></msub><mo>=</mo><mtext>MLP</mtext><mo stretchy="false">(</mo><mtext>LN</mtext><mo stretchy="false">(</mo><msubsup><mi>z</mi><mi>ϱ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><msubsup><mi>z</mi><mi>ϱ</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo separator="true">,</mo><mspace width="2em"></mspace><mi>ϱ</mi><mo>=</mo><mn>1</mn><mtext> </mtext><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mtext> </mtext><mi>L</mi></mrow><annotation encoding="application/x-tex">z_\varrho = \textup{MLP}(\textup{LN}(z&#x27;_\varrho)) + z&#x27;_\varrho,\qquad \varrho = 1 \ ...\ L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϱ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.135em;vertical-align:-0.3831em"></span><span class="mord text"><span class="mord textup">MLP</span></span><span class="mopen">(</span><span class="mord text"><span class="mord textup">LN</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-2.453em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϱ</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.135em;vertical-align:-0.3831em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-2.453em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϱ</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:2em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">ϱ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord">1</span><span class="mspace"> </span><span class="mord">...</span><span class="mspace"> </span><span class="mord mathnormal">L</span></span></span></span></span></p><ul><li>MLP head 의 수식으로, 위와 동일하다.</li></ul><p>(4) <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mtext>LN</mtext><mo stretchy="false">(</mo><msubsup><mi>z</mi><mi>L</mi><mn>0</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = \textup{LN}(z^0_L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0894em;vertical-align:-0.2753em"></span><span class="mord text"><span class="mord textup">LN</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-2.4247em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">L</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2753em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><ul><li>마지막으로, classification 으로 target 값을 찾는다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="intuctive-bias">Intuctive bias<a href="#intuctive-bias" class="hash-link" aria-label="Direct link to Intuctive bias" title="Direct link to Intuctive bias">​</a></h3><p>CNN 은 locality, 2D 이웃 구조체를 내재
반면, ViT 는 MLP 레이어만 지역성을 가지고, self-attention 레이어는 전역적이기 때문에 CNNs 보다 inductive bias 가 적다.</p><p>Transformer 에서는 이 2D 이웃 구조체는 극히 드물게 사용한다.</p><ul><li>모델 시작 부분에서 이미지를 패치로 자르는 것</li><li>fine-tuning 시 이미지의 position embedding 조절</li><li>position embedding 은 초기화 시 패치들의 2D position 정보가 없으며, 패치 간의 모든 관계를 처음부터 학습 해야함</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="hybris-architecture">Hybris Architecture<a href="#hybris-architecture" class="hash-link" aria-label="Direct link to Hybris Architecture" title="Direct link to Hybris Architecture">​</a></h3><p>원시적인 Image patch 대안으로, sequence 는 CNN 의 feature map 으로 구성하는 것이 가능하여 Hybrid model 을 만든다.</p><p>patch embedding projection <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> (식 1) 은 CNN feature map 으로 추출된 patch 에 적용한다.</p><p>특별한 경우, patch 는 1x1 사이즈도 가능 한데, 이는 sequence 가 feature map 의 차원을 flatten 하고 projecting 하여 Transformer 차원으로 얻을 수 있다는 것을 의미한다.</p><p><strong>CNN feature map 으로 patch 추출 → Flatten → embedding projection <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> (식 1)</strong></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-fine-tuning-and-higher-resolution">3.2 fine-tuning and higher resolution<a href="#32-fine-tuning-and-higher-resolution" class="hash-link" aria-label="Direct link to 3.2 fine-tuning and higher resolution" title="Direct link to 3.2 fine-tuning and higher resolution">​</a></h2><p>일반적으로 ViT 를 큰 데이터셋에 pre-training 하고 downstream task 로 fine-tuning</p><p>이를 위해 다음 사항들을 진행한다.</p><ul><li>prediction head 제거</li><li>0 으로 초기화된 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>×</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">D \times K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> 를 feedforward layer 에 부착 (여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> 는 downstream class 수)</li><li>pre-training 보다 높은 해상도로 fine-tune 하는 것이 이득</li><li>높은 해상도로 할 땐, patch size 를 동일하게 해야함 → 더 큰 effective sequence length 가 생성<ul><li>fine-tune 시, pre-trained 의 position embedding 은 의미가 없음</li><li>따라서 pre-trained 의 position embedding 을 2D 보간 (interpolation) 해야 함</li></ul></li></ul><p>위의 해상도 조정 및 패치 추출은 ViT 에 대한 이미지 구조의 inductive bias 를 삽입하는 방법</p><h1>4. Experiments</h1><p>Resnet, ViT, hybrid 를 각각 평가한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-setup">4.1 Setup<a href="#41-setup" class="hash-link" aria-label="Direct link to 4.1 Setup" title="Direct link to 4.1 Setup">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="datasets">Datasets<a href="#datasets" class="hash-link" aria-label="Direct link to Datasets" title="Direct link to Datasets">​</a></h3><ul><li>train dataset<ul><li>ILSVRC-2012 ImageNet-1k (1.3M images)</li><li>ImageNet-21k (14M images)</li><li>JFT-18k (303M high resolution images)</li></ul></li><li>transfer dataset<ul><li>transfer learning 평가<ul><li>ImageNet</li><li>ImageNet ReaL</li><li>CIFAR-10/100</li><li>Oxford-IIIT Pets/-Flowers102</li><li>VTAV </li></ul></li><li>또한 19-task VTAB 분류 평가로 여러 그룹의 작은 데이터셋 transfer 평가<ul><li>Natural : Pets, CIFAR</li><li>Specialized : medical, satellite imagery</li><li>Structured : geometric, localization</li></ul></li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-variants">Model Variants<a href="#model-variants" class="hash-link" aria-label="Direct link to Model Variants" title="Direct link to Model Variants">​</a></h3><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/d9ef2533-44a5-4e82-abca-0f14964f5f0c/image.png" class="img_ev3q"></p><p>Table 1 의 Base, Large, Huge 는 BERT 를 기반으로한 ViT 의 구성이다.</p><p>간단한 표기법으로 모델 사이즈 및 input patch 사이즈를 다음과 같이 나타낸다.</p><ul><li><strong>ViT-L/16</strong> : 16x16 input patch 및 &quot;Large&quot; 모델<ul><li>sequence length 는 patch 크기 제곱에 반비례 →  patch 가 작을수록 계산량 증가</li></ul></li></ul><p>CNN 베이스라인은 ResNet 으로, 다음과 같이 수정을 더하였다.</p><ul><li>Batch Normalization → Group Normalization &amp; Standardized Convolution<ul><li>이렇게 수정한 ResNet 을 BiT 라 표시</li><li>transfer learning 의 성능 향상</li></ul></li></ul><p>Hybrid 모델의 경우, 중간 feature map 으로 ViT 에 한 픽셀 (1x1) 의 patch 사이를 입력 </p><ul><li>각기 다른 sequence length 로 실험<ul><li>ResNet50 - stage 4</li><li>ResNet50 - stage 3 (단, Layer 수 유지) → 4배 더 긴 sequence length (stage 3 의 output 이 확장되므로)</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="training--fine-tuning">Training &amp; Fine-tuning<a href="#training--fine-tuning" class="hash-link" aria-label="Direct link to Training &amp; Fine-tuning" title="Direct link to Training &amp; Fine-tuning">​</a></h3><p>train model</p><ul><li>include ResNet</li><li>Adam ( <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">\beta_1 = 0.9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.9</span></span></span></span></span> , <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding="application/x-tex">\beta_2 = 0.999</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.999</span></span></span></span></span> )</li><li>batch size 4096</li><li>weight decay 0.1</li></ul><p>fine-tune</p><ul><li>SGD</li><li>batch size 512</li><li>고해상도 변경 : 512 - ViT-L/16, 518 - ViT-H/14</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="metrics">Metrics<a href="#metrics" class="hash-link" aria-label="Direct link to Metrics" title="Direct link to Metrics">​</a></h3><p>few-shot 정확도는 다음과 같이 얻는다.</p><ol><li>고정된 훈련 이미지의 representation 추출</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><msup><mo stretchy="false">}</mo><mi>K</mi></msup></mrow><annotation encoding="application/x-tex">\{ -1,1 \}^K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span></span></span></span></span></span></span></span></span>  target vector 로 매핑</li><li>위 과정으로 regularized least-squares regression 문제를 해결하여 얻음</li></ol><p>주로 fine-tuning 성능에 초점을 두지만, 비용이 많이 들어 linear few-shot 정확도를 빠르게 평가</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-comparison-to-state-of-the-art">4.2 Comparison to State Of The Art<a href="#42-comparison-to-state-of-the-art" class="hash-link" aria-label="Direct link to 4.2 Comparison to State Of The Art" title="Direct link to 4.2 Comparison to State Of The Art">​</a></h2><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/cb4ff314-2aa4-4e41-82fd-f833500ce529/image.png" class="img_ev3q"></p><p>ViT-H/14 와 ViT-L/16 를 CNN SOTA 모델들과 비교 한다.</p><ul><li>BiT</li><li>Large EfficientNet 으로 semi-supervised 훈련한 Noisy Student</li></ul><p>모든 모델은 TPUv3 로 훈련을 진행하였고, 결과는 위 Table 에서 나타난다.</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/1f27b8b9-8b16-44f8-beb8-6f6d5f6cd618/image.png" class="img_ev3q"></p><p>위 그림은 각 VTAB 작업 그룹에 따른 결과 비교이다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-pre-trained-data-requirements">4.3 Pre-trained Data Requirements<a href="#43-pre-trained-data-requirements" class="hash-link" aria-label="Direct link to 4.3 Pre-trained Data Requirements" title="Direct link to 4.3 Pre-trained Data Requirements">​</a></h2><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/6ee1a013-c334-4393-bb42-e426686c3987/image.png" class="img_ev3q"></p><p>각각 ImageNet, ImageNet-21k, JFT-300M 데이터셋으로 pre-train 한 결과 JFT-300M 이 가장 좋았다.</p><p>이는 ResNet 보다 inductive bias 가 적기 때문에 많은 데이터가 있어야 하기 때문이다.</p><p>각 pre-train 후 작은 데이터셋에서 성능을 올리기 위해, 세 가지 regularization 을 설정한다.</p><ul><li>weight decay</li><li>dropout</li><li>label smoothing</li></ul><p>두 번째로, JFT-300M 의 하위집합으로 9M, 30M, 90M 및 전체를 학습하여 비교한다.</p><p>하위 집합에는 추가적인 정규화를 수행하지 않고 모든 하이퍼파라미터를 동일하게 사용한다.</p><p>여기서 fine-tuning 정확도 대신 linear few-shot 정확도를 관찰한다.
위 그래프에서 보이듯 작은 데이터셋에는 ResNet 보다 오버피팅 발생한다.</p><p>이 결과로 다음을 알 수 있다.</p><ul><li>transfer learning 시 작은 데이터셋에는 convolution inductive bias 가 유리</li><li>transfer learning 시 ViT 는 큰 데이터셋에 유리</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="44-scailing-study">4.4 Scailing Study<a href="#44-scailing-study" class="hash-link" aria-label="Direct link to 4.4 Scailing Study" title="Direct link to 4.4 Scailing Study">​</a></h2><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/c310c997-636b-4de6-aedb-6b04e99d7474/image.png" class="img_ev3q"></p><p>각 Transformer, BiT, Hybrid 의 크기에 대한 실험</p><p>모델은 다음과 같다</p><ul><li>ResNets, R50x1, R50x2 R101x1, R152x1, R152x2 pre-train for 14 epochs
R152x2, R200x3 pre-train for 14 epochs</li><li>ViT-B/32, B/16, L/32, L/16 pre-train for 7 epochs
L/17, H/14 pre-train for 14 epochs</li><li>R50+ViT-B/32, B/16, L/32, L/16 pre-train for 7 epochs
R50-ViT-L/16 pre-train for 14 epochs</li></ul><p>이 실험에서 다음을 관찰할 수 있다.</p><ul><li>ViT 는 성능/계산 적으로 BiT 보다 뛰어남</li><li>적은 비용에선 Hybrid 가 ViT 보다 약간 더 좋음</li><li>ViT 는 포화되지 않아, 성능이 더 좋아질 수도 있다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="45-inspecting-vision-transformer">4.5 Inspecting Vision Transformer<a href="#45-inspecting-vision-transformer" class="hash-link" aria-label="Direct link to 4.5 Inspecting Vision Transformer" title="Direct link to 4.5 Inspecting Vision Transformer">​</a></h2><p>ViT 가 이미지를 처리하는 법을 이해하기 위해 internal representation 을 분석한다.</p><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary><b><u>internal representation</u></b></summary><div><div class="collapsibleContent_i85q"><p>  이미지 처리 과정에 모델은 입력 이미지의 다양한 특징을 추출하며, 이러한 특징을 모델 내부의 데이터에 맞는 representation 으로 변환한다.</p><p>  Transformer 에서는 이미지 정보를 2D attention 메커니즘으로 1D embedding 으로 변환한다.
이 embedding 이 Transformer 의 internal representation 이다.</p></div></div></details><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/33d06753-c156-4825-b77d-7c402e005a7f/image.png" class="img_ev3q"></p><ul><li><strong>왼쪽</strong> : ViT-L/32 로 RGB 의 initial linear embedding 로 Filter<ul><li>각 Filter 는 저차원의 CNN filter 와 유사</li></ul></li><li><strong>중앙</strong> : ViT-L/32 의 position embedding 유사도<ul><li>Projection 후 position embedding 이 patch representation 에 추가</li><li>즉, 더 가까운 패치 (같은 행/열 등)는 더 유사한 position embedding 을 가짐 → patch 간의 공간정보가 잘 학습</li></ul></li><li><strong>오른쪽</strong> : head 와 network depth 에 따른 attened area 사이즈</li></ul><p>self-attention 으로 전체 이미지 정보를 통합 가능한지 조사</p><ul><li>attention weight 를 기반으로 image space 간 평균 거리 계산</li><li>이 <strong>attention distance</strong> 는 CNN 의 receptive field 와 유사</li><li>낮은 layer 의 self-attention head (낮은 attention distance 지님) 는 CNN 의 초기 convolutional layer 와 유사한 기능을 가짐 → localization 효과</li></ul><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/e720a409-e57a-43e2-a7bf-600fb833852c/image.png" class="img_ev3q"></p><p>나아가 이미지 분류에 의미있는 영역에 attention 하는 것 발견</p><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary><b><u>receptive field</u></b></summary><div><div class="collapsibleContent_i85q"><p>  receptive field 는 입력된 이미지의 어떤 부분에서 반응을 보이는지를 나타내는 개념</p></div></div></details><h2 class="anchor anchorWithStickyNavbar_LWe7" id="46-self-supervision">4.6 self-supervision<a href="#46-self-supervision" class="hash-link" aria-label="Direct link to 4.6 self-supervision" title="Direct link to 4.6 self-supervision">​</a></h2><p>NLP task 에서 self-supervision 을 시도</p><ul><li>예로, BERT 에서의 masking</li><li>ViT 에선 masked patch 를 예측함으로 self-supervision</li></ul><p>ViT 에서 self-supervision 한 결과</p><ul><li>ViT-B/16 모델은 ImageNet 에서 79.9% 정확도 달성</li><li>Supervised Learning 보다는 4% 떨어짐</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-conclusion">5. Conclusion<a href="#5-conclusion" class="hash-link" aria-label="Direct link to 5. Conclusion" title="Direct link to 5. Conclusion">​</a></h2><ul><li>patch 추출 단계에서 image-specific inductive bias 를 도입하지 않음</li><li>대신, 이미지를 sequence 처럼 해석하여 표준 Transformer encoder 로 진행</li><li>큰 데이터셋으로 pre-train 하면 잘 작동 (위 실험의 JFT-300M)</li><li>남은 과제<ul><li>detection, segmentation 에서는 ViT 를 어떻게 적용할 것인가</li><li>self-supervised 와 큰 규모의 supervised pre-training 간의 갭</li></ul></li><li>ViT 의 scailing 으로 성능 향상 기대</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vi-t">ViT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-transformer">Vision Transformer</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/image-classification">Image Classification</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/computer-vision">Computer Vision</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/paper">Paper</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Computer Vision/Image Classification/2020-10-ViT.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Computer Vision/Generation/Interpretability/Info-Decomp"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Interpretable Diffusion via Information Decomposition</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Computer Vision/Image Classification/EfficientNetV2"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">EfficientNetV2: Smaller Models and Faster Training</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-vision-transformer-vit" class="table-of-contents__link toc-highlight">3.1 Vision Transformer (ViT)</a><ul><li><a href="#intuctive-bias" class="table-of-contents__link toc-highlight">Intuctive bias</a></li><li><a href="#hybris-architecture" class="table-of-contents__link toc-highlight">Hybris Architecture</a></li></ul></li><li><a href="#32-fine-tuning-and-higher-resolution" class="table-of-contents__link toc-highlight">3.2 fine-tuning and higher resolution</a></li><li><a href="#41-setup" class="table-of-contents__link toc-highlight">4.1 Setup</a><ul><li><a href="#datasets" class="table-of-contents__link toc-highlight">Datasets</a></li><li><a href="#model-variants" class="table-of-contents__link toc-highlight">Model Variants</a></li><li><a href="#training--fine-tuning" class="table-of-contents__link toc-highlight">Training &amp; Fine-tuning</a></li><li><a href="#metrics" class="table-of-contents__link toc-highlight">Metrics</a></li></ul></li><li><a href="#42-comparison-to-state-of-the-art" class="table-of-contents__link toc-highlight">4.2 Comparison to State Of The Art</a></li><li><a href="#43-pre-trained-data-requirements" class="table-of-contents__link toc-highlight">4.3 Pre-trained Data Requirements</a></li><li><a href="#44-scailing-study" class="table-of-contents__link toc-highlight">4.4 Scailing Study</a></li><li><a href="#45-inspecting-vision-transformer" class="table-of-contents__link toc-highlight">4.5 Inspecting Vision Transformer</a></li><li><a href="#46-self-supervision" class="table-of-contents__link toc-highlight">4.6 self-supervision</a></li><li><a href="#5-conclusion" class="table-of-contents__link toc-highlight">5. Conclusion</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.f5f9b19d.js"></script>
<script src="/assets/js/main.e0ea5c8f.js"></script>
</body>
</html>