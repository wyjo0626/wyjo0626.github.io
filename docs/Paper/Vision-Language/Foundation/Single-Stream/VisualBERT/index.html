<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/Foundation/Single-Stream/2019-08-VisualBERT">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">VisualBERT: A Simple And Performance Baseline For Visual And Language | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/Foundation/Single-Stream/VisualBERT"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="VisualBERT: A Simple And Performance Baseline For Visual And Language | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처:"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처:"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/Foundation/Single-Stream/VisualBERT"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/Foundation/Single-Stream/VisualBERT" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/Foundation/Single-Stream/VisualBERT" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.a65adec1.js" as="script">
<link rel="preload" href="/assets/js/main.d181d84f.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Reinforce Learning/PPO/RLAIF/Constitutional AI">Reinforce Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">CLIP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Foundation</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Contrastive Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Single-Stream/VLBERT">Single-Stream</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Single-Stream/VLBERT">VL-BERT: Pre-Training Of Generic Visual-Linguistic Representations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Single-Stream/VisualBERT">VisualBERT: A Simple And Performance Baseline For Visual And Language</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Single-Stream/UNITER">UNITER: UNiversal Image-TExt Representation Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Single-Stream/ImageBERT">ImageBERT: Cross-Modal Pre-Training with Large-Scale Weak-Supervised Image-Text Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Single-Stream/Pixel-BERT">Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Single-Stream/VD-BERT">VD-BERT: A Unified Vision and Dialog Transformer with BERT</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Two-Stream/LXMERT">Two-Stream</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">VQA-IC</a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Foundation</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Single-Stream</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">VisualBERT: A Simple And Performance Baseline For Visual And Language</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>VisualBERT: A Simple And Performance Baseline For Visual And Language</h1></header><p>논문 및 이미지 출처: <a href="https://arxiv.org/pdf/1908.03557.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1908.03557.pdf</a></p><h1>Abstract</h1><p>vision-language tasks 의 넓은 범위에 modeling 하기 위한 간단하고 유연한 프레임워크인 <strong>VisualBERT</strong> 제안</p><ul><li>input text 요소 및 input image 영역을 self-attention 으로 implicitly align 하는 Transformer layer stack 로 구성</li><li>image caption data 에 VisualBERT pre-training 을 위해 two visually-grounded language model objectives 제안</li><li>VQA, VCR, NLVR<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> 및 Flickr30K 을 포함한 vision-language tasks 실험에서 매우 간단하지만 SOTA 를 능가하거나 견줄 정도</li><li>추가 분석으로 supervision 없이 language 요소를 image 영역에 매칭시킬 수 있으며, 동사와 해당 이미지 영역간의 문법적 관계에도 민감</li></ul><h1>1. Introduction</h1><p>vision 과 language combining task 는 시각적 정보가 풍부한 시스템의 추론 능력에 대한 평가를 제공</p><ul><li>object 가 무엇인지 인식하는 이상으로, captioning, visual question answering 및 visual reasoning 과 같은 vision-language tasks 는 이미지의 <em>detailed semantics</em> 를 이해하도록 요구</li><li>이는 객체, 특성, 부분, 공간적 관계, 동작 및 의도를 포함한 세부 의미를 natural language 로 표현하고 언어로 어떻게 참조되고 구체화되는지를 이해하는데 도전</li></ul><hr><p>본 논문에서는 풍부한 vision 과 text 의 의미를 포착하기 위해 설계된 간단하고 유연한 모델인 VisualBERT 제안</p><ul><li>NLP 를 위한 Transformer 기반 모델인 BERT 와 Faster-RCNN 같은 pre-trained objective 시스템을 통합하여 vision-and-language tasks 에 적용</li><li>특히, object 에서 추출된 image features 는 unordered input tokens 로 처리되어 text 와 함께 VisualBERT 에 주입</li><li>text 및 image inputs 은 VisualBERT 의 multiple Transformer layers 에서 공동으로 처리</li><li>words 및 object 사이의 rich interaction 은 모델이 text 와 image 사이의 복잡한 관계를 포착할 수 있도록 함</li></ul><hr><p>BERT 와 유사하게, exteral resource 에서의 pre-training VisualBERT  는 downstream 적용에 이점을 주므로, images 와 text 간의 관계를 학습하기 위해 image 의 <em>detailed semantics</em> 가 natural language 로 표현되는 image caption data 에 VisualBERT pre-training 을 고려한다.</p><p>저자는 pre-training 을 위해 two <em>visually-grounded</em> language model 을 제안</p><ol><li>text 일부가 masking 되고 model 은 remaining text 와 visual context 를 기반으로 masked words 를 예측</li><li>모델은 text 가 image 와 일치하는지 여부를 결정하도록 훈련</li></ol><p>image caption data 에 대한 이런 pre-training 은 VisualBERT 가 transferable text 및 visual representations 를 학습하는데 중요함을 보여줌</p><p>저자는 vision-language tasks 에 대해 포괄적 실험 수행</p><ol><li>visual question answering (VQA)</li><li>visual commonsense reasoning (VCR)</li><li>natural language for visual reasoning (NLVR<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>)</li><li>region-to-phrase grounding (Flickr30K)</li></ol><p>결과 COCO image caption dataset 에 VisualBERT 를 pre-training 함으로써 VisualBERT 가 SOTA model 을 능가하거나 견주는 것을 보여줌</p><ul><li>또한 저자의 설계가 정당화하기 위해 ablation 연구 제공</li><li>나아가 VisualBERT 는 내부적으로 word 와 image regions 를 align 하기 위해 attention weights 를 할당하는 방법을 보여줌</li><li>pre-training 을 통해 VisualBERT 가 entities 를 grounding 하고 word 및 image region 사이의 특정 dependency relationships 를 인코딩하는 방법을 보여주며, 이는 이미지의 detailed semantics 에 대한 모델의 이해력을 향상시킴 (Fig. 1)</li></ul><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-08b90e71084829a3fbb3d6981213b458.png" width="2031" height="854" class="img_ev3q"></p><h1>2. Related Work</h1><p>vision-language tkas 해결을 위해 일반적으로 text encoder, image feature extractor, multi-modal fusion module 및 answer classifier 로 구성된다.</p><p>대부분 task specific 을 위해 설계되지만, VisualBERT 는 new task 에 적용되거나 다른 task-specific model 에 통합될 수 있다.</p><hr><p>image 의 <em>detailed semantics</em> 이해는 visual 이해에 중요하며, 이러한 의미의 modeling 은 vision-language model 에 도움이 될 수 있다는 이전 연구 결과가 있다.</p><p>예로, </p><ul><li>Visual Genome (Krishna et al., 2017) 의 attribute annotations 는 VQA 의 object detector 를 향상시키는데 사용</li><li>(Anderson et al., 2018). Santoro et al. (2017), Norcliffe-Brown et al. (2018) 및 Cadene et al. (2019)은 attention module 을 사용하여 이미지 내 objects 간의 관계를 explicitly modeling 을 탐구 </li><li>Li et al. (2019)는 object relations 를 명시적으로 인코딩하기 위해 그래프 구축</li></ul><p>VisualBERT 에서 self-attention mechanism 은  모델이 객체 간의 implicit relations 를 포착할 수 있도록 하며, image caption data 에 대한 pre-training 은 모델이 이러한 관계를 포착하는 법을 배우는데 효과적이다.</p><hr><p>저자는 NLP 의 transformer-based representation model 인 BERT 에서 영감을 얻음</p><ul><li>language modeling objective 로 pre-training 하여 universal language encoder 를 학습하는 연구에 속함</li><li>VideoBERT : video 를 음성으로 변환하고 이미지와 결합하여 Transformer 를 적용하여 joint representation 학습<ul><li>하지만 VideoBERT 는 cooking video 에 대한 captioning 을 평가했으며, vision-language task 에 대한 포괄적인 분석 수행</li></ul></li><li>ViLBERT : BERT 와 유사한 아키텍처를 사용하여 image 와 text 의 joint representation 을 학습하는 것을 제안했지만, vision 과 language 를 위한 별도의 Transformer 를 가지며 서로만 attend 할 수 있다. (결과, parameter 가 두 배)</li></ul><p>저자의 결과는 이전 연구와 일관성이 있다. (두 task 중 하나에서 우월) 하지만 방법들은 완전히 비교할 수 없는데, 서로 다른 visual representation 과 pre-training resource 가 사용되었기 때문이다.</p><h1>3. A Joint Representation Model For Vision And Lanugage</h1><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-1-a6a5c521562fd97586f165b7c28b46a7.png" width="2031" height="1003" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-background">3.1 Background<a href="#31-background" class="hash-link" aria-label="Direct link to 3.1 Background" title="Direct link to 3.1 Background">​</a></h2><p>BERT 는 subwords 를 input 으로 사용하는 Transformer 이며 language modeling objectives 로 훈련된다.</p><p>input sentence 의 all subwords 는 일련의 embeddings <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> 로 매핑된다.</p><p>각 embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mo>∈</mo><mi>E</mi></mrow><annotation encoding="application/x-tex">e \in E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> 은 다음과 같이 계산된다.</p><ol><li>subword 의 특정한 token embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">e_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span></li><li>token 이 text 의 어느 부분에서 온 것인지 나타내는 segment embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">e_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> (e.g. entailment pair)</li><li>sentence 내의 token position 을 나타내는 position embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">e_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span></li></ol><p>input embeddings <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> 은 multi-layer Transformer 를 통해 전달되어 subwords 의 contextualized representations 를 구축</p><p>BERT 는 두 단계로 훈련</p><ol><li>pre-training: 두 가지 language modeling objectives 의 조합<ol><li>일부 input tokens 를 special token (즉, <!-- -->[MASK]<!-- -->) 로 randomly replace 하는 masked language modeling</li><li>next sentence prediction 으로, 모델에게 sentence pair 을 제공하고 document 의 연속된 두 sentence 인지 분류하도록 훈련</li></ol></li><li>특정 task 에 적용하기 위해, task-specific input, output layer, objectives 및 task data 로 fine-tuning</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-visualbert">3.2 VisualBERT<a href="#32-visualbert" class="hash-link" aria-label="Direct link to 3.2 VisualBERT" title="Direct link to 3.2 VisualBERT">​</a></h2><p>저자의 핵심 아이디어는 Transformer 내부의 self-attention mechanism 을 재사용하여 input text 와 regions 를 implicitly align 하는 것</p><p>BERT 의 all components 에 더하여, image 를 모델링하기 위해 visual embedding set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span></span></span></span> 을 도입한다.</p><ul><li>각 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>∈</mo><mi>F</mi></mrow><annotation encoding="application/x-tex">f \in F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span></span></span></span> : object detector 에서 파생된 image 내의 bounding region 에 해당</li></ul><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span></span></span></span> 의 각 embedding 은 세 개의 embedding 을 합산하여 계산</p><ol><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">f_o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> : <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span></span></span></span></span> 의 bounding region 의 visual feature representation</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">f_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> : text embedding 과 대조적으로 image embedding 을 나타내는 segment embedding</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">f_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> : input 의 일부로 words 와 bounding regions 간의 alignment 가 제공될 때 사용되는 position mebedding 으로, aligned words 에 해당하는 position embeddings 의 합으로 설정</li></ol><p>이러한 visual embeddings 은 original text embedding set 과 함께 multi-layer Transformer 에 전달되어 모델이 두 입력 간의 유용한 alignment 를 발견하고, new joint representation 을 구축하도록 함</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-training-visualbert">3.3 Training VisualBERT<a href="#33-training-visualbert" class="hash-link" aria-label="Direct link to 3.3 Training VisualBERT" title="Direct link to 3.3 Training VisualBERT">​</a></h2><p>BERT 와 유사한 훈련 절차를 채택</p><p>하지만 VisualBERT 는 language 및 visual input 을 동시에 수용해야하므로 paired data 에 도달하게 된다.</p><p>각각 5 independent captions 와 images 의 paried 를 포함하는 COCO dataset 에서 다음 세 단계 훈련 절차를 구성</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="task-agnostic-pre-training">Task-Agnostic Pre-training<a href="#task-agnostic-pre-training" class="hash-link" aria-label="Direct link to Task-Agnostic Pre-training" title="Direct link to Task-Agnostic Pre-training">​</a></h4><p>VisualBERT 를 COCO dataset 에서 두 개의 <em>visually-grounded</em> language model objectives 로 훈련</p><ol><li>Masked language modeling with images<ul><li>text input 의 일부 요소가 masking 되어 예측되어야 하지만, image regions 에 해당하는 vector 는 masking 되지 않음</li></ul></li><li>Sentence-image prediction<ul><li>COCO 에서 한 image 에 대응하는 여러 caption 이 있는 경우, 두 개의 caption 으로 구성된 text segment 제공</li><li>한 caption 은 이미지를 설명하고, 다른 하나는 해당 이미지에 대응하거나 다른 caption 을 무작위로 선택된 caption 일 확률이 50%</li></ul></li></ol><p>모델은 위 두 가지 상황을 구별하는 데 훈련된다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="task-specific-pre-training">Task-Specific Pre-Training<a href="#task-specific-pre-training" class="hash-link" aria-label="Direct link to Task-Specific Pre-Training" title="Direct link to Task-Specific Pre-Training">​</a></h4><p>VisualBERT 를 downstream task 에 fine-tuning 하기 전에, masked language modeling 과 image objective 의 task data 를 사용한 model 훈련이 유익한 것을 발견</p><p>이 단계에선 모델이 new target domain 에 적응할 수 있도록 한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="fine-tuning">Fine-Tuning<a href="#fine-tuning" class="hash-link" aria-label="Direct link to Fine-Tuning" title="Direct link to Fine-Tuning">​</a></h4><p>BERT 의 fine-tuning 을 모방하며 tkas-specific input, output 및 objective 가 도입되고, Transformer 가 task 에 대한 성능을 최대화하도록 훈련</p><h1>4. Experiment</h1><p>vision-language application 의 네 가지 다양한 유형에 평가</p><ul><li>Visual Question Answering : VQA 2.0</li><li>Visual Commonsense Reasoning : VCR</li><li>Natural Language for Visual Reasoning (NLVR<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>)</li><li>Resion-to-Phrase Grounding : Flickr30K</li></ul><hr><p>all tasks 에 대해, COCO 의 Karpathy train split 을 사용하여 task-agnostic pre-training 에 대한 pre-training 수행</p><ul><li>이 split 에는 약 100K image 및 5 captions 존재</li><li>Transformer encoder 는 BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>BASE</mtext></msub></mrow><annotation encoding="application/x-tex">_{\text{BASE}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">BASE</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 와 동일한 구성: layers 12, hidden size 768, self-attention heads 12</li><li>parameter 는 pre-trained BERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>BASE</mtext></msub></mrow><annotation encoding="application/x-tex">_{\text{BASE}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">BASE</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> parameter 에 초기화</li></ul><hr><p>image representation 의 경우, 각 데이터셋마다 서로 다른 standard object detector 가 있어 region proposals 및 region features 를 생성한다.</p><p>이들과 비교를 위해, 설정을 따르며 결과적으로 다양한 task 에 대해 다른 image feautres 사용됨</p><p>일관성 유지를 위해 COCO 에 task-agnostic pre-training 중, end tasks 에 사용되는 것과 동일한 image features 사용</p><p>각 데이터셋에 대해, VisualBERT 의 세 가지 변형 사용</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="visualbert">VisualBERT<a href="#visualbert" class="hash-link" aria-label="Direct link to VisualBERT" title="Direct link to VisualBERT">​</a></h4><p>COCO 에서 pre-training 을 거친 BERT 의 parameter initialization 의 full model 로, task data 에 pre-training 하고, task 에 fine-tuning</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="visualbert-wo-early-fusion">VisualBERT w/o Early Fusion<a href="#visualbert-wo-early-fusion" class="hash-link" aria-label="Direct link to VisualBERT w/o Early Fusion" title="Direct link to VisualBERT w/o Early Fusion">​</a></h4><p>initial Transformer layer 에서 image representation 이 text 와 결합되지 않고, new Transformer layer 에서 very end 에서 결합되는 VisualBERT</p><p>이를 통해 whole Transformer stack 을 통해 language 및 vision 간의 상호작용이 성능에 중요한지 테스트</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="visualbert-wo-coco-pre-training">VisualBERT w/o COCO Pre-training<a href="#visualbert-wo-coco-pre-training" class="hash-link" aria-label="Direct link to VisualBERT w/o COCO Pre-training" title="Direct link to VisualBERT w/o COCO Pre-training">​</a></h4><p>COCO caption 에 대한 task-agnostic pre-training 을 skip 한 VisualBERT</p><p>이는 pre-training 의 중요성을 검증하기 위해 수행</p><hr><ul><li>BERT 를 따라, Adam 을 사용하여 SGD 로 optimize</li><li>warm-up step 수는 특별히 지정되지 않는 한 total training step 수 10% 로 설정</li><li>batch size 는 하드웨어 제약에 충족되게 선택하며, 128 보다 긴 sequence 는 상한으로 설정</li><li>Tesla V100s 및 GTX 1080Tis 에서 수행</li><li>모든 실험은 최대 4 Tesla V100s 에 각 16GB GPUs memory 를 사용하여 복제</li><li>pre-trained on COCO 은 4 cards 에서 하루 이내 완료되며 task-specific pre-training 및 fine-tuning 은 이보다 덜하다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-vqa">4.1 VQA<a href="#41-vqa" class="hash-link" aria-label="Direct link to 4.1 VQA" title="Direct link to 4.1 VQA">​</a></h2><p>주어진 image 및 question 에 대해 올바른 대답을 하는 task</p><p>VQA 2.0 을 사용하며, COCO 의 images 에 대한 1 million question 을 구성돼있다.</p><p>빈도 높은 3,129 answers 를 예측하도록 훈련하며 Visual Genome 에 pre-training 된 ResNeXt-based Faster-RCNN 의 image features 사용</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-2-3ad68c5a5b28d88e5ce48c367f7aeae3.png" width="1996" height="867" class="img_ev3q"></p><ul><li>same visual features 및 bounding region proposals 수를 사용하는 baselines, 저자의 모델 및 Visual Genome (+VG) external question-answer pairs, multiple detectors (+Multiple Detectors) 및 모델들의 앙상블을 사용한 다른 비교 불가한 방법을 포함하여 report</li><li>비교 가능한 설정에서 현존하는 연구보다 간단하고 우수한 성능</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-vcr">4.2 VCR<a href="#42-vcr" class="hash-link" aria-label="Direct link to 4.2 VCR" title="Direct link to 4.2 VCR">​</a></h2><p>VCR 은 110k movie scenes 에서 290K questions 로 구성되어, visual commonsense 에 초점을 맞춰있다.</p><ul><li>이 task 는 개별 모델을 훈련하는 two multi-choice sub-tasks: question answering (Q <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⇒</mo></mrow><annotation encoding="application/x-tex">\Rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em"></span><span class="mrel">⇒</span></span></span></span></span> A) 및 answering justification (QA <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⇒</mo></mrow><annotation encoding="application/x-tex">\Rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em"></span><span class="mrel">⇒</span></span></span></span></span> R) 로 분해</li><li>image feature 는 ResNet50 에서 얻으며, dataset 에서 제공되는 &quot;gold&quot; detection bounding boxes 및 segmentations 를 사용</li><li>dataset 은 text 에서 참조된 words 및 bounding regions 간의 alignments 를 제공하며, 일치하는 words 및 regions 에 대해 동일한 position embeddings 를 사용하여 활용</li><li></li></ul><hr><p><img loading="lazy" alt="Table 2" src="/assets/images/image-3-496fd1b9e940ccb477125b4213bfea43.png" width="1996" height="690" class="img_ev3q"></p><p>저자는 dataset 과 함께 발표된 BERT (R2C) 를 기반으로 하는 모델과 리더보드 (B2T2) 에서 SOTA model 과 비교</p><ul><li>축약된 VisualBERT w/o COCO Pre-training 은 R2C 와 동일한 리소스를 사용하지만 더 간단하며 뛰어난 성능을 보임</li><li>full model 은 더욱 향상</li><li>COCO 및 VCR 간에는, VCR 이 movie scenes, COCO 는 image caption 이라는 domain 차이가 있음에도 COOCO pre-training 이 도움을 줌</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-nlvr2">4.3 NLVR<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><a href="#43-nlvr2" class="hash-link" aria-label="Direct link to 43-nlvr2" title="Direct link to 43-nlvr2">​</a></h2><p>NLVR<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> 은 natural language 및 image 에 대한 joint reasoning dataset 으로 semantic 다양성, compositionality 및 visiual reasoning challenge 포함</p><ul><li>이 task 는 image pair 에 대한 natural language caption 이 참인지 여부 판별</li><li>이 dataset 은 web unage 로 English senteces paired 의 100k 예제 포함</li><li>VisualBERT 의 segment embedding mechanism 을 수정하여 서로 다른 image 및 서로 다른 segment embedding 의 features 를 할당</li></ul><p><img loading="lazy" alt="Table 3" src="/assets/images/image-4-6fa8465227b34679091ad9d2612cdc63.png" width="1955" height="535" class="img_ev3q"></p><ul><li>Detectron 의 기존 detector 를 사용하여 image feature 를 제공하고 이미지 당 144 proposals 사용</li><li>결과, VisualBERT w/o Early Fusion과 VisualBERT w/o COCO pre-training 은 SOTA 인 MaxEnt 를 크게 능가하며 VisualBERT는 더욱 큰 성능 향상을 보임</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="44-flickr30k-entities">4.4 Flickr30K Entities<a href="#44-flickr30k-entities" class="hash-link" aria-label="Direct link to 4.4 Flickr30K Entities" title="Direct link to 4.4 Flickr30K Entities">​</a></h2><p>Flickr30K Entities dataset 은 caption 의 phrase 를 이미지 내의 bounding regions 에 연결하는 능력을 테스트</p><ul><li>이 task 는 sentence 로부터 span 이 주어졌을 때, 해당 span 과 일치하는 bounding regions 선택하는 것</li><li>30k images 와 약 250k annotations 구성</li><li>BAN 설정을 채택하며 Faster R-CNN pre-trained on Visual Genome 의 image features 사용</li><li>task specific fine-tuning 을 위해, additional self-attention block 도입하고 각 heads 의 average attention weights 를 사용하여 boxes 와 phrases 사이의 alignment 예측</li><li>phrase 에 grounding 하기 위해, model prediction 으로 phrase 의 last sub-word 로부터 attention 을 가장 많이 받는 box 선택</li></ul><p><img loading="lazy" alt="Table 4" src="/assets/images/image-5-ae9af6a8243f9343ed20f8a100c78bab.png" width="1955" height="602" class="img_ev3q"></p><ul><li>결과, VisualBERT SOTA 인 BAN 보다 우수한 성능을 보임</li><li>early fusion 없는 ablation model 및 full model 간의 유의미한 차이를 관찰하진 못함. 아마 task 에 더 얕은 아키텍처만으로도 충분하다는 것을 시사</li></ul><h1>5. Analysis</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-ablation-study">5.1 Ablation Study<a href="#51-ablation-study" class="hash-link" aria-label="Direct link to 5.1 Ablation Study" title="Direct link to 5.1 Ablation Study">​</a></h2><p>NLVR<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> 에 ablation study 수행하며 두 가지 ablation model 포함 및 VisualBERT 의 네 가지 변형을 비교</p><p>이 모델들은 image 당 36 features 에만 훈련 (full model 포함)</p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-6-989d754710e95222ddf10b658d926caa.png" width="1016" height="822" class="img_ev3q"></p><p>저자의 분석 (Table 5) 은 VisualBERT 의 네 가지 요소에 대한 기여를 조사</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="c1-task-agnostic-pre-training">C1: Task-agnostic Pre-training<a href="#c1-task-agnostic-pre-training" class="hash-link" aria-label="Direct link to C1: Task-agnostic Pre-training" title="Direct link to C1: Task-agnostic Pre-training">​</a></h4><p>task-agnostic pre-training 의 기여를 조사를 위해 pre-training 을 skipping 한 것 (VisualBERT w/o COCO Pre-training)과 COCO 의 image 없이 text 만 pre-training (VisualBERT w/o Grounded Pre-training)</p><p>두 변형은 성능이 저하되며, vision 및 lanugage paired 에 pre-training 하는 것이 중요함을 보여줌</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="c2-early-fusion">C2: Early Fusion<a href="#c2-early-fusion" class="hash-link" aria-label="Direct link to C2: Early Fusion" title="Direct link to C2: Early Fusion">​</a></h4><p>VisualBERT w/o Early Fusion 을 포함하여 image 와 text features 간의 early interaction 의 중요성을 검증</p><p>vision 및 language 간의 multiple interaction layers 가 중요</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="c3-bert-initialization">C3: BERT Initialization<a href="#c3-bert-initialization" class="hash-link" aria-label="Direct link to C3: BERT Initialization" title="Direct link to C3: BERT Initialization">​</a></h4><p>BERT initialization 의 기여를 이해하기 위해, randomly initialized parapmeter 로 변형을 도입하고, 이후 full model 처럼 훈련된다.</p><p>language-only pre-trained BERT 의 weights 는 중요해보이지만, 성능이 예상만큼 저하되지 않고, COCO pre-training 중 grounded language 에 대한 많은 유용한 측면을 학습하고 있음을 주장</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="c4-the-sentence-image-prediction-objective">C4: The sentence-image prediction objective<a href="#c4-the-sentence-image-prediction-objective" class="hash-link" aria-label="Direct link to C4: The sentence-image prediction objective" title="Direct link to C4: The sentence-image prediction objective">​</a></h4><p>task-agnostic pre-training 중 sentence-image prediction objective 가 없는 모델 도입 (VisualBERT w/o Objective 2)</p><p>결과 이 objective 가 다른 구성 요소와 비교하여 긍정적이지만 그 영향이 덜 중요하다는 것을 시사</p><hr><p>전반적인 결과로 중요한 설계 선택은 task-agnostic pre-training (C1) 및 vision-language early fusion (C2)임을 확인</p><p>pre-training 에선 additional COCO data 포함과 images 및 captions 모두 사용하는 것은 필수</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-dissecting-attention-weights">5.2 Dissecting Attention Weights<a href="#52-dissecting-attention-weights" class="hash-link" aria-label="Direct link to 5.2 Dissecting Attention Weights" title="Direct link to 5.2 Dissecting Attention Weights">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="entity-grounding">Entity Grounding<a href="#entity-grounding" class="hash-link" aria-label="Direct link to Entity Grounding" title="Direct link to Entity Grounding">​</a></h4><p>먼저 VisualBERT 의 entity grounding 을 수행할 수 있는 attention heads 를 찾는다. 즉, sentence 의 entity 에 해당하는 bounding region 에 attending 하는 것이다.</p><p>구체적으로, Flickr30K 의 평가셋에서 ground truth alignment 를 사용</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-7-57254d5d3e4ec48d91f962fcdde54d8d.png" width="1016" height="855" class="img_ev3q"></p><ul><li>sentence 내의 각 entity 및 VisualBERT 의 각 attention head 에 가장 많은 attention weight 를 받는 bounding region 을 살펴봄</li><li>word 는 image regions 뿐 아니라 text 의 words 에도 attending 할 가능성이 높아, 이 평가에선 words 에 대한 head&#x27; attention 을 masking 하고 image regions 는 유지한 후, 어느 attention 의 attending 이 Flickr30K 의 annotation 과 일치하는지 계산 <ul><li>VisualBERT 의 all 144 attention heads 에 대한 정확도를 layer 별로 나열한 것을 Fig. 3 에서 확인</li></ul></li><li>항상 가장 높은 detection confidence 의 region 을 선택하는 baseline 고 고려<ul><li>VisualBERT 는 entity grounding 을 위해 직접 supervision 을 받지 않았더라도 높은 정확도에 달성하는 것 발견</li><li>grounding accuracy 는 higher layers 에서 더 향상</li><li>모델이 lower layer 에서 two input 을 합성할 때 덜 확실하지만 어떻게 정렬해야 하는지 인식하는 것으로 나타남</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="syntactic-grounding">Syntactic Grounding<a href="#syntactic-grounding" class="hash-link" aria-label="Direct link to Syntactic Grounding" title="Direct link to Syntactic Grounding">​</a></h4><p>BERT 의 attention head 가 syntactic relationships 를 발견할 수 있는 것을 이미 관찰되어, VisualBERT 가 발견한 syntactic relationships 는 어떻게 grounding information 을 전달하는지 분석</p><p>특히, dependency relation 으로 연결된 two words <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo><mover><mo><mo>→</mo></mo><mn>2</mn></mover></mo><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">w_1 \overset{2}{\rightarrow} w_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.268em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel"><span class="mop op-limits"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.118em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop">→</span></span></span><span style="top:-3.5669em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 가 주어진 경우, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">w_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 의 attention head 가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 에 해당하는 region 에 얼마나 자주 attending 하는지, 그 반대도 분석</p><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-8-a7004e39482b169dabad8962a0516c6b.png" width="2064" height="1557" class="img_ev3q"></p><ul><li>Fig. 1 의 &quot;walking&quot; 과 &quot;man&quot; 이 관련된 경우, Stanford Dependency Parsing 하에 &quot;nsubj&quot; 관계를 통해 서로 연결되어 있기 때문에 &quot;walking&quot; 에서 &quot;man&quot; 에 해당하는 region 에 attending 하는 attention head 를 확인하려 함.</li><li>VisualBERT 의 syntactic sensitivity 를 평가하기 위해 AllenNLP 의 dependency parser 를 사용하여 Flickr30K 의 all sentence 를 parsing. 이후 각 attention head 에 대해 특정 dependency relationship 을 가진 두 단어가 있고 그 중 하나가 Flickr30K 의 ground-truth grounding 이 있는 경우, head attention weight 가 ground-truth grounding 을 얼마나 정확히 예측하는지 계산</li><li>all dependency relationships 를 조사한 결과, VisualBERT 는 확신있는 bounding region 을 추측하기 보단 훨씬 더 우수한 성능을 보이는 각 관계에 대해 최소 one head 가 존재</li><li>특히 몇 가지 dependency relationship 을 Fig. 4 에서 강조<ul><li>많은 heads 가 동사와 인수를 정확히 연결시키는 것 (즉, &quot;pobj&quot;, &quot;nsub&quot; 및 &quot;dobj&quot; dependency relations) 으로 보임</li><li>이는 visual 요소에 암시적, 그리고 supervision 없이 해결함을 보여줌</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="53-qualitative-analysis">5.3 Qualitative Analysis<a href="#53-qualitative-analysis" class="hash-link" aria-label="Direct link to 5.3 Qualitative Analysis" title="Direct link to 5.3 Qualitative Analysis">​</a></h2><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-9-2a3f58f24af3fc07c92ea4d17d7d96c3.png" width="2011" height="3323" class="img_ev3q"></p><p>VisualBERT 가 image 및 text 를 처리하는 동안 layer 별로 어떻게 attending 하는지 Fig. 1 및 Fig. 5 에서 보여줌</p><ul><li>각 ground-truth box 에 해당 bounding region 과 가장 가까운 predicted bounding region 을 수동으로 다른 카테고리와 그룹화</li><li>모델이 활발히 attending 하고 있는 region 도 포함</li><li>이후 동일한 카테고리 내의 words 에 해당 region 의 attention weights 를 집계</li><li>highest entity grounding accuracy 를 달성한 6 layers 의 best heads 를 보여줌</li></ul><p>전반적으로 Transformer layer 를 통해 alignment 를 개선하는 것으로 보임</p><ul><li>Fig. 5 의 왼쪽 하단 이미지에, 처음엔 &quot;husband&quot; 와 &quot;woman&quot; 모두 여성에 해당하는 region 에 attending</li><li>계산 끝에는 woman 과 man 을 분리하여 올바르게 정렬</li><li>syntactic alignment 로도 많은 예시가 있으며, 같은 이미지에 &quot;teased&quot; 라는 단어는 남성과 여성 둘 다에 정렬되고 &quot;by&quot; 는 남성에 정렬되고, &quot;her&quot; 은 여성에게 정렬됨</li></ul><h1>6. Conclusion and Future Work</h1><p>본 논문은 joint vision-language representation 을 위한 pre-trained model 인 VisualBERT 제안</p><ul><li>네 가지 평가에서 강력한 성능을 달성하면서도 간단함</li><li>추가 분석으로, attention mechanism 으로 해석 가능한 방식으로 포착하는 것을 시사</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">Vision-Language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/visual-bert">VisualBERT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/single-stream">Single-Stream</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/Foundation/Single-Stream/2019-08-VisualBERT.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/Foundation/Single-Stream/VLBERT"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">VL-BERT: Pre-Training Of Generic Visual-Linguistic Representations</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/Foundation/Single-Stream/UNITER"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">UNITER: UNiversal Image-TExt Representation Learning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-background" class="table-of-contents__link toc-highlight">3.1 Background</a></li><li><a href="#32-visualbert" class="table-of-contents__link toc-highlight">3.2 VisualBERT</a></li><li><a href="#33-training-visualbert" class="table-of-contents__link toc-highlight">3.3 Training VisualBERT</a></li><li><a href="#41-vqa" class="table-of-contents__link toc-highlight">4.1 VQA</a></li><li><a href="#42-vcr" class="table-of-contents__link toc-highlight">4.2 VCR</a></li><li><a href="#43-nlvr2" class="table-of-contents__link toc-highlight">4.3 NLVR^2</a></li><li><a href="#44-flickr30k-entities" class="table-of-contents__link toc-highlight">4.4 Flickr30K Entities</a></li><li><a href="#51-ablation-study" class="table-of-contents__link toc-highlight">5.1 Ablation Study</a></li><li><a href="#52-dissecting-attention-weights" class="table-of-contents__link toc-highlight">5.2 Dissecting Attention Weights</a></li><li><a href="#53-qualitative-analysis" class="table-of-contents__link toc-highlight">5.3 Qualitative Analysis</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.a65adec1.js"></script>
<script src="/assets/js/main.d181d84f.js"></script>
</body>
</html>