<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/VQA-IC/Few-shot/2022-09-MAGMA">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Linearly Mapping from Image to Text Space | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Linearly Mapping from Image to Text Space | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.7caf321c.js" as="script">
<link rel="preload" href="/assets/js/main.a0120155.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">CLIP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Foundation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">VQA-IC</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Few-shot</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Multimodal Few-Shot Learning with Frozen Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA">xGQA: Cross-Lingual Visual Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP">CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA">Linearly Mapping from Image to Text Space</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper">Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA">Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/CoTMT">A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MAPD">Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering</a></li></ul></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">VQA-IC</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Few-shot</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Linearly Mapping from Image to Text Space</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Linearly Mapping from Image to Text Space</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/abs/2209.15162" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2209.15162</a></p><h1>Abstract</h1><p>텍스트만 사용하는 language model (LM) 이 비언어적 세계의 특성을 얼마나 학습하여 표현하는지는 여전히 열린 질문이다. 기존 연구에서는 pre-trained LM 이 vision model 의 parameter 를 최적화하여 이미지를 language space 에 인코딩하도록 하면 image captioning 을 학습할 수 있음을 보였다. </p><p>저자는 더 강한 가설을 검증한다. 즉, frozen text-only model 과 vision-only model 이 학습한 conceptual representation 이 충분히 유사하여, 단순한 linear map 만으로 이를 달성할 수 있다는 것이다. </p><ul><li>저자는 vision model 의 image representation 을 single linear projection 만 학습하여 frozen LM 에 continuous prompt 로 전달할 수 있음을 보인다.</li><li>이를 이용해 LM 을 prompting 하면, image encoder 와 text decoder (e.g., MAGMA model) 를 모두 튜닝하는 방법과 비교하여 captioning 및 visual question answering task 에서 경쟁력 있는 성능을 달성한다. </li></ul><p>저자는 pre-training 과정에서 서로 다른 수준의 linguistic supervision 을 받은 세 가지 image encoder 를 비교한다: BEIT (linguistic 정보 없음), NF-ResNET (lexical category 정보), CLIP (자연어 설명 전체). </p><p>세 encoder 모두 크기와 같은 visual property information (e.g., 동물이 크거나 작은지) 를 LM 에 효과적으로 전달하지만, linguistic supervision 으로 pre-trained image encoder 는 category information (e.g., 하마 vs. 코끼리 구분) 를 더 뚜렷하게 인코딩하여 language-and-vision benchmark task 에서 유의하게 더 좋은 성능을 보였다. 이러한 결과는 LM 이 이미지로만 학습된 model 을 포함한 vision-based model 과 구조적으로 유사한 방식으로 개념 정보를 인코딩함을 시사한다.</p><h1>1 Introduction</h1><p>최근 NLP 연구의 많은 부분은 오직 text data 의 형식적 특성에만 기반한 학습이 representation capacity 에 어떤 한계를 부과하는지에 초점을 두고 있다. 이러한 논의는, 명시적인 grounding 이 없다면 language model 은 인간이 물리적이고 비언어적인 세계와 상호작용하면서 습득하는 풍부한 conceptual knowledge 를 반영하는 conceptual representation 을 학습하지 못할 것이라는 주장과 연결된다. 그럼에도 불구하고, large language model 이 비언어적 현상에 대해 일반화하고 추론하는 능력에서 놀라운 발견들이 있었다. 따라서 이 분야의 열린 질문은, 오직 text-only data 로 학습된 language model 이 물리적 세계의 측면을 어느 정도까지 학습할 수 있는가 하는 것이다.</p><p>이 논문에서는 language model 과 image encoder 의 representation 사이의 관계에 대한 특정 가설을 검증한다. 즉, 이러한 conceptual representation 이 linear transformation 을 통해 서로 근사적으로 대응될 수 있다는 것이다. 이를 위해 저자는 image representation space 에서 generative LM 의 language space 로 사영하는 single linear layer 를 학습한다. 다른 model parameter 는 전혀 튜닝하지 않으며, 이를 LiMBeR (Linearly Mapping Between Representation spaces) 라고 부른다. 구체적으로, image representation 을 discrete language token 과 대응되지 않는 embedding space 상의 vector 인 “soft prompts” 로 linear transformation 한다. 이 linear projection 의 weight 는 image captioning task 를 통해 학습된다 (Fig. 1 참조). 이후 vision-language (VL) task 에 대해 LM 이 생성하는 텍스트를 탐구함으로써 성능을 평가한다. linear transformation 의 단순성 때문에, 두 model 의 conceptual representation space 가 구조적으로 유사하다면 이 transfer 는 성공적일 것이며, LM 은 이미지 내용을 기술하는 데 큰 어려움이 없을 것이라 예상된다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-70-74b4261ba0a6c8dad83aeedfff6442b3.png" width="1179" height="309" class="img_ev3q"></p><p>저자는 pre-training 과정에서 서로 다른 수준의 linguistic supervision 을 받은 세 가지 image encoder 를 사용하여 LM 으로 projection 을 학습한다: BEIT, Normalizer Free ResNet50 (NFRN50), CLIP. </p><ul><li>여기서 linguistic supervision 은 image encoder 가 pre-training 중 language data 에 어느 정도 노출되었는지를 의미하며, 이는 LM 과의 representation 유사성에 영향을 미친다. </li><li>CLIP 은 이미지와 자연어 caption 을 shared image-text representation space 에 맞추도록 pre-training 되었지만, </li><li>BEIT 은 language 에 전혀 노출되지 않고 이미지의 masked 영역 내용을 예측하는 방식으로 학습되었다. </li><li>NFRN50 은 두 극단의 중간에 위치한다. ImageNet1k class 집합에서 이미지 subject 를 식별하는 image classification task 로 pre-training 되었기 때문이다. </li><li>이 task 에는 자연어가 포함되지 않지만, WordNet hierarchy 에서 유래한 lexical category 개념 (image class) 에 따라 시각적 feature 를 매핑하도록 model 을 유도한다.</li></ul><p>실험 결과, 세 encoder 중 어느 것을 사용하든 LM 에 prompt 로 입력하면 이미지의 semantic content 가 효과적으로 전달되어 LM 이 자연어로 기술할 수 있음을 보인다. 그러나 성능은 image encoder 가 가진 linguistic supervision 의 강도에 비례하는 것으로 나타났다. CLIP 과 NFRN50 은 자유롭게 model 을 튜닝하는 방법과 비교해도 경쟁력 있는 성능을 보였지만, BEIT 은 주로 거친 수준의 visual properties 만 전달하며 LM 이 정확한 lexical category 를 생성하도록 유도하는 데는 어려움을 보였다. 이는 language data 또는 vision data 로 학습된 model 이 개념 공간을 구조적으로 유사하게 학습한다는 증거로 해석되지만, 정확한 유사도의 정도는 image encoder 가 받은 supervision 의 종류에 달려 있음을 시사한다.</p><p>요약하면, 저자는 다음을 보인다.</p><ol><li>visual semantic information 은 어떤 model parameter 도 튜닝하지 않고 soft prompts 의 형태로 language model 에 linear mapping 될 수 있다.</li><li>이러한 mapping 은 generative model 이 이미지를 설명하고 이미지에 관한 질문에 답할 수 있도록 하며, 이는 image 와 language representation 을 공동으로 튜닝하는 multimodal model 과 비교해도 경쟁력 있는 수준이다.</li><li>서로 다른 image encoder backbone 을 사용하여 prompting pipeline 을 학습함으로써, pre-training 에서의 linguistic supervision 이 model 의 개념 형성에 핵심적인 역할을 하며, 따라서 vision-to-text 공간 간 visual feature 의 transferability 에도 중요한 영향을 미친다는 것을 보인다.</li></ol><h1>2 Related Work</h1><p>저자의 접근법은 pre-trained language model 을 image representation 을 입력으로 받을 수 있도록 적응시키려는 최근 연구들에서 영감을 얻었다. 특히 Frozen 과 MAGMA model, 그리고 Sung et al., Alayrac et al., Mokady et al., Luo et al., Lin et al., Zhai et al. 등의 연구는, pre-trained image network 와 text network 을 함께 image captioning task 에서 튜닝한 뒤 downstream vision-language (VL) task 에 적용할 수 있음을 보였다. 이러한 접근법은 pre-trained model 을 fine-tuning 하거나, modality 간에 non-linear MLP projection/fusion network 을 학습하는 방식을 택한다. 이는 저자의 접근법과 달리 representation 의 해석을 어렵게 만든다. </p><p>Scialom et al. 은 trained linear transformation 만으로도 BERT 가 image region representation 을 인코딩하여 text decoder 로 전달해 이미지에 대한 질문을 생성할 수 있음을 보였으나, 이러한 transformation 을 통해 LM 이 어떤 abstraction 을 전달받을 수 있는지는 충분히 이해되지 않았으며, text decoder 가 visual encoding 의 linear transformation 만으로도 직접 동작할 수 있는지 역시 불분명하다.</p><p>과거에는 pretrained LM 또는 scratch 에서 trained LM 이 주로 image captioning 에 사용되었으며, 이때는 image representation 을 LM 의 입력으로 직접 전달하였다. </p><ul><li>Gui et al., Yuan et al. 은 image-caption data 로부터 vision-language model 을 scratch 에서 학습하였다. </li><li>Zeng et al., Xie et al., Wang et al. 은 LM training 을 통해 습득된 knowledge 를 활용하기 위해 VL model 로부터 파생된 text prompt 를 LM 에 주입하여 multimodal 성능을 향상시켰다. </li><li>이러한 연구들은 LM 이 text 로 기술된 visual input 과 상호작용할 수 있음을 보여준다. </li><li>그러나 저자의 연구는, visual input 이 language 를 거치지 않고 직접 LM 에 입력될 수 있는가를 묻는다. 앞선 model 들이 VL task 에서 성공한 사실은 text model 과 image model 이 독립적으로 학습하더라도 representation 유사성을 가지게 됨을 시사하며, 저자는 이를 본 논문에서 탐구한다.</li></ul><p>저자의 연구는 또한 model “stitching” 개념과 밀접하게 관련된다. stitching 은 두 서로 다른 model 을 특정 layer 에서 연결하는 방식이다. </p><ul><li>LiMBeR 는 image encoder output 을 soft prompt 형태로 LM 의 입력에 연결하는 stitching 으로 설명할 수 있다. </li><li>stitching 은 RSA 나 CKA 와 같은 기존 방법보다 두 model 간 representation similarity 를 평가하는 데 뚜렷한 장점을 제공한다. </li><li>예를 들어, LiMBeR 는 CLIP encoding 이 BEIT representation 보다 text encoding 에 더 유사하다는 것을 보여줄 뿐 아니라, BEIT representation 역시 LM 으로 visual property 정보를 transfer 할 수 있음을 보여준다 (Sec. 5.3).</li></ul><p>최근 연구에서는 LM 이 언어를 모델링하기 위해 비언어적 세계의 측면을 모델링하는지에 대한 관심이 크다. </p><ul><li>Lu et al. 은 pretrained LM 의 weight 가 서로 다른 modality 의 task 에 일반화될 수 있음을 보였다. </li><li>Hao et al. 역시 LM 이 여러 modality 의 인터페이스로 동작할 수 있음을 보였다. </li><li>Li et al. 은 entity 와 situation model 이 contextual word representation 으로부터 도출될 수 있음을 보였다. </li><li>Patel &amp; Pavlick 은 매우 large LM (GPT-3 scale) 이 text 로 표현된 non-linguistic conceptual domain 을 in-context 학습할 수 있음을 보였다. </li></ul><p>저자의 연구는 이러한 연구들과 달리, LM 이 model weight 를 전혀 변경하지 않고 non-text data 와 직접적으로 인터페이스하도록 하며, text-only LM 의 representation space 가 여러 vision-based model 의 representation space 와 비자명한 유사성을 공유함을 보인다는 점에서 차별화된다.</p><h1>3 Method: Linearly Mapping from Image to Text Representations</h1><p>기존 연구에서는 image 를 language model 의 soft prompt 로 mapping 하여 multimodal pretraining 을 수행하는 방법이 성공적으로 사용되었지만 (e.g., Frozen, Magma; Sec. 2 참조), 이 mapping 의 메커니즘을 제한하여 그 작동 방식을 이해하려는 시도는 없었다. 저자의 기본 접근법은, pre-trained image encoder 의 hidden size <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>I</mi></msub></mrow><annotation encoding="application/x-tex">h_I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 로부터 generative language model 의 input space <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>L</mi></msub></mrow><annotation encoding="application/x-tex">e_L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 로 project 하는 single linear layer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span></span> 를 학습하는 것이다. 이때 학습은 image captioning task 에 기반한다. projected input 은 discrete language token 에 대응되지 않으며, image 를 나타내는 soft prompt (Lester et al., 2021) 로 생각할 수 있다. 편의상, 저자는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span></span> 의 학습을 <strong>Linearly Mapping Between Representation spaces (LiMBeR)</strong> 라고 부른다.</p><p>이 접근법은 Tsimpoukelli et al. 과 Eichenberg et al. 의 방법을 단순화한 것으로, projection <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span></span> 만 학습되는 구조이다. projection 양쪽의 image encoder <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> 와 LM 은 frozen 상태로 두기 때문에, LM 이 image 입력을 설명하거나 관련 task 를 수행할 수 있는 능력의 함수로 두 representation space 간의 유사성을 탐구할 수 있다. 저자는, 만약 language model 이 vision encoder 가 학습한 visual conceptual information 을 구조적으로 유사하게 표현한다면, 단순한 linear transformation 만으로도 visual feature 를 language model 로 transfer 할 수 있을 것이라고 기대한다.</p><p>훈련 절차를 설명하기 전에, model 의 기본 구성 요소와 선택된 변형을 설명한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="language-model-lm--image-encoders-e">Language Model LM &amp; Image Encoders E<a href="#language-model-lm--image-encoders-e" class="hash-link" aria-label="Direct link to Language Model LM &amp; Image Encoders E" title="Direct link to Language Model LM &amp; Image Encoders E">​</a></h4><p>저자는 LM 이 학습한 conceptual representation 이 linear transformation 을 거쳐 image encoder <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> 가 learned representation 과 동등하다고 가정한다. 사용된 language model 은 6 billion parameter 를 가진 decoder-only GPT-J (Wang &amp; Komatsuzaki, 2021) 이다. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span></span> 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>I</mi></msub></mrow><annotation encoding="application/x-tex">h_I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 로부터 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>L</mi></msub><mo>=</mo><mn>4096</mn></mrow><annotation encoding="application/x-tex">e_L = 4096</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">4096</span></span></span></span></span> 의 LM input space 로 projection 되도록 학습된다.</p><p>저자는 서로 다른 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> 를 사용해 여러 model 을 학습하고, encoding 간의 호환성을 평가한다. 특히, pre-training 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> 가 받은 linguistic supervision 정도가 성능에 어떤 영향을 미치는지 실험한다 (Sec. 1 참조). <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> 로부터 얻은 image encoding (dimention <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>I</mi></msub></mrow><annotation encoding="application/x-tex">h_I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>) 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>L</mi></msub><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">e_L \times k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 의 soft prompt sequence 로 projection 되며, 이를 image prompt 라고 부른다. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 값은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> 의 구조에 의해 결정된다.</p><p>예를 들어, MAGMA model 과의 일관성을 위해 CLIP 에서는 pooling 이전의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn><mo>×</mo><mn>12</mn><mo>×</mo><mn>3072</mn><mi>d</mi></mrow><annotation encoding="application/x-tex">12 \times 12 \times 3072d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">12</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">12</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord">3072</span><span class="mord mathnormal">d</span></span></span></span></span> feature map 을 사용하고, 이를 flatten 하여 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>12</mn><mo>×</mo><mn>12</mn><mo>=</mo><mn>144</mn></mrow><annotation encoding="application/x-tex">k = 12 \times 12 = 144</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">12</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">12</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">144</span></span></span></span></span> 로 설정한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="encoders">Encoders<a href="#encoders" class="hash-link" aria-label="Direct link to Encoders" title="Direct link to Encoders">​</a></h4><ol><li><strong>CLIP RN50x16</strong> (Radford et al., 2021)<ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>144</mn></mrow><annotation encoding="application/x-tex">k = 144</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">144</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>I</mi></msub><mo>=</mo><mn>3072</mn></mrow><annotation encoding="application/x-tex">h_I = 3072</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3072</span></span></span></span></span></li><li>CLIP 은 multimodal image-text embedding 을 학습하도록 훈련되었기 때문에, vision-only encoder 보다 language space projection 학습이 더 용이할 것으로 예상된다.</li></ul></li><li><strong>NFRN50</strong> (Brock et al., 2021)<ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">k = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>I</mi></msub><mo>=</mo><mn>2048</mn></mrow><annotation encoding="application/x-tex">h_I = 2048</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">2048</span></span></span></span></span></li><li>NF-ResNet50 을 세 가지 변형으로 실험한다:<ul><li>NFRN50: pre-trained model 을 caption training 동안 frozen</li><li>NFRN50 Tuned: caption training 동안 encoder 는 튜닝, LM 은 frozen 유지</li><li>NFRN50 Random: 무작위 초기화</li></ul></li><li>NFRN50 은 WordNet hypo/hypernym 구조에 따라 labeling 된 image classification task 로 pre-training 되었다. 이 과정에서 object class 가 lexical category 개념에 따라 구분되므로, 간접적인 linguistic supervision 을 받았다고 볼 수 있다.</li></ul></li><li><strong>BEIT-Large</strong> (Bao et al., 2021)<ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>196</mn></mrow><annotation encoding="application/x-tex">k = 196</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">196</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>I</mi></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">h_I = 1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1024</span></span></span></span></span></li><li>BEIT 은 self-supervised masked visual token modeling task 로 pre-training 되었으며, linguistic 구조에 대한 inductive bias 를 줄 수 있는 label data 에 전혀 접근하지 않았다. 여기서는 16-pixel patch 버전을 사용하며, ImageNet22k 만으로 pre-trained model 을 사용한다.</li><li>추가적으로 두 가지 변형을 실험한다:<ul><li>BEIT Random: 무작위 초기화</li><li>BEIT FT: 동일한 pre-training 후, 동일한 dataset 으로 image classification 에 finetuning 된 모델</li></ul></li><li>이를 통해 captioning task 에서 성능 향상을 이끄는 요인이 pretraining objective 의 linguistic supervision 임을 보인다.</li></ul></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-training-procedure">3.1 TRAINING PROCEDURE<a href="#31-training-procedure" class="hash-link" aria-label="Direct link to 3.1 TRAINING PROCEDURE" title="Direct link to 3.1 TRAINING PROCEDURE">​</a></h2><p>MAGMA 와 Frozen model (Eichenberg et al., Tsimpoukelli et al.) 을 따라, 저자는 image captioning task 를 통해 projection 을 학습하여 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span></span></span></span></span> 와 LM 의 representation space 를 정렬하도록 한다. 모든 model 은 MAGMA 논문에서 기술된 기본 hyperparameter 및 설정을 동일하게 사용하며, Conceptual Captions 3M dataset (CC3M) 에서 15,000 training step 동안 학습된다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="baselines">Baselines<a href="#baselines" class="hash-link" aria-label="Direct link to Baselines" title="Direct link to Baselines">​</a></h4><ul><li><strong>NFRN50 Random</strong></li><li><strong>NFRN50 Tuned</strong></li><li>저자가 학습한 <strong>MAGMA_base</strong></li></ul><p>여기서 NFRN50 Tuned 는 Frozen model 을 대체하는 baseline 으로 사용된다. 구조적으로는 동일하지만, MAGMA model 학습에 사용된 hyperparameter 를 적용한 점이 다르다. NFRN50 Random 은 image encoder backbone 이 유용한 visual feature 를 학습하지 않은 경우 LiMBeR 의 효용성을 검증하기 위한 것이다.</p><p>저자가 학습한 MAGMA 는 CLIP RN50x16 image encoder, GPT-J LM, 그리고 attention block 내에 downsample factor 4 를 가지는 adapter sequence 를 사용한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-limitations">3.2 LIMITATIONS<a href="#32-limitations" class="hash-link" aria-label="Direct link to 3.2 LIMITATIONS" title="Direct link to 3.2 LIMITATIONS">​</a></h2><p>계산 자원 제약으로 인해, 각 image encoder 의 prompt 길이 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 는 통제되지 않았다. Tsimpoukelli et al. 은 Frozen model 에 대해 작은 범위의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 값을 실험하였고, 일부 차이는 있으나 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 는 주로 hyperparameter tuning 의 요소이며 model 간 비교에 큰 영향을 주지 않는다고 보였다. 본 연구에서는 CLIP 과 BEIT 에 대해 훨씬 더 큰 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 값을 사용했으므로, 이는 엄밀하게 통제되지 않은 변수로 남는다.</p><p>또 다른 잠재적 교란 요인은 <strong>LM runoff</strong> 현상이다. 즉, LM 이 어떤 개념과 관련된 단어 (e.g., &quot;the beach&quot;) 를 인식하고 생성하기 시작하면, 해당 개념에 강한 linguistic prior 가 존재하기 때문에 추가적인 관련 정보를 생성할 수 있다 (e.g., &quot;building a sandcastle&quot;). 이 경우 LM 이 실제로는 보지 못한 요소 (&quot;the sandcastle&quot;) 도 인식하는 것처럼 보일 수 있다. 그러나 이 문제의 범위는 매우 제한적이며, 여러 large dataset 에 걸친 실험에서 여전히 이미지 정보의 회수가 가능함을 보인다. 다만, 그 정확하고 완전한 범위는 알 수 없다.</p><p>추가적으로, 저자는 visual question answering 분석에서 ‘blind’ model 을 포함시켜 이 문제를 더 잘 통제한다.</p><h1>4 Performance on Vision-Language Tasks</h1><p>저자는 먼저, image representation 이 LM 의 input space 로 linear projection 되었을 때 LM 이 해당 이미지의 semantic information 을 이해할 수 있는지 검증한다. projection 은 image encoder 와 text decoder 사이의 single layer 만 학습되므로, LM 내의 prompt token 은 결국 image representation 을 linear transformation 한 것과 동등하다. 만약 LM 이 비언어적이고 순수하게 시각적 공간을 반영하는 image encoder 의 conceptual spaces 를 유사하게 학습한다면, LM 은 image 정보를 포착하고 이를 텍스트로 기술할 수 있어야 한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="data">Data<a href="#data" class="hash-link" aria-label="Direct link to Data" title="Direct link to Data">​</a></h4><p>평가는 각 image encoder 로부터 생성된 image prompt 를 기반으로 수행하며, 사용된 dataset 은 다음과 같다.</p><ul><li><strong>Image captioning</strong>: MSCOCO, NoCaps</li><li><strong>Visual question answering</strong>: VQA2</li></ul><p>SimVLM 및 MAGMA 의 convention 을 따라, 각 image 입력 뒤에 “A picture of” 라는 prefix 를 추가하여 model 을 prompting 한다. 기존 연구들과 동일하게, 이 prompt 가 성능을 향상시키는 데 유리함을 확인하였다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics">Metrics<a href="#metrics" class="hash-link" aria-label="Direct link to Metrics" title="Direct link to Metrics">​</a></h4><ul><li><strong>Image captioning</strong>: CIDEr-D, CLIPScore, RefCLIPScore 를 보고한다.<ul><li>CIDEr-D 는 시각적으로 유의미한 정확한 단어 생성을 보상한다.</li><li>CLIPScore 는 reference 가 없어도 image 와 caption 간의 semantic similarity 를 측정할 수 있다. 따라서 ground truth 와 표현이 크게 달라도 의미적으로 유사하다면 (e.g., pool 을 lake 로 기술), 정당하게 점수를 부여할 수 있다.</li><li>추가 captioning metric 은 Appendix B 에 포함된다.</li></ul></li><li><strong>Visual question answering</strong>: Eichenberg et al. 의 few-shot 절차를 따른다. model 은 “<!-- -->[image]<!-- --> Q: <!-- -->[q]<!-- --> A:” 형식으로 prompting 된다. 생성된 output 의 첫 번째 단어를 사용하며, MAGMA 논문과 같이 ground truth answer 중 가장 긴 길이에 맞추어 잘라낸다. 또한 VQA repo 에서 정의한 normalization 절차와 accuracy metric 을 사용한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h4><p>주요 결과는 Tab. 1 에 제시된다.</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-72-3eb2cbffac4a88f23457e7a0a6600fea.png" width="1231" height="928" class="img_ev3q"></p><ul><li>MAGMA 와 CLIP, NFRN50 tuned 와 frozen 을 비교하면, encoder 나 decoder 의 parameter 를 학습하는 것이 상대적으로 큰 이점을 주지 않는다는 것을 확인하였다.</li><li>저자가 구현한 MAGMA model 은 frozen CLIP model 과 동일하지만, MAGMA 는 image encoder 와 LM 을 모두 튜닝한다. 그러나 captioning 및 VQA task 에서 공동 튜닝된 model (MAGMA, NFRN50 Tuned) 의 성능은 일관되게 더 좋지 않았으며, 오히려 projection 만 학습한 frozen model 보다 성능이 낮은 경우도 많았다. 이 경향은 Appendix B 에 기술된 10 개 이상의 자동 captioning metric 전반에서 동일하게 나타났다.</li><li>저자의 결과는 pre-training task 의 linguistic supervision 정도가 LM 으로 transfer 성능에 직접적으로 관련됨을 보여준다. <ul><li>즉, CLIP &gt; NFRN50 &gt; BEIT 순으로 성능이 나타났다. </li><li>이를 확인하기 위해 저자는 image classification 으로 finetuning 된 BEIT (BEIT FT) 에 LiMBeR 를 적용하였다. </li><li>그 결과, 성능이 크게 향상되었으며 NoCaps 에서는 CLIP 을 능가했고, CLIP-Score 를 포함한 모든 metric 에서 원래 BEIT 보다 9–10 점 더 높은 성능을 기록했다. </li><li>이는 성공적인 transfer 에서 architecture 보다는 <strong>pretraining task 의 linguistic supervision</strong> 이 더 중요한 요인임을 시사한다.</li></ul></li></ul><p>흥미롭게도, pre-training 에서 전혀 linguistic supervision 을 받지 않은 vanilla BEIT 도 여전히 captioning task 에서 LM 으로 잘 transfer 되었으며, 시각적 feature 를 학습하지 않은 random NFRN50 보다 월등히 높은 성능을 보였다. 다만 BEIT caption 은 더 모호한 언어를 사용하거나, 의미적으로는 유사하지만 객체에 대한 잘못된 설명을 생성하는 경우가 있었다 (Fig. 2). </p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-71-f6bcee69ebfad2ebbb291fb3db278bf7.png" width="1182" height="730" class="img_ev3q"></p><ul><li>이는 CLIPScore 에도 반영되었는데, caption 과 reference 의 정확한 lexical 일치보다 semantic similarity 를 보상하기 때문이다. </li><li>BEIT caption 은 NoCaps 와 COCO 에서 각각 62, 63.6 점을 기록했으며, 평균적으로 NFRN50 보다 4.5 점 낮지만 random NFRN50 보다는 14.3 점 높았다.<ul><li>그러나 BEIT 의 가장 큰 한계는 LM 이 이미지를 기반으로 질문에 답할 수 있도록 transfer 할 수 있는 세부 정보를 제공하지 못하는 점이다. </li><li>예를 들어, 4-shot VQA 에서 BEIT 은 31.72% 를 기록했지만, image 입력이 없는 ‘blind’ LM 은 36.99% 를 기록했다. 저자는 이것이 BEIT representation 이 lexical category 와 잘 대응되는 visual information 을 인코딩하지 못하기 때문이라고 가설을 세운다. </li></ul></li></ul><p>Sec. 5 에서 저자는 이 가설을 지지하는 증거를 제시하고, frozen encoder 별로 LM 으로 transfer 되는 prompt detail 의 granularity 를 탐구한다.</p><h1>5 TRANSFER OF VISUAL CONCEPTS</h1><p>image prompt 이 LM 으로 성공적으로 transfer 되는 조건과 실패하는 조건을 분석하면 text 와 image representation space 의 차이를 이해하는 데 도움이 된다. 또한 이는 BEIT prompt 가 captioning 에서는 어느 정도 성능을 보이지만 VQA 에서는 매우 낮은 성능을 보이는 이유를 설명하는 데도 기여할 수 있다. Sec. 5.1 에서는 이미지에 특정 lexical category 가 등장할 때 이를 caption 에 정확히 생성할 수 있는 능력을 분석한다 (e.g., 다람쥐 사진이 주어졌을 때 “squirrel” 이라는 단어를 언급하는 경우).</p><p>이후 Sec. 5.3 에서는 model 이 생성한 잘못된 caption 을 분석한다. 즉, LM 이 잘못된 caption 을 생성할 때, 실제 entity 와 시각적 속성이 유사한 entity 를 기술하는 경향이 있는지 확인한다. 예를 들어, “small”, “woodland”, “furry” 동물의 이미지를 보고 LM 이 실제 대상(다람쥐)을 언급하지 못한다면, 대신 비슷한 털이 있는 동물(토끼 등)을 언급하는지 살펴본다.</p><p>저자의 분석에 따르면, <strong>linguistic supervision 을 받은 image encoder (NFRN50, CLIP) 만이 lexical category 와 정렬되는 개념을 강하게 인코딩</strong>하는 경향이 있었다. 그러나 BEIT 을 포함한 모든 pretrained model 은 property information 을 대체로 비슷한 수준으로 인코딩하였으며, 무작위로 초기화된 image encoder baseline 보다 훨씬 뛰어났다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-transfer-of-lexical-categorical-concepts">5.1 TRANSFER OF LEXICAL CATEGORICAL CONCEPTS<a href="#51-transfer-of-lexical-categorical-concepts" class="hash-link" aria-label="Direct link to 5.1 TRANSFER OF LEXICAL CATEGORICAL CONCEPTS" title="Direct link to 5.1 TRANSFER OF LEXICAL CATEGORICAL CONCEPTS">​</a></h2><p>COCO validation set 을 사용하여, ground truth caption 에 나타나는 상위 50 개의 <strong>명사, 수식어(형용사 등), 관계(동사, 전치사구 등)</strong> 를 집계한 후, Table 1 의 점수 산출에 사용된 generated caption 에서 이들이 얼마나 자주 등장하는지를 계산하였다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-1">Metrics<a href="#metrics-1" class="hash-link" aria-label="Direct link to Metrics" title="Direct link to Metrics">​</a></h4><ul><li>각 단어에 대해 precision / recall / F1 을 conceptual category 별로 계산하였다.</li><li>BEIT 이 더 거친 정보만 transfer 한다는 가설을 검증하기 위해, ground truth 단어와 generated caption 에 등장한 가장 유사한 단어 간의 <strong>Wu-Palmer similarity (Wup)</strong> 도 측정하였다. Wup score 는 WordNet taxonomy 상에서 두 단어 간 거리를 계산하여, 정답 단어와 생성 단어가 ‘얼마나 가까운지’를 측정하는 방법이다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-1">Results<a href="#results-1" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h4><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-73-57f8c834f7814041830efece3c3f550f.png" width="1232" height="601" class="img_ev3q"></p><ul><li>Fig. 3 에서 보이듯이, BEIT 의 <strong>명사 recall</strong> 은 ‘people’, ‘environment’, ‘vehicles’, ‘objects’ 와 같은 category 에서 NFRN50 이나 CLIP 보다 낮았다. </li><li>그러나 많은 category 에서 <strong>Wup similarity</strong> 는 두 encoder 와 비교할 만한 수준이었다. <ul><li>이는 BEIT 의 pretraining 이 NFRN50 과 CLIP 의 pretraining 과 달리, 서로 다른 단어로 표현되는 시각적으로 유사한 객체 간 개념적 차이를 학습하도록 유도하지 않았기 때문이다.</li></ul></li><li>그러나 무작위 초기화된 NFRN50 prompt 와 비교하면, BEIT prompt 는 여전히 LM 이 폭넓은 개념적 의미를 추출할 수 있도록 하였다. 이는 높은 Wup similarity 와 Table 1 의 CLIPScore 결과로도 확인된다.</li></ul><p>이 결과는 BEIT prompt 가 순수한 visual space 로부터 순수한 text space 로 개념 정보를 transfer 한다는 가설을 뒷받침한다. 다만 이는 lexical category 가 아니라 시각적 속성에 대응되는 거친 수준(coarse-grained)의 개념 정보에 한정된다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-probing">5.2 PROBING<a href="#52-probing" class="hash-link" aria-label="Direct link to 5.2 PROBING" title="Direct link to 5.2 PROBING">​</a></h2><p>BEIT representation 이 lexical concept information 을 인코딩하고 있음에도 불구하고, representation 차이로 인해 단순히 LM 으로 linear transfer 되지 못하는 가능성을 배제하기 위해, 저자는 여러 image classification dataset 에 대해 linear probe 를 학습하였다. 그 결과, BEIT 은 NFRN50 이나 CLIP 만큼 fine-grained information 을 잘 인코딩하지는 못하지만, 무작위로 초기화된 NFRN50 baseline 보다는 훨씬 뛰어난 성능을 보였다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="53-transfer-of-coarse-grained-perceptual-concepts">5.3 TRANSFER OF COARSE-GRAINED PERCEPTUAL CONCEPTS<a href="#53-transfer-of-coarse-grained-perceptual-concepts" class="hash-link" aria-label="Direct link to 5.3 TRANSFER OF COARSE-GRAINED PERCEPTUAL CONCEPTS" title="Direct link to 5.3 TRANSFER OF COARSE-GRAINED PERCEPTUAL CONCEPTS">​</a></h2><p>BEIT 이 단어 category 정보가 아닌 무엇을 인코딩하는지 더 잘 이해하기 위해, 저자는 오류가 발생하는 지점과 각 frozen image encoder 의 embedding space 구조 차이를 조사하였다. 분석의 편의를 위해 task 를 <strong>동물 사진 caption 생성</strong> 으로 한정하였다. 이렇게 좁은 범위를 택한 이유는, caption 분석이 더 용이하기 때문이다. 즉, 동물 사진을 설명하는 caption 은 해당 동물의 이름을 언급해야 하며, 사용되는 단어는 대체로 모호하지 않다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="data-1">Data<a href="#data-1" class="hash-link" aria-label="Direct link to Data" title="Direct link to Data">​</a></h4><p>이 task 에는 <strong>Animals With Attributes 2 (AWA)</strong> dataset 을 사용하였다. 이 dataset 은 총 37k 개의 이미지와 50 개 동물 class 를 포함한다. 각 class 는 동물을 설명하는 85 개의 속성(annotation) (e.g., ‘claws’, ‘stripes’, ‘jungle’) 과 함께 제공되며, 이를 통해 특정 encoder 의 prompt 가 특정 차원에서 일관되게 오류를 발생시키는지를 분석할 수 있다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics-2">Metrics<a href="#metrics-2" class="hash-link" aria-label="Direct link to Metrics" title="Direct link to Metrics">​</a></h4><p>image prompt 가 caption 을 생성했을 때, caption 에 등장한 동물이 ground truth animal label 의 WordNet synset 과 얼마나 유사한지를 측정할 수 있다. 또한 AWA dataset 의 속성 annotation 을 활용하여 유사도를 측정할 수 있다. 예를 들어, ground truth 가 “squirrel” 일 때, 가장 자주 잘못 인식되는 다른 동물 (“rabbit”) 과의 속성 공유 비율을 비교할 수 있다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-2">Results<a href="#results-2" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h4><p>각 image 에 대해 각 frozen image encoder 로부터 prompt 를 생성하고 caption 을 얻었다. caption 이 해당 동물의 이름을 포함하면 ‘정답’으로 간주하였다.</p><ul><li>CLIP: 59% 정답</li><li>NFRN50: 43% 정답</li><li>BEIT: 13% 정답</li><li>Random NFRN50: 0.4% 정답</li></ul><p>이는 BEIT 이 fine-grained lexical 개념을 인코딩하는 데 어려움을 겪는다는 이전의 관찰과 일치한다.</p><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-74-d2681a7e21ff5fc75aabf1f33b075c1c.png" width="1224" height="1072" class="img_ev3q"></p><p>각 model 의 failure case 를 살펴본 결과, model 이 잘못된 동물을 예측할 때, 그 동물은 ground truth 와 매우 유사한 동물인 경우가 많았다. Fig. 4a 에 따르면, 잘못 분류된 동물은 다음과 같이 높은 유사도를 보였다.</p><ul><li>Wu-Palmer similarity 평균: BEIT 0.8, NFRN50 0.81, CLIP 0.8</li><li>AWA 속성 overlap 평균: BEIT 0.62, NFRN50 0.68, CLIP 0.59</li></ul><p>즉, BEIT prompt 는 LM 으로 정확한 동물 개념을 transfer 하지는 못하지만, <strong>coarse-grained perceptual information</strong> 는 transfer 되어 LM 이 이를 ‘이해’할 수 있었다.</p><p>Fig. 4b 에서 저자는 AWA 의 각 image encoding 을 UMAP projection 으로 시각화하였다. 그 결과,</p><ul><li>NFRN50 과 CLIP 은 lexical category (동물 종류) 에 따라 밀집 cluster 를 형성하였다.</li><li>BEIT 은 habitat, flippers 보유 여부 등 <strong>perceptual feature</strong> 를 기준으로 더 강하게 cluster 되었다.</li></ul><h1>6 DISCUSSION &amp; FUTURE WORK</h1><p>저자는 image representation 을 LM 의 입력으로 연결하기 위해 극도로 단순한 변환, 즉 linear projection 을 사용하였다. 이 접근법으로 이미지와 관련된 텍스트를 성공적으로 생성할 수 있다는 사실은 language representation 과 vision representation 사이에 충분히 탐구되지 않은 representation similarity 가 존재함을 보여준다.</p><p>pre-training 과정에서 image encoder 가 받은 linguistic guidance 의 정도에 따라 성능이 달라진다. vision-only encoder (BEIT) 를 사용할 경우, LM 이 생성하는 텍스트는 종종 부정확하지만 perceptual relatedness 기준에서는 근접한 경우가 많았다. image classification 으로 finetuning 되지 않는 한, BEIT 은 pre-training 에서 서로 지각적으로 유사하지만 언어적으로는 다른 개념을 구분하도록 하는 inductive bias 를 가지지 않는다. 오직 linguistic supervision 을 받은 image encoder 만이 이러한 구분을 학습할 수 있다는 사실은, 언어가 category 형성에서 어떤 역할을 하는지에 대한 향후 연구 가능성을 제시한다.</p><p>linear transformation 으로도 강력한 성능을 보였음에도 불구하고, 이러한 model 의 representation space 는 language space 로 근사할 수 없는 차이를 포함한다. 이상적인 multimodal model 은 이러한 차이를 활용하여 더 풍부한 representation 을 학습할 수 있어야 한다. 따라서 현재 multimodal pretraining objective 가 이러한 점을 어떻게 성공하거나 실패하는지 이해하는 것이 중요하다. LiMBeR 는 vision 과 language 간의 최소한의 mapping 을 기준으로 비교할 수 있는 강력한 baseline 이 될 수 있다.</p><p>저자는 LiMBeR 가 서로 다른 modality 로부터 학습된 representation 이 얼마나 유사하거나 다른지를 이해하는 데 유용한 도구라고 본다.</p><ul><li>representation 이 유사한 개념의 경우, 이를 활용해 좋은 text representation 을 학습하는 데 필요한 data 양을 줄일 수 있는가?</li><li>representation 이 다른 개념의 경우, multimodal model 은 양쪽 정보를 통합하여 더 풍부한 representation 을 학습할 수 있는가?</li></ul><p>예를 들어, vision data 는 text corpus 에 내재된 reporting bias 를 완화하는 데 도움을 줄 수 있다. 이러한 질문에 답하는 것은 text-only pretraining 의 한계를 이해하고, LM 을 비언어적 data 에 더 잘 grounding 하는 방법을 탐구하는 데 기여할 수 있다.</p><h1>7 CONCLUSION</h1><p>이 논문에서는 pretrained image representation 과 text representation 이 얼마나 유사한지를 검증하기 위해, image representation 을 LM 의 입력 공간으로 projection 하는 linear map 을 학습하였다. 이를 통해 LM 이 이미지의 내용을 정확히 설명할 수 있도록 하였다.</p><p>LiMBeR (Linearly Mapping Between Representation spaces) 로 학습된 model 은, image 와 text network 을 모두 튜닝하는 MAGMA 와 같은 기존 model 과 비교하여 image captioning 과 visual question answering benchmark 에서 경쟁력 있는 성능을 보였다. 그러나 이러한 transfer 성능은 image encoder backbone 이 pre-training 중 받은 linguistic supervision 의 정도에 크게 의존함을 발견하였다. vision-only image encoder 인 BEIT 은 image classification 으로 학습된 ResNet model 보다 성능이 낮았고, ResNet 은 다시 자연어 caption 과 함께 pre-trained CLIP 보다 성능이 낮았다.</p><p>저자는 어떤 개념 정보가 성공적으로 transfer 되는지를 탐구하였으며, 생성된 텍스트, clustering, probing 분석을 통해 LM 과 vision-only image representation 사이의 유사성은 주로 coarse-grained perceptual feature 개념에 국한됨을 보였다. 반면, linguistic supervision 을 받은 vision model 은 lexical concept transfer 가 가능하였다.</p><p>이러한 발견은 LM 과 vision model 이 개념적으로 유사한 representation space 를 학습하며, 최소한의 linear transformation 으로도 image 정보를 transfer 하는 데 충분히 근사할 수 있음을 시사한다. 다만, 이러한 representation similarity 의 범위는 아직 잘 이해되지 않았으며, 이는 향후 탐구할 만한 흥미로운 연구 방향이다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">Vision-Language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multimodal">Multimodal</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vqa">VQA</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/VQA-IC/Few-shot/2022-09-MAGMA.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-training-procedure" class="table-of-contents__link toc-highlight">3.1 TRAINING PROCEDURE</a></li><li><a href="#32-limitations" class="table-of-contents__link toc-highlight">3.2 LIMITATIONS</a></li><li><a href="#51-transfer-of-lexical-categorical-concepts" class="table-of-contents__link toc-highlight">5.1 TRANSFER OF LEXICAL CATEGORICAL CONCEPTS</a></li><li><a href="#52-probing" class="table-of-contents__link toc-highlight">5.2 PROBING</a></li><li><a href="#53-transfer-of-coarse-grained-perceptual-concepts" class="table-of-contents__link toc-highlight">5.3 TRANSFER OF COARSE-GRAINED PERCEPTUAL CONCEPTS</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.7caf321c.js"></script>
<script src="/assets/js/main.a0120155.js"></script>
</body>
</html>