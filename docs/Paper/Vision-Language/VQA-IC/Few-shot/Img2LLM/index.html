<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/VQA-IC/Few-shot/2022-12-Img2LLM">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.618c4439.js" as="script">
<link rel="preload" href="/assets/js/main.079b9612.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Reinforce Learning/DPO/RLHF/Self-RLM">Reinforce Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">CLIP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Foundation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">VQA-IC</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Few-shot</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Multimodal Few-Shot Learning with Frozen Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA">xGQA: Cross-Lingual Visual Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP">CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA">Linearly Mapping from Image to Text Space</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper">Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA">Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/CoTMT">A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MAPD">Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering</a></li></ul></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">VQA-IC</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Few-shot</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</h1></header><p>논문 및 이미지 출처 : <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_From_Images_to_Textual_Prompts_Zero-Shot_Visual_Question_Answering_With_CVPR_2023_paper.pdf" target="_blank" rel="noopener noreferrer">https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_From_Images_to_Textual_Prompts_Zero-Shot_Visual_Question_Answering_With_CVPR_2023_paper.pdf</a></p><h1>Abstract</h1><p>Large language models (LLMs) 은 새로운 language task 에 대해 뛰어난 zero-shot generalization 을 보여왔다. 그러나 LLM 을 zero-shot visual question-answering (VQA) 에 효과적으로 활용하는 것은 여전히 도전적인 문제로 남아있다. 이는 주로 LLM 과 VQA task 간의 modality disconnect 와 task disconnect 때문이라고 할 수 있다. Multimodal data 에 대한 end-to-end training 은 이러한 disconnect 를 완화할 수 있지만, 유연성이 떨어지고 computation cost 가 매우 크다.</p><p>이 문제를 해결하기 위해 저자는 <strong>Img2LLM</strong> 을 제안한다. 이는 plug-and-play module 로서, end-to-end training 없이도 LLM 이 zero-shot VQA task 를 수행할 수 있도록 LLM prompt 를 제공한다. 저자는 LLM-agnostic model 을 개발하여 image content 를 exemplar question-answer pair 로 기술하고, 이를 효과적인 LLM prompt 로 활용한다.</p><p>Img2LLM 은 다음과 같은 이점을 제공한다:</p><ul><li>End-to-end training 에 의존하는 방법들보다 동등하거나 더 나은 성능을 달성한다. 예를 들어, VQAv2 에서 Flamingo 보다 5.6% 높은 성능을 보인다. 또한 도전적인 A-OKVQA dataset 에서 저자의 방법은 few-shot 방법보다 최대 20% 더 우수한 성능을 보인다.</li><li>다양한 LLM 과 유연하게 연결되어 VQA 를 수행할 수 있다.</li><li>End-to-end finetuning 을 통한 LLM 의 특수화를 제거하고, highly specialized LLM 을 end user 에게 제공할 필요가 없어져 비용을 절감할 수 있다.</li></ul><h1>1. Introduction</h1><p>Visual question answering (VQA) 는 시각장애인이 주변 환경을 이해하도록 돕는 것과 같은 다양한 실제 응용을 가진 대표적인 vision-language task 이다. 지금까지 다양한 VQA dataset 이 제안되었는데, 일부는 image recognition 에 초점을 맞추고, 다른 일부는 logical reasoning 에 초점을 맞춘다. 그러나 human annotation 은 비용이 많이 들고 다양한 human bias 를 유발할 수 있어 VQA system 이 새로운 답변 스타일과 질문 유형에 취약하게 된다. 이러한 문제로 인해 ground-truth question-answer annotation 을 필요로 하지 않는 zero-shot VQA 방법들이 제안되었고, 이를 통해 보다 generalizable VQA system 이 가능해졌다.</p><p>최근 large language models (LLMs) 은 in-domain data 가 전혀 없는 상황에서도 task 수행, logical reasoning, commonsense knowledge 활용 등의 능력을 보여주었다. 이에 따라 최근 접근법들은 zero-shot VQA 를 위해 LLM 을 활용하고 있다. 그러나 LLM 을 VQA task 에 적용하는 것은 단순하지 않다. 그 이유는 (1) vision 과 language 간의 modality disconnect 와 (2) language modeling 과 question answering 간의 task disconnect 때문이다. 일반적인 접근법은 vision encoder 와 LLM 을 함께 finetune 하여 vision 과 language representation space 를 정렬하는 것이다. 하지만 이는 막대한 computation 및 data cost 를 요구한다. 예를 들어, Flamingo 는 수십억 개의 image-text pair 를 사용해 수천 개의 TPU 로 finetune 을 수행한다. 게다가 finetuning 은 vision encoder 와 LLM 사이의 강한 상호 의존성을 초래하여, 새로운 버전의 LLM 으로 교체할 때 전체 model 을 다시 고비용 training 해야 한다.</p><p>이에 반해, 저자는 LLM 을 end-to-end 로 VQA system 에 통합하는 대신, frozen off-the-shelf LLM 에 기반한 modular VQA system 을 제안한다. 이는 두 가지 장점을 가진다. </p><ol><li>deployment cost 를 줄이고 과정을 단순화할 수 있다. </li><li>LLM upgrade 가 용이하다. 그러나 end-to-end training 없이 modality disconnect 와 task disconnect 를 해소하는 것은 도전적이다. <ul><li>PICa 는 이미지를 caption 으로 변환하고, training data 로부터 얻은 exemplar QA pair 를 LLM 의 prompt 로 제공한다. </li><li>하지만 이는 annotation 된 training data 의 존재를 전제로 하며, few-shot exemplar 선택에 따라 성능이 민감하게 변한다.</li></ul></li></ol><p>저자는 <strong>Img2LLM</strong> 을 제안한다. </p><ul><li>이는 plug-and-play module 로서 off-the-shelf LLM 이 zero-shot VQA 를 수행할 수 있도록 한다. </li><li>Img2LLM 의 핵심 통찰은 vision-language model (e.g., BLIP) 과 question-generation model 을 활용하여 image content 를 synthetic QA pair 로 변환하고, 이를 LLM prompt 의 일부로 제공하는 것이다. </li><li>이러한 exemplar QA pair 는 이미지 내용을 언어로 설명함으로써 modality disconnect 를 해소하고, LLM 에 QA task 를 시연함으로써 task disconnect 를 해소한다. </li><li>특히, exemplar QA pair 는 전적으로 test image 와 question 에 기반하여 구성되므로, PICa 가 요구하는 few-shot 예제가 필요하지 않다. <ul><li>이는 practical zero-shot 환경에서 매우 유리하다. </li></ul></li><li>Open-source OPT language model 에 적용했을 때, Img2LLM 은 비용이 큰 end-to-end training 방법들과 비교해 동등하거나 더 우수한 zero-shot VQA 성능을 달성한다.</li></ul><p>본 논문의 기여는 다음과 같다.</p><ul><li>저자는 Img2LLM 을 제안한다. 이는 현재 주어진 image 와 question 만으로 synthetic QA pair 를 생성하는 plug-and-play module 이다. Img2LLM 은 language 와 vision 간의 modality disconnect 와, language modeling 과 VQA 간의 task disconnect 를 동시에 해소한다.</li><li>Img2LLM 은 off-the-shelf LLM 이 zero-shot VQA 를 수행할 수 있게 하며, 비용이 큰 end-to-end training 이나 특수한 textual QA network 를 필요로 하지 않는다. 따라서 저비용, 유연한 model deployment 와 손쉬운 LLM upgrade 가 가능하다.</li><li>실험 결과, Img2LLM 을 장착한 OPT model 은 end-to-end trained model 과 비교해 경쟁력 있는 성능을 보이며, 일부 경우에는 더 우수하다. 예를 들어, VQAv2 에서 Flamingo 보다 5.6% 더 높은 성능을 달성하며, 다수의 few-shot VQA 방법보다도 성능이 뛰어나다.</li></ul><h1>2. Related Work</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-recent-advances-in-vqa-methods">2.1. Recent Advances in VQA Methods<a href="#21-recent-advances-in-vqa-methods" class="hash-link" aria-label="Direct link to 2.1. Recent Advances in VQA Methods" title="Direct link to 2.1. Recent Advances in VQA Methods">​</a></h2><p>Visual Question Answering (VQA) 는 주어진 image 를 기반으로 natural language question 에 답해야 하는 multi-modal evaluation benchmark 로, 활발한 연구의 초점이 되어왔다. 최근 몇 년 동안 large-scale image-text pretraining 과 그 후 VQA dataset 에 대한 finetuning 을 통해 성능이 빠르게 향상되었다. Knowledge-based VQA 를 다루기 위해 최근 연구들은 ConceptNet, Wikipedia 와 같은 external knowledge 를 통합하기도 했다. 그러나 실험 결과, 이러한 방법들은 여전히 복잡한 reasoning 을 필요로 하는 질문에는 어려움을 겪고 있음이 보고되었다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-llm-for-zerofew-shot-vqa-tasks">2.2. LLM for Zero/Few-Shot VQA Tasks<a href="#22-llm-for-zerofew-shot-vqa-tasks" class="hash-link" aria-label="Direct link to 2.2. LLM for Zero/Few-Shot VQA Tasks" title="Direct link to 2.2. LLM for Zero/Few-Shot VQA Tasks">​</a></h2><p>Large language models (LLMs) 는 web-scale corpus 로 학습되어 natural language 이해와 reasoning 에서 강력한 성능을 보인다. 일반적으로 LLM 은 task data 에 대해 target token 을 autoregressive 하게 생성한다. 구체적으로, prompt <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span></span> 와 task input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 가 주어졌을 때, LLM 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>arg</mi><mo>⁡</mo><mi>max</mi><mo>⁡</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y = \{ y_i \}_{i=1}^n, \quad y_i = \arg \max p_\theta(y_i \mid y_{&lt;i}, C, x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0087em;vertical-align:-0.2587em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">ar<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">max</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> 를 생성하며, 여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span></span> 는 model parameter 를 의미한다. LLM 을 활용한 기존 VQA 방법들은 크게 <strong>multi-modal pretraining</strong> 과 <strong>language-mediated VQA</strong> 두 가지 범주로 나뉜다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-57-eba9a2bf30e7aa26d44e74ecca615d17.png" width="1677" height="669" class="img_ev3q"></p><ul><li><strong>Multi-modal pretraining</strong>
이 접근법은 vision 과 language embedding 을 정렬하기 위해 추가 alignment module 을 학습한다 (Fig. 1(a)). LLM 은 너무 커서 효율적으로 finetune 하기 어렵기 때문에, 일부 연구는 visual encoder 만 finetune 하거나, Flamingo 와 같이 cross-modality interaction 을 학습하기 위해 cross-attention layer 를 추가 학습한다. 그러나 이 paradigm 은 두 가지 단점이 있다:<ol><li><strong>Compute-inefficient</strong>. Vision backbone 과 LLM 을 함께 정렬하는 것은 막대한 연산 자원이 필요하다. 예를 들어, Flamingo 학습에는 1536 TPUv4 가 2 주간 필요하다. 따라서 다른 LLM 으로 전환하는 것이 사실상 불가능할 정도로 비용이 크다.</li><li><strong>Catastrophic forgetting</strong>. Visual model 과 함께 joint training 하면 alignment 단계가 LLM 의 reasoning 능력을 저해할 수 있다.</li></ol></li><li><strong>Language-mediated VQA</strong>
이 접근법은 vectorized representation 대신 natural language 를 image 의 intermediate representation 으로 사용하여, 고비용 pretraining 을 요구하지 않는다 (Fig. 1(b)). 즉, 현재 image 를 language description 으로 변환하고, 이를 in-context exemplar 와 함께 frozen LLM 에 입력한다. Few-shot setting 에서 PICa 는 image caption 을 생성하고 training data sample 을 in-context exemplar 로 선택하지만, exemplar 가 생략되면 성능이 크게 저하된다. 동시에 제안된 zero-shot 접근법은 question-relevant caption 을 생성한다. 그러나 zero-shot 조건에서는 in-context exemplar 를 제공할 수 없으므로 in-context learning 의 이점을 활용할 수 없다. 그 결과, UnifiedQAv2 와 같은 QA-specific LLM 에 의존해야 높은 성능을 얻을 수 있다.</li></ul><h1>3. Method</h1><p>LLMs 를 zero-shot VQA 에 효과적으로 활용하는 데에는 주로 두 가지 장애가 존재한다. (i) <strong>Modality disconnection</strong>: LLM 은 이미지를 직접 처리하지 못하며, visual information 을 LLM 이 처리할 수 있는 형식으로 변환하는 것은 도전적이다. (ii) <strong>Task disconnection</strong>: LLM 은 일반적으로 generative 또는 denoising objective 를 기반으로 language modeling task 에 대해 pretrain 되어 있다. 따라서 LLM 은 question answering 또는 VQA task 자체를 알지 못하며, 답변을 생성할 때 contextual information 을 충분히 활용하지 못한다.</p><p>Language-mediated VQA 접근법에서는 (§2.2), modality disconnection 을 dense vector 대신 intermediate language description 으로 변환함으로써 해결한다. 그러나 task disconnection 은 few-shot in-context exemplar 또는 textual QA 에 직접 finetune 된 LLM 을 필요로 한다. 따라서 zero-shot 환경에서 generic LLM 의 task disconnection 을 어떻게 다룰 수 있는지는 명확하지 않다.</p><p>저자는 이 문제를 해결하기 위해 새로운 zero-shot 기법 <strong>Img2LLM</strong> (Fig. 1(c)) 을 제안한다. </p><ul><li>Img2LLM 은 image-relevant exemplar prompt 를 LLM 에 제공한다. </li><li>question <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span></span> 와 이미지가 주어졌을 때, 저자의 핵심 아이디어는 현재 이미지로부터 synthetic QA pair 를 in-context exemplar 로 생성하는 것이다. </li><li>이 exemplar 는 LLM 에게 QA task 자체를 시연할 뿐만 아니라, 이미지의 내용을 전달하여 question <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span></span> 에 답할 수 있도록 한다. 즉, 한 번에 두 가지 문제(modality disconnection 과 task disconnection)를 해결한다. </li><li>Img2LLM 은 LLM-agnostic 으로, off-the-shelf LLM 의 knowledge 와 reasoning 능력을 활용할 수 있어 강력하면서도 유연한 zero-shot VQA 해법을 제공한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-answer-extraction">3.1. Answer Extraction<a href="#31-answer-extraction" class="hash-link" aria-label="Direct link to 3.1. Answer Extraction" title="Direct link to 3.1. Answer Extraction">​</a></h2><p>이미지 내용을 in-context learning 의 exemplar 에 통합하기 위해, 주어진 VQA image 로부터 synthetic question 의 잠재적 답변으로 사용할 수 있는 단어를 먼저 추출한다. 저자는 off-the-shelf question-relevant caption generation module (§3.3) 을 사용해 여러 caption 을 생성한다. 이어서 noun phrase (named entity 포함), verb phrase, adjective phrase, number, &quot;yes&quot;, &quot;no&quot; 와 같은 boolean type 단어를 잠재적 answer 로 추출한다. Fig. 2 와 Appendix A.3 에 일부 예시가 제시되어 있다.</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-58-90e18fcb777103569129ab6191305425.png" width="1619" height="589" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-question-generation">3.2. Question Generation<a href="#32-question-generation" class="hash-link" aria-label="Direct link to 3.2. Question Generation" title="Direct link to 3.2. Question Generation">​</a></h2><p>추출된 answer candidate 집합 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mover accent="true"><mi>a</mi><mo>^</mo></mover><mo>∗</mo><mi>j</mi></mrow><mo>∗</mo><msup><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>U</mi></msup></mrow><annotation encoding="application/x-tex">{\hat{a}*j}*{j=1}^U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.0852em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.05724em">j</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord">1</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8908em"><span style="top:-3.1124em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">U</span></span></span></span></span></span></span></span></span></span></span></span> 가 주어졌을 때, 저자는 question generation network 를 사용하여 각 answer candidate 에 대한 구체적 질문을 생성한다. 본 논문에서는 template-based 방법과 neural 방법을 모두 실험하였다. 중요한 점은, zero-shot 조건을 위반하지 않기 위해 본 방법은 오직 textual data 만 사용하며 어떠한 VQA data 도 사용하지 않는다는 것이다.</p><ul><li><p><strong>Template-based Question Generation</strong>
Off-the-shelf parser 를 이용해 각 answer 의 part-of-speech 를 얻고, POS type 별로 특정 질문 template 을 설계한다. 예를 들어, noun answer 의 경우 &quot;What object is in this image?&quot; 를 사용하고, verb answer 의 경우 &quot;What action is being taken in this image?&quot; 를 사용한다. 전체 template 목록은 Appendix A.5 에 포함된다.</p></li><li><p><strong>Neural Question Generation</strong>
최근 연구에서 영감을 받아, 저자는 textual QA dataset 에서 neural question generation model 을 학습하였다. 구체적으로, pretrained T5-large model 을 finetune 하여 answer 로부터 question 을 생성하도록 한다. Model input 은 &quot;Answer: <code>[answer]</code>. Context: <code>[context]</code>&quot; 로 구성되며, <code>[answer]</code> 는 추출된 answer text, <code>[context]</code> 는 textual QA dataset 의 context text 이다. Inference 시에는 <code>[answer]</code> 를 추출된 answer candidate 로, <code>[context]</code> 를 해당 answer 가 추출된 caption 으로 대체한다. Model 은 SQuAD2.0, MultiRC, BookQA, CommonsenseQA, Social IQA 등 다섯 개 textual QA dataset 으로 finetune 된다.</p></li></ul><p>위 과정을 통해 synthetic QA pair 집합 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mi>j</mi></msub><mo separator="true">,</mo><msub><mover accent="true"><mi>a</mi><mo>^</mo></mover><mi>j</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>U</mi></msubsup></mrow><annotation encoding="application/x-tex">\{\hat{q}_j, \hat{a}_j\}_{j=1}^U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.3948em"></span><span class="mopen">{</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">a</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.25em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">U</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em"><span></span></span></span></span></span></span></span></span></span></span> 를 얻을 수 있으며, 이를 LLM 의 in-context learning exemplar 로 사용한다. 이 과정은 LLM 이 QA task 를 수행하도록 유도하며, language modeling 과 VQA 사이의 task disconnection 을 해소한다.</p><p>미리 결과를 보여주자면, Tab. 1 에서 exemplar QA pair 의 효과를 확인할 수 있다. 세부 instruction 은 §3.4 에 설명된다. Exemplar QA prompt 는 caption 만 사용하는 경우보다 훨씬 나은 성능을 보이며, 이는 LLM pretraining 과 VQA task 사이의 task disconnection 을 해소하는 데 효과적임을 보여준다. 또한 exemplar prompt 는 image 의 많은 내용을 이미 설명하므로, caption 을 추가해도 새로운 정보가 거의 없고 성능 향상은 제한적이다.</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-59-6d3b33287d66d6ffe174b49cdd3e0ec8.png" width="1345" height="274" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-question-relevant-caption-prompt">3.3. Question-relevant Caption Prompt<a href="#33-question-relevant-caption-prompt" class="hash-link" aria-label="Direct link to 3.3. Question-relevant Caption Prompt" title="Direct link to 3.3. Question-relevant Caption Prompt">​</a></h2><p>Synthetic exemplar QA pair 와 더불어, 저자는 question-relevant image caption 도 LLM 에 제공한다. 질문은 종종 image 내 특정 object 나 region 을 대상으로 하지만, 기존 network 가 생성하는 generic caption 은 관련 정보를 포함하지 않을 수 있다. 예를 들어, Fig. 2 의 질문 “What items are spinning in the background which can be used to control electricity?” 는 wind turbine 과만 관련이 있다. 그러나 전체 image 로부터 생성된 caption 은 눈에 띄는 orange boat 에 초점을 맞출 가능성이 높아, LLM 이 질문에 답할 수 있는 정보가 부족하게 된다. 이를 해결하기 위해, 저자는 image 의 question-relevant 부분을 대상으로 caption 을 생성하여 LLM 의 prompt 에 포함시킨다.</p><p>이를 위해 먼저 BLIP 의 Image-grounded Text Encoder (ITE) 를 사용하여 image region 과 textual question 의 유사도 점수 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>sim</mtext><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{sim}(v, q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">sim</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mclose">)</span></span></span></span></span> 를 계산한다. 이어서 GradCAM 을 활용하여 주어진 질문과 매칭되는 image region 을 coarse localization map 으로 생성한다. 간단히 말해, GradCAM 은 Transformer network 의 cross-attention score 에 대한 ITE similarity function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>sim</mtext><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{sim}(v, q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">sim</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span><span class="mclose">)</span></span></span></span></span> 의 gradient 를 활용하여 cross-attention score 를 보정한다.</p><p>Patch relevance <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span></span></span></span> 을 얻은 후, patch relevance <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span></span></span></span> 에 비례하는 확률로 image patch subset 을 sampling 한다. Sampling 된 image patch 로부터 caption 을 생성할 때는 text decoder 에 짧은 prompt (“a picture of,”) 를 함께 제공하고, top-k sampling 을 통해 caption 을 생성한다. 이를 각 image 에 대해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span></span> 번 반복하여 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span></span> 개의 다양한 caption 을 생성한 뒤, 다른 caption 의 substring 인 경우는 제외한다.</p><p>그러나 top-k sampling 의 비결정적 특성 때문에 caption model 이 noisy caption 을 생성할 수 있으며, 이는 성능에 부정적 영향을 미칠 수 있다. 이를 방지하기 위해, 저자는 ITE 를 사용하여 생성된 caption 과 question-relevant image patch 간의 유사도를 계산하고, similarity score 가 0.5 미만인 caption 은 제거한다. 전체 과정은 question-relevant 하면서도 다양하고 noise 가 적은 synthetic caption 을 산출하며, visual 정보와 language 정보 사이의 연결고리를 제공한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-prompt-design">3.4. Prompt Design<a href="#34-prompt-design" class="hash-link" aria-label="Direct link to 3.4. Prompt Design" title="Direct link to 3.4. Prompt Design">​</a></h2><p>저자는 synthetic question-relevant caption 과 QA pair 를 결합하여 LLM prompt 를 구성한다. Prompt 는 instruction, caption, QA exemplar 로 이어붙인다. Instruction 은</p><ul><li><strong>Instruction text</strong>: “Please reason the answers of question according to the contexts.”</li><li><strong>Caption prompt</strong>: “Contexts: <code>[all captions]</code>”</li><li><strong>QA exemplar</strong>: “Question: <code>[question]</code> Answer: <code>[answer]</code>”</li></ul><p>형태로 작성된다. 마지막에 현재 question 을 배치하며,</p><ul><li><strong>Current question</strong>: “Question: <code>[question]</code>. Answer:”</li></ul><p>형태로 마무리한다. 최종 답변은 LLM 에 greedy decoding 을 수행해 얻으며, Flamingo 와 같이 의미 없는 token 은 제거한다.</p><p>또한, LLM 의 입력에는 최대 길이 제한(e.g., OPT, GPT-3 의 경우 2048 token) 이 있기 때문에, question-relevant caption 과 QA pair 의 subset 을 선택하여 prompt 를 구성해야 한다. 이를 위해, 먼저 100 개의 caption 을 생성하고 synthetic answer candidate 의 등장 빈도를 집계한다. 이어서 등장 빈도가 가장 높은 30 개 answer candidate 를 선택하고 각각에 대해 질문을 생성한다. 또한, 등장 빈도가 가장 낮은 30 개 answer candidate 와 각 answer 를 포함하는 caption 도 함께 포함시킨다. Caption selection 전략에 대한 분석은 §4.5 에 제시된다.</p><h1>4. Experiment</h1><p>본 섹션에서는 먼저 Img2LLM 의 효과를 검증하기 위해 다른 zero-shot 및 few-shot VQA 방법과 비교한다. 이어서 prompt pattern 과 caption selection 전략 등 중요한 설계 선택에 대한 ablation study 를 수행하여 그 영향을 분석한다. 또한 qualitative example 을 제시하고, 관찰된 실패 사례에 대한 논의를 포함한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-environment-setup">4.1. Environment Setup<a href="#41-environment-setup" class="hash-link" aria-label="Direct link to 4.1. Environment Setup" title="Direct link to 4.1. Environment Setup">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="datasets">Datasets<a href="#datasets" class="hash-link" aria-label="Direct link to Datasets" title="Direct link to Datasets">​</a></h4><p>저자는 Img2LLM 을 VQAv2, OK-VQA, A-OKVQA dataset 에서 검증한다. </p><p>이들은 perception, reasoning, commonsense 를 필요로 하는 질문들을 포함한다. VQAv2 는 validation set 에 214,354 개, test-dev set 에 107,394 개의 질문을 포함한다. OK-VQA 는 5,046 개의 test question, A-OKVQA 는 1,100 개의 validation question 과 6,700 개의 test question 을 포함한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="implementation-details">Implementation details<a href="#implementation-details" class="hash-link" aria-label="Direct link to Implementation details" title="Direct link to Implementation details">​</a></h4><p>Question-relevant caption prompt 를 얻기 위해 BLIP 을 사용하여 caption 을 생성하고 image-question matching 을 수행한다. 이어 BLIP image-grounded text encoder 의 cross-attention layer 로부터 GradCAM 을 생성하여 question-relevant region 을 localization 한다. GradCAM 기반으로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mtext>’</mtext><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">K’=20</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="mord">’</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">20</span></span></span></span></span> 개의 image patch 를 sampling 하고, 이를 사용해 100 개의 question-relevant caption 을 생성한다. LLM 으로는 open-source OPT model 의 여러 사이즈를 주로 사용한다. Ablation study 에서는 다양한 다른 LLM 도 실험하여 저자의 방법의 generalization 능력을 보인다. 답변 생성은 LLM 의 autoregressive decoding 으로 수행하며, answer list 나 training sample 은 사용하지 않으므로 완전한 zero-shot VQA 를 이룬다. Official evaluation protocol 을 따라 각 dataset 의 VQA score 를 보고한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="competing-methods">Competing methods<a href="#competing-methods" class="hash-link" aria-label="Direct link to Competing methods" title="Direct link to Competing methods">​</a></h4><p>비교 대상은 세 가지 범주로 나눌 수 있다.</p><ol><li><strong>Zero-shot + frozen LLM</strong>: PICa. 본 연구도 여기에 속하나, PICa 와 달리 Img2LLM 은 training sample 없이 prompt 를 구성한다.</li><li><strong>Zero-shot + multi-modal pretraining</strong>: Flamingo, Frozen, VL-T5, FewVLM, VLKD 등이 있으며, 이들은 large-scale vision-language dataset 이 필요하고 update 비용이 크다. 또한 VQ2A, WeaQA 도 포함되지만, 이들은 answer candidate 접근을 가정하므로 실제 적용성에 제약이 있다.</li><li><strong>Few-shot methods</strong>: 비교를 위해 PICa, FewVLM, ClipCap 의 few-shot 결과도 포함한다.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-main-results">4.2. Main Results<a href="#42-main-results" class="hash-link" aria-label="Direct link to 4.2. Main Results" title="Direct link to 4.2. Main Results">​</a></h2><p><img loading="lazy" alt="Table 2" src="/assets/images/image-60-b023f3433ccf2523e17bf2caa09d2fc5.png" width="922" height="768" class="img_ev3q"></p><p>주요 정량적 결과는 Tab. 2 에 제시된다. 요약은 다음과 같다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="state-of-the-art-results-on-zero-shot-evaluation-with-plug-in-frozen-llms">State-of-the-art results on zero-shot evaluation with plug-in frozen LLMs.<a href="#state-of-the-art-results-on-zero-shot-evaluation-with-plug-in-frozen-llms" class="hash-link" aria-label="Direct link to State-of-the-art results on zero-shot evaluation with plug-in frozen LLMs." title="Direct link to State-of-the-art results on zero-shot evaluation with plug-in frozen LLMs.">​</a></h4><ul><li>Img2LLM 은 PICa 대비 큰 폭으로 향상된 성능을 보인다 (e.g., OK-VQA 에서 45.6 vs 17.7). </li><li>PICa 는 frozen LLM 을 사용했지만 prompt 를 구성하기 위해 training sample 이 필요하다. </li><li>반면 저자의 방법은 VQA sample 접근 없이 QA pair 를 생성하므로, 완전한 zero-shot 조건을 충족한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-effect-of-llms-and-their-emergent-capabilities-on-vqa">Scaling effect of LLMs and their emergent capabilities on VQA.<a href="#scaling-effect-of-llms-and-their-emergent-capabilities-on-vqa" class="hash-link" aria-label="Direct link to Scaling effect of LLMs and their emergent capabilities on VQA." title="Direct link to Scaling effect of LLMs and their emergent capabilities on VQA.">​</a></h4><ul><li>LLM parameter 규모를 6.7B 에서 175B 로 확장하면 dataset 전반에서 3~10 점 성능 향상이 나타난다. <ul><li>이는 language modeling 능력이 강할수록 질문 이해가 향상되고 정확한 답변이 가능함을 보여준다. </li></ul></li><li>특히 commonsense reasoning 과 external knowledge 가 필요한 OK-VQA 와 A-OKVQA 에서 이러한 추세가 더욱 명확하다. </li><li>또한 LLM scaling 효과는 model 이 충분히 커졌을 때(e.g., 30B 이상) 명확히 드러나며, 소규모 모델(6.7B, 13B)에서는 일관적이지 않다. <ul><li>이는 LLM 의 emergent ability 가 language task 에서 보고된 최근 결과와 일치하며, frozen LLM 을 vision(-language) task 에 적용했을 때도 동일한 현상이 처음 확인된 것이다.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="competitive-performance-with-end-to-end-pretraining-and-few-shot-models">Competitive performance with end-to-end pretraining and few-shot models.<a href="#competitive-performance-with-end-to-end-pretraining-and-few-shot-models" class="hash-link" aria-label="Direct link to Competitive performance with end-to-end pretraining and few-shot models." title="Direct link to Competitive performance with end-to-end pretraining and few-shot models.">​</a></h4><ul><li>Img2LLM 은 대부분의 end-to-end pretrained model 및 few-shot 방법보다 우수하다. <ul><li>예를 들어, VQAv2 에서 저자의 방법은 Flamingo-80B (500K TPU-hour 와 수십억 scale dataset 을 사용하여 학습됨) 보다 5.6 점 더 높다. </li></ul></li><li>A-OKVQA 에서는 기존 best 모델인 ClipCap 대비 성능을 2 배 이상 개선하였다. </li><li>예외적으로 OK-VQA 에서는 Flamingo-9B 보다는 낫지만 Flamingo-80B 와는 비슷한 수준에 도달하지 못했다. 그러나 Img2LLM 은 추가 training cost 없이 강화된 LLM 으로 쉽게 교체 가능하므로, practical VQA system 적용에 더 현실적인 해법이라 할 수 있다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-experimental-results-of-different-llms">4.3. Experimental Results of Different LLMs<a href="#43-experimental-results-of-different-llms" class="hash-link" aria-label="Direct link to 4.3. Experimental Results of Different LLMs" title="Direct link to 4.3. Experimental Results of Different LLMs">​</a></h2><p><img loading="lazy" alt="Table 3" src="/assets/images/image-61-afebebfc78b780d212a2ecb3b8cac0c3.png" width="800" height="434" class="img_ev3q"></p><ul><li>Tab. 3 은 OPT 이외의 open-source LLM, 즉 GPT-J, GPT-Neo, BLOOM 에 Img2LLM 을 적용한 성능을 보여준다. </li><li>결과에 따르면 Img2LLM 은 다양한 LLM 이 zero-shot VQA task 를 수행할 수 있도록 하며, 모든 경우에서 zero-shot PICa 와 Frozen 보다 우수한 성능을 달성한다. </li><li>이는 제안된 방법이 서로 다른 LLM 에서도 강력한 generalization 능력을 가진다는 강력한 증거이다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="44-analysis-on-question-generation-methods">4.4. Analysis on Question Generation Methods<a href="#44-analysis-on-question-generation-methods" class="hash-link" aria-label="Direct link to 4.4. Analysis on Question Generation Methods" title="Direct link to 4.4. Analysis on Question Generation Methods">​</a></h2><p>Tab. 4 는 Sec. 3.2 에서 설명한 question generation 전략의 성능을 비교한다. </p><p>세 가지 question generation 기법을 평가했는데, </p><ul><li><strong>Image-agnostic</strong>: 다른 image 에서 샘플링된 질문을 사용하는 방식, </li><li><strong>Template-based</strong>: POS 에 따라 설계된 template question 을 사용하는 방식, </li><li><strong>Neural-based</strong>: neural model 로 생성된 질문을 사용하는 방식이다. 또한 두 가지 synthetic QA selection 전략을 비교한다. </li><li><strong>Random</strong>: QA pair 를 무작위로 선택하는 방식.</li><li><strong>Max Freq.</strong>: caption 에 가장 자주 등장하는 answer candidate 를 선택하고, 해당 synthetic question 을 함께 가져오는 방식이다.</li></ul><p><img loading="lazy" alt="Table 4" src="/assets/images/image-62-ac6a3385889e4ea20242e7f48d716e3e.png" width="708" height="381" class="img_ev3q"></p><ul><li>세 기법 중 <strong>Agnostic</strong> 이 가장 성능이 낮고, <strong>Neural</strong> 이 가장 높았다. <ul><li>그 이유는 QA pair 의 품질 차이에 있다. </li></ul></li><li>Agnostic QA pair 는 현재 image 와 무관한 정보를 포함해 LLM 을 혼란시킬 수 있다. </li><li>Template question 은 언어적 다양성이 부족해 서로 다른 QA 전략을 시연하지 못한다. </li><li>반면 Neural 은 가장 관련성 있는 정보를 포함하고 언어적 다양성도 가장 크다. </li><li>또한 Max Freq. 기반 QA pair 는 Random 보다 더 나은 성능을 보였다. 이는 caption 에서 가장 빈번하게 등장하는 answer 가 image 의 핵심적이고 중요한 요소를 묘사하기 때문이라고 가정할 수 있다.</li></ul><p>저자는 exemplar prompt 의 visual information 품질을 평가하기 위해 <strong>Answer Hit Rate (AHR)</strong> 와 <strong>Answer Noise Rate (ANR)</strong> 를 도입했다.</p><ul><li>AHR 은 QA pair 가 ground-truth answer 를 포함하는 비율이다.</li><li>ANR 은 exemplar prompt 전체 token 수 대비 ground-truth answer 의 비율이다.</li></ul><p><img loading="lazy" alt="Table 7" src="/assets/images/image-65-e098a518e7016e2dae31f6ab31b02afd.png" width="1444" height="335" class="img_ev3q"></p><ul><li>Tab. 7 은 question-relevant caption 기반 exemplar prompt 가 더 높은 AHR 을 가지며, 이로 인해 VQA 성능이 향상됨을 보여준다. </li><li>또한 caption filtering 과정이 noisy caption 을 제거하여 더 높은 ANR 을 달성하게 한다. </li><li>이는 AHR 과 ANR 모두를 개선하는 것이 prompt 품질과 VQA 성능을 향상시키는 데 중요함을 보여준다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="45-ablation-on-caption-selection">4.5. Ablation on Caption Selection<a href="#45-ablation-on-caption-selection" class="hash-link" aria-label="Direct link to 4.5. Ablation on Caption Selection" title="Direct link to 4.5. Ablation on Caption Selection">​</a></h2><p><img loading="lazy" alt="Table 6" src="/assets/images/image-64-2ca1e323e0a8b4a3fb69b507ef8a3797.png" width="749" height="217" class="img_ev3q"></p><p>Tab. 6 은 caption selection 전략의 성능을 비교한다. </p><ul><li><strong>Max Frequency</strong> 전략은 가장 자주 등장하는 30 개 answer 를 포함하는 caption 을 선택하고, <strong>Min Frequency</strong> 전략은 가장 드물게 등장하는 answer 를 포함하는 caption 을 선택한다. </li><li>Exemplar prompt 자체가 이미 가장 빈번한 answer 를 사용하므로 Max Frequency 전략은 추가 정보를 제공하지 못한다. </li><li>반대로 Min Frequency 전략은 QA pair 에 포함되지 않은 정보를 제공할 수 있어 성능 향상을 가져온다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="46-ablation-study-on-prompt-design">4.6. Ablation Study on Prompt Design<a href="#46-ablation-study-on-prompt-design" class="hash-link" aria-label="Direct link to 4.6. Ablation Study on Prompt Design" title="Direct link to 4.6. Ablation Study on Prompt Design">​</a></h2><p>LLM prompt 를 구성하는 데 두 가지 선택지가 있다.</p><ol><li>caption 과 해당 caption 으로부터 생성된 synthetic QA pair 를 연속으로 붙이는 방식이다. 이는 <code>CQA-CQA-CQA</code> 로 표현할 수 있으며, 여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span> 는 각각 caption, synthetic question, synthetic answer 를 의미한다.</li><li>모든 caption 을 먼저 제시한 뒤, 이어서 모든 QA pair 를 제시하는 방식으로, <code>CCC-QAQAQA</code> 로 나타낼 수 있다.</li></ol><p><img loading="lazy" alt="Table 5" src="/assets/images/image-63-12fd345f6cef145ffac154750ff79812.png" width="641" height="207" class="img_ev3q"></p><ul><li>실험적으로(Tab. 5), 두 번째 설계가 첫 번째보다 훨씬 좋은 성능을 보였다. </li><li>저자는 첫 번째 설계가 LLM 이 single caption 만 읽고 답변하도록 유도할 수 있다고 가정한다. </li><li>Prompt 상에서 이 caption 하나에 이미 질문에 필요한 모든 정보가 들어 있기 때문이다. </li><li>실제 메커니즘을 정확히 특정하기는 어렵지만, 결과는 QA prompt 와 그 위치가 매우 중요함을 보여준다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="47-examples-and-failure-case-analysis">4.7. Examples and Failure Case Analysis<a href="#47-examples-and-failure-case-analysis" class="hash-link" aria-label="Direct link to 4.7. Examples and Failure Case Analysis" title="Direct link to 4.7. Examples and Failure Case Analysis">​</a></h2><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-66-fad181d2f5af6bcbc44dbff6fa2c99f2.png" width="1203" height="816" class="img_ev3q"></p><p>Fig. 3 에서는 caption, exemplar prompt, 그리고 예측 결과의 예시 네 가지를 성공 및 실패 사례와 함께 보여준다. </p><ul><li>Fig. 3(a) 에서는 caption 과 synthetic QA pair 가 &quot;a man is making drinks at a bar&quot; 는 정보를 제공한다. </li><li>LLM 은 background knowledge 를 활용하여 그의 직업이 bartender 임을 정확히 추론한다. </li><li>반면 Fig. 3(c) 에서는 예측이 이해 가능하지만(비록 문법적으로 완벽하지 않더라도), LLM 이 정성적 물리학적 추론을 하지 못하여 정답을 맞히지 못한다. </li><li>이러한 결과는 open-ended VQA 에 적절한 commonsense knowledge 를 적용하는 것이 중요함을 보여준다.</li></ul><h1>5. Limitation</h1><p>제안된 접근법의 한계 중 하나는 image caption 과 QA pair 를 생성하는 과정이 추가적인 inference overhead 를 발생시킨다는 점이다. 8×A100 머신에서, 저자의 구현은 175B OPT inference time 대비 약 24.4% 의 추가 연산 시간을 필요로 한다. Prompt 를 더 짧게 하여 속도를 높이는 대신 정확도를 일부 희생함으로써 overhead 를 줄일 수 있다. 그러나 본 방법은 Flamingo 의 경우처럼 500K TPU hour 이상이 소요되는 고비용 end-to-end multimodal representation alignment 를 피한다는 점에서 여전히 효율적이다.</p><h1>6. Conclusion</h1><p>본 논문에서는 zero-shot VQA task 를 위해 off-the-shelf large language models (LLMs) 의 knowledge 와 reasoning 능력을 활용할 수 있는 plug-and-play module 인 <strong>Img2LLM</strong> 을 제안하였다. Img2LLM 은 visual information 과 task guidance 를 LLM 이 쉽게 처리할 수 있는 prompt 형식으로 제공한다. 이를 통해 고비용의 end-to-end vision-language alignment 필요성을 제거하며, model deployment 의 유연성을 높이고 비용을 낮춘다.</p><p>실험 결과, Img2LLM 은 서로 다른 LLM 이 zero-shot VQA 에서 성능을 발휘하도록 하며, 비싼 end-to-end training 이 필요한 기존 방법과 비교해 동등하거나 더 나은 성능을 달성함을 보여준다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">Vision-Language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multimodal">Multimodal</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vqa">VQA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/img-2-llm">Img2LLM</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/VQA-IC/Few-shot/2022-12-Img2LLM.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Linearly Mapping from Image to Text Space</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-recent-advances-in-vqa-methods" class="table-of-contents__link toc-highlight">2.1. Recent Advances in VQA Methods</a></li><li><a href="#22-llm-for-zerofew-shot-vqa-tasks" class="table-of-contents__link toc-highlight">2.2. LLM for Zero/Few-Shot VQA Tasks</a></li><li><a href="#31-answer-extraction" class="table-of-contents__link toc-highlight">3.1. Answer Extraction</a></li><li><a href="#32-question-generation" class="table-of-contents__link toc-highlight">3.2. Question Generation</a></li><li><a href="#33-question-relevant-caption-prompt" class="table-of-contents__link toc-highlight">3.3. Question-relevant Caption Prompt</a></li><li><a href="#34-prompt-design" class="table-of-contents__link toc-highlight">3.4. Prompt Design</a></li><li><a href="#41-environment-setup" class="table-of-contents__link toc-highlight">4.1. Environment Setup</a></li><li><a href="#42-main-results" class="table-of-contents__link toc-highlight">4.2. Main Results</a></li><li><a href="#43-experimental-results-of-different-llms" class="table-of-contents__link toc-highlight">4.3. Experimental Results of Different LLMs</a></li><li><a href="#44-analysis-on-question-generation-methods" class="table-of-contents__link toc-highlight">4.4. Analysis on Question Generation Methods</a></li><li><a href="#45-ablation-on-caption-selection" class="table-of-contents__link toc-highlight">4.5. Ablation on Caption Selection</a></li><li><a href="#46-ablation-study-on-prompt-design" class="table-of-contents__link toc-highlight">4.6. Ablation Study on Prompt Design</a></li><li><a href="#47-examples-and-failure-case-analysis" class="table-of-contents__link toc-highlight">4.7. Examples and Failure Case Analysis</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.618c4439.js"></script>
<script src="/assets/js/main.079b9612.js"></script>
</body>
</html>