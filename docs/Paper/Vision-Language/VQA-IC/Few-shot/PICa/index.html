<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/VQA-IC/Few-shot/2021-09-PICa">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미주 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미주 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.74164ba1.js" as="script">
<link rel="preload" href="/assets/js/main.82abd802.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">CLIP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Foundation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">VQA-IC</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Few-shot</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Multimodal Few-Shot Learning with Frozen Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA">xGQA: Cross-Lingual Visual Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP">CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA">Linearly Mapping from Image to Text Space</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper">Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA">Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/CoTMT">A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MAPD">Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering</a></li></ul></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">VQA-IC</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Few-shot</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</h1></header><p>논문 및 이미주 출처 : <a href="https://arxiv.org/pdf/2109.05014" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2109.05014</a></p><h1>Abstract</h1><p>Knowledge-based visual question answering (VQA) 는 이미지에 존재하지 않는 external knowledge 가 필요한 질문에 답변하는 것을 포함한다. 기존 방법들은 먼저 외부 리소스에서 knowledge 를 검색한 후, 선택된 knowledge 와 input image, question 을 함께 reasoning 하여 answer 를 예측한다. 그러나 이러한 two-step approach 는 VQA 성능을 제한할 수 있는 mismatch 로 이어질 수 있다. 예를 들어, retrieved knowledge 가 question 과 무관하거나 noisy 할 수 있으며, reasoning 과정에서 re-embedding 된 knowledge feature 가 knowledge base (KB) 에서의 원래 의미와 달라질 수 있다.</p><p>이 문제를 해결하기 위해, 저자는 knowledge-based VQA 를 위해 Image Caption 을 활용해 GPT-3 를 prompting 하는 간단하면서도 효과적인 방법인 <strong>PICa</strong> 를 제안한다. </p><ul><li>GPT-3 의 knowledge retrieval 및 question answering 능력에서 영감을 받아, 이전 연구에서 사용한 structured KB 대신 GPT-3 를 관련 knowledge 를 획득하고 처리할 수 있는 implicit 하고 unstructured 한 KB 로 간주한다. </li><li>구체적으로, 먼저 이미지를 GPT-3 가 이해할 수 있는 caption (또는 tag) 으로 변환한 후, in-context VQA 예시 몇 개만 제공하여 GPT-3 를 few-shot 방식으로 VQA task 에 적응시킨다.</li></ul><p>또한, 저자는 성능을 향상시키기 위해 다음 두 가지를 면밀히 조사하였다.</p><ol><li>image content 를 가장 잘 설명하는 text format 은 무엇인지</li><li>in-context example 을 더 효과적으로 선택하고 사용하는 방법은 무엇인지</li></ol><p>PICa 는 GPT-3 를 multimodal task 에 최초로 사용하도록 한다. 단 16 examples 만 사용하여, PICa 는 OK-VQA dataset 에서 supervised SOTA 대비 절대 +8.6 points 를 초과하는 성능을 달성한다. 또한 VQAv2 dataset 에서도 PICa 는 우수한 few-shot 성능을 보인다.</p><h1>Introduction</h1><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-37-b5d003e52a69d4ffa2a20b987aad0fda.png" width="673" height="746" class="img_ev3q"></p><p>Knowledge-based visual question answering (VQA) 는 이미지 내용 external knowledge 가 필요한 질문을 제시함으로써 standard VQA task 를 확장한 것이다. 이러한 knowledge 를 획득하기 위해 기존 방법들은 Wikipedia article, ConceptNet concept 등의 다양한 리소스에서 external knowledge 를 먼저 검색한다. 이를 기반으로, retrieved knowledge 와 image-question pair 를 결합하여 reasoning 을 수행하고 answer 를 예측한다(Fig. 1(a)).</p><p>그러나 이러한 two-step approach 는 최적이 아닐 수 있다. </p><ul><li>예를 들어, 첫 번째 단계에서 knowledge retrieval 에 사용된 image-question feature 가 두 번째 reasoning 단계에서의 representation 과 일치하지 않아 noisy 하거나 무관한 knowledge 가 검색될 수 있다. </li><li>또한, retrieved knowledge 의 re-embedded textual feature 가 원래 knowledge source 에서의 의미와 달라질 수 있다. </li></ul><p>이러한 mismatch 는 VQA 성능을 제한할 가능성이 있다. 게다가, 효과적인 joint knowledge-image-question representation 을 학습하기 위해서는 충분한 training data 가 필요하므로 새로운 유형의 질문으로의 전이는 어렵다.</p><p>이 연구에서는 최근 language model 의 흥미로운 특성에서 영감을 얻어 대안을 탐구한다. 특히, GPT-3 와 같은 large-scale language model 은 knowledge retrieval 과 question answering 과 같은 NLP task 에서 강력한 능력을 보여주었으며, few in-context example 만으로도 new task 에 빠르게 적응할 수 있는 strong few-shot learner 이다.</p><p>이에 영감을 받아, 저자는 GPT-3 의 도움으로 위의 knowledge retrieval 과 reasoning 단계를 통합하는 간단하면서도 효과적인 방법인 <strong>PICa</strong> 를 제안한다. </p><ul><li>이전 연구처럼 <em>explicit</em> 하고 <em>structured</em> knowledge bases (KBs) 대신, PICa 는 prompt engineering 을 통해 GPT-3 를 <em>implicit</em> 하고 <em>unstructured</em> KB 로 취급한다. </li><li>구체적으로, 이미지를 GPT-3 가 이해할 수 있는 textual description (i.e., caption 또는 tag) 으로 변환한 뒤, question 과 textual description 을 기반으로 GPT-3 에 질의하여 직접 answer 를 예측한다.</li></ul><p>PICa 는 supervised fine-tuning 없이, GPT-3 의 few-shot learning 능력을 그대로 계승하며 inference 시에 few in-context example 만으로 VQA task 에 적응한다. 실험적으로, GPT-3 는 관련 knowledge 를 암묵적으로 검색하고, question 과 context 를 효과적으로 reasoning 하여 answer prediction 을 수행할 수 있음을 보인다. 성능 향상을 위해 저자는 (i) image context 를 textual description 으로 효과적으로 표현하는 방법, (ii) in-context example 의 선택과 multi-query ensemble 을 통해 GPT-3 의 성능을 극대화하는 방법을 면밀히 조사하였다.</p><p>OK-VQA dataset 에서 pre-trained captioning model 인 VinVL 을 사용한 PICa 는 few-shot 방식에서 46.9% 의 accuracy 를 달성하며, supervised SOTA 대비 절대 7.5 points 향상된 성능을 보였다. 예측된 image tag 를 추가하면 성능은 48.0 으로 더 향상된다. 또한, PICa 의 효과성을 이해하기 위해 상세한 ablation study 와 qualitative analysis 를 제공한다.</p><p>저자의 주요 기여는 다음과 같다.</p><ol><li>GPT-3 를 knowledge-based VQA 에 활용하는 간단하면서도 효과적인 방법 PICa 를 제안하며, GPT-3 를 multimodal task 에 최초로 적용하였다.</li><li>PICa 는 image 를 textual description 으로 표현하고, in-context example selection 및 multi-query ensemble 을 통해 GPT-3 의 성능을 향상시켰다.</li><li>PICa 는 few-shot 방식으로 OK-VQA 에서 48.0% accuracy 를 달성하며, 기존 SOTA 인 39.4% 를 크게 초과하였고, VQAv2 에서도 우수한 few-shot 성능을 보였다.</li></ol><h1>Related Work</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="knowledge-based-vqa">Knowledge-based VQA.<a href="#knowledge-based-vqa" class="hash-link" aria-label="Direct link to Knowledge-based VQA." title="Direct link to Knowledge-based VQA.">​</a></h4><p>Knowledge-based VQA 는 질문에 답하기 위해 이미지 내용뿐 아니라 external knowledge 가 필요하다. 초기 연구로는 KB-VQA 와 F-VQA 가 있으며, 최근의 OK-VQA dataset 은 COCO image 에 기반하며, input question 은 광범위한 knowledge category 를 포함한다. 기존 연구들은 knowledge 를 검색하고 사용하는 다양한 방법을 제안하였으며, question 의 관련 knowledge 를 포괄하기 위해 Wikipedia, ConceptNet, Google image, 그리고 language model 의 implicit knowledge 등 여러 knowledge resource 를 활용해야 한다고 보았다.</p><p>external knowledge retrieval 이후, 연구들은 획득한 knowledge 와 image-question pair 를 결합하여 reasoning 을 수행해 answer prediction 을 하는 데 집중했으며, 이때 graph convolution network 가 multimodal fusion 에 효과적인 방법임이 제시되었다. 최근 KRISP 는 structured KB 에 추가적인 knowledge resource 로서 pre-trained language model 에 저장된 implicit knowledge 를 검색하는 방법을 제안하였다. MAVEx 는 noisy 한 검색 knowledge 를 더 잘 활용하기 위한 answer validation 접근법을 제시하였다. 그러나 이러한 two-step approach 는 retrieval 단계에서 가장 관련성 높은 knowledge 를 얻지 못하거나, reasoning 단계에서 QA 에 적합하게 knowledge 를 encoding 하지 못할 수 있다. 본 연구에서는 두 단계를 결합하여, GPT-3 를 prompting 함으로써 VQA 를 위해 knowledge 를 공동으로 획득하고 처리하는 model 을 제시한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="multimodal-few-shot-learning">Multimodal few-shot learning.<a href="#multimodal-few-shot-learning" class="hash-link" aria-label="Direct link to Multimodal few-shot learning." title="Direct link to Multimodal few-shot learning.">​</a></h4><p>GPT-3 는 놀라운 in-context few-shot learning 능력을 보여주었다. 최근 Frozen 은 pre-trained language model 을 재사용하여 vision-and-language task 로 이러한 few-shot 능력을 확장하는 방법을 제안하였다. Frozen 은 대규모 text corpus 로 pre-train 된 GPT 계열 language model (약 7B parameter) 로 시작한다. 이후 language model 을 freeze 하고, input image 를 language model 이 이해할 수 있는 visual feature 로 projection 하는 visual encoder 를 학습한다. visual encoder 는 image captioning task 로 학습되며, gradient 는 frozen language model 로부터 back-propagation 된다. Frozen 은 최초의 multimodal few-shot learner 로서, VQA 와 같은 task 에서 random guess 보다 훨씬 나은 성능을 보인다. 그러나 성능은 SOTA 와 비교해 여전히 만족스럽지 않으며, 예를 들어 OK-VQA dataset 에서 accuracy 가 12.6% 에 불과하다.</p><p>저자의 pre-trained language model 활용 아이디어는 Frozen 과 밀접하게 관련되어 있다. 그러나 본 연구에서는 이를 한층 발전시켜 stronger language model 을 사용하고, knowledge-based VQA task 에 초점을 맞춘다. 이를 통해 supervised SOTA 를 초월하는 최초의 few-shot 접근법을 제시한다.</p><h1>Approach</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="gpt-3-for-in-context-learning">GPT-3 for In-context Learning<a href="#gpt-3-for-in-context-learning" class="hash-link" aria-label="Direct link to GPT-3 for In-context Learning" title="Direct link to GPT-3 for In-context Learning">​</a></h2><p>GPT-3 는 강력한 in-context few-shot learning 능력을 보여주었다. pre-trained model 을 downstream task 에 맞게 fine-tuning 하는 대신, in-context few-shot learner 는 inference 시 few example 만으로 new task 에 빠르게 적응하며 parameter update 가 필요하지 않다. 구체적으로, inference 동안 new task 의 target <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 는 주어진 context <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span></span> 와 new task 의 input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 를 조건으로 한 text sequence generation task 로 직접 예측된다. 모든 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">C</mi></mrow><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.05834em">C</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 는 text sequence 이며, 예를 들어 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = (y_1, \dots, y_T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 일 때, 각 decoding step <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span> 에서</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><msub><mi>y</mi><mi>t</mi></msub></munder><msub><mi>p</mi><mtext>LM</mtext></msub><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><mi mathvariant="script">C</mi><mo separator="true">,</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y_t = \arg\max_{y_t} p_{\text{LM}}(y_t | \mathcal{C}, x, y_{&lt;t})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.5861em;vertical-align:-0.8361em"></span><span class="mop">ar<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.4em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8361em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">LM</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathcal" style="margin-right:0.05834em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div><ul><li>여기서 LM 은 pre-trained language model 의 weight 를 나타내며, new task 에 대해서는 frozen 된다. </li><li>context <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">C</mi><mo>=</mo><mo stretchy="false">{</mo><mi>h</mi><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{C} = \{ h, x_1, y_1, \dots, x_n, y_n \}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.05834em">C</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord mathnormal">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span> 는 optional prompt head <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">h</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> 개의 in-context example (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">\{x_i, y_i\}_{i=1}^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0087em;vertical-align:-0.2587em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span></span></span></span></span>) 으로 구성된다. </li><li>GPT-3 의 strong few-shot 능력과 다양한 knowledge 를 활용하여, 저자는 GPT-3 를 이용한 few-shot knowledge-based VQA 방법을 제안한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="gpt-3-for-vqa">GPT-3 for VQA<a href="#gpt-3-for-vqa" class="hash-link" aria-label="Direct link to GPT-3 for VQA" title="Direct link to GPT-3 for VQA">​</a></h2><p>GPT-3 를 VQA 에 사용하는 데 있어 한 가지 도전 과제는 GPT-3 가 이미지 입력을 본질적으로 이해하지 못한다는 점이다. 실험적으로, 이미지 context 를 textual description 으로 변환하는 것이 VQA 에서 강력한 baseline 이 됨을 보인다.</p><p>PICa 는 inference 시 GPT-3 에 구성된 input prompt 를 제공하여 VQA task 를 수행한다. prompt 는 context <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span></span></span></span></span> (prompt head <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">h</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> 개의 in-context example 포함) 와 VQA input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 로 구성된 word sequence 이다. 먼저, 최신 captioning (또는 tagging) model 을 사용해 VQA image 를 caption (또는 tag list) 으로 변환한다.</p><p>예를 들어, VQA input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 는 변환된 image context string (“<code>Context: People are standing in a parking lot with some umbrellas as it snows.</code>”) 과 question string (“<code>Q: What is the warmest temperature at which this weather can happen? A:</code>”) 을 연결한 것이다. target <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 는 open-ended text generation 방식으로 생성된 answer (“<code>32 degrees</code>”) 이다. answer 는 GPT-3 의 전체 vocabulary 에서 임의 길이의 word 를 포함할 수 있다.</p><p>context <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">C</mi></mrow><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.05834em">C</span></span></span></span></span> 는 모든 sample 에 대해 고정된 string (“<code>Please answer the question according to the above context.</code>”) 인 prompt head <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">h</span></span></span></span></span> 로 시작한다. 나머지 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">C</mi></mrow><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.05834em">C</span></span></span></span></span> 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> 개의 in-context example string <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_i, y_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 의 연결로 구성된다. 이렇게 구성된 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">C</mi></mrow><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.05834em">C</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 를 연결해 prompt 를 만들고, GPT-3 는 이를 입력으로 받아 language model 내에서 암묵적으로 knowledge 를 retrieval 및 reasoning 하여 answer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 를 예측한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="in-context-examples">In-context Examples<a href="#in-context-examples" class="hash-link" aria-label="Direct link to In-context Examples" title="Direct link to In-context Examples">​</a></h2><p>일반적으로 in-context example 수가 많을수록 few-shot 성능이 향상된다. 그러나 new task 에서 사용 가능한 example 수와 model 의 최대 입력 길이 제한이 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> 의 최대값을 결정한다. 실제로는, model 의 최대 입력 길이가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> 을 제한하는 경우가 많다(예: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">n=16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">16</span></span></span></span></span>). 이를 더 잘 활용하기 위해 저자는 다음 두 가지 방법을 탐구하였다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="in-context-example-selection">In-context example selection<a href="#in-context-example-selection" class="hash-link" aria-label="Direct link to In-context example selection" title="Direct link to In-context example selection">​</a></h4><p>In-context example selection 은 각 inference-time input <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 에 대해 가능한 모든 example 중에서 optimal example 을 선택한다. 특정 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 가 좋은 example 이려면, 해당 question feature 가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 와 유사해야 한다. 이를 위해 CLIP model (ViT-B/16 variant) 의 text encoder 로 inference-time question 의 textual feature 를 추출하고, 모든 in-context example question 과의 cosine similarity 를 계산한다. </p><p>question text similarity 와 image visual similarity 의 평균을 사용하여 example selection 을 진행하고, highest similarities <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> 개 question 의 example 을 선택한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="multi-query-ensemble">Multi-query ensemble<a href="#multi-query-ensemble" class="hash-link" aria-label="Direct link to Multi-query ensemble" title="Direct link to Multi-query ensemble">​</a></h4><p>Multi-query ensemble 은 더 많은 example 을 활용하기 위한 대안이다. inference-time example <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 에 대해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 개의 in-context example 을 사용해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 개의 prompt 를 만든다. GPT-3 에 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 번 질의하여 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 개의 answer prediction 을 얻고, 각 prediction 의 log-probability sum <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>t</mi></msub><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mtext>LM</mtext></msub><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sum_t \log p_{\text{LM}}(y_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1308em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">LM</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 이 가장 높은 answer 를 최종 결과로 선택한다. </p><p>이 방법은 in-context example selection 과 결합할 수 있으며, 상위 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> example 을 선택한 뒤 이를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> prompts 에 분배하여 두 접근법의 장점을 모두 활용한다.</p><h1>Experiments on OK-VQA</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="dataset-and-setup">Dataset and Setup<a href="#dataset-and-setup" class="hash-link" aria-label="Direct link to Dataset and Setup" title="Direct link to Dataset and Setup">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dataset">Dataset.<a href="#dataset" class="hash-link" aria-label="Direct link to Dataset." title="Direct link to Dataset.">​</a></h4><ul><li>OK-VQA 는 현재 가장 큰 knowledge-based VQA dataset 으로, 14,055 image-question pairs 로 구성된다. </li><li>question 은 external knowledge 가 필요하도록 수작업으로 필터링되며, 각 question 은 5 ground-truth answers 를 가진다. </li><li>평가는 VQAv2 의 soft accuracy 를 사용한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup.<a href="#setup" class="hash-link" aria-label="Direct link to Setup." title="Direct link to Setup.">​</a></h4><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-38-af1ab5c67b42c6166df8d29d4d3624a7.png" width="1348" height="569" class="img_ev3q"></p><p>저자는 두 가지 방법 변형을 비교하였다.</p><ul><li><strong>PICa-Base</strong>: Fig. 2 에 제시된 prompt 를 사용한다. image 는 VinVL 로 생성한 caption 으로 표현하거나, Microsoft Azure tagging API 로 예측한 tag 를 caption 에 추가하여 표현한다. in-context example 은 무작위로 선택된다.</li><li><strong>PICa-Full</strong>: in-context example selection 과 multi-query ensemble 을 모두 포함한 전체 model.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparison-with-state-of-the-art">Comparison with State-of-the-art<a href="#comparison-with-state-of-the-art" class="hash-link" aria-label="Direct link to Comparison with State-of-the-art" title="Direct link to Comparison with State-of-the-art">​</a></h2><p><img loading="lazy" alt="Table 1" src="/assets/images/image-39-bc90b2566f5da52cfb1ec47bc6b62b45.png" width="1351" height="416" class="img_ev3q"></p><p>Tab. 1 은 OK-VQA dataset 의 결과를 요약한다. 표 상단은 전체 OK-VQA training set 에 대해 supervised 학습된 model 들이고, 하단은 few-shot 결과이다. </p><ul><li>“Image Repr.” 은 VQA 에서 image 를 표현하는 방식을 나타낸다. </li><li>“Feature Emb.” 는 학습 가능한 network 로 image 를 feature vector 로 encoding 하는 기존 방식을 의미한다. </li><li>GPT-3 의 end-to-end fine-tuning 은 비용이 높으므로, 저자는 image 를 GPT-3 가 이해할 수 있는 text sequence 로 변환한다. </li><li>“Caption” 은 COCO-train14 split 에서 fine-tuning 된 VinVL-base model 이 생성한 caption 을 의미하며, </li><li>“Tags” 는 tagging API 가 예측한 tag 를 의미한다. </li><li>“Knowledge Resource” 는 사용된 external knowledge resource 를 포함한다. </li><li>대부분의 기존 방법은 Wikipedia, ConceptNet 등 external knowledge resource 에서 명시적으로 knowledge 를 검색한다. 반면 few-shot 방법은 pre-trained language model 을 직접 사용하여 knowledge 를 획득 및 처리한다.</li></ul><p>다음은 observation 을 요악한 것.</p><ul><li><strong>Fine-tuning 없이 SOTA 초과</strong>: PICa-Full 은 16 in-context VQA examples 만으로 accuracy 48.0% 를 달성하며, OK-VQA 전체 training set 으로 학습한 supervised SOTA (39.4%) 를 초과한다. 이는 external knowledge 를 명시적으로 검색하는 대신 GPT-3 의 implicit knowledge retrieval 과 reasoning 능력의 강점을 보여준다.</li><li><strong>Example selection 효과</strong>: 무작위 example 을 사용하는 PICa-Base 대비, PICa-Full 은 in-context example 을 더 효과적으로 활용해 성능을 향상시켰다. Caption 만 사용 시 42.0% → 46.9%, Caption+Tag 사용 시 43.3% → 48.0% 로 개선되었다. 세부 ablation study 는 Tab. 5 에 제시된다.</li></ul><p><img loading="lazy" alt="Table 5" src="/assets/images/image-44-4a1c11c496ed32b79bf97c0ca81e1815.png" width="815" height="376" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="few-shot-ability">Few-shot Ability<a href="#few-shot-ability" class="hash-link" aria-label="Direct link to Few-shot Ability" title="Direct link to Few-shot Ability">​</a></h2><p><img loading="lazy" alt="Table 2" src="/assets/images/image-40-d3a1bbdbb896da5cafd9835856c5528a.png" width="1354" height="251" class="img_ev3q"></p><p>Tab. 1 하단을 집중적으로 분석하기 위해, 저자는 OK-VQA dataset 에서 few-shot 성능을 Tab. 2 에 제시하였다. 표 상단은 example pool 에서 무작위로 in-context example 을 선택하는 strict few-shot setting 결과이고, 하단은 in-context example selection 과 multi-query ensemble 을 적용한 결과이다.</p><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> 은 in-context example 개수(shot 수)이며, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">n=16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">16</span></span></span></span></span> 은 GPT-3 의 max input length 2049 에서 수용 가능한 최대 example 수이다. </li><li>prompt 가 최대 길이를 초과할 경우, 더 짧은 example 로 재선택하지만, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">n=16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">16</span></span></span></span></span> 에서는 거의 발생하지 않는다.</li></ul><p>다음은 observation</p><ul><li>shot 수가 증가할수록 성능이 향상된다. e.g., <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span> 일 때 40.8% → <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">n=16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">16</span></span></span></span></span> 일 때 48.0% (row e). <ul><li>이는 가능한 한 많은 example 을 사용하는 것이 유리하다는 저자의 동기를 뒷받침한다.</li></ul></li><li>PICa-Full 은 모든 경우에서 PICa-Base 대비 일관적으로 약 5% 의 accuracy 향상을 보였다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="textual-representation-for-images">Textual Representation for Images<a href="#textual-representation-for-images" class="hash-link" aria-label="Direct link to Textual Representation for Images" title="Direct link to Textual Representation for Images">​</a></h2><p>저자는 GPT-3 를 위해 이미지를 textual format 으로 표현하는 최적의 방법에 대한 ablation study 를 수행하였다. 구체적으로, 다음 방법들을 비교하였다.</p><ul><li><strong>Blind</strong>: 이미지를 빈 문자열로 표현하는 baseline 으로, 이미지를 보지 않았을 때의 VQA 성능을 나타낸다. in-context example selection 은 question similarity 만을 사용하여 blind setting 을 유지한다.</li><li><strong>Tags</strong>: 자동 tagging model 이 예측한 tag list 로 이미지를 표현하며, 모든 tag 는 콤마로 구분된 문자열로 연결된다.</li><li><strong>VinVL-Caption-COCO</strong>: Tab. 1, 2 의 결과에 사용된 caption 으로, VinVL-base pre-trained checkpoint 를 COCO 2014 training set 에 fine-tuning 하여 OK-VQA test set 이미지(COCO 2014 validation set 이미지)에 대한 caption 을 생성한다.</li><li><strong>VinVL-Caption-CC</strong>: 동일 COCO dataset 이미지 노출을 피하기 위해 Conceptual Captions dataset 으로 VinVL-base captioning model 을 학습한 경우.</li><li><strong>API-Caption</strong>: 공개 Azure API 가 생성한 caption (Footnote 2 참조).</li><li><strong>GT-Caption</strong>: ground-truth COCO caption 을 oracle 로 사용하여 이상적인 image description 을 제공한다. 1 개의 무작위 ground truth 또는 모든 5 개 caption 을 연결하여 image description 으로 사용한다.</li><li><strong>Caption+Tags</strong>: caption 문자열과 tag list 문자열을 연결하여 image 를 표현한다.</li></ul><p><img loading="lazy" alt="Table 3" src="/assets/images/image-41-0b2945094dcc5d1992f5f5e9235c842a.png" width="833" height="561" class="img_ev3q"></p><p>Tab. 3 은 서로 다른 textual representation 형식에서의 OK-VQA accuracy 를 보여준다. 주요 발견 사항은 다음과 같다.</p><ul><li>(b)<!-- -->~<!-- -->(j) 행의 모든 textual description format 은 row (a)의 blind baseline 대비 유의미하게 높은 성능을 보이며, 이는 단순 question-answer pair 만 입력으로 사용할 때보다 이미지 표현력이 뛰어남을 나타낸다.</li><li>COCO 이미지를 전혀 보지 않은 VinVL-Caption-CC (row c) 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">n=16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">16</span></span></span></span></span> 에서 37.0% 의 준수한 accuracy 를 보였다. in-context example selection 과 multi-query ensemble 을 포함하면 성능이 44.0% 로 향상되며, 이는 supervised SOTA 를 초과한다.</li><li>예측된 caption 간 비교에서 VinVL-Caption-COCO (row e) 가 가장 높은 성능을 달성하였다. 전반적으로, 세밀하고 충분한 description 이 더 나은 VQA 성능으로 이어진다.</li><li>Ground-truth COCO caption (row f) 은 더 정확한 이미지 설명을 제공하여 oracle accuracy 48.7% 를 달성하였다. 모든 ground-truth caption 을 연결(row g) 하면 이미지 내용에 대한 설명이 더욱 완전해져 accuracy 가 53.3% 로 향상된다.</li><li>여러 caption 을 연결하는 접근의 효과에 영감을 받아, 서로 다른 textual description 형식을 결합하는 실험을 수행하였다. caption 과 tag 를 결합하면 상호 보완적인 정보를 제공하여 VQA 성능이 향상되었다. <ul><li>예를 들어, row (j) 에서 VinVL caption 과 tag 를 결합하면 16-shot accuracy 가 48.0% 로, row (b) 와 (e) 의 44.6% 및 46.9% 대비 성능이 상승하였다. 다른 조합(row h, i) 에서도 유사한 향상이 관찰되었다.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="example-selection-and-multi-query-ensemble">Example Selection and Multi-query Ensemble<a href="#example-selection-and-multi-query-ensemble" class="hash-link" aria-label="Direct link to Example Selection and Multi-query Ensemble" title="Direct link to Example Selection and Multi-query Ensemble">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="in-context-example-selection-1">In-context example selection<a href="#in-context-example-selection-1" class="hash-link" aria-label="Direct link to In-context example selection" title="Direct link to In-context example selection">​</a></h4><p><img loading="lazy" alt="Table 4" src="/assets/images/image-42-73ece168f991e18c84ab389bf9050c44.png" width="820" height="434" class="img_ev3q"></p><p>결과는 Tab. 4 에 요약되어 있다.</p><ul><li><strong>Row (a)</strong>: example 을 무작위로 선택한 PICa-Base 성능.</li><li><strong>Row (b)</strong>: question textual feature 의 similarity 기반으로 example 선택.</li><li><strong>Row (c)</strong>: 가장 dissimilar 한 example 을 선택한 경우로, “bad” example 이 실제로 성능을 악화시킴을 보여준다.</li><li><strong>Row (d)</strong>: example selection 시 answer similarity 까지 포함한 oracle 값으로, upper bound 역할을 하며 적절한 example selection 이 VQA accuracy 를 크게 향상시킬 수 있음을 보여준다.</li><li><strong>Row (e,f)</strong>: example selection 에 image visual feature 를 포함한 경우. row (e) 는 CLIP 으로 계산한 image feature similarity 기반 selection, row (f) 는 PICa-Full 의 방법으로 question 과 image similarity 를 모두 고려.</li></ul><p>주요 observation 은 다음과 같다.</p><ul><li>row (a) → row (b) 의 향상은 in-context example selection 이 few-shot VQA 에 실질적으로 도움이 됨을 보여준다.</li><li>dissimilar example 을 사용한 row (c) 는 accuracy 가 40.1% 로 떨어져, question similarity 기반 selection 의 중요성을 나타낸다.</li><li>“이상적인” example 을 선택한 row (d) 의 oracle accuracy 는 49.1% 에 도달하였다.</li><li>CLIP text encoder 로 feature 를 계산한 경우, RoBERTa 기반보다 약간 더 나은 성능을 보였다.</li><li>image similarity 만 사용한 selection(row e) 도 random baseline 을 개선하지만, question similarity 기반(row b)보다 향상 폭은 작았다.</li><li>PICa-Full(row f) 은 question similarity 와 image similarity 를 결합하여 accuracy 를 46.5% 로 향상시켰다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="multi-query-ensemble-1">Multi-query ensemble<a href="#multi-query-ensemble-1" class="hash-link" aria-label="Direct link to Multi-query ensemble" title="Direct link to Multi-query ensemble">​</a></h4><p>Multi-query ensemble 은 inference 시 더 많은 in-context example 을 사용할 수 있도록 하여 성능을 추가적으로 향상시킬 수 있다. 이 방법은 in-context example selection 과 자연스럽게 결합될 수 있다.</p><p>Tab. 5 에 두 방법을 함께 사용한 결과를 제시하였다.</p><ul><li><strong>Row (a,b)</strong>: multi-query ensemble 을 사용하지 않은 baseline 결과.</li><li><strong>Row (c,d)</strong>: prompt 수 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 를 늘리면, shot 수 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span> 전반에 걸쳐 OK-VQA accuracy 가 일관적으로 향상됨을 보여준다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="qualitative-analysis">Qualitative Analysis<a href="#qualitative-analysis" class="hash-link" aria-label="Direct link to Qualitative Analysis" title="Direct link to Qualitative Analysis">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="representative-cases">Representative cases<a href="#representative-cases" class="hash-link" aria-label="Direct link to Representative cases" title="Direct link to Representative cases">​</a></h4><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-43-013c2c6ce63dfcc3b500cdff8e3193a4.png" width="1695" height="787" class="img_ev3q"></p><p>Fig. 3 상단은 PICa 예측의 정성적 예시를 보여준다. PICa 는 다양한 종류의 external knowledge 가 필요한 질문에 대해 잘 동작한다.</p><ul><li>예: Fig. 3(a) 에서 GPT-3 는 question 의 “<code>this type of transportation</code>” 이 image 의 “<code>train</code>” 을 가리킨다는 것을 이해하고, “<code>train was invented in 1804</code>” 라는 정답을 제공한다.</li><li>Fig. 3(b) 에서도 model 은 “<code>motorcycle was invented in 1885</code>” 라는 사실을 알고 있다.</li><li>사실(encyclopedia) 기반 knowledge 뿐 아니라, Fig. 3(c) 와 같이 commonsense knowledge 가 필요한 질문에도 잘 동작한다. 여기서 model 은 사람이 grocery store 에서 banana 를 얻을 수 있다는 것을 이해한다. 이 예시에서 ground-truth answer 간 불일치는 open-ended answer generation 이 정답의 다양한 형식을 생성할 수 있음을 보여주며, 평가를 어렵게 만든다.</li><li>Fig. 3(d,e) 에서도 model 은 “<code>train stops at the train station</code>” 과 “<code>there could be sharks in the sea when surfacing</code>” 과 같은 implicit knowledge 를 활용해 올바른 답을 도출한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="answer-rationale">Answer rationale<a href="#answer-rationale" class="hash-link" aria-label="Direct link to Answer rationale" title="Direct link to Answer rationale">​</a></h4><p>GPT-3 가 fine-tuning 없이 open-ended 방식으로 knowledge-based question 에 올바르게 답변할 수 있는 이유에 대해 궁금할 수 있다. GPT-3 raw model 에 접근할 수 없으므로 language model 의 동작을 심층 분석하기는 어렵다. 대신, 저자는 zero-shot 방식의 answer rationale prediction 을 수행하여, open-ended text generation task 로 rationale 을 생성하였다.</p><p>구체적으로, question string <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span>, predicted answer <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span>, 그리고 prompt head “This is because” 를 연결하여 prompt 를 구성한 뒤, GPT-3 가 생성한 text 를 answer rationale 로 사용한다.</p><p>Fig. 3 하단은 예측된 answer 에 대한 rationale 을 보여준다. GPT-3 는 다양한 유형의 knowledge 가 필요한 question 에 대해 합리적인 rationale 을 생성한다.</p><ul><li>e.g., Fig. 3(a) 에서 rationale 은 “<code>the first locomotive was invented in 1804</code>” 라는 핵심 encyclopedia knowledge 이다.</li><li>Fig. 3(c) 에서는 “<code>grocery store is a common place to get food</code>” 라는 commonsense knowledge 를 제공한다.</li></ul><h1>Experiments on VQAv2</h1><p>knowledge-based VQA 에서의 우수한 성능에도 불구하고, 저자의 방법에는 image 가 text 로 추상화된다는 한계가 있다. caption 이나 tag 는 image 의 부분적 설명만을 제공하며, detailed visual attribute prediction 과 같이 question answering 에 필요한 중요한 시각적 세부 정보를 놓칠 수 있다. 본 절에서는 detailed image content 에 초점을 맞춘 question 이 포함된 VQAv2 dataset 에서 PICa 를 benchmark 한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dataset-and-setup-1">Dataset and setup.<a href="#dataset-and-setup-1" class="hash-link" aria-label="Direct link to Dataset and setup." title="Direct link to Dataset and setup.">​</a></h4><p>VQAv2 dataset 은 COCO image corpus 를 기반으로 question-answer pair 를 annotation 하였으며, question 은 image content 와 높은 관련성을 갖도록 설계되었다. 사람의 성능은 question 만 사용 시 40.8%, question+caption 사용 시 57.5%, question+image 사용 시 83.3% 로 보고된다. 저자는 Frozen 의 설정을 따라 validation set 에서 accuracy 를 보고한다. VQA 를 사전 선택된 answer vocabulary 기반 classification task 로 처리하는 대신, open-ended text generation 방식으로 answer 를 예측한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results.<a href="#results" class="hash-link" aria-label="Direct link to Results." title="Direct link to Results.">​</a></h4><p><img loading="lazy" alt="Table 6" src="/assets/images/image-46-622f57d63bac244f081d346516b3d09a.png" width="825" height="421" class="img_ev3q"></p><ul><li>Tab. 6 에 따르면, PICa-Full 은 accuracy 56.1% 를 달성하며, 기존 few-shot 성능 38.2% 를 큰 폭으로 초과한다. </li><li>제안한 in-context example selection 과 multi-query ensemble 방법은 VQAv2 dataset 에서도 효과적이다(PICa-Base: 54.3%, PICa-Full: 56.1%). </li><li>supervised learning model Oscar 의 73.8% 와 비교하면 여전히 약 17% 낮으며, Fig. 4 에 failure case 가 제시된다. </li><li>그럼에도 불구하고, 이러한 유망한 few-shot 결과는 제안 방법이 few-shot vision-language task 접근에서 강력한 baseline 임을 보여준다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="limitations">Limitations.<a href="#limitations" class="hash-link" aria-label="Direct link to Limitations." title="Direct link to Limitations.">​</a></h4><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-45-69d30b395dc42c618944b85591583be3.png" width="1585" height="564" class="img_ev3q"></p><p>Fig. 4(a-d) 와 (e,f) 는 각각 PICa-Full 의 성공 및 실패 사례이다. VQAv2 question 의 일부는 commonsense knowledge 로 답변할 수 있으며, 이 경우 PICa 는 대체로 잘 동작한다. 예를 들어, Fig. 4(a) 의 “<code>the sign above doorway can be the exit sign</code>” 과 Fig. 4(b) 의 “<code>cow laying down because of being tired</code>” 과 같은 암묵적 knowledge 가 있다.</p><p>그러나 VQAv2 question 의 많은 부분은 object color 와 같은 detailed image content 에 관한 것이다. 성공 사례에서 PICa 는 관련 textual description 이 있는 경우 이를 사용하거나, object property 를 통해 추론한다. 예: Fig. 4(c) 의 “<code>a silver refrigerator</code>” 와 Fig. 4(d) 의 “<code>bedroom walls are usually white</code>”. 하지만 불완전한 textual description 만을 보면 많은 question 에 실패한다. 예: Fig. 4(e) 에서 정확한 색상을 예측하지 못하거나, Fig. 4(f) 에서 기린의 수를 맞히지 못한다. 이러한 질문에는 end-to-end vision encoder tuning 이 도움이 될 것으로 기대된다.</p><h1>Conclusion</h1><p>저자는 GPT-3 를 활용한 few-shot knowledge-based VQA 접근법인 PICa 를 제안하였다. 명시적 structured knowledge base 를 사용해 external knowledge 를 검색·추론 하는 대신, PICa 는 GPT-3 prompting 을 통해 관련 knowledge 를 공동으로 획득·처리한다. GPT-3 의 강력한 few-shot 능력을 계승하여 OK-VQA 에서 supervised SOTA 를 큰 폭으로 초과하였다. 분석 결과, 제안 방법은 question 에 답하기 위해 관련 knowledge 를 암묵적으로 획득함을 확인하였다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">Vision-Language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multimodal">Multimodal</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/few-shot-learning">few-shot learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/gpt-3">GPT-3</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/prompting">Prompting</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/VQA-IC/Few-shot/2021-09-PICa.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Multimodal Few-Shot Learning with Frozen Language Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">xGQA: Cross-Lingual Visual Question Answering</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#gpt-3-for-in-context-learning" class="table-of-contents__link toc-highlight">GPT-3 for In-context Learning</a></li><li><a href="#gpt-3-for-vqa" class="table-of-contents__link toc-highlight">GPT-3 for VQA</a></li><li><a href="#in-context-examples" class="table-of-contents__link toc-highlight">In-context Examples</a></li><li><a href="#dataset-and-setup" class="table-of-contents__link toc-highlight">Dataset and Setup</a></li><li><a href="#comparison-with-state-of-the-art" class="table-of-contents__link toc-highlight">Comparison with State-of-the-art</a></li><li><a href="#few-shot-ability" class="table-of-contents__link toc-highlight">Few-shot Ability</a></li><li><a href="#textual-representation-for-images" class="table-of-contents__link toc-highlight">Textual Representation for Images</a></li><li><a href="#example-selection-and-multi-query-ensemble" class="table-of-contents__link toc-highlight">Example Selection and Multi-query Ensemble</a></li><li><a href="#qualitative-analysis" class="table-of-contents__link toc-highlight">Qualitative Analysis</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.74164ba1.js"></script>
<script src="/assets/js/main.82abd802.js"></script>
</body>
</html>