---
slug: xGQA
title: "xGQA: Cross-Lingual Visual Question Answering"
tags: [Vision-Language, Multimodal, few-shot learning, GPT-3, Prompting]
---

논문 및 이미지 출처 : <https://arxiv.org/pdf/2109.06082>

# Abstract

최근 multimodal vision 과 language modeling 의 발전은 주로 multilingual multimodal dataset 의 부족 때문에 대부분 English 언어에 집중되어 왔다. 이 연구에서는 이러한 격차를 해소하기 위해 visual question answering task 를 위한 새로운 multilingual evaluation benchmark 인 xGQA 를 제안한다. 저자는 기존 English GQA dataset 을 7 개의 유형적으로 다양한 언어로 확장하여, cross-lingual visual question answering 에서 중요한 도전 과제를 탐지하고 분석할 수 있게 한다. 또한 multimodal transformer 기반 model 을 multilingual 로 확장하기 위한 adapter 기반 접근법, 그리고 그 반대로 multilingual model 을 multimodal 로 확장하기 위한 접근법을 제안한다. 제안된 방법은 zero-shot cross-lingual setting 에서 현재 state-of-the-art multilingual multimodal model (e.g., M3P) 보다 우수한 성능을 보이지만, 전체적으로 accuracy 는 여전히 낮다. target language 에서 약 38 accuracy point 의 성능 저하는 이 task 에서 zero-shot cross-lingual transfer 의 어려움을 보여준다. 이러한 결과는 multimodal model 의 단순한 cross-lingual transfer 가 잠재적인 multilingual multimodal misalignment 를 초래한다는 점을 시사하며, vision 과 multilingual language modeling 을 위한 더 정교한 방법이 필요함을 보여준다.

# 1 Introduction

Transformer 기반 architecture 는 NLP 와 CV 분야에서 널리 사용되며 뛰어난 task 성능을 보여준다. 여러 modality 를 위한 공통 architecture 는 정보의 효과적인 fusion 가능성을 열어주었고, image captioning, phrase grounding, visual question answering, referring expression comprehension, image-text retrieval 과 같은 다양한 multimodal task 에서 인상적인 성능 향상을 이끌어냈다. 그러나 이러한 발전은 대부분 English 언어에 국한되어 있으며, 주요 multimodal dataset 이 English text 로만 구성되어 있기 때문이다. Multilingual evaluation benchmark 의 부족으로 인해 이 문제를 다루는 model 개발은 제한적이었다.

이 격차를 해소하기 위해, 저자는 visual question answering task 를 위한 multilingual evaluation benchmark 인 xGQA 를 제안하며, monolingual English 전용 GQA dataset 을 확장한다. xGQA 를 위해 저자는 balanced GQA test-dev set 을 7 개 언어 계열에서 온 7 개 언어로 수작업 번역 및 적응하였으며, 이는 5 개의 서로 다른 script 를 포함한다 (Sec. 1 과 Tab. 1 참조). 추가로, target language 에서 소수의 예제만 활용하는 cross-lingual few-shot learning experiment 를 위한 새로운 고정 data split 도 제공한다.

Pretraining 은 (i) 고자원 언어의 경우 계산 비용이 매우 크고 (ii) multilingual multimodal resource 의 양이 제한적이기 때문에, 저자는 multilingual multimodal model 을 구축하기 위한 추가 baseline 으로 계산 효율적인 adapter 기반 접근법을 제안한다. 즉, English text 로만 pretraining 된 multimodal model 을 multilingual 로 확장하고, 반대로 multilingual model 을 multimodal 로 확장한다. 이를 위해 저자는 Artetxe et al. 과 Pfeiffer et al. 의 접근을 따르며, monolingual 및 multilingual model 을 새로운 언어와 script 로 확장하기 위해 새로운 tokenizer 와 word-embedding matrix, target language 용 adapter 를 학습한다. 이어서 해당 multilingual multimodal adapter 기반 model 을 target task 로 transfer 하기 위해, 저자는 modality 별 adapter weight 를 사용하는 새로운 modality-specific split architecture 를 제안한다 (Sec. 2 참조).

결과적으로 제안된 adapter 기반 architecture 는 최근 state-of-the-art multilingual multimodal pretrained model 인 M3P 를 zero-shot cross-lingual setting 에서 능가한다. 그러나 전반적인 zero-shot transfer 성능은 여전히 낮으며, target language 전반에서 평균 약 38 accuracy point 의 성능 저하가 발생한다. Few-shot setup 에서 target language 예제를 소수 사용하면 모든 접근법에서 성능이 크게 향상되지만, cross-lingual transfer 성능은 source language 성능에 비해 여전히 상당히 낮다. 이는 질문이 template 기반이며 평균 8.5 개 단어만 포함하는 비교적 단순한 질문임에도 불구하고, 해당 task 의 본질적인 어려움을 보여준다.

**Contributions**

1. cross-lingual visual question answering 을 위한 최초의 evaluation benchmark 를 제안하며, 7 개의 다양한 target language 를 포함한다.
2. multilingual multimodal model 생성을 위한 새로운 adapter 기반 접근법을 제안한다.
3. state-of-the-art model 과 새로운 multilingual multimodal model 을 zero-shot 과 few-shot learning setup 에서 체계적으로 benchmark 하여 task 의 어려움을 입증하고 향후 연구를 위한 강력한 reference point 를 제공한다.
4. 다양한 접근법을 철저히 분석하여 model failure 를 유발하는 요소와 question type 을 밝혀내고, 이 영역에서의 향후 연구 필요성을 다시금 강조한다.

# 2 Background and Related Work

**Multilingual Language Models.** Pretrained multilingual transformer 기반 LM 인 mBERT 와 XLM-R 은 각각 monolingual counterpart 인 BERT 와 RoBERTa 와 동일한 pretraining regime 을 따른다. 이들은 100 개 이상의 언어가 포함된 concatenated text corpus 에 대해 self-supervised masked language modelling (MLM) objective 로 pretraining 된다. Text 는 WordPiece, SentencePiece, 혹은 BytePair encoding 으로 tokenization 된다. 이러한 multilingual model 은 직접적인 cross-lingual supervision (예: parallel data, translation dictionary) 없이도 cross-lingual task 에서 놀라운 성능을 보이는 것으로 나타났다.

**Vision and Language Models.** 대부분의 transformer 기반 multimodal model 은 text token 과 image region feature 를 함께 인코딩한다. 이미지는 Faster R-CNN 과 같은 object detection model 을 이용하여 region of interest (RoI) feature 를 추출한다. 추출된 image region feature 는 affine layer 를 통과하며 multimodal transformer 의 joint embedding space 로 projection 된다. RoI 의 bounding box 좌표는 visual feature 의 positional embedding 으로 작동하며, 이 또한 affine transformation 을 거쳐 image region representation 과 결합된다. Position-aware image region embedding 은 transformer 로 전달되고, multi-head attention 은 각 layer 에서 모든 text 와 image 입력에 대해 attention 을 수행하며 joint representation 을 학습한다.

Kamath et al. 은 object detector 를 black-box 로 사전 feature 추출에 사용하는 대신, 이를 multimodal transformer architecture 의 중심 요소로 통합하였다. Object detector 를 multimodal transformer 와 함께 end-to-end 로 학습하면 더 큰 유연성과 표현력을 확보할 수 있다. Multimodal transformer 기반 model 은 MLM 과 유사하게 self-supervised objective 로 학습되며, masked feature regression, masked object detection, masked attribute detection, cross-modality matching 과 같은 contrastive loss 가 사용된다. 주로 COCO, Flickr30k, Conceptual Captions (CC), SBU 와 같은 image captioning dataset 이 pretraining 에 활용된다. Unimodal language model 과 유사하게 \[CLS] token 이 classification task 에서 contextual representation 으로 사용된다.

최근에는 multilingual multimodal model 도 제안되었다. 예를 들어, M3P 는 50 개 언어의 Wikipedia 와 English multimodal CC dataset 으로 학습된다. English 이외의 언어 token 을 image representation 과 정렬하기 위해, M3P 는 code-switching 메커니즘을 사용하며, English CC 예제의 단어를 대응되는 bilingual dictionary 의 단어로 무작위 대체한다. UC2 에서는 English multimodal dataset 을 machine translation 으로 다른 언어로 확장하고, masked region-to-token modeling 및 visual translation language modeling 을 제안한다.

**Adapters.** Adapter 는 NLP 와 CV 에서 transfer learning 을 위한 효율적인 fine-tuning 전략으로 도입되었다. Pretrained model 의 모든 weight 를 fine-tuning 하는 대신, 각 layer 에 작은 feed-forward layer 를 삽입한다. Task fine-tuning 시에는 adapter weight 만 업데이트되고 pretrained parameter 는 고정(frozen)된다. Adapter 는 학습 효율성이 매우 뛰어난 것으로 나타났으며, domain 과 task 간 transfer, machine translation, cross-lingual transfer 와 같은 다양한 응용 분야에서 사용된다.

**Datasets.** Multilingual multimodal model 을 위한 pretraining 및 fine-tuning data 는 주로 Wikipedia 기반 multimodal 정보(WikiCaps, WIT) 또는 downstream task data 에 의존한다. Multi30k 는 English, German, French, Czech 을 포함하는 multilingual image captioning dataset 이며 retrieval task 를 다룬다. GEM 은 image 와 video retrieval task 를 각각 20, 30 개 언어에서 다룬다. HowTo100M 은 multilingual, multimodal pretraining dataset 으로 image 와 video retrieval 에 사용된다. MultiSubs 는 빈칸 채우기 및 lexical translation task 에 집중하며 English, Spanish, German, Portuguese, French 를 포함한다. Gao et al. 과 Shimizu et al. 은 bilingual visual question answering dataset 을 각각 English–Chinese, English–Japanese 로 제안하였다. Liu et al. 은 MaRVL dataset 을 제안하였는데, 이는 NLVR2 와 유사한 binary multilingual question answering dataset 으로 Chinese, Tamil, Swahili, Indonesian, Turkish 등 5 개의 다양한 언어를 포함한다.

기존 dataset 은 retrieval 중심 task (상대적으로 단순), 소수의 유사 언어만 포함, 혹은 binary question 만 다루는 경우가 많았다. 이에 반해 저자는 더 다양한 언어 집합을 포괄하는 최초의 multilingual visual question answering dataset 을 제안한다.

가장 최근에는 IGLUE 가 제안되었으며, 이는 multilingual multimodal benchmark 로서 xGQA 를 통합한다. IGLUE 는 visual question answering, cross-modal retrieval, grounded reasoning, grounded entailment task 를 20 개 이상의 다양한 언어에서 다룬다.

# 3 xGQA

원래 English GQA dataset 은 Visual Genome scene graph 를 활용하여 구축되었다. English question engine 은 content (즉, object, attribute, relation 에 관한 정보) 와 structure (수백 개의 구조적 패턴과 정교한 lexical semantic resource 를 결합한 언어적 grammar) 를 활용하여 image scene graph 에 시각적으로 기반한 2,200 만 개 이상의 다양한 question 을 생성하였다. Question 은 template 을 사용하여 자동 생성되었기 때문에 자연 언어의 광범위한 스펙트럼을 반드시 반영하지는 않으며, 실제 환경에서의 성능에 대해 가정하기는 어렵다.

각 question 은 추가 metadata 와 연관되며, 이는 구조적 유형에 따라 다음과 같이 구분된다.

1. **verify**: yes/no question (예: "Do you see any cats?")
2. **query**: open question (예: "Who is wearing jeans?")
3. **choose**: 두 개의 대안을 제시하는 question (예: "Is it red or blue?")
4. **logical**: 논리적 추론을 포함하는 question (예: "Is the field soft and snowy?")
5. **compare**: 두 개 이상의 object 를 비교하는 question (예: "Are all the animals zebras?")

자세한 metadata 는 Hudson and Manning 의 연구를 참고한다.

**Dataset Design.** xGQA 를 설계할 때의 주요 목표는 visual question answering 을 위한 유형적으로 진정으로 다양한 multilingual multimodal evaluation benchmark 를 만드는 것이었다. 저자는 GQA 의 balanced test-dev set 을 활용하였으며, 이는 398 개 image 에 대한 12,578 개 question 으로 구성된다. 정해진 구조적 패턴 덕분에 question 의 표현은 단순하며, 평균 길이는 8.5 단어이다. 최종적으로 xGQA dataset 은 7 개 언어로 번역을 포함하며, 각 언어는 서로 다른 언어 계열을 대표하고, 5 개의 script 를 포함한다 (Tab. 1 참조).

**Few-Shot Data Splits.** Cross-lingual few-shot learning experiment 를 수행하기 위해, 저자는 다양한 크기의 새로운 data split 을 제공한다. Split 은 image 단위로 진행되며, 해당 image 와 연관된 모든 question 을 해당 set 에 추가한다. Development set 과 test set 은 각각 50 개와 300 개 image 로 구성된다. Training split 은 1, 5, 10, 20, 25, 48 개 image 로 구성된다 (Tab. 2 참조). 각 set 에서 structural type 의 분포가 유지되도록 보장하였다.

xGQA 는 최초의 진정한 유형적으로 다양한 multilingual multimodal benchmark 로, cross-lingual zero-shot 및 few-shot 시나리오에서 새로운 실험과 분석 기회를 제공한다. xGQA 의 question 은 직관적이고 인간이 풀기에는 쉽지만, 이후 결과에서 보이듯 현재 state-of-the-art model 들은 여전히 transfer 에 어려움을 겪는다.

# 4 Baselines

xGQA 의 성능과 현재의 한계를 분석하기 위해, 먼저 multilingual 및 multimodal data 로 pretraining 된 최근의 M3P model 을 평가한다. 그러나 pretraining 은 계산 비용이 크고 multilingual multimodal resource 는 제한적이다. 따라서 저자는 보다 효율적인 새로운 접근법을 제안한다. (1) state-of-the-art multilingual language model 을 multimodal domain 으로 확장하고, (2) state-of-the-art multimodal model 에 multilingual 기능을 제공하는 방식이다.

별도의 언급이 없는 한, 저자는 GQA 에서 일반적으로 사용되는 fine-tuning 전략을 따른다. Pretrained transformer 의 output 위에 prediction head 를 얹고, GQA task 의 가능한 1853 개의 답변을 class label 로 매핑한다. Image 와 연관된 question 과 position-aware region feature 가 transformer 의 입력으로 전달되며, cross-entropy loss 로 학습된다.

## 4.1 Multimodal → Multilingual

**OSCAR+Emb.**
Artetxe et al. 은 monolingual transformer LM 을 multilingual domain 으로 확장하기 위해 target language 에서 새로운 word-embedding layer 를 fine-tuning 하였다. 이 아이디어를 기반으로, 저자는 monolingual multimodal transformer model 인 OSCAR+ 를 target language 용 embedding 을 학습하도록 확장한다.

* **Language-extension phase:** OSCAR+ 의 embedding matrix 를 무작위로 초기화된 embedding matrix 로 교체한다. Transformer weight 는 고정되고, 새로운 embedding 만 target language 의 unlabeled text data 로 MLM objective 를 사용하여 fine-tuning 된다.
* **Target-task phase:** 원래 OSCAR+ model 은 English GQA training data 로 fine-tuning 되며, 이때 transformer layer 는 fine-tuning 되고 embedding layer 는 고정된다. Inference 시에는 embedding layer 가 target language 의 embedding 으로 교체된다.

**OSCAR+Ada.**
여기에 adapter 를 추가한 확장 방식이다.

* **Language-extension phase:** Pfeiffer et al. 의 방법을 따르며, OSCAR+Emb 와 유사하게 새로운 embedding layer 를 학습한다. 또한 각 transformer layer 에 language adapter 를 추가한다. OSCAR+ 가 English text 로 학습되었기 때문에, English language adapter module 은 embedding matrix 를 교체하지 않고 학습된다. Transformer weight 는 고정되고, 새로운 embedding 과 language adapter weight 만 target language 의 unlabeled text data 로 fine-tuning 된다.

* **Target-task phase:** 저자는 cross-lingual transfer 방법에서 영감을 받아 새로운 modality-split architecture 를 제안한다 (Fig. 2). 각 transformer layer 에서 text 와 image representation 은 pretrained multi-head attention (MHA) 와 feed-forward (FFN) layer 를 통과한다. 동시에 image 와 text representation 은 pretrained language adapter 를 통과한다. 이후 각 modality 는 modality-specific task adapter (text, image) 를 거치고, 마지막으로 shared multimodal alignment adapter 를 통과한다. Training 시 transformer, embedding, language adapter weight 는 고정되며, task adapter 와 multimodal aligner adapter weight, prediction head 만 fine-tuning 된다. Inference 시에는 embedding layer 와 language adapter 가 target language weight 로 교체된다.

## 4.2 Multilingual → Multimodal

**mBERTAda.**
Multilingual model 을 multimodal 로 확장하는 실험에서는 mBERT 를 사용한다.

* **Language-extension phase:** mBERT 는 이미 다양한 언어를 표현할 수 있으므로 target language 를 위한 새로운 embedding layer 를 학습할 필요가 없다. 대신 AdapterHub.ml 에서 제공되는 mBERT 호환 language adapter 를 활용한다.

* **Target-task phase:** Image representation layer 는 OSCAR+ 방식과 동일하게, image feature 를 positional information 과 결합한 후 affine transformation layer 를 통과시킨다. Fig. 2 의 adapter architecture 를 OSCAR+Ada 와 동일하게 사용한다. Training 시 transformer, embedding, language adapter weight 는 고정된다. 그러나 OSCAR+ 와 달리 affine image transformation layer 는 무작위 초기화 후 fine-tuning 된다. Task adapter, multimodal aligner adapter weight, prediction head 도 GQA task 에 대해 fine-tuning 된다. Inference 시에는 embedding layer 와 language adapter 가 target language weight 로 교체된다.

# 5 Experimental Setup

## 5.1 Language-Extension Phase

OSCAR+Emb 과 OSCAR+Ada 의 경우, 저자는 Pfeiffer et al. 이 제안한 일반적인 설정을 따른다. 각 target language 에 대해 vocabulary size 가 30k 인 새로운 word-piece tokenizer 를 학습한다. 무작위로 초기화된 embedding layer 와 (OSCAR+Ada 의 경우) adapter layer 를 batch size 64, learning rate \$1e^{-4}\$ 로 100k update step 동안 fine-tuning 한다. mBERTAda 에 대해서는 AdapterHub.ml 의 language adapter 를 활용한다.

## 5.2 Fine-tuning on GQA

저자는 Li et al. 이 제안한 standard setup 을 따르며, \[CLS] token 의 representation 을 prediction head 로 전달한다. 각 model 은 cross-entropy loss 로 fine-tuning 되며, label 은 GQA dataset 의 모든 가능한 answer 로 설정된다. Li et al. 의 prior work 에 따라 batch size 는 192, 학습 epoch 은 5 로 설정하고, unbalanced GQA training set 을 사용한다.

* **M3P:** pretrained model 의 모든 weight 를 learning rate \$3e^{-5}\$ 로 fine-tuning 한다.
* **OSCAR+Emb, OSCAR+Ada, mBERTAda:** Zhang et al. 이 제공한 pretrained weight 와 image region feature 를 사용한다. 그러나 object attribute label 은 입력으로 전달하지 않는다. 이는 English 로만 제공되며, cross-lingual scenario 에 활용하기가 비직관적이기 때문이다. 해당 부분은 future work 으로 남긴다.

  * **OSCAR+Emb:** transformer weight 와 prediction head 만 fine-tuning 하고, embedding layer 는 freeze 한다. Learning rate 는 \$3e^{-5}\$ 를 사용한다.
  * **OSCAR+Ada, mBERTAda:** Sec. 4.1 과 Fig. 2 에서 설명된 대로 adapter layer 를 추가한다. Embedding, transformer layer, language adapter 를 포함한 모든 pretrained weight 는 freeze 하고, 새로 추가된 adapter 와 prediction head 만 fine-tuning 한다.
  * **mBERTAda:** affine image transformation layer 를 추가하고 학습한다. Adapter 기반 model 은 learning rate \$1e^{-4}\$ 로 fine-tuning 한다.

# 5.3 Zero-Shot Cross-Lingual Transfer

Zero-shot cross-lingual evaluation 을 위해, 저자는 GQA training data 로 fine-tuning 된 model 을 multilingual xGQA test data 에서 평가한다. English GQA validation data 에서 가장 좋은 성능을 보인 model checkpoint 가 transfer 에 사용된다.

* **M3P:** 이 model 은 pretraining 시 xGQA 언어를 포함하므로 cross-lingual transfer 를 위해 추가적인 단계가 필요 없다.
* **OSCAR+Emb:** English embedding layer 를 target language embedding layer 로 교체한다.
* **OSCAR+Ada:** English embedding 과 language adapter layer 를 target language 의 embedding 과 adapter layer 로 교체한다.
* **mBERTAda:** language adapter layer 를 target language 의 adapter layer 로 교체한다.

# 5.4 Few-Shot Cross-Lingual Transfer

Few-shot cross-lingual 시나리오에서는 Lauscher et al. 의 접근을 따르며, zero-shot transfer 와 동일한 fine-tuning model (§5.3) 에서 시작한다. 이후 §5.2 의 English training data 학습과 동일한 부분을 target language 의 소량 multimodal data 로 fine-tuning 한다.

Training 은 1, 5, 10, 15, 20, 25, 48 개 image 로 구성된 서로 다른 data split (Tab. 2 참조) 에 대해 수행된다. Epoch 수는 5, 10, learning rate 는 M3P 와 OSCAR+Emb 의 경우 \$1e^{-5}\$, \$5e^{-5}\$ 를, OSCAR+Ada 와 mBERTAda 의 경우 \$5e^{-5}\$, \$1e^{-4}\$ 를 사용하여 실험하였다.

그 결과, 더 긴 학습과 더 큰 learning rate 를 사용할 때 모든 setting 에서 가장 좋은 성능을 보였다.

# 6 Results and Discussion

주요 결과는 Tab. 3 (zero-shot 실험) 과 Tab. 4 (few-shot 실험) 에 제시된다.

## 6.1 Zero-Shot Cross-Lingual Transfer

핵심적인 발견 중 하나는 multimodal zero-shot cross-lingual transfer 가 매우 어렵다는 것이다. xGQA dataset 의 target language 에서 accuracy 가 English GQA 점수와 비교했을 때 평균 38 point 이상 감소하는 현상이 관찰된다 (예: M3P 결과 비교).

예상대로 OSCAR+ 가 English test set 에서 가장 높은 accuracy 를 기록했지만, massively multilingual model 인 M3P 와 mBERT 가 cross-lingual transfer 에서는 훨씬 더 좋은 성능을 보인다. 이는 joint multilingual pretraining 이 중요하며, 단순한 multilingual adapter 기반 또는 embedding 기반의 monolingual model 확장은 cross-lingual 성능이 떨어짐을 의미한다.

Pretraining 기반의 M3P 가 English test set 에서 더 나은 accuracy 를 기록하였지만, adapter 기반 multimodal 확장을 적용한 mBERT 는 cross-lingual transfer 에서 M3P 를 능가하였다. 저자는 monolingual multimodal data 로 transformer weight 를 모두 fine-tuning 하면, M3P 내의 cross-lingual alignment 가 깨질 수 있다고 가정한다. 그러나 adapter 기반 setting 에서는 multilingual weight 가 freeze 되어 alignment 가 유지되므로 이러한 문제가 발생하지 않는다.

**Structural Question Type 분석.**
Fig. 3 은 zero-shot 실험에서 structural question type 에 대한 분석을 보여준다. 특히 query 와 choose 유형 question 에서 accuracy 가 크게 떨어지는 것이 관찰된다.

* **Query type question** 은 자유형이며 의미적으로 가장 어렵다. Source language (English) 에서조차 답변하기 어려운 유형이기 때문에, zero-shot setting 에서 전반적으로 낮은 accuracy 가 나타난다.
* **Choose type question** 은 source language 에서 model 이 잘 수행하는 유형이지만, zero-shot cross-lingual transfer 에서 accuracy 가 크게 감소한다. 이는 question 의 구조와 model 구현 방식에 기인한다. Choose 유형 question 은 "Is it red or blue?" 와 같이 정답이 question 에 포함된 단어 또는 구로 구성된다. Label class 와 prediction head 는 dataset 에 등장하는 모든 answer 로 구축되며, model 은 최종 layer 에서 각 answer 의 분산 표현을 학습한다. 따라서 cross-lingual transfer 시에는 translated question 내의 option ("red" 또는 "blue") 을 model 의 prediction head 에서 학습된 English latent representation 과 자동으로 정렬해야 한다. 이 유형에서 매우 낮은 성능은 zero-shot 시나리오에서 이러한 cross-lingual word alignment 가 무너짐을 보여준다.

종합적으로, 저자가 제안한 multimodal adapter 기반 mBERT 확장(mBERTAda) 은 best accuracy 를 기록하며, M3P 대비 약 3 point, OSCAR+ 대비 약 5 point 향상되었다. 그러나 모든 접근법의 overall accuracy 는 English 결과와 비교하면 여전히 낮다. 이는 zero-shot multimodal cross-lingual transfer 가 매우 어렵고, visual representation 과 cross-lingual internal representation 간의 misalignment 문제 때문일 가능성이 크다는 것을 보여준다. 이를 더 조사하기 위해 저자는 few-shot setup 에서 유사한 실험을 수행하였으며, 이는 zero-shot setup 에서 관찰된 misalignment 문제를 완화할 가능성이 있다.

# 6.2 Few-Shot Cross-Lingual Transfer

Few-shot 실험의 주요 결과는 Tab. 4 에 제시되며, training data 양의 영향을 보여주는 plot 은 Fig. 5 에 나타나 있다. 핵심적인 발견은 예상대로 target language 의 data 양이 증가할수록 모든 방법에서 accuracy 가 꾸준히 향상된다는 점이다. Target language 에서 단 48 개 image 로 model 을 학습했을 때 accuracy 가 최대 20 point 개선되었다. 이는 소수의 target language 예제만으로도 model 이 내부 cross-lingual multimodal alignment 를 부분적으로 복구할 수 있음을 시사한다. 흥미롭게도 단 5 개 image 와 그에 해당하는 question 만으로도 M3P 가 mBERTAda (zero-shot setting 에서 best performing model) 보다 더 나은 성능을 보이기 시작한다.

저자는 structural question type 별로 few-shot learning 의 영향을 다시 분석했으며, 결과는 Fig. 4 에 제시된다. Zero-shot 시나리오(Fig. 3) 와 비교했을 때 전반적으로 accuracy 가 모든 유형에서 향상되었다. 특히 zero-shot setup 에서 가장 낮은 성능을 보였던 query 와 choose 유형에서 두드러진 향상이 나타났다. 이는 target language 에서 소수의 예제로 fine-tuning 하는 과정이 latent multimodal representation 과 multilingual representation 간 alignment 를 개선했음을 의미한다.

# 6.3 Language Transfer

저자의 model 은 기존 연구에서 보고된 것과 유사한 cross-lingual transfer 패턴을 보였다. Typologically 가까운 언어에서 가장 좋은 성능을 보였는데, German (de) 과 Portuguese (pt) 가 그 예이다. 두 언어는 모두 Indo-European (IE) 계열에 속하며 source language 인 English (en) 과 같은 Latin script 를 공유한다. Russian (ru), Indonesian (id), Chinese (zh) 에서는 accuracy 가 소폭 감소했고, Bengali (bn), Korean (ko) 에서는 더 큰 감소가 관찰되었다. 이들 언어는 source language 와 유형적으로 다르며 대부분 script 도 공유하지 않는다. 이러한 차이는 cross-lingual transfer 에서 언어 다양성이 중요함을 강조한다. 저자의 benchmark 는 이러한 다양성을 포괄함으로써 multilingual multimodal model 을 진정으로 유형적으로 다양한 언어 집합에서 실험하고 평가할 수 있게 한다.

# 7 Contemporary Work

Multilingual vision 과 language learning 에 대한 관심이 최근 급격히 증가하면서, contemporay work 에서는 이미 제안된 xGQA dataset 을 더 분석하고 확장하였다.

* **Further Analysis.** Liu et al. (2022) 는 cross-lingual visual question answering 으로 학습된 multilingual, multimodal model 을 광범위하게 분석하고, Sec. 6.1 에서 논의된 multilingual misalignment 문제를 완화하기 위한 여러 접근법을 제안하였다. 그 결과, text-only cross-lingual transfer scenario 에서 사용되던 일반적인 접근법은 pretrained model 의 multilingual capability 를 충분히 활용하지 못한다는 사실이 드러났다. 흥미롭게도 prediction head 를 더 깊게 만드는 것은 source language 성능에 영향을 주지 않지만, 모든 target language 의 zero-shot transfer 성능은 상당히 개선되었다.

* **Translated Test Data.** Bugliarello et al. (2022) 는 modality, task, language 를 아우르는 transfer learning benchmark 를 처음으로 제안했으며, 여기에는 visual question answering, cross-modal retrieval, grounded reasoning, grounded entailment task 가 포함되며 총 20 개의 다양한 언어를 다룬다. 이들은 machine translation 으로 번역된 test set question 을 추가하여 xGQA dataset 을 확장하고, translate-test setup 에서 state-of-the-art monolingual multimodal model 을 평가하였다. 이 setting 에서는 약간 더 나은 성능을 보였으나, 여전히 source language 성능에는 크게 미치지 못했다. Translate-test data 는 [iglue-benchmark.github.io](https://iglue-benchmark.github.io) 에서 확인할 수 있다.

# 8 Conclusion

저자는 visual question answering task 를 위한 최초의 cross-lingual evaluation benchmark 인 xGQA 를 제안하였다. xGQA 는 English GQA dataset 을 7 개의 유형적으로 다양한 언어로 확장하였으며, 5 개 script 를 포함한 development 및 test data 를 제공한다. 추가 baseline 으로, 저자는 unimodal multilingual model 을 multimodal 로 확장하거나 그 반대로 monolingual multimodal model 을 multilingual 로 확장하는 adapter 기반 방법을 제안하였다.

실험 결과, (1) 효율적인 adapter 기반 방법은 zero-shot scenario 에서 pretrained multilingual multimodal model 인 M3P 를 약간 능가했지만, (2) 모든 model 은 English 성능과 비교할 때 zero-shot cross-lingual transfer 에서 심각한 accuracy 감소를 보였다. 그러나 target language 의 소량 training data 를 활용하는 few-shot learning 을 통해 accuracy 를 부분적으로 복구할 수 있었다. 그럼에도 불구하고 큰 성능 격차는 여전히 남아 있으며, 이는 해당 task 가 직관적이고 인간(특히 bilingual) 에게는 매우 쉬움에도 불구하고 본질적으로 높은 복잡성을 가진다는 점을 시사한다.

저자는 제안된 dataset 과 error analysis 가 이 task 는 물론, 더 넓게는 multilingual multimodal representation learning 이라는 새롭게 부상하는 흥미로운 연구 영역에서 향후 연구를 촉진하기를 기대한다.

