<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/VQA-IC/Few-shot/2021-10-FEWVLM">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.c7613d59.js" as="script">
<link rel="preload" href="/assets/js/main.21e44b4c.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">CLIP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Foundation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">VQA-IC</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Few-shot</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Multimodal Few-Shot Learning with Frozen Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP">CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA">Linearly Mapping from Image to Text Space</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper">Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA">Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches</a></li></ul></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">VQA-IC</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Few-shot</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2110.08484" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2110.08484</a></p><h1>Abstract</h1><p>Large pre-trained vision-language (VL) model 은 few examples 만으로 새로운 task 를 학습하고, fine-tuning 없이도 새로운 task 로 generalization 할 수 있다. 그러나 이러한 VL model 은 크기가 지나치게 크고 inference 속도가 느려 실제 응용에 배포하기 어렵다. </p><p>이러한 한계를 해결하기 위해, 저자는 제안한 방법인 <strong>FEWVLM</strong> 을 사용하여 prompt-based low-resource VL task 학습을 연구한다. </p><ul><li>FEWVLM 은 최근 few-shot learner 에 비해 상대적으로 작은 크기를 가진다. FEWVLM 을 위해, 저자는 sequence-to-sequence transformer model 을 prefix language modeling (PrefixLM) 과 masked language modeling (MaskedLM) 으로 pre-training 한다. </li><li>또한 few-shot task 에 대해 다양한 prompt 의 효과를 분석한다. </li></ul><p>VQA 에 대한 실험 결과, prompt 기반 학습을 적용한 FEWVLM 은 FEWVLM 보다 31× 더 큰 Frozen 보다 18.2%p 높은 성능을 보였으며, FEWVLM 보다 246× 더 큰 PICa 와 유사한 성능을 달성하였다. 분석 결과, </p><ol><li>prompt 는 zero-shot 성능에 큰 영향을 주지만 few-shot 성능에는 미미한 영향을 준다.</li><li>noisy prompt 를 사용한 model 은 더 많은 training data 가 주어지면 hand-crafted prompt 와 동일한 속도로 학습한다.</li><li>MaskedLM 은 VQA task 에 도움을 주고 PrefixLM 은 captioning 성능을 향상시킨다.</li></ol><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-23-e90b88ee53b667780759ae72c16b358b.png" width="749" height="851" class="img_ev3q"></p><h1>1 Introduction</h1><p>Large pre-trained language model (PLM) 의 fine-tuning 은 vision-language task 를 포함한 다양한 domain 에서 강력한 결과를 이끌어냈다. 이러한 large PLM 은 few examples 만으로 새로운 task 를 학습하거나, 어떠한 training example 도 없이 새로운 task 로 generalization 할 수 있다. 즉, few-shot 및 zero-shot learning 이 가능하다. Few-shot learning 은 human-labeled data 수집이 비용이 크고 느리다는 data-hungry supervised learning 의 문제를 극복한다. 그러나 GPT3, Frozen, PICa 와 같은 최근의 few-shot model 은 지나치게 큰 model size 로 인해 소형 또는 중형 규모의 컴퓨팅 환경에서는 배포가 어렵다.</p><p>본 논문에서는 저자가 제안하는 moderate-sized vision-language model 인 <strong>FEWVLM</strong> 을 사용하여, training example 이 전혀 없거나 소수만 있는 상황에서 fine-tuning 하는 low-resource VL task 학습을 연구한다. </p><ul><li>FEWVLM 을 위해, 저자는 sequence-to-sequence transformer model 을 prefix language modeling (PrefixLM) 과 masked language modeling (MaskedLM) 으로 pre-training 한다. </li><li>이러한 설정은 표준 컴퓨팅 하드웨어에서 경제적으로 training 및 inference 가 가능하며, 실제 환경에서 대규모의 고품질 training example 을 확보하는 것은 비용이 크다는 점에서 더 실용적이다. </li><li>이러한 few-shot setting 에서는 task-specific prompt 나 task description 이 중요하며, few-shot NLP task 에서 그 효과가 이미 입증되었다.</li></ul><p>이러한 성공을 VL task 로 확장하기 위해, 저자는 prompt 기반 low-resource VL learning 에 대해 다음 질문에 답하고자 한다.</p><ul><li>Q1) prompt 설계는 새로운 task 의 zero/few-shot learning 에 어떤 영향을 미치는가?</li><li>Q2) 더 큰 training data 가 주어졌을 때도 prompt 설계가 여전히 중요한가?</li><li>Q3) 서로 다른 pre-training objective 가 zero/few-shot learning 에 어떤 영향을 미치는가?</li></ul><p>이 질문에 답하기 위해, 저자는 zero/few-shot VL learning dataset 에서 hand-crafted prompt 와 noisy prompt 를 포함한 다양한 prompt format 을 탐구한다. 또한 Raffel et al. 의 연구에서 영감을 받아, prefix language modeling (PrefixLM) 과 masked language modeling (MaskedLM) 의 pre-training objective 를 few-shot task 에 적용하여 연구한다. 이를 위해, visual question answering, captioning, miniImageNet 등 다양한 few-shot VL task 에서 model 의 성능을 조사한다.</p><p>실험 분석에서, prompt 기반 학습을 적용한 FEWVLM 은 zero-shot VQAv2 에서 FEWVLM 보다 31× larger Frozen 보다 18.2%p 높은 성능을 보였으며, 246× larger PICa 와 유사한 결과를 달성하였다. 추가적으로, </p><ol><li>prompt 는 zero-shot 성능에 큰 영향을 미치지만 새로운 task 의 few-shot 성능에는 미미한 영향을 준다 (§6.2, §6.3), </li><li>noisy prompt 를 사용한 model 은 더 많은 training data 가 주어졌을 때 hand-crafted prompt 와 동일한 속도로 학습한다 (§6.5), </li><li>MaskedLM 은 few-shot VQA task 에 도움을 주고 PrefixLM 은 captioning 성능을 향상시킨다 (§6.6) 는 것을 관찰하였다.</li></ol><h1>2 Related Work</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="vision-language-few-shot-learning">Vision-language few-shot learning.<a href="#vision-language-few-shot-learning" class="hash-link" aria-label="Direct link to Vision-language few-shot learning." title="Direct link to Vision-language few-shot learning.">​</a></h4><p>최근 vision-language task 에서의 few-shot learner 들이 제안되었는데, 여기에는 GPT, Frozen, PICa, SimVLM 등이 포함된다. </p><ul><li>Frozen 은 GPT-2 기반의 대규모 language model 로, soft prompting 을 확장하여 이미지와 텍스트의 집합을 통합함으로써 multimodal few-shot learner 로 변환된다. <ul><li>이 접근법은 visual question answering 과 image classification task 에서 few-shot 성능을 보인다. </li></ul></li><li>유사하게, PICa 는 GPT-3 를 사용하여 few in-context VQA 예시를 제공함으로써 few-shot 방식으로 VQA task 를 해결한다. <ul><li>이 과정에서 이미지를 textual description 으로 변환하여 GPT-3 가 이미지를 이해할 수 있도록 한다. </li></ul></li><li>SimVLM 은 weakly-supervised dataset 에서 prefix language modeling 으로 학습되며, zero-shot captioning task 에서 효과를 입증하였다.</li></ul><p>그러나 이러한 model 들은 few-shot task 에서 성능 향상을 이루었음에도 불구하고, model size 가 커서 실제 응용에 사용하기에는 비실용적이다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="language-model-prompting">Language model prompting.<a href="#language-model-prompting" class="hash-link" aria-label="Direct link to Language model prompting." title="Direct link to Language model prompting.">​</a></h4><p>Prompt 나 task description 제공은 많은 task 에서 pre-trained language model 의 성능 향상에 중요한 역할을 한다. 특히 GPT 계열 model 은 NLP task 에서 prompt 나 task demonstration 을 통해 큰 성공을 거두었다. 이러한 방향성을 바탕으로, prompt 기반 접근법은 small pre-trained model 의 few-shot text classification task 성능을 향상시킨다. </p><p>또한 CLIP 은 image classification 에서 zero-shot 성능에 영향을 미치는 prompt template 을 탐구하였다. </p><p>본 연구는 이러한 핵심 아이디어를 따라 vision-language task 에서 prompt 를 활용하여 zero-shot 및 few-shot 성능을 향상시키는 것을 목표로 한다.</p><h1>3 Analysis Setup</h1><p>본 연구에서는 vision-language model <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">L</mi></mrow><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal">L</span></span></span></span></span> 의 zero-shot 및 few-shot 성능을 분석한다. 이를 위해 problem formulation, analysis question, downstream task 및 dataset, evaluation metric, 그리고 baseline 을 소개한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-problem-formulation">3.1 Problem Formulation<a href="#31-problem-formulation" class="hash-link" aria-label="Direct link to 3.1 Problem Formulation" title="Direct link to 3.1 Problem Formulation">​</a></h2><p>Zero-shot task 에서는 pre-trained VL model <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">L</mi></mrow><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal">L</span></span></span></span></span> 이 training set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mtext>train</mtext></msub></mrow><annotation encoding="application/x-tex">D_{\text{train}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">train</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 및 development set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mtext>dev</mtext></msub></mrow><annotation encoding="application/x-tex">D_{\text{dev}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">dev</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 에 접근하지 못하며, test instance <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mtext>test</mtext></msub></mrow><annotation encoding="application/x-tex">D_{\text{test}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">test</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 에 대해 직접 inference 를 수행한다. Few-shot task 에서는 training data 로부터 dev set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mtext>dev</mtext></msub></mrow><annotation encoding="application/x-tex">D_{\text{dev}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">dev</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 를 구성하고, Perez et al. 과 Gao et al. 의 방식에 따라 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><msub><mi>D</mi><mtext>train</mtext></msub><mi mathvariant="normal">∣</mi><mo>=</mo><mi mathvariant="normal">∣</mi><msub><mi>D</mi><mtext>dev</mtext></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|D_{\text{train}}| = |D_{\text{dev}}|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">train</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">dev</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></span> 로 맞추어 hyper-parameter 조정 및 model 선택을 진행한다. 제한된 데이터로 학습하는 목표를 충족하기 위해 training set 과 development set 의 크기를 제한하며, 본 연구에서는 두 set 모두 크기를 16 으로 설정한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-analysis-questions">3.2 Analysis Questions<a href="#32-analysis-questions" class="hash-link" aria-label="Direct link to 3.2 Analysis Questions" title="Direct link to 3.2 Analysis Questions">​</a></h2><p>본 연구는 여러 VL dataset 에서의 실험을 통해 다음 세 가지 질문에 답하고자 한다.</p><ul><li><p><strong>Q1) Prompt 설계가 새로운 task 의 zero/few-shot learning 에 어떤 영향을 미치는가?</strong>
Task-specific prompt 제공은 NLP 분야에서 zero-shot 및 few-shot 성능을 크게 향상시키는 것으로 알려져 있다. 이를 위해 vision-language task 에서 다양한 ad-hoc prompt 를 실험하고, hand-crafted prompt 와 noisy prompt 가 zero-shot 및 few-shot 성능에 얼마나 영향을 미치는지 Sec. 6.5 에서 분석한다.</p></li><li><p><strong>Q2) larger training data 가 주어졌을 때도 prompt 설계가 여전히 중요한가?</strong>
Prompt 가 zero/few-shot 성능에 영향을 주더라도, training data 의 크기에 따라 그 영향이 달라질 수 있다. 이를 검증하기 위해 training data 크기와 prompt 를 달리하여 학습한 model 의 성능을 비교한다.</p></li><li><p><strong>Q3) 서로 다른 pre-training objective 가 zero/few-shot 성능에 어떤 영향을 미치는가?</strong>
Prefix language modeling (PrefixLM) 과 masked language modeling (MaskedLM) 두 가지 pre-training objective 를 비교한다. 이를 위해 각 objective 로 pre-training 한 model 을 zero-shot 및 few-shot task 에서 평가하며, 결과는 Sec. 6.6 에서 제시한다.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-downstream-tasks-and-datasets">3.3 Downstream Tasks and Datasets<a href="#33-downstream-tasks-and-datasets" class="hash-link" aria-label="Direct link to 3.3 Downstream Tasks and Datasets" title="Direct link to 3.3 Downstream Tasks and Datasets">​</a></h2><p>본 연구는 주로 세 가지 task 에 초점을 맞춘다: visual question answering, captioning, categorical learning.</p><ul><li><strong>Visual question answering (VQA)</strong>: 주어진 이미지와 질문에 대해 정답을 생성하는 task 로, zero-shot 환경에서도 model 이 답변을 생성할 수 있도록 generation task 로 변환한다.</li><li><strong>Captioning</strong>: 주어진 이미지에 대한 설명을 생성하는 task.</li><li><strong>Categorical learning</strong>: 올바른 category 또는 class 를 선택하는 task 로, 다른 classification 방법과 달리 open-ended 방식으로 평가하며, 정답 label 을 직접 생성해야 한다.</li></ul><p>Dataset 은 다음과 같다.</p><ul><li><strong>VQA</strong>: VQAv2, OKVQA, GQA</li><li><strong>Captioning</strong>: NoCaps, Flickr30k (Karpathy split 사용: train/val/test = 29,000 / 1,014 / 1,000)</li><li><strong>Categorical learning</strong>: miniImageNet (meta learning dataset) – Tsimpoukelli et al. 의 설정을 따라 meta test data 만 사용하며, 5-way k-shot setup 으로 평가 (5 개 class 와 class 당 k 개 예시 제공).</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-evaluation-metrics">3.4 Evaluation Metrics<a href="#34-evaluation-metrics" class="hash-link" aria-label="Direct link to 3.4 Evaluation Metrics" title="Direct link to 3.4 Evaluation Metrics">​</a></h2><p>Few-shot 성능을 평가하기 위해, training set 과 dev set 을 각각 5 가지로 무작위 샘플링하여 5 개 split 의 평균 성능을 측정한다. Few-shot 설정에서는 vision-language model 을 200 epoch 동안 fine-tuning 하며, dev set 에서 가장 좋은 checkpoint 를 선택한다. NoCaps task 의 경우, 자체 training data 가 없으므로 Wang et al. 의 방식에 따라 COCO captioning 의 training data 를 사용한다.</p><p>평가는 다음과 같이 진행한다.</p><ul><li><strong>VQAv2</strong>: validation set</li><li><strong>GQA</strong>: test-dev</li><li><strong>OK-VQA</strong>: test set</li><li><strong>Flickr30k captioning</strong>: Karpathy split 의 test set</li><li><strong>NoCaps</strong>: validation set</li></ul><p>평가 지표로는 VQA dataset 과 miniImageNet 에 대해 accuracy 를 사용하며, captioning 에 대해서는 CIDEr 와 SPICE 를 사용한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="35-baselines">3.5 Baselines<a href="#35-baselines" class="hash-link" aria-label="Direct link to 3.5 Baselines" title="Direct link to 3.5 Baselines">​</a></h2><p>비교를 위해 강력한 zero/few-shot vision-language learner 를 평가 대상으로 포함한다.</p><ul><li><strong>VQA dataset</strong>: Frozen, PICa</li><li><strong>Captioning dataset</strong>: SimVLM</li><li><strong>Few-shot VQAv2, Flickr30k</strong>: Unified VLP 포함</li></ul><p>또한, 각 task 에 대해 fully fine-tuned model <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mtext>full</mtext></msub></mrow><annotation encoding="application/x-tex">L_{\text{full}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">full</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 을 few-shot model 의 upper bound 로 비교한다. </p><ul><li>Fully fine-tuned model 은 전체 dataset 으로 fine-tuning 된 반면, </li><li>few-shot model 은 극히 일부 데이터만 접근 가능하다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mtext>full</mtext></msub></mrow><annotation encoding="application/x-tex">L_{\text{full}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">full</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 의 경우, VQAv2 는 Uniter<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>large</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{large}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6222em;vertical-align:-0.2861em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">large</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span>, </li><li>GQA 는 Oscar, NoCaps CIDEr 는 SimVLM, SPICE 는 VinVL, Flickr30k captioning 은 Unified VLP 의 결과를 인용한다.</li><li>추가적으로, VQA dataset 으로 pre-training 하지 않은 baseline 인 VL-T5<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>no-vqa</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{no-vqa}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4375em;vertical-align:-0.2861em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">no-vqa</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 를 포함한다. </li><li>miniImageNet 의 경우 Frozen 과 AFHN 을 포함하며, Frozen 은 few-shot learning 용, AFHN 은 meta learning 용으로 설계되어 더 작고 빠르다.</li></ul><h1>4 Method</h1><p>본 분석에 앞서, VL task 의 zero/few-shot learning 을 수행하고 앞서 제기한 분석 질문에 답하기 위해 제안하는 model 인 <strong>FEWVLM</strong> 을 소개한다. 여기서는 FEWVLM 의 architecture 와 pre-training objective 를 설명한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-encoder-decoder-vision-language-model">4.1 Encoder-decoder Vision-language Model<a href="#41-encoder-decoder-vision-language-model" class="hash-link" aria-label="Direct link to 4.1 Encoder-decoder Vision-language Model" title="Direct link to 4.1 Encoder-decoder Vision-language Model">​</a></h2><p>저자는 visual 과 text input 을 encoding 하고 target text 를 생성하기 위해 encoder-decoder architecture 를 채택한다. input image 는 Visual Genome 에서 학습된 Faster R-CNN 으로부터 추출한 36 object regions 로 표현한다. 이 region representation 집합은 text 와 함께 encoder 입력에 결합된다.</p><p>Model parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span></span> 는 주어진 입력 text <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 와 image <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span></span> 에 대해 target text <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> token 의 negative log-likelihood 를 최소화하도록 학습된다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>L</mi><mi>θ</mi></msub><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi mathvariant="normal">∣</mi><mi>y</mi><mi mathvariant="normal">∣</mi></mrow></munderover><mi>log</mi><mo>⁡</mo><msub><mi>P</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo separator="true">,</mo><mi>x</mi><mo separator="true">,</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">L_{\theta} = - \sum_{i=1}^{|y|} \log P_{\theta}(y_i \mid y_{&lt;i}, x, v) \tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.2387em;vertical-align:-1.2777em"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.961em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.386em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">y</span><span class="mord mtight">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:3.2387em;vertical-align:-1.2777em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></div><p>이 model 은 특정 task 에 종속되지 않으므로 zero/few-shot setting 에 적합하다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-pre-training-objectives">4.2 Pre-training Objectives<a href="#42-pre-training-objectives" class="hash-link" aria-label="Direct link to 4.2 Pre-training Objectives" title="Direct link to 4.2 Pre-training Objectives">​</a></h2><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-25-e49b4f826bdf05a739815d458dce2bca.png" width="721" height="802" class="img_ev3q"></p><p>저자는 Prefix language modeling (PrefixLM) 과 Masked language modeling (MaskedLM) 두 가지 objective 로 model 을 pre-training 한다.</p><ul><li><p><strong>Prefix language modeling (PrefixLM)</strong>
Raffel et al. 의 방식을 따라, 주어진 이미지와 텍스트 span 을 임의로 두 개의 부분으로 분할한다. 전반부 텍스트와 이미지는 encoder 입력으로 사용되고, 후반부 텍스트는 decoder 가 생성해야 할 target text 로 사용된다.</p></li><li><p><strong>Masked language modeling (MaskedLM)</strong>
Cho et al. 의 방식을 따라, 입력 텍스트에서 무작위 span 을 선택하여 sentinel token (e.g., <code>&lt;text_1&gt;</code>) 으로 치환한 뒤, 이를 encoder 입력으로 사용한다. Decoder 는 마스킹된 span 을 target text 로 생성한다. 입력 텍스트 token 의 15% 를 무작위로 masking 한다.</p></li></ul><p><strong>Pre-training data</strong>
FEWVLM pre-training 에 사용한 데이터는 MS COCO 와 Visual Genome 의 image-caption data 로, 총 9.18M image-text pair 와 180K 개의 고유 이미지를 포함한다.</p><h1>5 Low-resource Adaptation</h1><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-24-31f5a3e93d905cea588136e8bb6813b5.png" width="1518" height="576" class="img_ev3q"></p><p>Downstream task 에서는 few examples 만을 사용하여 model 을 학습한다. Fig. 2 는 inference 시 FEWVLM 의 구조를 보여준다. Prompt template <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span></span></span></span></span> 가 주어지면, 먼저 template 을 이용해 입력 텍스트와 target 텍스트 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="script">P</mi><mo stretchy="false">(</mo><mtext>input</mtext><mo separator="true">,</mo><mtext>label</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x, y) = \mathcal{P}(\text{input}, \text{label})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathcal" style="margin-right:0.08222em">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">input</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord text"><span class="mord">label</span></span><span class="mclose">)</span></span></span></span></span> 을 생성한다. 이후 Eq. (1) 의 negative log-likelihood 를 최소화하여 model parameter 를 학습한다. Inference 시에는 동일한 prompt 를 사용하여 model 이 label text 를 생성하며, 최종 label 은 target prompt template 을 제거하여 얻는다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-prompt-design">5.1 Prompt Design<a href="#51-prompt-design" class="hash-link" aria-label="Direct link to 5.1 Prompt Design" title="Direct link to 5.1 Prompt Design">​</a></h2><p><img loading="lazy" alt="Table 1" src="/assets/images/image-26-0d4afabe9cd256002f26e49fa22073bf.png" width="1639" height="602" class="img_ev3q"></p><p>Prompt 는 vision-language model 의 성능에 영향을 미치며, 본 연구에서는 downstream task 의 zero-shot 및 few-shot 성능에 미치는 영향을 분석한다. 사용된 prompt 는 Tab. 1 과 Tab. 11 에 제시된다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="511-visual-question-answering">5.1.1 Visual Question Answering<a href="#511-visual-question-answering" class="hash-link" aria-label="Direct link to 5.1.1 Visual Question Answering" title="Direct link to 5.1.1 Visual Question Answering">​</a></h3><p>VQA, OKVQA, GQA 와 같은 visual question answering task 는 주어진 이미지에 대한 질문에 답하는 것을 요구한다. 기존 접근법은 사전에 정의된 answer 후보 집합에 대해 multi-label classification 으로 문제를 해결하였으나, 저자는 task-specific head 를 추가하지 않고도 답변을 생성할 수 있도록 <strong>generation task</strong> 로 변환하였다. 이 설정에서 prompt 는 model 이 올바른 형식의 답변을 생성하도록 제약을 제공한다. Prompt 가 없을 경우, model 이 문장을 생성하여 VQA 형식에 맞지 않는 출력을 할 수 있다.</p><p>이를 위해 Tab. 1 과 Tab. 11 에 나타난 바와 같이 다양한 input/output prompt 를 실험하였다. 여기에는 hand-crafted prompt 와 ablation study 를 위한 noisy prompt 가 포함된다.</p><ul><li><strong>Hand-crafted prompt</strong>:<ul><li>Input prompt: &quot;question: <!-- -->[Q]<!-- --> answer:&quot; 또는 마지막에 <code>&lt;text_1&gt;</code> sentinel token 을 포함한 형태. MaskedLM 과 유사하게 sentinel token 이 단어 생성을 유도할 것으로 기대된다.</li><li>Target prompt: &quot;<!-- -->[A]<!-- -->&quot; 또는 &quot;<code>&lt;text_1&gt;</code> <!-- -->[A]<!-- -->&quot; 형식. MaskedLM 의 target text 형식을 모방하여 model 이 새로운 task 에 빠르게 적응하도록 한다. Prompt ID 는 Tab. 1 에 정의된다.</li></ul></li><li><strong>Noisy prompt</strong>:
Zero/few-shot 학습에서 noisy prompt 의 영향을 분석하기 위해, irrelevant prompt, noisy token, random sentence 를 포함하였다.<ul><li>Irrelevant prompt: 무작위 질문이나 지시문을 사용하여 model 이 잘못된 질문에 답하거나 관련 없는 지시를 따르도록 유도.</li><li>Noisy token: T5 vocabulary 에서 무작위 선택된 token. Model 이 random token 에 얼마나 robust 한지 측정.</li><li>Random sentence: MS COCO 의 caption 을 사용하여 model 에 잘못된 정보를 제공.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="512-captioning">5.1.2 Captioning<a href="#512-captioning" class="hash-link" aria-label="Direct link to 5.1.2 Captioning" title="Direct link to 5.1.2 Captioning">​</a></h3><p>NoCaps 와 Flickr30k dataset 에서는 세 가지 hand-crafted input prompt (&quot;a picture of&quot;, &quot;a photo of&quot;, &quot;an image of&quot;) 를 실험한다. 세 표현은 의미가 유사하지만, 실험 결과에서 zero-shot 및 few-shot 성능이 다르게 나타난다. Target prompt 는 별도의 추가 prompt 없이 original caption 을 그대로 사용한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="513-miniimagenet">5.1.3 MiniImageNet<a href="#513-miniimagenet" class="hash-link" aria-label="Direct link to 5.1.3 MiniImageNet" title="Direct link to 5.1.3 MiniImageNet">​</a></h3><p>MiniImageNet dataset 에서는 hand-crafted input prompt 로 &quot;This is <code>&lt;text_1&gt;</code>&quot; 를, target prompt 로 &quot;<code>&lt;text_1&gt;</code> <!-- -->[A]<!-- -->&quot; 를 사용한다. Prompt 사용 여부에 따른 categorical learning 성능 차이를 비교한다.</p><h1>6 Results and Discussion</h1><p>본 절에서는 zero-shot 및 few-shot task 에 대한 주요 결과를 논의한 후, 앞서 제기한 질문인 <strong>prompt design 이 zero/few-shot learning 에서 중요한가?</strong> 에 대해 답한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="61-experiment-details">6.1 Experiment Details<a href="#61-experiment-details" class="hash-link" aria-label="Direct link to 6.1 Experiment Details" title="Direct link to 6.1 Experiment Details">​</a></h2><p>Pre-training 시, FEWVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo><mtext>base</mtext></mrow><annotation encoding="application/x-tex">*\text{base}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord">∗</span><span class="mord text"><span class="mord">base</span></span></span></span></span></span> 는 batch size 1,280, FEWVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo><mtext>large</mtext></mrow><annotation encoding="application/x-tex">*\text{large}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord">∗</span><span class="mord text"><span class="mord">large</span></span></span></span></span></span> 는 batch size 800 으로 설정하고 30 epoch 동안 학습한다. Learning rate 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1 \times 10^{-4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></span> 로 설정하며, 5% linear warmup 을 적용한다.</p><p>Few-shot learning 시, model 은 200 epoch 동안 학습하며 learning rate 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5 \times 10^{-5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span></span> 로, 5% linear warmup 을 적용한다. Dev set 에서 가장 성능이 높은 checkpoint 를 선택한다.</p><p>Prompt 설정은 다음과 같다.</p><ul><li><strong>VQA</strong>: Input prompt — <code>&quot;question: [Q] answer &lt;text_1&gt;&quot;</code> (P3), Target prompt — &quot;<code>&lt;text_1&gt; [A]</code>&quot;</li><li><strong>Captioning</strong>: Input prompt — <code>&quot;an image of&quot;</code> (Q3), Target prompt — 원래 caption</li><li><strong>MiniImageNet</strong>: Input prompt — <code>&quot;This is &lt;text_1&gt;&quot;</code>, Target prompt — &quot;<code>&lt;text_1&gt; [A]</code>&quot;</li></ul><p>VQA 및 captioning task 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mtext>train</mtext></msub></mrow><annotation encoding="application/x-tex">D_{\text{train}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">train</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 과 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mtext>dev</mtext></msub></mrow><annotation encoding="application/x-tex">D_{\text{dev}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">dev</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 크기는 각각 16 으로 설정하였다. MiniImageNet 의 경우, 클래스당 {1, 3, 5}-shot 환경에서 실험을 진행한다. Prompt 의 효과는 Sec. 6.5 에서 분석한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="62-performance-on-zero-shot-learning">6.2 Performance on Zero-shot Learning<a href="#62-performance-on-zero-shot-learning" class="hash-link" aria-label="Direct link to 6.2 Performance on Zero-shot Learning" title="Direct link to 6.2 Performance on Zero-shot Learning">​</a></h2><p><img loading="lazy" alt="Table 2" src="/assets/images/image-27-c098237c4fe866684c49359665a994d3.png" width="799" height="580" class="img_ev3q"></p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-29-a480115771475886a2a5915e23ab66e5.png" width="801" height="414" class="img_ev3q"></p><p>Zero-shot 환경에서는 model 이 어떤 training data 도 보지 못한 상태에서 평가를 진행한다. Tab. 2 와 Tab. 4 는 각각 VQA 와 captioning dataset 결과를 보여준다.</p><ul><li>FEWVLM(P3, hand-crafted prompt 사용) 은 VQA dataset 에서 다른 baseline 보다 우수한 성능을 기록하였다. </li><li>특히, FEWVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo><mtext>base</mtext></mrow><annotation encoding="application/x-tex">*\text{base}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord">∗</span><span class="mord text"><span class="mord">base</span></span></span></span></span></span> 는 약 31× larger Frozen 을 크게 능가하였다. </li><li>OK-VQA 에서는 GPT-3 기반의 PICa 가 가장 높은 성능을 보였으나, FEWVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo><mtext>large</mtext></mrow><annotation encoding="application/x-tex">*\text{large}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord">∗</span><span class="mord text"><span class="mord">large</span></span></span></span></span></span> 는 246× 더 작은 모델임에도 PICa 와 유사한 결과를 달성하였다.</li></ul><p>또한, 동일한 architecture 를 사용하되 VQA dataset 없이 pre-training 한 VL-T5<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo><mtext>no-vqa</mtext></mrow><annotation encoding="application/x-tex">*\text{no-vqa}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6597em;vertical-align:-0.1944em"></span><span class="mord">∗</span><span class="mord text"><span class="mord">no-vqa</span></span></span></span></span></span> 와 비교했을 때, FEWVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo><mtext>base</mtext></mrow><annotation encoding="application/x-tex">*\text{base}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord">∗</span><span class="mord text"><span class="mord">base</span></span></span></span></span></span> 는 VQAv2 성능을 약 30%p 향상시켰다. 이는 pre-training objective 와 prompt 가 VQA 성능을 향상시킨다는 것을 시사한다.</p><p>Captioning task 인 NoCaps 에서는 SimVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo><mtext>huge</mtext></mrow><annotation encoding="application/x-tex">*\text{huge}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord">∗</span><span class="mord text"><span class="mord">huge</span></span></span></span></span></span> 가 최고 성능을 기록했으나, FEWVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo><mtext>base</mtext></mrow><annotation encoding="application/x-tex">*\text{base}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord">∗</span><span class="mord text"><span class="mord">base</span></span></span></span></span></span> 역시 VL-T5<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>no-vqa</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{no-vqa}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4375em;vertical-align:-0.2861em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">no-vqa</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 대비 성능을 크게 향상시켰다. 이 역시 pre-training objective 와 prompt 의 기여로 해석된다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="63-performance-on-few-shot-learning">6.3 Performance on Few-shot Learning<a href="#63-performance-on-few-shot-learning" class="hash-link" aria-label="Direct link to 6.3 Performance on Few-shot Learning" title="Direct link to 6.3 Performance on Few-shot Learning">​</a></h2><p><img loading="lazy" alt="Table 3" src="/assets/images/image-28-853e17a124942864d7fa674562e69662.png" width="795" height="734" class="img_ev3q"></p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-30-58d4ca544781795fce1213df1f286bdc.png" width="798" height="473" class="img_ev3q"></p><p>Tab. 3 과 Tab. 5 는 VQA 와 captioning dataset 에 대한 few-shot 성능을 보여준다. FEWVLM, VL-T5<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>no-vqa</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{no-vqa}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4375em;vertical-align:-0.2861em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">no-vqa</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span>, Unified VLP 는 training set 과 validation set 크기를 각각 16 으로 설정하며, Frozen 과 PICa 는 각각 4 개와 16 개의 in-context demonstration example 을 사용한다.</p><ul><li>VQAv2 와 OK-VQA 에서 PICa 가 최고 성능을 보였으며, FEWVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo><mtext>large</mtext></mrow><annotation encoding="application/x-tex">*\text{large}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord">∗</span><span class="mord text"><span class="mord">large</span></span></span></span></span></span> 는 VQAv2 에서 이에 필적하는 성능을 달성하였다. </li><li>OK-VQA 는 다른 VQA dataset 과 달리 정답을 위해 external knowledge 가 필요하므로, larger model 과 large-scale pre-training data(prior knowledge)가 성능 향상에 중요하다. </li><li>흥미롭게도, 4 training examples 만으로 학습한 FEWVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mstyle mathcolor="#cc0000"><mtext>\*</mtext></mstyle></msup><mo>∗</mo><mtext>base</mtext></mrow><annotation encoding="application/x-tex">^\**\text{base}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight" style="color:#cc0000"><span class="mord mtight" style="color:#cc0000">\*</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord text"><span class="mord">base</span></span></span></span></span></span> 가 Frozen 을 능가하였다.</li></ul><p>Captioning dataset 에서 FEWVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo><mtext>base</mtext></mrow><annotation encoding="application/x-tex">*\text{base}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord">∗</span><span class="mord text"><span class="mord">base</span></span></span></span></span></span> 는 NoCaps CIDEr 기준 VL-T5<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo><mtext>no-vqa</mtext></mrow><annotation encoding="application/x-tex">*\text{no-vqa}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6597em;vertical-align:-0.1944em"></span><span class="mord">∗</span><span class="mord text"><span class="mord">no-vqa</span></span></span></span></span></span> 대비 31.1%p 높은 성능을 기록하였다. Flickr30k captioning task 에서는 Unified VLP 가 FEWVLM 보다 약간 낮은 성능을 보였는데, 이는 Unified VLP 의 architecture 가 encoder-decoder transformer 기반이며 captioning task 로 pre-training 되었기 때문이라고 추정된다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="64-miniimagenet">6.4 MiniImageNet<a href="#64-miniimagenet" class="hash-link" aria-label="Direct link to 6.4 MiniImageNet" title="Direct link to 6.4 MiniImageNet">​</a></h2><p><img loading="lazy" alt="Table 6" src="/assets/images/image-31-84cf705cbbfd9efdb9e7b6194e4a2c96.png" width="802" height="507" class="img_ev3q"></p><p>Tab. 6 은 miniImageNet 결과를 보여준다. 이 task 에서 model 은 각 이미지에 대해 올바른 class 를 선택해야 한다. </p><ul><li>FEWVLM 은 generative 방식으로 학습 및 평가되며, correct label text 를 정확히 생성해야 credit 을 얻는다. </li><li>FEWVLM 은 all shots 환경에서 Frozen 을 크게 능가하였다. </li><li>FEWVLM 은 few training sample 로 학습하지만, Frozen 은 이를 in-context demonstration 으로 사용한다. </li><li>흥미롭게도, hand-crafted prompt 를 사용한 FEWVLM 은 1-shot 환경에서 성능이 크게 향상되었으나, 5-shot 환경에서는 향상이 미미했다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="65-study-of-prompt-design">6.5 Study of Prompt Design<a href="#65-study-of-prompt-design" class="hash-link" aria-label="Direct link to 6.5 Study of Prompt Design" title="Direct link to 6.5 Study of Prompt Design">​</a></h2><p><img loading="lazy" alt="Table 7" src="/assets/images/image-32-1e819499387d8b5b9864b6d169939b7d.png" width="818" height="430" class="img_ev3q"></p><p>FEWVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>base</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{base}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4861em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">base</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 의 prompt 효과를 Tab. 7, Fig. 6, Fig. 5, Fig. 4 에서 분석하였다. 평가 dataset 은 VQAv2 와 Flickr30k 이다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="651-zero-shot-predictions">6.5.1 Zero-shot Predictions<a href="#651-zero-shot-predictions" class="hash-link" aria-label="Direct link to 6.5.1 Zero-shot Predictions" title="Direct link to 6.5.1 Zero-shot Predictions">​</a></h3><ul><li>Tab. 7 에서 보듯이, 두 dataset 모두에서 zero-shot 결과는 input prompt 에 크게 영향을 받았다. </li><li>VQAv2 의 경우, P1 과 P3 의 <code>&lt;text_1&gt;</code> 은 “no prompt” 및 P2 대비 zero-shot 성능을 크게 향상시켰다. <ul><li>이는 <code>&lt;text_1&gt;</code> 이 MaskedLM 과 유사하게 model 이 masked span 을 예측하도록 유도하기 때문으로 추정된다.</li></ul></li></ul><p>Flickr30k 에서는 prompt 의 단어 선택 차이를 분석하였다: “a picture of” (Q1), “a photo of” (Q2), “an image of” (Q3). 예를 들어, “an image of” 사용 시 no prompt 대비 21.4p 높은 성능을 보였다. 즉, 단어 선택 차이가 zero-shot 결과에 상당한 영향을 미친다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="652-few-shot-predictions">6.5.2 Few-shot Predictions<a href="#652-few-shot-predictions" class="hash-link" aria-label="Direct link to 6.5.2 Few-shot Predictions" title="Direct link to 6.5.2 Few-shot Predictions">​</a></h3><p>VQAv2 에서 irrelevant prompt, noisy token, random sentence 를 포함한 다양한 input prompt 를 실험하였다(Fig. 4). </p><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-33-e6794f2ebcf1bf9d6283b317743fbc2b.png" width="641" height="721" class="img_ev3q"></p><ul><li>Zero-shot 환경에서는 noisy prompt 와 no prompt 모두 거의 0 accuracy 를 기록했다. </li><li>그러나 few-shot 환경에서는 noisy prompt 도 충분한 데이터가 주어지면 hand-crafted prompt 와 비슷한 속도로 학습하였다. <ul><li>예를 들어, noisy prompt 사용 시 최적의 hand-crafted prompt 와 유사한 성능을 달성했다.</li></ul></li><li>Noisy prompt 중 random sentence 가 성능을 가장 많이 저하시켰는데, 이는 random sentence 가 MS COCO caption 에서 가져온 것이므로 model 이 이미지를 보지 않고 잘못된 caption 에서 정답을 선택할 수 있기 때문이다. </li><li>흥미롭게도, no prompt 는 다른 noisy prompt 보다 성능이 좋았으며, 더 많은 training data 가 주어졌을 때는 hand-crafted prompt 와 유사하거나 더 나은 성능을 보였다. </li></ul><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-34-3aa0b1207b2d115595102ad6420e054f.png" width="633" height="554" class="img_ev3q"></p><p>Flickr30k 에서도 유사한 현상이 관찰되었으며(Fig. 5), no prompt 가 hand-crafted prompt 와 비슷한 성능을 기록했다.</p><p>추가적으로, 저자는 두 가지 다른 target prompt 인 &quot;<code>&lt;text_1&gt; [A]</code>&quot; 와 &quot;<code>[A]</code>&quot; 를 실험하였다. </p><p><img loading="lazy" alt="Figure 6" src="/assets/images/image-35-2c856fbae87a4392612b507f8d4080ee.png" width="625" height="583" class="img_ev3q"></p><ul><li>VQA 에서 target prompt 에 &quot;<code>&lt;text_1&gt;</code>&quot; 을 추가한 것은 MaskedLM 의 target text 형식을 모방하기 위함이며, 동일한 target prompt 를 공유함으로써 model 이 새로운 task 에 빠르게 적응하도록 돕는 효과를 기대하였다. </li><li>Fig. 6 에 따르면, &quot;<code>[A]</code>&quot; 형식은 다른 경우보다 더 큰 분산을 보여주었는데, 이는 &quot;<code>&lt;text_1&gt;</code>&quot; 을 추가하는 것이 model 의 빠른 적응을 돕는다는 것을 시사한다. <ul><li>그러나 training data 가 충분히 많아지는 경우(e.g., 300)에는 두 prompt 모두 유사한 결과를 보였다.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="66-pre-training-objectives">6.6 Pre-training Objectives<a href="#66-pre-training-objectives" class="hash-link" aria-label="Direct link to 6.6 Pre-training Objectives" title="Direct link to 6.6 Pre-training Objectives">​</a></h2><p>저자는 pre-training objective 가 서로 다른 task 에 어떤 영향을 미치는지 분석하였다. 이를 위해 FEWVLM 을 masked language modeling (MaskedLM) 과 prefix language modeling (PrefixLM) 각각으로 pre-training 하였다.</p><p><img loading="lazy" alt="Table 8" src="/assets/images/image-36-58b043dfde9cdf2b9e69cd8e42002aad.png" width="617" height="538" class="img_ev3q"></p><ul><li>Tab. 8 에 따르면, MaskedLM 은 zero-shot 및 few-shot 환경에서 VQA task 성능 향상에 기여하고, PrefixLM 은 captioning task 성능 향상에 기여한다. </li><li>저자는 그 이유를 다음과 같이 추론한다. MaskedLM 은 span 예측을 목표로 하는데, 이는 질문에 대한 정답을 예측하는 VQA 와 유사하다. </li><li>반면, PrefixLM 은 주어진 prefix 이후의 텍스트를 생성하는 방식이며, 이는 captioning task 와 유사하다. 즉, pre-training task 가 downstream task 와 유사할수록 성능 향상에 도움이 된다.</li></ul><p>또한 두 objective 를 모두 사용하여 pre-training 할 경우, 상호 보완적 효과(synergetic effect) 가 발생하여 cross-task generalization 이 향상된다.</p><h1>7 Conclusion</h1><p>본 연구에서는 vision-language task 에서의 few-shot prompt 기반 learner 인 <strong>FEWVLM</strong> 을 제안하였다. 다양한 dataset 에서 FEWVLM 은 baseline 을 능가하였으며, 246× 더 큰 PICa 와 유사한 성능을 보였다. Prompt 는 zero-shot 및 few-shot task 에서 매우 중요한 역할을 하며, 각 pre-training objective 는 서로 다른 few-shot task 에 도움을 준다는 점을 확인하였다. 또한, 더 많은 training data 를 사용할 경우 noisy prompt 의 영향이 크지 않음을 발견하였다.</p><p>향후 연구로는 automatic prompt generation 과 multiple-choice VQA 와 같은 다양한 few-shot task 형식 탐구를 계획하고 있다. 최적의 prompt 를 찾는 과정은 성능 향상을 위해 상당한 엔지니어링 노력을 요구하며, 그 결과 인상적인 성능 향상을 이끌 수 있다. 이러한 방향에 대한 탐구는 향후 연구로 남긴다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">Vision-Language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multimodal">Multimodal</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/few-shot-learning">few-shot learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/prefix-language-modeling">prefix language modeling</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/VQA-IC/Few-shot/2021-10-FEWVLM.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-problem-formulation" class="table-of-contents__link toc-highlight">3.1 Problem Formulation</a></li><li><a href="#32-analysis-questions" class="table-of-contents__link toc-highlight">3.2 Analysis Questions</a></li><li><a href="#33-downstream-tasks-and-datasets" class="table-of-contents__link toc-highlight">3.3 Downstream Tasks and Datasets</a></li><li><a href="#34-evaluation-metrics" class="table-of-contents__link toc-highlight">3.4 Evaluation Metrics</a></li><li><a href="#35-baselines" class="table-of-contents__link toc-highlight">3.5 Baselines</a></li><li><a href="#41-encoder-decoder-vision-language-model" class="table-of-contents__link toc-highlight">4.1 Encoder-decoder Vision-language Model</a></li><li><a href="#42-pre-training-objectives" class="table-of-contents__link toc-highlight">4.2 Pre-training Objectives</a></li><li><a href="#51-prompt-design" class="table-of-contents__link toc-highlight">5.1 Prompt Design</a><ul><li><a href="#511-visual-question-answering" class="table-of-contents__link toc-highlight">5.1.1 Visual Question Answering</a></li><li><a href="#512-captioning" class="table-of-contents__link toc-highlight">5.1.2 Captioning</a></li><li><a href="#513-miniimagenet" class="table-of-contents__link toc-highlight">5.1.3 MiniImageNet</a></li></ul></li><li><a href="#61-experiment-details" class="table-of-contents__link toc-highlight">6.1 Experiment Details</a></li><li><a href="#62-performance-on-zero-shot-learning" class="table-of-contents__link toc-highlight">6.2 Performance on Zero-shot Learning</a></li><li><a href="#63-performance-on-few-shot-learning" class="table-of-contents__link toc-highlight">6.3 Performance on Few-shot Learning</a></li><li><a href="#64-miniimagenet" class="table-of-contents__link toc-highlight">6.4 MiniImageNet</a></li><li><a href="#65-study-of-prompt-design" class="table-of-contents__link toc-highlight">6.5 Study of Prompt Design</a><ul><li><a href="#651-zero-shot-predictions" class="table-of-contents__link toc-highlight">6.5.1 Zero-shot Predictions</a></li><li><a href="#652-few-shot-predictions" class="table-of-contents__link toc-highlight">6.5.2 Few-shot Predictions</a></li></ul></li><li><a href="#66-pre-training-objectives" class="table-of-contents__link toc-highlight">6.6 Pre-training Objectives</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.c7613d59.js"></script>
<script src="/assets/js/main.21e44b4c.js"></script>
</body>
</html>