<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/VQA-IC/Few-shot/2022-10-PNP-VQA">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/PNP-VQA"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/PNP-VQA"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/PNP-VQA" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/PNP-VQA" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.72402260.js" as="script">
<link rel="preload" href="/assets/js/main.c8c47fc5.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Reinforce Learning/DPO/RLHF/Self-RLM">Reinforce Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">CLIP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Foundation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">VQA-IC</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Few-shot</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Multimodal Few-Shot Learning with Frozen Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA">xGQA: Cross-Lingual Visual Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/ClipCap">ClipCap: CLIP Prefix for Image Captioning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP">CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA">Linearly Mapping from Image to Text Space</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/PNP-VQA">Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper">Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Prophet">Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/LAMOC">Zero-shot Visual Question Answering with Language Model Feedback</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA">Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/CoTMT">A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MAPD">Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/ZeroCap">Zero-shot</a></div></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">VQA-IC</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Few-shot</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2210.08773" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2210.08773</a></p><h1>Abstract</h1><p>visual question answering (VQA) 은 vision 과 language reasoning 의 대표적 과제이며, zero-shot setting 에서 특히 challenging 하다. </p><p>본 연구는 zero-shot VQA 를 위한 modular framework 인 <strong>Plug-and-Play VQA (PNP-VQA)</strong> 를 제안한다. </p><p>대부분의 기존 연구가 pretrained language model (PLM) 을 vision modality 에 적응시키기 위해 상당한 양의 추가 adaptation 을 요구하는 것과 달리, PNP-VQA 는 PLM 의 추가 training 을 전혀 요구하지 않는다. 대신 natural language 와 network interpretation 을 pretrained model 들을 연결하는 intermediate representation 으로 사용한다. </p><p>저자는 먼저 question-guided informative image caption 을 생성하고, 이 caption 을 question answering 을 위한 context 로 PLM 에 전달한다. </p><p>end-to-end training baseline 을 능가하며, PNP-VQA 는 zero-shot VQAv2 와 GQA 에서 state-of-the-art 성능을 달성한다. </p><p>11B parameter 환경에서는, PNP-VQA 가 80B-parameter Flamingo model 보다 VQAv2 에서 8.5% 더 높은 성능을 보인다. 738M PLM parameter 환경에서는, PNP-VQA 가 740M parameter FewVLM 대비 GQA 에서 9.1% 향상을 달성한다.</p><h1>1 Introduction</h1><p>최근 몇 년간 PLM 과 training data 규모의 증가를 기반으로, 특히 zero-shot 및 few-shot setting 에서 다양한 natural language reasoning task 의 성능이 전례 없이 향상되었다. 이러한 성공에 영감을 받아, PLM 을 vision-language reasoning task 에 적용하면 zero-shot 성능 역시 향상될 것이라는 자연스러운 기대가 등장한다.</p><p>그러나 vision-language task 에 PLM 을 적용하기 위해서는, 대부분의 기존 방법에서 PLM 을 vision modality 에 맞게 복잡한 adaptation 이 필요하며, 이는 새로운 network component 및 training objective 의 설계를 요구한다. 예를 들어 일부 연구는 PLM 내부에 새 layer 를 추가하여 scratch 부터 training 하거나, frozen PLM 에 question-dependent soft prompt 를 vision encoder 가 출력하도록 training 하거나, vision encoder 와 PLM 에 삽입된 layer 를 함께 training 한다. zero-shot setting 에서는 image captioning 혹은 image-conditioned masked language modeling 등의 vision-language pretraining objective 가 사용된다.</p><p>general-purpose AI 관점에서 보면, architecture 변경이나 추가 training 없이, 단지 large-scale pretrained model (foundation model) 들을 재조합하여 새로운 task 를 수행할 수 있다면 매우 바람직하다. 이러한 시스템은 소수의 foundation model 을 재배치(rewiring) 하는 것만으로 unseen task 에도 동적으로 적응할 수 있다. 하지만 end-to-end training 없이 높은 performance 를 얻는 것은 어려운 문제이다.</p><p>본 논문은 Plug-and-Play VQA (PNP-VQA) 를 제안하며, </p><ul><li>이는 zero-shot visual question answering 을 위해 large pretrained model 들을 <strong>추가 training 없이 결합</strong>하는 framework 이며, zero-shot VQAv2 및 GQA 에서 state-of-the-art 성능을 달성한다. </li><li>vision 과 language modality 를 연결하기 위해 pretrained vision-language model (PVLM) 을 사용하여 visual information 을 textual caption 으로 변환한다. </li><li>question-relevant 한 caption 을 생성하기 위해, network interpretability technique 을 적용하여 question 과 관련된 image patch 를 검출한다. </li><li>그 후 해당 image patch 에 대해 stochastic caption 을 생성한다. 마지막으로 PLM 을 이용해 caption 으로부터 question 에 답하도록 한다.</li></ul><p>cognitive science 및 neuroscience 분야의 연구는 인간의 cognitive system 이 상당히 modular 하다는 것을 시사한다. 예컨대 인간의 low-level cognition 은 빠르고 자율적이며 domain-specific 한 여러 module 로 구성된다는 관점이다. practical 측면에서 modular 설계는 artificial general intelligence 의 각 component 를 독립적으로 교체·업데이트 할 수 있으므로 시스템 유지 및 발전에 유리하다. </p><p>본 논문에서는 PLM 및 PVLM 의 최근 발전과 network interpretability 의 결합을 통해 zero-shot VQA 를 위한 modular design 을 제안한다.</p><p>저자의 기여는 다음과 같다:</p><ul><li>zero-shot VQA 를 위한 training-free modular framework 인 PNP-VQA 를 제안하며, pretrained model 의 발전에 따라 PNP-VQA 역시 공동으로 진화할 수 있는 flexibility 를 가진다.</li><li>natural language 뿐 아니라, pretrained LM 과 VLM 을 연결하는 interface 로 network interpretation 을 활용한다. interpretability technique 을 사용해 question-relevant 정보를 폭넓게 담은 image caption 을 생성하여 accurate QA 를 가능하게 한다.</li><li>다양한 benchmark 에서 state-of-the-art zero-shot VQA 성능을 보여준다. VQAv2 에서 PNP-VQA<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>11B</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{11B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">11B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 extensive end-to-end V-L pretraining 을 적용한 Flamingo<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>80B</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{80B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">80B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 대비 8.5% 향상되며, GQA 에서 PNP-VQA<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>large</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{large}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6222em;vertical-align:-0.2861em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">large</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 는 FewVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>large</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{large}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6222em;vertical-align:-0.2861em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">large</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 대비 9.1% 더 높은 성능을 보인다.</li></ul><h1>2 Related Work</h1><p>large-scale image-text pretraining of neural networks 는 활발한 연구 방향이다. image-conditioned language modeling, masked language modeling, prefix language modeling, image-text matching, image-text contrastive learning 등 다양한 vision-language pretraining task 가 제안되었다. pretraining 이후 일부 model 은 image-text retrieval 및 image captioning 에서 zero-shot capability 를 보인다. 그러나 zero-shot VQA 는 model 의 reasoning ability 에 대한 요구가 높아 여전히 challenging 한 task 로 남아 있다.</p><p>zero-shot VQA 를 위해 PLM 을 adapting 하는 방식은 유망한 성과를 보여 왔다. PLM 에 vision information 을 통합하기 위해, 대부분의 기존 방법은 image-text data 에 대한 추가 vision-language training 을 수행한다. </p><ul><li><em>Frozen</em> 은 vision encoder 를 training 하되 거대한 PLM 은 frozen 상태로 유지하여 question answering 에 필요한 knowledge 를 보존한다. vision encoder 의 output 은 frozen language model 의 prompt 로 사용된다. </li><li><em>FewVLM</em> 은 prefix language modeling 및 masked language modeling objective 로 PLM 을 fine-tune 한다. </li><li><em>VLKD</em> 는 finetuning 동안 CLIP 을 teacher 로 사용하여 PLM 에 multimodal knowledge 를 distill 한다. </li><li><em>Flamingo</em> 는 pretrained vision model 과 PLM 모두에 새로운 layer 를 추가하고, 수십억 개의 image-text pair 에 대해 이 layer 들을 training 한다.</li></ul><p>위 접근들과 달리, </p><ul><li><em>PNP-VQA</em> 는 architectural modification 이나 추가 training 없이 pretrained model 을 직접 활용한다. </li><li>가장 유사한 연구는 <em>PICa</em> 로, image 를 single caption 으로 변환하고 GPT-3 을 사용해 zero-shot VQA 를 수행한다. </li><li>그러나 PNP-VQA 는 question-guided caption 을 multiple 하게 생성하며, encoding 이후 caption 통합(fusion) 을 수행하여 많은 caption 을 효과적으로 활용하고, 그 결과 상당한 성능 향상을 얻는다.</li></ul><p>또 다른 orthogonal 연구 방향은 caption 으로부터 생성된 synthetic VQA example 로 VLM 을 training 하는 것이다. 그러나 PNP-VQA 는 추가 training 을 필요로 하지 않는다.</p><p>natural language 를 서로 다른 model 간 혹은 reasoning 의 여러 step 사이의 intermediate representation 또는 interface 로 사용하는 전략은 최근 부상한 machine learning 접근이다. 이는 오래전 제안된 아이디어이며, PLM 의 확산과 함께 최근 다시 관심을 얻고 있다. </p><ul><li>일부 연구는 natural language description 을 학습하여 image-text matching model 내부의 few-shot classifier 로 활용한다. </li><li>다른 연구는 finetuned PLM 으로 intermediate reasoning step 을 생성하거나, 복잡한 문제를 subproblem description 으로 분해하여 PLM 이 hierarchical 하게 해결하도록 한다. </li><li>일부 연구는 PLM 의 output 과 input 을 chaining 한다. language-conjoined LM 과 VLM 이 captioning 과 retrieval 은 수행할 수 있으나 VQA 에 대해서는 평가되지 않은 연구도 있다.</li></ul><p>이에 비해 PNP-VQA 는 <strong>natural language 와 network interpretation</strong> 을 모두 pretrained model 간 interface 로 사용한다.</p><h1>3 Method</h1><p>Plug-and-Play VQA (PNP-VQA) 의 중심 아이디어는 pretrained language model 과 pretrained vision-language model 사이에 <strong>training 없이 작동하는 interface</strong> 를 구축하는 것이다. </p><p>저자는 natural language image caption 과 network saliency map 이 효과적인 interface 로 작동함을 보인다. 이상적으로, 생성되는 caption 은 image 에 존재하는 정보 전체를 충분히 coverage 하면서, question 과의 관련성도 갖추어야 한다. </p><p>본 연구는 <em>saliency map-based interpretability technique</em> 을 사용해 question 과 가장 관련된 image patch 를 식별하고, 해당 patch 들로부터 caption 을 생성함으로써 relevance 를 확보한다. 또한 relevant image patch 의 random sampling 및 caption 생성 과정의 <em>token-level stochasticity</em> 를 주입하여 coverage 를 높인다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-116-3f67e1fbb8dc0daaf8264a0df6c5cfe1.png" width="1226" height="705" class="img_ev3q"></p><p>전체 시스템 architecture(Fig. 1) 는 세 가지 module 로 구성된다:</p><ol><li>question 에 대해 relevant 한 image patch 를 식별하는 <strong>image-question matching module</strong>,</li><li>선택된 image patch 집합으로부터 diverse caption 을 생성하는 <strong>image captioning module</strong>,</li><li>생성된 caption 과 question 을 입력받아 answer 를 출력하는 <strong>question answering module</strong>.</li></ol><p>본 절에서는 세 module 을 상세히 소개한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-matching-image-patches-and-questions">3.1 Matching Image Patches and Questions<a href="#31-matching-image-patches-and-questions" class="hash-link" aria-label="Direct link to 3.1 Matching Image Patches and Questions" title="Direct link to 3.1 Matching Image Patches and Questions">​</a></h2><p>image 는 풍부한 정보를 포함하지만, 주어진 question 은 특정 object 또는 region 에만 초점을 둘 가능성이 크다. 따라서 PNP-VQA 가 무목적의 generic caption 이 아니라, <em>question 에 실제로 관련된 region 을 묘사하는 caption</em> 을 생성하도록 유도한다.</p><p>이를 위해 저자는 large-scale pretrained vision-language model 인 BLIP 을 활용한다. </p><ul><li>BLIP 은 image <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span></span> 와 text <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span> 간 similarity score <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">m</mi></mrow><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{sim}(v, t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathrm">sim</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span> 를 출력하는 network branch 를 포함하며, 이를 <strong>Image-grounded Text Encoder (ITE)</strong> 라 한다. </li><li>ITE 는 vision transformer 를 이용해 image 를 encoding 하고, textual encoder 가 cross-attention 을 통해 image feature 에 attend 한다. </li><li>image encoder 의 입력으로 image 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> 개 patch 로 균등 분할된다.</li></ul><p>question 과 관련된 image patch 를 식별하기 위해, image <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span></span> 와 question <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span> 를 ITE 에 입력하고, GradCAM 기반 feature-attribution interpretability 기법을 적용한다. 이 기법은 gradient weight 와 cross-attention map 을 결합하여 중요한 patch 를 도출한다.</p><p>image patch feature 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>K</mi><mo>×</mo><msub><mi>D</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">X \in \mathbb{R}^{K \times D_v}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.0278em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, textual feature 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>M</mi><mo>×</mo><msub><mi>D</mi><mi>t</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">Y \in \mathbb{R}^{M \times D_t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal" style="margin-right:0.22222em">Y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em"><span style="top:-2.357em;margin-left:-0.0278em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 라 하자. 모든 cross-attention head 에 대해 parameter matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>D</mi><mi>t</mi></msub><mo>×</mo><msub><mi>D</mi><mi>t</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W_Q \in \mathbb{R}^{D_t \times D_t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em"><span style="top:-2.357em;margin-left:-0.0278em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em"><span style="top:-2.357em;margin-left:-0.0278em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>D</mi><mi>v</mi></msub><mo>×</mo><msub><mi>D</mi><mi>t</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W_K \in \mathbb{R}^{D_v \times D_t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.0278em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em"><span style="top:-2.357em;margin-left:-0.0278em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> 가 존재한다. </p><p>cross-attention score <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>M</mi><mo>×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in \mathbb{R}^{M \times K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span></span></span></span></span></span></span></span></span></span> 는 다음과 같이 계산된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>A</mi><mo>=</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Y</mi><msub><mi>W</mi><mi>Q</mi></msub><msubsup><mi>W</mi><mi>K</mi><mi mathvariant="normal">⊤</mi></msubsup><msup><mi>X</mi><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><msub><mi>D</mi><mi>t</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">A = \mathrm{softmax} \left( \frac{ Y W_Q W_K^\top X^\top }{ \sqrt{D_t} } \right). \tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4761em;vertical-align:-0.95em"></span><span class="mord"><span class="mord mathrm">softmax</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.2583em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8517em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.8117em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1883em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em">Y</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.4247em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2753em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">.</span></span><span class="tag"><span class="strut" style="height:2.4761em;vertical-align:-0.95em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span> 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span> 번째 row 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span> 번째 textual token 이 각 image patch 에 할당한 attention 양을 의미한다. </li><li>특정 layer 에서 similarity score 의 gradient <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">m</mi></mrow><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi mathvariant="normal">∂</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">\partial \mathrm{sim}(v,t) / \partial A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord" style="margin-right:0.05556em">∂</span><span class="mord"><span class="mord mathrm">sim</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mord">/</span><span class="mord" style="margin-right:0.05556em">∂</span><span class="mord mathnormal">A</span></span></span></span></span> 를 계산하고, 이를 cross-attention score 와 element-wise 로 곱한다. </li></ul><p>image patch <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span> 의 relevance 는 모든 attention head <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span></span></span> 및 textual token <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span></span> 에 대해 다음과 같이 평균한다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">l</mi></mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>H</mi></mfrac><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>H</mi></munderover><mi>max</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mn>0</mn><mo separator="true">,</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">m</mi></mrow><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msubsup><mi>A</mi><mrow><mi>j</mi><mi>i</mi></mrow><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></mfrac><mo fence="true">)</mo></mrow><msubsup><mi>A</mi><mrow><mi>j</mi><mi>i</mi></mrow><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\mathrm{rel}(i)= \frac{1}{H} \sum_{j=1}^{M} \sum_{h=1}^{H} \max\left( 0, \frac{\partial \mathrm{sim}(v,t)}{\partial A^{(h)}_{ji}} \right) A^{(h)}_{ji}, \tag{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathrm">rel</span></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.2421em;vertical-align:-1.4138em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em">H</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8479em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">max</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size4">(</span></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.11em"><span class="pstrut" style="height:3.0448em"></span><span class="mord"><span class="mord" style="margin-right:0.05556em">∂</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ji</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">h</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em"><span></span></span></span></span></span></span></span></span><span style="top:-3.2748em"><span class="pstrut" style="height:3.0448em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.7218em"><span class="pstrut" style="height:3.0448em"></span><span class="mord"><span class="mord" style="margin-right:0.05556em">∂</span><span class="mord"><span class="mord mathrm">sim</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3478em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size4">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ji</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">h</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em"><span></span></span></span></span></span></span><span class="mpunct">,</span></span><span class="tag"><span class="strut" style="height:3.2421em;vertical-align:-1.4138em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></div><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(h)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mclose">)</span></span></span></span></span> 는 attention head index 이다.</li><li>caption 을 생성할 때마다, relevance 에 비례하는 확률로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">K_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 개의 image patch 를 sampling 하여 captioning module 에 전달한다.</li></ul><p>이 기술의 motivation 은 다음과 같다. </p><ul><li>attention matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">A</span></span></span></span></span> 자체가 patch importance 를 나타낼 수 있지만, attention head 간 redundancy 가 크고 많은 head 가 제거되어도 성능 손실이 적다는 연구 결과가 있다. <ul><li>이는 일부 attention score 가 informative 하지 않음을 의미한다. </li></ul></li><li>GradCAM 에서 영감을 얻어, 저자는 gradient 를 곱하여 실제 similarity 증가에 기여하는 informative 한 attention score 만 강조한다.</li></ul><p>Fig. 2 는 generic caption 과 question-guided caption, 그리고 relevance heatmap 예시를 보여준다. question-guided caption 이 더 많은 relevant 정보를 포함하여 올바른 answer 생성에 도움이 됨을 확인할 수 있다.</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-117-7a7aff3cf6e015c58bb2c4f58deaa2d2.png" width="1217" height="928" class="img_ev3q"></p><p>Table 1 은 서로 다른 patch selection 방법에 따른 zero-shot VQA 성능 변화를 정량적으로 분석한다. </p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-118-083767c575fde23d45cfd22d55d8a8fa.png" width="1518" height="524" class="img_ev3q"></p><ul><li>question-guided patch sampling 은 전체 patch 를 사용한 generic captioning 및 random patch sampling 을 크게 능가하며, 특히 caption 수가 많은 경우 성능 향상이 더욱 두드러진다. </li><li>100 개의 question-guided caption 은 MS COCO 의 human-written caption 5 개보다 VQAv2 에서 5.2%, OK-VQA 에서 6.0% 더 높은 성능을 보이며, 제안 기법의 장점을 보여준다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-informative-image-captioning">3.2 Informative Image Captioning<a href="#32-informative-image-captioning" class="hash-link" aria-label="Direct link to 3.2 Informative Image Captioning" title="Direct link to 3.2 Informative Image Captioning">​</a></h2><p>relevant image region 이 주어지더라도, 이를 서술하는 방식은 여전히 여러 가지가 존재할 수 있다. 어떤 description 은 question 에 대한 정답을 포함할 수 있지만, 다른 description 은 그렇지 않을 수 있다. 사전에 answer 를 식별할 수 없으므로, 저자는 가능한 answer space 를 폭넓게 coverage 하기 위해 다양성이 최대화된 caption 을 생성하는 것을 목표로 한다.</p><p>이를 위해 BLIP 의 image captioning network branch 를 사용하고, beam search 대신 <strong>stochastic top-k sampling</strong> 을 적용한다. </p><ul><li>beam search 는 dull 하고 반복적인 caption 을 생성하는 것으로 알려져 있기 때문이다. </li><li>network input 은 relevance 에 따라 sampling 된 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">K_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 개의 image patch 이며(§3.1), 저자는 text decoder 의 입력에 “a picture of ” 라는 짧은 prompt 를 prepend 한다. </li><li>이러한 방식으로 <em>한 image 당 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> captions</em> 를 반복 생성하여 caption 의 다양성과 visual content coverage 를 확보한다.</li><li>또한 repetition 을 방지하기 위해, 새로 생성된 caption 이 이전에 생성된 caption 의 exact substring 으로 포함될 경우 해당 caption 은 폐기한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-answering-the-question">3.3 Answering the Question<a href="#33-answering-the-question" class="hash-link" aria-label="Direct link to 3.3 Answering the Question" title="Direct link to 3.3 Answering the Question">​</a></h2><p>question-answering encoder-decoder model 은 text-only data 로 pretraining 되어 있으며 text 입력만 처리할 수 있다. 따라서 question 과 생성된 caption 들을 함께 model 의 입력으로 제공한다. §3.2 에서 언급했듯 image captioning module 은 다양한 caption 을 여러 개 생성하므로, 이를 효율적으로 처리하기 위해 <strong>Fusion-in-Decoder (FiD)</strong> 전략을 채택한다.</p><p>FiD 전략은 Fig. 3 에 FiE(Fusion-in-Encoder) 와 비교하여 설명된다.</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-119-ec8b172a411d6458fad0815317d51578.png" width="1526" height="646" class="img_ev3q"></p><ul><li><strong>FiE 방식</strong>: question 과 모든 caption 을 하나의 long paragraph 로 concatenation 하여 encoder 입력으로 사용한다.</li><li><strong>FiD 방식</strong>: question 과 각 caption 을 <em>별도로 encoding</em> 하고, encoder 가 출력한 token representation 들을 모두 concat 하여 decoder 의 cross-attention 입력으로 사용한다.</li></ul><p>self-attention 의 시간 복잡도는 입력 길이에 대해 제곱으로 증가하는 반면, <em>cross-attention 은 encoder output 길이에 대해 선형</em>으로 증가한다. 따라서 FiD 는 FiE 보다 훨씬 효율적이다. 또한 FiE 는 encoder 의 positional encoding 으로 인해 maximum input length 제약을 받지만, FiD 는 이러한 제약이 없다. 그 결과 PNP-VQA 는 FiD 를 통해 훨씬 많은 caption 의 이점을 활용할 수 있다.</p><p>Fig. 4 는 caption 수에 따른 FiD 와 FiE 의 성능 변화를 나타낸다. </p><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-120-9d7b701b4086c1ceabd302cfe4bf6d06.png" width="1504" height="474" class="img_ev3q"></p><p>caption 수가 증가하면 두 방법 모두 초기에는 성능이 향상되지만, FiE 는 encoder 입력 길이 한계를 넘어서는 약 40 caption 부근에서 성능 향상이 정체된다. 반면 FiD 는 caption 수 증가에 따라 성능이 계속 상승한다.</p><h1>4 Experiments</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-datasets-and-evaluation">4.1 Datasets and Evaluation<a href="#41-datasets-and-evaluation" class="hash-link" aria-label="Direct link to 4.1 Datasets and Evaluation" title="Direct link to 4.1 Datasets and Evaluation">​</a></h2><p>저자는 여러 zero-shot VQA benchmark 를 사용한다. 여기에는 VQAv2 의 validation set (214,354 questions) 및 test-dev set (107,394 questions), OK-VQA 의 test set (5,046 questions), GQA-balanced 의 test-dev set (12,578 questions) 이 포함된다. 일부 최근 연구들은 VQAv2 validation set 에 대해서만 평가를 수행하므로, 본 연구 역시 해당 set 을 포함한다.</p><p>answer 는 open-ended generation 방식으로 생성하며, exact matching 기반으로 평가한다. VQAv2 및 OK-VQA 에 대해서는 multiple ground truth answer 를 반영하기 위해 soft-accuracy 를 보고하며, GQA 에 대해서는 standard accuracy 를 보고한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-implementation-details">4.2 Implementation Details<a href="#42-implementation-details" class="hash-link" aria-label="Direct link to 4.2 Implementation Details" title="Direct link to 4.2 Implementation Details">​</a></h2><p>image-question matching module 및 image captioning module 을 구축하기 위해 저자는 BLIP 을 사용하며, 이는 129M image-text pair 로 pretrained 된 ViT-L/16 architecture 기반이다. 공개된 BLIP-ITM 과 BLIP-Caption model 은 COCO Captions 2017 train split 에 대해 추가로 finetune 되어 있는데, 이는 VQAv2 및 OK-VQA 와 부분적 overlap 을 가진다. </p><p>data leak 을 방지하기 위해, 저자는 COCO Captions 2014 train split 에 대해 finetune 한다. 이러한 조치는 공개된 BLIP 보다 <em>추가 training 을 줄이는 방식</em>임을 강조한다. </p><p>question answering module 로는 diverse textual QA dataset 에 대해 training 된 UnifiedQAv2 를 사용한다. UnifiedQAv2 는 training 동안 visual modality 를 전혀 보지 않았으므로, VQA dataset 과의 overlap 은 존재하지 않는다.</p><p>별도 언급이 없는 한, question 당 총 100 개의 caption 을 사용한다. GradCAM 을 위해 ITE network 의 8번째 cross-attention layer 를 선택한다. caption 생성 시 patch sampling 크기는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mn>0</mn></msub><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">K_0 = 20</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">20</span></span></span></span></span> 이며, top-k decoding 에는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding="application/x-tex">k=50</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">50</span></span></span></span></span> 을 사용한다.</p><p>VQAv2 및 OK-VQA 에 대해서는 FiD 를 적용하며 question 을 caption 하나씩과 함께 encoding 한다. 하지만 GQA 는 compositional visual reasoning 이 필요하므로 더 많은 contextual information 이 유리하여, question 을 5 개 caption 그룹과 함께 encoding 한다.</p><p>실험은 8개의 Nvidia A100 GPU 위에서 LAVIS framework 를 사용하여 수행하였다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-comparison-with-state-of-the-arts">4.3 Comparison with State of the Arts<a href="#43-comparison-with-state-of-the-arts" class="hash-link" aria-label="Direct link to 4.3 Comparison with State of the Arts" title="Direct link to 4.3 Comparison with State of the Arts">​</a></h2><p>저자는 zero-shot VQA 를 open-ended answer generation 문제로 정식화하는 state-of-the-art 방법들과 비교한다. 비교는 pretrained network 를 어떻게 결합(conjoin)하는지를 기준으로 분류된다.</p><ol><li>Group 1: VE → PLM 으로 결합 + end-to-end VL training</li></ol><p>이 그룹의 방법들은 vision encoder(VE) 가 image 를 dense matrix 로 embedding 하고, 이를 PLM 에 전달한다. 이후 image captioning 등의 task 로 end-to-end vision-language(VL) training 을 수행한다.</p><p>이 그룹에는 VL-T5no-vqa, FewVLM, VLKD, Flamingo, Frozen 등이 포함된다.</p><ul><li>VL-T5no-vqa, FewVLM: VE 를 freeze 하고 PLM 을 finetune</li><li>Frozen: PLM 을 freeze 하고 VE 를 training</li><li>VLKD: PLM 과 VE 일부를 함께 finetune</li><li>Flamingo: VE 와 PLM 모두에 새 layer 를 추가하고, 대규모 image-text pair 로 training</li></ul><ol start="2"><li>Group 2: caption 을 intermediate representation 으로 사용 (joint training 없음)</li></ol><p>이 그룹은 두 foundation model 을 joint training 하지 않으며, image 를 caption 형태의 natural language 로 변환하여 PLM 에 전달한다. </p><p>이 그룹에는 PICa 와 본 연구의 PNP-VQA 가 속한다.</p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-121-63b8ce6a4b1f70086b47dedbc080b256.png" width="1523" height="830" class="img_ev3q"></p><p>Tab. 2 결과를 정리하면 다음과 같다.</p><ul><li>PNP-VQA 는 VQAv2 와 GQA 에서 기존 모든 방법을 큰 폭으로 능가한다.<ul><li>VQAv2 test-dev 기준, PNP-VQA<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>11B</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{11B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">11B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 두 번째로 좋은 Flamingo<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>80B</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{80B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">80B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 보다 8.5% 더 높다.</li><li>PNP-VQA3B 는 Flamingo<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>80B</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{80B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">80B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 대비 7.2% 더 높으며, 유사 규모의 Flamingo3B 보다 14.3% 더 높다.</li><li>GQA 기준, PNP-VQA<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>large</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{large}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6222em;vertical-align:-0.2861em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">large</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 는 FewVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>large</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{large}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6222em;vertical-align:-0.2861em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">large</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 보다 <strong>9.1%</strong> 높으며, end-to-end training 없이 달성한 결과이다.</li></ul></li><li>OK-VQA 에 대해서는 Flamingo 가 PNP-VQA 보다 성능이 높다. <ul><li>OK-VQA 는 image 에 존재하지 않는 external knowledge 를 요구하며, 단순히 caption 품질이 좋아도 해결할 수 없는 경우가 많다. </li></ul></li><li>저자는 Flamingo 의 massive VL pretraining 이 image 와 knowledge concept 간 mapping 을 형성하여 OK-VQA 성능을 높였을 것이라고 가정한다. <ul><li>그럼에도 PNP-VQA 는 Flamingo 의 거대 training data 없이도 다른 모든 baseline 보다 OK-VQA 에서 우수하다.</li></ul></li><li>175B parameter 를 가진 PICa 대비하여, PNP-VQA<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>11B</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{11B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">11B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 18.2% 개선을 보인다.<ul><li>이 결과는 VL training 없이 pure language model 기반 zero-shot VQA 가 얼마나 어려운지 보여준다. </li></ul></li><li>PICa 는 PLM 규모가 175B 임에도 FewVLM<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mtext>large</mtext></msub></mrow><annotation encoding="application/x-tex">_\text{large}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6222em;vertical-align:-0.2861em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">large</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 와 유사한 성능을 보이며, 후자는 VL data 로 finetune 되었다. <ul><li>반면 billion-scale PLM 을 finetune 하는 것은 computational cost 가 크며 catastrophic forgetting 위험이 존재한다.</li></ul></li></ul><p>PNP-VQA 는 이러한 문제를 피하면서도, 아무 training 없이 billion-scale PLM 을 사용해 VQA 를 수행할 수 있음을 보여주는 새로운 패러다임임이 실험을 통해 확인된다.</p><h1>5 Analysis</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-are-pnp-vqa-captions-informative">5.1 Are PNP-VQA captions informative?<a href="#51-are-pnp-vqa-captions-informative" class="hash-link" aria-label="Direct link to 5.1 Are PNP-VQA captions informative?" title="Direct link to 5.1 Are PNP-VQA captions informative?">​</a></h2><p>직관적으로 caption 이 correct answer 를 포함한다면, QA model 이 정답을 생성할 가능성이 높아진다. caption 의 utility 를 측정하기 위해 저자는 <strong>answer hit rate (AHR)</strong> 를 계산한다. 이는 적어도 하나의 caption 이 ground-truth answer 를 verbatim 형태로 포함하는 question 의 비율이다. yes/no question 은 “yes”, “no” 가 caption 에 거의 등장하지 않으므로 제외한다.</p><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-122-50c90a69b21d5758dff36e23e539680f.png" width="1511" height="606" class="img_ev3q"></p><ul><li>Fig. 5(a) 는 VQAv2 validation set 에 대해, 세 가지 image patch sampling 방식(질문 기반 sampling, uniform random sampling, 전체 patch 사용)별로 AHR 과 VQA accuracy 사이의 correlation 을 보여준다. <ul><li>각 sampling 방식 내에서, AHR 이 증가할수록 VQA accuracy 또한 증가한다. </li><li>이는 caption 이 answer 를 포함할수록 correct answer 생성이 용이하다는 저자의 가설을 뒷받침한다.</li></ul></li><li>AHR 은 answer accuracy 에 영향을 미치는 모든 요소를 포착하지는 못한다(e.g., 문장 내 answer 의 위치, 등장 횟수 등). 그러나 caption 의 정보 품질을 평가하는 데 쉽고 유용한 지표를 제공한다.</li><li>Fig. 5(b) 는 caption 수 증가에 따른 AHR 변화를 보여준다. <ul><li>세 방식 중 question-guided sampling 이 가장 높은 AHR 을 제공한다. 따라서 PNP-VQA 의 우수한 성능은 question-guided caption 이 실제로 correct answer 를 포함하는 비율이 높기 때문이라는 점을 부분적으로 설명할 수 있다. </li><li>caption 수가 20 에서 100 으로 증가할 때, question-guided AHR 은 71.8%에서 84.0%로 상승한다. <ul><li>이는 PNP-VQA 가 많은 caption 을 활용할 수 있게 하는 Fusion-in-Decoder 의 장점을 보여준다.</li></ul></li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-how-sensitive-is-pnp-vqa-to-the-caption-decoding-method">5.2 How sensitive is PNP-VQA to the caption decoding method?<a href="#52-how-sensitive-is-pnp-vqa-to-the-caption-decoding-method" class="hash-link" aria-label="Direct link to 5.2 How sensitive is PNP-VQA to the caption decoding method?" title="Direct link to 5.2 How sensitive is PNP-VQA to the caption decoding method?">​</a></h2><p>caption content 은 PNP-VQA 성능에 핵심적이므로, caption decoding 방법의 민감도를 분석한다. 저자는 네 가지 decoding 방법을 비교한다: deterministic beam search 와 세 가지 stochastic 방법–temperature sampling, nucleus sampling, top-k sampling. 각 방법으로 100 개 caption 을 생성하고 Tab. 3 에 결과를 제시한다.</p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-123-e7eb781390fd8b9f3b2004ffa35d5735.png" width="748" height="520" class="img_ev3q"></p><ul><li>stochastic decoding 방법들 사이에서는 PNP-VQA 성능이 유사하지만, beam search 는 명확한 성능 저하를 보인다. </li><li>분석 결과 beam search 는 반복적이고 다양성이 부족한 caption 을 생성하여 image 의 다양한 측면을 충분히 cover 하지 못하는 것으로 나타났다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="53-can-pnp-vqa-work-with-other-textual-qa-models">5.3 Can PNP-VQA work with other textual QA models?<a href="#53-can-pnp-vqa-work-with-other-textual-qa-models" class="hash-link" aria-label="Direct link to 5.3 Can PNP-VQA work with other textual QA models?" title="Direct link to 5.3 Can PNP-VQA work with other textual QA models?">​</a></h2><p>저자는 question answering module 로 두 가지 PLM 을 추가로 실험한다: T0 및 GPT-J.</p><ul><li><strong>T0</strong>: multi-task 기반으로 사전학습된 encoder-decoder model</li><li><strong>GPT-J</strong>: large-scale text corpus 로 사전학습된 decoder-only model</li></ul><p><img loading="lazy" alt="Table 4" src="/assets/images/image-124-84ea3e5278a619fe966f02d30f111bc9.png" width="753" height="460" class="img_ev3q"></p><ul><li>Tab. 4 결과를 보면, UnifiedQAv2 가 VQA task 에서 T0 및 GPT-J 보다 우수한 성능을 보인다. <ul><li>이는 UnifiedQAv2 가 task-specific QA model 로서 textual QA 성능이 뛰어나기 때문이라고 저자는 해석한다. </li><li>결과적으로 zero-shot VQA 에서는 PLM 선택이 매우 중요함을 시사한다. </li></ul></li><li>또한 PNP-VQA 의 modular 한 설계는 향후 더 우수한 PLM 이 등장할 때 이를 쉽게 도입할 수 있는 여지를 남긴다.</li></ul><h1>6 Conclusion</h1><p>저자는 zero-shot VQA 를 위해 off-the-shelf pretrained model 을 결합하는 <strong>zero-training framework</strong> 인 PNP-VQA 를 제안한다. </p><p>PNP-VQA 는 image-question matching module 로 question 과 관련된 image patch 를 식별하고, image captioning module 이 question-guided caption 을 생성하며, question answering module 이 이를 기반으로 answer 를 생성한다. </p><p>PNP-VQA 는 여러 VQA benchmark 에서 state-of-the-art 성능을 달성한다. 본 연구가 vision-language task 를 해결하기 위한 flexible 하고 modular 한 AI system 개발에 영감을 제공하기를 기대한다.</p><h1>7 Limitations</h1><p>PNP-VQA 의 장점과 한계는 모두 zero-training modular system 설계에서 비롯된다. PNP-VQA 는 pretrained model 의 강점을 활용하지만, 동시에 해당 model 들의 bias 를 그대로 물려받는다. </p><p>training 이 없기 때문에 효율적이지만, multi-step 과정으로 인해 inference cost 가 증가한다. 그럼에도 저자는 PNP-VQA 의 장점이 한계를 압도한다고 판단하며, pretrained model 의 bias 완화 및 inference 속도 개선을 위한 후속 연구를 환영한다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">Vision-Language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multimodal">Multimodal</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vqa">VQA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/zero-shot">Zero-shot</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/plug-and-play">Plug-and-Play</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/VQA-IC/Few-shot/2022-10-PNP-VQA.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Linearly Mapping from Image to Text Space</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-matching-image-patches-and-questions" class="table-of-contents__link toc-highlight">3.1 Matching Image Patches and Questions</a></li><li><a href="#32-informative-image-captioning" class="table-of-contents__link toc-highlight">3.2 Informative Image Captioning</a></li><li><a href="#33-answering-the-question" class="table-of-contents__link toc-highlight">3.3 Answering the Question</a></li><li><a href="#41-datasets-and-evaluation" class="table-of-contents__link toc-highlight">4.1 Datasets and Evaluation</a></li><li><a href="#42-implementation-details" class="table-of-contents__link toc-highlight">4.2 Implementation Details</a></li><li><a href="#43-comparison-with-state-of-the-arts" class="table-of-contents__link toc-highlight">4.3 Comparison with State of the Arts</a></li><li><a href="#51-are-pnp-vqa-captions-informative" class="table-of-contents__link toc-highlight">5.1 Are PNP-VQA captions informative?</a></li><li><a href="#52-how-sensitive-is-pnp-vqa-to-the-caption-decoding-method" class="table-of-contents__link toc-highlight">5.2 How sensitive is PNP-VQA to the caption decoding method?</a></li><li><a href="#53-can-pnp-vqa-work-with-other-textual-qa-models" class="table-of-contents__link toc-highlight">5.3 Can PNP-VQA work with other textual QA models?</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.72402260.js"></script>
<script src="/assets/js/main.c8c47fc5.js"></script>
</body>
</html>