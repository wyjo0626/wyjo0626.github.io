<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/VQA-IC/Few-shot/2021-09-xGQA">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">xGQA: Cross-Lingual Visual Question Answering | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="xGQA: Cross-Lingual Visual Question Answering | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.335bb705.js" as="script">
<link rel="preload" href="/assets/js/main.58fe3837.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">CLIP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Foundation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">VQA-IC</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Few-shot</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Multimodal Few-Shot Learning with Frozen Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA">xGQA: Cross-Lingual Visual Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP">CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA">Linearly Mapping from Image to Text Space</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper">Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA">Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/CoTMT">A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MAPD">Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering</a></li></ul></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">VQA-IC</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Few-shot</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">xGQA: Cross-Lingual Visual Question Answering</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>xGQA: Cross-Lingual Visual Question Answering</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2109.06082" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2109.06082</a></p><h1>Abstract</h1><p>최근 multimodal vision 과 language modeling 의 발전은 주로 multilingual multimodal dataset 의 부족 때문에 대부분 English 언어에 집중되어 왔다. 이 연구에서는 이러한 격차를 해소하기 위해 visual question answering task 를 위한 새로운 multilingual evaluation benchmark 인 <strong>xGQA</strong> 를 제안한다. </p><ul><li>저자는 기존 English GQA dataset 을 7 개의 유형적으로 다양한 언어로 확장하여, cross-lingual visual question answering 에서 중요한 도전 과제를 탐지하고 분석할 수 있게 한다. </li><li>또한 multimodal transformer 기반 model 을 multilingual 로 확장하기 위한 adapter 기반 접근법, 그리고 그 반대로 multilingual model 을 multimodal 로 확장하기 위한 접근법을 제안한다. </li><li>제안된 방법은 zero-shot cross-lingual setting 에서 현재 SOTA multilingual multimodal model (e.g., M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P) 보다 우수한 성능을 보이지만, 전체적으로 accuracy 는 여전히 낮다. </li><li>target language 에서 약 38 accuracy point 의 성능 저하는 이 task 에서 zero-shot cross-lingual transfer 의 어려움을 보여준다. </li><li>이러한 결과는 multimodal model 의 단순한 cross-lingual transfer 가 잠재적인 multilingual multimodal misalignment 를 초래한다는 점을 시사하며, vision 과 multilingual language modeling 을 위한 더 정교한 방법이 필요함을 보여준다.</li></ul><h1>1 Introduction</h1><p>Transformer-based architecture 는 NLP 와 CV 분야에서 널리 사용되며 뛰어난 task 성능을 보여준다. 여러 modality 를 위한 공통 architecture 는 정보의 효과적인 fusion 가능성을 열어주었고, image captioning, phrase grounding, visual question answering, referring expression comprehension, image-text retrieval 과 같은 다양한 multimodal task 에서 인상적인 성능 향상을 이끌어냈다. 그러나 이러한 발전은 대부분 English 언어에 국한되어 있으며, 주요 multimodal dataset 이 English text 로만 구성되어 있기 때문이다. Multilingual evaluation benchmark 의 부족으로 인해 이 문제를 다루는 model 개발은 제한적이었다.</p><p>이 격차를 해소하기 위해, 저자는 visual question answering task 를 위한 multilingual evaluation benchmark 인 <strong>xGQA</strong> 를 제안하며, monolingual English 전용 GQA dataset 을 확장한다. xGQA 를 위해 저자는 balanced GQA test-dev set 을 7 개 언어 계열에서 온 7 개 언어로 수작업 번역 및 적응하였으며, 이는 5 개의 서로 다른 script 를 포함한다 (Fig. 1 과 Tab. 1 참조). </p><p>추가로, target language 에서 소수의 예제만 활용하는 cross-lingual few-shot learning experiment 를 위한 새로운 고정 data split 도 제공한다.</p><p>Pretraining 은 (i) 고자원 언어의 경우 계산 비용이 매우 크고 (ii) multilingual multimodal resource 의 양이 제한적이기 때문에, 저자는 multilingual multimodal model 을 구축하기 위한 추가 baseline 으로 계산 효율적인 adapter 기반 접근법을 제안한다. 즉, English text 로만 pretraining 된 multimodal model 을 multilingual 로 확장하고, 반대로 multilingual model 을 multimodal 로 확장한다. 이를 위해 저자는 Artetxe et al. 과 Pfeiffer et al. 의 접근을 따르며, monolingual 및 multilingual model 을 새로운 언어와 script 로 확장하기 위해 새로운 tokenizer 와 word-embedding matrix, target language 용 adapter 를 학습한다. 이어서 해당 multilingual multimodal adapter 기반 model 을 target task 로 transfer 하기 위해, 저자는 modality 별 adapter weight 를 사용하는 새로운 <strong>modality-specific split architecture</strong> 를 제안한다 (Fig. 2 참조).</p><p>결과적으로 제안된 adapter 기반 architecture 는 최근 SOTA multilingual multimodal pretrained model 인 M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 를 zero-shot cross-lingual setting 에서 능가한다. 그러나 전반적인 zero-shot transfer 성능은 여전히 낮으며, target language 전반에서 평균 약 38 accuracy point 의 성능 저하가 발생한다. Few-shot setup 에서 target language 예제를 소수 사용하면 모든 접근법에서 성능이 크게 향상되지만, cross-lingual transfer 성능은 source language 성능에 비해 여전히 상당히 낮다. 이는 질문이 template 기반이며 평균 8.5 개 단어만 포함하는 비교적 단순한 질문임에도 불구하고, 해당 task 의 본질적인 어려움을 보여준다 (Fig. 1).</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-75-94e2bbd61d3c6c64115a9afcabced83e.png" width="761" height="766" class="img_ev3q"></p><p><strong>Contributions</strong></p><ol><li>cross-lingual visual question answering 을 위한 최초의 evaluation benchmark 를 제안하며, 7 개의 다양한 target language 를 포함한다.</li><li>multilingual multimodal model 생성을 위한 새로운 adapter 기반 접근법을 제안한다.</li><li>SOTA model 과 새로운 multilingual multimodal model 을 zero-shot 과 few-shot learning setup 에서 체계적으로 benchmark 하여 task 의 어려움을 입증하고 향후 연구를 위한 강력한 reference point 를 제공한다.</li><li>다양한 접근법을 철저히 분석하여 model failure 를 유발하는 요소와 question type 을 밝혀내고, 이 영역에서의 향후 연구 필요성을 다시금 강조한다.</li></ol><h1>2 Background and Related Work</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="multilingual-language-models">Multilingual Language Models.<a href="#multilingual-language-models" class="hash-link" aria-label="Direct link to Multilingual Language Models." title="Direct link to Multilingual Language Models.">​</a></h4><p>Pretrained multilingual transformer 기반 LM 인 mBERT 와 XLM-R 은 각각 monolingual counterpart 인 BERT 와 RoBERTa 와 동일한 pretraining regime 을 따른다. 이들은 100 개 이상의 언어가 포함된 concatenated text corpus 에 대해 self-supervised masked language modelling (MLM) objective 로 pretraining 된다. Text 는 WordPiece, SentencePiece, 혹은 BytePair encoding 으로 tokenization 된다. 이러한 multilingual model 은 직접적인 cross-lingual supervision (e.g., parallel data, translation dictionary) 없이도 cross-lingual task 에서 놀라운 성능을 보이는 것으로 나타났다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="vision-and-language-models">Vision and Language Models.<a href="#vision-and-language-models" class="hash-link" aria-label="Direct link to Vision and Language Models." title="Direct link to Vision and Language Models.">​</a></h4><p>대부분의 transformer 기반 multimodal model 은 text token 과 image region feature 를 함께 인코딩한다. 이미지는 Faster R-CNN 과 같은 object detection model 을 이용하여 region of interest (RoI) feature 를 추출한다. 추출된 image region feature 는 affine layer 를 통과하며 multimodal transformer 의 joint embedding space 로 projection 된다. RoI 의 bounding box 좌표는 visual feature 의 positional embedding 으로 작동하며, 이 또한 affine transformation 을 거쳐 image region representation 과 결합된다. Position-aware image region embedding 은 transformer 로 전달되고, multi-head attention 은 각 layer 에서 모든 text 와 image 입력에 대해 attention 을 수행하며 joint representation 을 학습한다.</p><p>Kamath et al. 은 object detector 를 black-box 로 사전 feature 추출에 사용하는 대신, 이를 multimodal transformer architecture 의 중심 요소로 통합하였다. Object detector 를 multimodal transformer 와 함께 end-to-end 로 학습하면 더 큰 유연성과 표현력을 확보할 수 있다. Multimodal transformer 기반 model 은 MLM 과 유사하게 self-supervised objective 로 학습되며, masked feature regression, masked object detection, masked attribute detection, cross-modality matching 과 같은 contrastive loss 가 사용된다. 주로 COCO, Flickr30k, Conceptual Captions (CC), SBU 와 같은 image captioning dataset 이 pretraining 에 활용된다. Unimodal language model 과 유사하게 <!-- -->[CLS]<!-- --> token 이 classification task 에서 contextual representation 으로 사용된다.</p><p>최근에는 multilingual multimodal model 도 제안되었다. 예를 들어, M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 는 50 개 언어의 Wikipedia 와 English multimodal CC dataset 으로 학습된다. English 이외의 언어 token 을 image representation 과 정렬하기 위해, M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 는 code-switching 메커니즘을 사용하며, English CC 예제의 단어를 대응되는 bilingual dictionary 의 단어로 무작위 대체한다. UC2 에서는 English multimodal dataset 을 machine translation 으로 다른 언어로 확장하고, masked region-to-token modeling 및 visual translation language modeling 을 제안한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="adapters">Adapters.<a href="#adapters" class="hash-link" aria-label="Direct link to Adapters." title="Direct link to Adapters.">​</a></h4><p>Adapter 는 NLP 와 CV 에서 transfer learning 을 위한 효율적인 fine-tuning 전략으로 도입되었다. Pretrained model 의 모든 weight 를 fine-tuning 하는 대신, 각 layer 에 작은 feed-forward layer 를 삽입한다. Task fine-tuning 시에는 adapter weight 만 업데이트되고 pretrained parameter 는 고정(frozen)된다. Adapter 는 학습 효율성이 매우 뛰어난 것으로 나타났으며, domain 과 task 간 transfer, machine translation, cross-lingual transfer 와 같은 다양한 응용 분야에서 사용된다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="datasets">Datasets.<a href="#datasets" class="hash-link" aria-label="Direct link to Datasets." title="Direct link to Datasets.">​</a></h4><p>Multilingual multimodal model 을 위한 pretraining 및 fine-tuning data 는 주로 Wikipedia 기반 multimodal 정보(WikiCaps, WIT) 또는 downstream task data 에 의존한다. </p><ul><li>Multi30k 는 English, German, French, Czech 을 포함하는 multilingual image captioning dataset 이며 retrieval task 를 다룬다. GEM 은 image 와 video retrieval task 를 각각 20, 30 개 언어에서 다룬다. </li><li>HowTo100M 은 multilingual, multimodal pretraining dataset 으로 image 와 video retrieval 에 사용된다. </li><li>MultiSubs 는 빈칸 채우기 및 lexical translation task 에 집중하며 English, Spanish, German, Portuguese, French 를 포함한다. </li><li>Gao et al. 과 Shimizu et al. 은 bilingual visual question answering dataset 을 각각 English–Chinese, English–Japanese 로 제안하였다. </li><li>Liu et al. 은 MaRVL dataset 을 제안하였는데, 이는 NLVR2 와 유사한 binary multilingual question answering dataset 으로 Chinese, Tamil, Swahili, Indonesian, Turkish 등 5 개의 다양한 언어를 포함한다.</li></ul><p>기존 dataset 은 retrieval 중심 task (상대적으로 단순), 소수의 유사 언어만 포함, 혹은 binary question 만 다루는 경우가 많았다. 이에 반해 저자는 더 다양한 언어 집합을 포괄하는 최초의 multilingual visual question answering dataset 을 제안한다.</p><p>가장 최근에는 IGLUE 가 제안되었으며, 이는 multilingual multimodal benchmark 로서 xGQA 를 통합한다. IGLUE 는 visual question answering, cross-modal retrieval, grounded reasoning, grounded entailment task 를 20 개 이상의 다양한 언어에서 다룬다.</p><h1>3 xGQA</h1><p>original English GQA dataset 은 Visual Genome scene graph 를 활용하여 구축되었다. English question engine 은 content (즉, object, attribute, relation 에 관한 정보) 와 structure (수백 개의 구조적 패턴과 정교한 lexical semantic resource 를 결합한 언어적 grammar) 를 활용하여 image scene graph 에 시각적으로 기반한 2,200 만 개 이상의 다양한 question 을 생성하였다. Question 은 template 을 사용하여 자동 생성되었기 때문에 자연 언어의 광범위한 스펙트럼을 반드시 반영하지는 않으며, 실제 환경에서의 성능에 대해 가정하기는 어렵다.</p><p>각 question 은 추가 metadata 와 연관되며, 이는 구조적 유형에 따라 다음과 같이 구분된다.</p><ol><li><strong>verify</strong>: yes/no question (e.g., &quot;Do you see any cats?&quot;)</li><li><strong>query</strong>: open question (e.g., &quot;Who is wearing jeans?&quot;)</li><li><strong>choose</strong>: 두 개의 대안을 제시하는 question (e.g., &quot;Is it red or blue?&quot;)</li><li><strong>logical</strong>: 논리적 추론을 포함하는 question (e.g., &quot;Is the field soft and snowy?&quot;)</li><li><strong>compare</strong>: 두 개 이상의 object 를 비교하는 question (e.g., &quot;Are all the animals zebras?&quot;)</li></ol><p>자세한 metadata 는 Hudson and Manning 의 연구를 참고한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dataset-design">Dataset Design.<a href="#dataset-design" class="hash-link" aria-label="Direct link to Dataset Design." title="Direct link to Dataset Design.">​</a></h4><p>xGQA 를 설계할 때의 주요 목표는 visual question answering 을 위한 유형적으로 진정으로 다양한 multilingual multimodal evaluation benchmark 를 만드는 것이었다. 저자는 GQA 의 balanced test-dev set 을 활용하였으며, 이는 398 개 image 에 대한 12,578 개 question 으로 구성된다. 정해진 구조적 패턴 덕분에 question 의 표현은 단순하며, 평균 길이는 8.5 단어이다. 최종적으로 xGQA dataset 은 7 개 언어로 번역을 포함하며, 각 언어는 서로 다른 언어 계열을 대표하고, 5 개의 script 를 포함한다 (Tab. 1 참조).</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-76-511dd86e8909e67430b1001e16365702.png" width="838" height="531" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="few-shot-data-splits">Few-Shot Data Splits.<a href="#few-shot-data-splits" class="hash-link" aria-label="Direct link to Few-Shot Data Splits." title="Direct link to Few-Shot Data Splits.">​</a></h4><p>Cross-lingual few-shot learning experiment 를 수행하기 위해, 저자는 다양한 크기의 새로운 data split 을 제공한다. Split 은 image 단위로 진행되며, 해당 image 와 연관된 모든 question 을 해당 set 에 추가한다. Development set 과 test set 은 각각 50 개와 300 개 image 로 구성된다. Training split 은 1, 5, 10, 20, 25, 48 개 image 로 구성된다 (Tab. 2 참조). 각 set 에서 structural type 의 분포가 유지되도록 보장하였다.</p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-77-ee7ef89a3348bbd11567b7fedc7e00d7.png" width="840" height="374" class="img_ev3q"></p><p>xGQA 는 최초의 진정한 유형적으로 다양한 multilingual multimodal benchmark 로, cross-lingual zero-shot 및 few-shot 시나리오에서 새로운 실험과 분석 기회를 제공한다. xGQA 의 question 은 직관적이고 인간이 풀기에는 쉽지만, 이후 결과에서 보이듯 현재 SOTA model 들은 여전히 transfer 에 어려움을 겪는다.</p><h1>4 Baselines</h1><p>xGQA 의 성능과 현재의 한계를 분석하기 위해, 먼저 multilingual 및 multimodal data 로 pretraining 된 최근의 M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P model 을 평가한다. 그러나 pretraining 은 계산 비용이 크고 multilingual multimodal resource 는 제한적이다. 따라서 저자는 보다 효율적인 새로운 접근법을 제안한다. (1) SOTA multilingual language model 을 multimodal domain 으로 확장하고, (2) SOTA multimodal model 에 multilingual 기능을 제공하는 방식이다.</p><p>별도의 언급이 없는 한, 저자는 GQA 에서 일반적으로 사용되는 fine-tuning 전략을 따른다. Pretrained transformer 의 output 위에 prediction head 를 얹고, GQA task 의 가능한 1853 개의 답변을 class label 로 매핑한다. Image 와 연관된 question 과 position-aware region feature 가 transformer 의 입력으로 전달되며, cross-entropy loss 로 학습된다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-multimodal--multilingual">4.1 Multimodal → Multilingual<a href="#41-multimodal--multilingual" class="hash-link" aria-label="Direct link to 4.1 Multimodal → Multilingual" title="Direct link to 4.1 Multimodal → Multilingual">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="oscaremb">OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>E</mi><mi>m</mi><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Emb}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">E</span><span class="mord mathnormal mtight">mb</span></span></span></span></span></span></span></span></span></span></span></span></span>.<a href="#oscaremb" class="hash-link" aria-label="Direct link to oscaremb" title="Direct link to oscaremb">​</a></h4><p>Artetxe et al. 은 monolingual transformer LM 을 multilingual domain 으로 확장하기 위해 target language 에서 새로운 word-embedding layer 를 fine-tuning 하였다. 이 아이디어를 기반으로, 저자는 monolingual multimodal transformer model 인 OSCAR+ 를 target language 용 embedding 을 학습하도록 확장한다.</p><ul><li><strong>Language-extension phase:</strong> OSCAR+ 의 embedding matrix 를 무작위로 초기화된 embedding matrix 로 교체한다. Transformer weight 는 고정되고, 새로운 embedding 만 target language 의 unlabeled text data 로 MLM objective 를 사용하여 fine-tuning 된다.</li><li><strong>Target-task phase:</strong> original OSCAR+ model 은 English GQA training data 로 fine-tuning 되며, 이때 transformer layer 는 fine-tuning 되고 embedding layer 는 고정된다. Inference 시에는 embedding layer 가 target language 의 embedding 으로 교체된다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="oscarada">OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>.<a href="#oscarada" class="hash-link" aria-label="Direct link to oscarada" title="Direct link to oscarada">​</a></h4><p>여기에 adapter 를 추가한 확장 방식이다.</p><ul><li><strong>Language-extension phase:</strong> Pfeiffer et al. 의 방법을 따르며, OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>E</mi><mi>m</mi><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Emb}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">E</span><span class="mord mathnormal mtight">mb</span></span></span></span></span></span></span></span></span></span></span></span></span> 와 유사하게 새로운 embedding layer 를 학습한다. 또한 각 transformer layer 에 language adapter 를 추가한다. OSCAR+ 가 English text 로 학습되었기 때문에, English language adapter module 은 embedding matrix 를 교체하지 않고 학습된다. Transformer weight 는 고정되고, 새로운 embedding 과 language adapter weight 만 target language 의 unlabeled text data 로 fine-tuning 된다.</li><li><strong>Target-task phase:</strong> 저자는 cross-lingual transfer 방법에서 영감을 받아 새로운 modality-split architecture 를 제안한다 (Fig. 2). </li></ul><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-78-dbb2a6e0eac3de7d1d5824ba490c1034.png" width="910" height="1015" class="img_ev3q"></p><ul><li>각 transformer layer 에서 text 와 image representation 은 pretrained multi-head attention (MHA) 와 feed-forward (FFN) layer 를 통과한다. </li><li>동시에 image 와 text representation 은 pretrained language adapter 를 통과한다. </li><li>이후 각 modality 는 modality-specific task adapter (text, image) 를 거치고, 마지막으로 shared multimodal alignment adapter 를 통과한다. </li><li>Training 시 transformer, embedding, language adapter weight 는 고정되며, task adapter 와 multimodal aligner adapter weight, prediction head 만 fine-tuning 된다. </li><li>Inference 시에는 embedding layer 와 language adapter 가 target language weight 로 교체된다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-multilingual--multimodal">4.2 Multilingual → Multimodal<a href="#42-multilingual--multimodal" class="hash-link" aria-label="Direct link to 4.2 Multilingual → Multimodal" title="Direct link to 4.2 Multilingual → Multimodal">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mbertada">mBERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>.<a href="#mbertada" class="hash-link" aria-label="Direct link to mbertada" title="Direct link to mbertada">​</a></h4><p>Multilingual model 을 multimodal 로 확장하는 실험에서는 mBERT 를 사용한다.</p><ul><li><strong>Language-extension phase:</strong> mBERT 는 이미 다양한 언어를 표현할 수 있으므로 target language 를 위한 새로운 embedding layer 를 학습할 필요가 없다. 대신 AdapterHub.ml 에서 제공되는 mBERT 호환 language adapter 를 활용한다.</li><li><strong>Target-task phase:</strong> Image representation layer 는 OSCAR+ 방식과 동일하게, image feature 를 positional information 과 결합한 후 affine transformation layer 를 통과시킨다. <ul><li>Fig. 2 의 adapter architecture 를 OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span> 와 동일하게 사용한다. </li><li>Training 시 transformer, embedding, language adapter weight 는 고정된다. 그러나 OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span> 와 달리 affine image transformation layer 는 무작위 초기화 후 fine-tuning 된다. </li><li>Task adapter, multimodal aligner adapter weight, prediction head 도 GQA task 에 대해 fine-tuning 된다. </li><li>Inference 시에는 embedding layer 와 language adapter 가 target language weight 로 교체된다.</li></ul></li></ul><h1>5 Experimental Setup</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-language-extension-phase">5.1 Language-Extension Phase<a href="#51-language-extension-phase" class="hash-link" aria-label="Direct link to 5.1 Language-Extension Phase" title="Direct link to 5.1 Language-Extension Phase">​</a></h2><ul><li>OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>E</mi><mi>m</mi><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Emb}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">E</span><span class="mord mathnormal mtight">mb</span></span></span></span></span></span></span></span></span></span></span></span></span> 과 OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span> 의 경우, 저자는 Pfeiffer et al. 이 제안한 일반적인 설정을 따른다. </li><li>각 target language 에 대해 vocabulary size 가 30k 인 새로운 word-piece tokenizer 를 학습한다. </li><li>무작위로 초기화된 embedding layer 와 (OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span> 의 경우) adapter layer 를 batch size 64, learning rate <!-- -->$<!-- -->1e^{-4}<!-- -->$<!-- --> 로 100k update step 동안 fine-tuning 한다. </li><li>mBERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span> 에 대해서는 AdapterHub.ml 의 language adapter 를 활용한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-fine-tuning-on-gqa">5.2 Fine-tuning on GQA<a href="#52-fine-tuning-on-gqa" class="hash-link" aria-label="Direct link to 5.2 Fine-tuning on GQA" title="Direct link to 5.2 Fine-tuning on GQA">​</a></h2><p>저자는 Li et al. 이 제안한 standard setup 을 따르며, <!-- -->[CLS]<!-- --> token 의 representation 을 prediction head 로 전달한다. 각 model 은 cross-entropy loss 로 fine-tuning 되며, label 은 GQA dataset 의 모든 가능한 answer 로 설정된다. Li et al. 의 prior work 에 따라 batch size 는 192, 학습 epoch 은 5 로 설정하고, unbalanced GQA training set 을 사용한다.</p><ul><li><strong>M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P:</strong> pretrained model 의 모든 weight 를 learning rate <!-- -->$<!-- -->3e^{-5}<!-- -->$<!-- --> 로 fine-tuning 한다.</li><li><strong>OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>E</mi><mi>m</mi><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Emb}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">E</span><span class="mord mathnormal mtight">mb</span></span></span></span></span></span></span></span></span></span></span></span></span>, OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>, mBERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>:</strong> Zhang et al. 이 제공한 pretrained weight 와 image region feature 를 사용한다. 그러나 object attribute label 은 입력으로 전달하지 않는다. 이는 English 로만 제공되며, cross-lingual scenario 에 활용하기가 비직관적이기 때문이다. 해당 부분은 future work 으로 남긴다.<ul><li><strong>OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>E</mi><mi>m</mi><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Emb}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">E</span><span class="mord mathnormal mtight">mb</span></span></span></span></span></span></span></span></span></span></span></span></span>:</strong> transformer weight 와 prediction head 만 fine-tuning 하고, embedding layer 는 freeze 한다. Learning rate 는 <!-- -->$<!-- -->3e^{-5}<!-- -->$<!-- --> 를 사용한다.</li><li><strong>OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>, mBERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>:</strong> Sec. 4.1 과 Fig. 2 에서 설명된 대로 adapter layer 를 추가한다. Embedding, transformer layer, language adapter 를 포함한 모든 pretrained weight 는 freeze 하고, 새로 추가된 adapter 와 prediction head 만 fine-tuning 한다.</li><li><strong>mBERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>:</strong> affine image transformation layer 를 추가하고 학습한다. Adapter 기반 model 은 learning rate <!-- -->$<!-- -->1e^{-4}<!-- -->$<!-- --> 로 fine-tuning 한다.</li></ul></li></ul><h1>5.3 Zero-Shot Cross-Lingual Transfer</h1><p>Zero-shot cross-lingual evaluation 을 위해, 저자는 GQA training data 로 fine-tuning 된 model 을 multilingual xGQA test data 에서 평가한다. English GQA validation data 에서 가장 좋은 성능을 보인 model checkpoint 가 transfer 에 사용된다.</p><ul><li><strong>M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P:</strong> 이 model 은 pretraining 시 xGQA 언어를 포함하므로 cross-lingual transfer 를 위해 추가적인 단계가 필요 없다.</li><li><strong>OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>E</mi><mi>m</mi><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Emb}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">E</span><span class="mord mathnormal mtight">mb</span></span></span></span></span></span></span></span></span></span></span></span></span>:</strong> English embedding layer 를 target language embedding layer 로 교체한다.</li><li><strong>OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>:</strong> English embedding 과 language adapter layer 를 target language 의 embedding 과 adapter layer 로 교체한다.</li><li><strong>mBERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>:</strong> language adapter layer 를 target language 의 adapter layer 로 교체한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="54-few-shot-cross-lingual-transfer">5.4 Few-Shot Cross-Lingual Transfer<a href="#54-few-shot-cross-lingual-transfer" class="hash-link" aria-label="Direct link to 5.4 Few-Shot Cross-Lingual Transfer" title="Direct link to 5.4 Few-Shot Cross-Lingual Transfer">​</a></h2><p>Few-shot cross-lingual 시나리오에서는 Lauscher et al. 의 접근을 따르며, zero-shot transfer 와 동일한 fine-tuning model (§5.3) 에서 시작한다. 이후 §5.2 의 English training data 학습과 동일한 부분을 target language 의 소량 multimodal data 로 fine-tuning 한다.</p><p>Training 은 1, 5, 10, 15, 20, 25, 48 개 image 로 구성된 서로 다른 data split (Tab. 2 참조) 에 대해 수행된다. Epoch 수는 5, 10, learning rate 는 M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 와 OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>E</mi><mi>m</mi><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Emb}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em">E</span><span class="mord mathnormal mtight">mb</span></span></span></span></span></span></span></span></span></span></span></span></span> 의 경우 <!-- -->$<!-- -->1e^{-5}<!-- -->$<!-- -->, <!-- -->$<!-- -->5e^{-5}<!-- -->$<!-- --> 를, OSCAR+<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span> 와 mBERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span> 의 경우 <!-- -->$<!-- -->5e^{-5}<!-- -->$<!-- -->, <!-- -->$<!-- -->1e^{-4}<!-- -->$<!-- --> 를 사용하여 실험하였다.</p><p>그 결과, 더 긴 학습과 더 큰 learning rate 를 사용할 때 모든 setting 에서 가장 좋은 성능을 보였다.</p><h1>6 Results and Discussion</h1><p>주요 결과는 Tab. 3 (zero-shot 실험) 과 Tab. 4 (few-shot 실험) 에 제시된다.</p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-79-4ae998b9d00bc87fa931cfa968ce3296.png" width="1529" height="353" class="img_ev3q"></p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-81-4bb557ffa74920ca41d5b943a3f50d23.png" width="755" height="1100" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="61-zero-shot-cross-lingual-transfer">6.1 Zero-Shot Cross-Lingual Transfer<a href="#61-zero-shot-cross-lingual-transfer" class="hash-link" aria-label="Direct link to 6.1 Zero-Shot Cross-Lingual Transfer" title="Direct link to 6.1 Zero-Shot Cross-Lingual Transfer">​</a></h2><p>핵심적인 발견 중 하나는 multimodal zero-shot cross-lingual transfer 가 매우 어렵다는 것이다. xGQA dataset 의 target language 에서 accuracy 가 English GQA 점수와 비교했을 때 평균 38 point 이상 감소하는 현상이 관찰된다 (e.g., M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 결과 비교).</p><p>예상대로 OSCAR+ 가 English test set 에서 가장 높은 accuracy 를 기록했지만, massively multilingual model 인 M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 와 mBERT 가 cross-lingual transfer 에서는 훨씬 더 좋은 성능을 보인다. 이는 joint multilingual pretraining 이 중요하며, 단순한 multilingual adapter 기반 또는 embedding 기반의 monolingual model 확장은 cross-lingual 성능이 떨어짐을 의미한다.</p><p>Pretraining 기반의 M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 가 English test set 에서 더 나은 accuracy 를 기록하였지만, adapter 기반 multimodal 확장을 적용한 mBERT 는 cross-lingual transfer 에서 M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 를 능가하였다. 저자는 monolingual multimodal data 로 transformer weight 를 모두 fine-tuning 하면, M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 내의 cross-lingual alignment 가 깨질 수 있다고 가정한다. 그러나 adapter 기반 setting 에서는 multilingual weight 가 freeze 되어 alignment 가 유지되므로 이러한 문제가 발생하지 않는다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="analysis-of-structural-question-types">Analysis of Structural Question Types<a href="#analysis-of-structural-question-types" class="hash-link" aria-label="Direct link to Analysis of Structural Question Types" title="Direct link to Analysis of Structural Question Types">​</a></h4><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-80-d070ea15968617b8c66bf8c79348e50f.png" width="1543" height="2142" class="img_ev3q"></p><p>Fig. 3 은 zero-shot 실험에서 structural question type 에 대한 분석을 보여준다. 특히 query 와 choose 유형 question 에서 accuracy 가 크게 떨어지는 것이 관찰된다.</p><ul><li><strong>Query type question</strong> 은 자유형이며 의미적으로 가장 어렵다. Source language (English) 에서조차 답변하기 어려운 유형이기 때문에, zero-shot setting 에서 전반적으로 낮은 accuracy 가 나타난다.</li><li><strong>Choose type question</strong> 은 source language 에서 model 이 잘 수행하는 유형이지만, zero-shot cross-lingual transfer 에서 accuracy 가 크게 감소한다. 이는 question 의 구조와 model 구현 방식에 기인한다. <ul><li>Choose 유형 question 은 &quot;Is it red or blue?&quot; 와 같이 정답이 question 에 포함된 단어 또는 구로 구성된다. </li><li>Label class 와 prediction head 는 dataset 에 등장하는 모든 answer 로 구축되며, model 은 최종 layer 에서 각 answer 의 분산 표현을 학습한다. </li><li>따라서 cross-lingual transfer 시에는 translated question 내의 option (&quot;red&quot; 또는 &quot;blue&quot;) 을 model 의 prediction head 에서 학습된 English latent representation 과 자동으로 정렬해야 한다. </li><li>이 유형에서 매우 낮은 성능은 zero-shot 시나리오에서 이러한 cross-lingual word alignment 가 무너짐을 보여준다.</li></ul></li><li>종합적으로, 저자가 제안한 multimodal adapter 기반 mBERT 확장(mBERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span>) 은 best accuracy 를 기록하며, M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 대비 약 3 point, OSCAR+ 대비 약 5 point 향상되었다. 그러나 모든 접근법의 overall accuracy 는 English 결과와 비교하면 여전히 낮다.<ul><li>이는 zero-shot multimodal cross-lingual transfer 가 매우 어렵고, visual representation 과 cross-lingual internal representation 간의 misalignment 문제 때문일 가능성이 크다는 것을 보여준다. </li><li>이를 더 조사하기 위해 저자는 few-shot setup 에서 유사한 실험을 수행하였으며, 이는 zero-shot setup 에서 관찰된 misalignment 문제를 완화할 가능성이 있다.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="62-few-shot-cross-lingual-transfer">6.2 Few-Shot Cross-Lingual Transfer<a href="#62-few-shot-cross-lingual-transfer" class="hash-link" aria-label="Direct link to 6.2 Few-Shot Cross-Lingual Transfer" title="Direct link to 6.2 Few-Shot Cross-Lingual Transfer">​</a></h2><p>Few-shot 실험의 주요 결과는 Tab. 4 에 제시되며, training data 양의 영향을 보여주는 plot 은 Fig. 5 에 나타나 있다. </p><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-82-05abc7eb5ec650d119f110317e73e95a.png" width="752" height="548" class="img_ev3q"></p><ul><li>핵심적인 발견은 예상대로 target language 의 data 양이 증가할수록 모든 방법에서 accuracy 가 꾸준히 향상된다는 점이다. </li><li>Target language 에서 단 48 개 image 로 model 을 학습했을 때 accuracy 가 최대 20 point 개선되었다. </li><li>이는 few target language examples 만으로도 model 이 내부 cross-lingual multimodal alignment 를 부분적으로 복구할 수 있음을 시사한다. </li><li>흥미롭게도 단 5 개 image 와 그에 해당하는 question 만으로도 M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 가 mBERT<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mrow><mi>A</mi><mi>d</mi><mi>a</mi></mrow></msup></mrow><annotation encoding="application/x-tex">^{Ada}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">a</span></span></span></span></span></span></span></span></span></span></span></span></span> (zero-shot setting 에서 best performing model) 보다 더 나은 성능을 보이기 시작한다.</li></ul><p>저자는 structural question type 별로 few-shot learning 의 영향을 다시 분석했으며, 결과는 Fig. 4 에 제시된다. </p><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-83-356d5606e3b5ee4c1d939081f5166284.png" width="1543" height="2142" class="img_ev3q"></p><ul><li>Zero-shot 시나리오(Fig. 3) 와 비교했을 때 전반적으로 accuracy 가 모든 유형에서 향상되었다. </li><li>특히 zero-shot setup 에서 가장 낮은 성능을 보였던 query 와 choose 유형에서 두드러진 향상이 나타났다. <ul><li>이는 target language 에서 소수의 예제로 fine-tuning 하는 과정이 latent multimodal representation 과 multilingual representation 간 alignment 를 개선했음을 의미한다.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="63-language-transfer">6.3 Language Transfer<a href="#63-language-transfer" class="hash-link" aria-label="Direct link to 6.3 Language Transfer" title="Direct link to 6.3 Language Transfer">​</a></h2><p>저자의 model 은 기존 연구에서 보고된 것과 유사한 cross-lingual transfer 패턴을 보였다. Typologically 가까운 언어에서 가장 좋은 성능을 보였는데, German (de) 과 Portuguese (pt) 가 그 예이다. </p><p>두 언어는 모두 Indo-European (IE) 계열에 속하며 source language 인 English (en) 과 같은 Latin script 를 공유한다. Russian (ru), Indonesian (id), Chinese (zh) 에서는 accuracy 가 소폭 감소했고, Bengali (bn), Korean (ko) 에서는 더 큰 감소가 관찰되었다. 이들 언어는 source language 와 유형적으로 다르며 대부분 script 도 공유하지 않는다. 이러한 차이는 cross-lingual transfer 에서 언어 다양성이 중요함을 강조한다. 저자의 benchmark 는 이러한 다양성을 포괄함으로써 multilingual multimodal model 을 진정으로 유형적으로 다양한 언어 집합에서 실험하고 평가할 수 있게 한다.</p><h1>7 Contemporary Work</h1><p>Multilingual vision 과 language learning 에 대한 관심이 최근 급격히 증가하면서, contemporay work 에서는 이미 제안된 xGQA dataset 을 더 분석하고 확장하였다.</p><ul><li><strong>Further Analysis.</strong> Liu et al. (2022) 는 cross-lingual visual question answering 으로 학습된 multilingual, multimodal model 을 광범위하게 분석하고, Sec. 6.1 에서 논의된 multilingual misalignment 문제를 완화하기 위한 여러 접근법을 제안하였다. <ul><li>그 결과, text-only cross-lingual transfer scenario 에서 사용되던 일반적인 접근법은 pretrained model 의 multilingual capability 를 충분히 활용하지 못한다는 사실이 드러났다. </li><li>흥미롭게도 prediction head 를 더 깊게 만드는 것은 source language 성능에 영향을 주지 않지만, 모든 target language 의 zero-shot transfer 성능은 상당히 개선되었다.</li></ul></li><li><strong>Translated Test Data.</strong> Bugliarello et al. (2022) 는 modality, task, language 를 아우르는 transfer learning benchmark 를 처음으로 제안했으며, 여기에는 visual question answering, cross-modal retrieval, grounded reasoning, grounded entailment task 가 포함되며 총 20 개의 다양한 언어를 다룬다. <ul><li>이들은 machine translation 으로 번역된 test set question 을 추가하여 xGQA dataset 을 확장하고, translate-test setup 에서 SOTA monolingual multimodal model 을 평가하였다. </li><li>이 setting 에서는 약간 더 나은 성능을 보였으나, 여전히 source language 성능에는 크게 미치지 못했다.</li></ul></li></ul><h1>8 Conclusion</h1><p>저자는 visual question answering task 를 위한 최초의 cross-lingual evaluation benchmark 인 xGQA 를 제안하였다. xGQA 는 English GQA dataset 을 7 개의 유형적으로 다양한 언어로 확장하였으며, 5 개 script 를 포함한 development 및 test data 를 제공한다. 추가 baseline 으로, 저자는 unimodal multilingual model 을 multimodal 로 확장하거나 그 반대로 monolingual multimodal model 을 multilingual 로 확장하는 adapter 기반 방법을 제안하였다.</p><p>실험 결과, (1) 효율적인 adapter 기반 방법은 zero-shot scenario 에서 pretrained multilingual multimodal model 인 M<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span>P 를 약간 능가했지만, (2) 모든 model 은 English 성능과 비교할 때 zero-shot cross-lingual transfer 에서 심각한 accuracy 감소를 보였다. 그러나 target language 의 소량 training data 를 활용하는 few-shot learning 을 통해 accuracy 를 부분적으로 복구할 수 있었다. 그럼에도 불구하고 큰 성능 격차는 여전히 남아 있으며, 이는 해당 task 가 직관적이고 인간(특히 bilingual) 에게는 매우 쉬움에도 불구하고 본질적으로 높은 복잡성을 가진다는 점을 시사한다.</p><p>저자는 제안된 dataset 과 error analysis 가 이 task 는 물론, 더 넓게는 multilingual multimodal representation learning 이라는 새롭게 부상하는 흥미로운 연구 영역에서 향후 연구를 촉진하기를 기대한다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">Vision-Language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multimodal">Multimodal</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/few-shot-learning">few-shot learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/gpt-3">GPT-3</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/prompting">Prompting</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/VQA-IC/Few-shot/2021-09-xGQA.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#41-multimodal--multilingual" class="table-of-contents__link toc-highlight">4.1 Multimodal → Multilingual</a></li><li><a href="#42-multilingual--multimodal" class="table-of-contents__link toc-highlight">4.2 Multilingual → Multimodal</a></li><li><a href="#51-language-extension-phase" class="table-of-contents__link toc-highlight">5.1 Language-Extension Phase</a></li><li><a href="#52-fine-tuning-on-gqa" class="table-of-contents__link toc-highlight">5.2 Fine-tuning on GQA</a></li><li><a href="#54-few-shot-cross-lingual-transfer" class="table-of-contents__link toc-highlight">5.4 Few-Shot Cross-Lingual Transfer</a></li><li><a href="#61-zero-shot-cross-lingual-transfer" class="table-of-contents__link toc-highlight">6.1 Zero-Shot Cross-Lingual Transfer</a></li><li><a href="#62-few-shot-cross-lingual-transfer" class="table-of-contents__link toc-highlight">6.2 Few-Shot Cross-Lingual Transfer</a></li><li><a href="#63-language-transfer" class="table-of-contents__link toc-highlight">6.3 Language Transfer</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.335bb705.js"></script>
<script src="/assets/js/main.58fe3837.js"></script>
</body>
</html>