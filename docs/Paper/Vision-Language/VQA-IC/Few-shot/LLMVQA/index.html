<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/VQA-IC/Few-shot/2024-03-LLMVQA">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.ec4b5b0a.js" as="script">
<link rel="preload" href="/assets/js/main.d9f0a85a.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">CLIP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Foundation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">VQA-IC</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Few-shot</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Multimodal Few-Shot Learning with Frozen Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA">xGQA: Cross-Lingual Visual Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP">CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA">Linearly Mapping from Image to Text Space</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper">Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA">Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/CoTMT">A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/MAPD">Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering</a></li></ul></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">VQA-IC</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Few-shot</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2403.11317" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2403.11317</a></p><h1>Abstract</h1><p>이미지를 large language models (LLMs) 에 입력하는 두 가지 접근 방식이 등장했다. 첫 번째는 이미지를 caption 으로 변환하여 자연어로 입력하는 방식이다. 두 번째는 image feature embedding 을 LLM 의 domain 으로 매핑하고, 매핑된 embedding 을 직접 LLM 에 전달하는 방식이다.</p><p>최근 few-shot multimodal 연구의 대부분은 이 두 가지 접근 방식 중 하나의 변형을 사용하는 architecture 를 통해 성능을 보고한다. 그러나 이러한 연구들은 두 접근 방식 간의 중요한 비교를 간과하고 있다. 저자는 few-shot visual question answering (VQA) 와 LLM 에 대한 이 두 가지 접근 방식을 비교하기 위해 통제되고 집중된 실험을 설계하였다.</p><p>실험 결과, 3B parameter LLM 인 Flan-T5 XL 에 대해서는 visual embedding 을 LLM 의 embedding space 에 직접 연결하는 것이 image caption 을 사용하는 것보다 항상 성능 향상을 보장하지 않는다는 것을 확인하였다. Zero-shot 환경에서는 textual image caption 을 사용하는 것이 더 우수한 결과를 보였다. Few-shot 환경에서는 in-context examples 가 어떻게 선택되는지가 어떤 접근 방식이 더 나은지를 결정하는 핵심 요인임을 발견하였다.</p><h1>1 Introduction</h1><p>인공지능의 근본적인 특징 중 하나는 multiple modality 를 통합하는 능력이다. 이는 많은 모호성을 줄이고 knowledge 획득을 지원한다. 특히, 근본적인 multimodal task 는 visual question answering (VQA) 이다. VQA 는 이미지를 활용하여 질문에 답하는 task 이다. Fig. 1 은 세 가지 예시를 보여준다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-67-5ec251a958fbdb0bc1123e8f890c93ba.png" width="712" height="369" class="img_ev3q"></p><p>vision 과 language 의 modality 를 통합하기 위한 노력에서, 이 두 분야의 대표적인 architecture 인 large language models (LLMs) 과 vision transformers 를 결합하려는 시도가 활발히 이루어지고 있다. LLM 은 text-to-text task 를 위한 범용 interface 이다. Vision transformer 는 이미지를 일반적인 feature representation 으로 변환할 수 있다. 그러나 이 두 모델은 별도로 학습되었기 때문에, 한 모델의 representation 을 다른 모델에서 직접 사용할 수 없다.</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-68-8070a675ca61391904c97d25d1ee48c7.png" width="1488" height="860" class="img_ev3q"></p><p>이를 해결하기 위해 Mokady et al. 은 image representation 을 language representation 의 domain 으로 변환하는 network 를 학습하였다. 이 변환 과정을 통해, 변환된 image representation 이 LLM 에 입력될 경우 image caption 을 생성하도록 한다. Tsimpoukelli et al. 은 이러한 변환된 image representation 을 LLM 에 전달할 때 질문을 함께 추가하여 VQA 를 수행하였다. 이 접근 방식은 Fig. 2a 에 시각화되어 있다.</p><p>VQA 의 또 다른 접근 방식은 이미지를 caption 으로 변환한 후, 이 caption 을 질문과 함께 LLM 에 자연어로 입력하는 것이다. 이 접근 방식은 Fig. 2b 에 시각화되어 있다.</p><p>few-shot VQA 에서는 in-context example 또한 LLM 의 prompt 에 함께 제공된다. 이는 올바른 응답과 함께 제시된 입력 예시로서, 원하는 동작을 나타낸다. In-context example 을 포함하면 few-shot VQA 성능이 향상된다는 것이 실험적으로 입증되었다.</p><p>최근 VQA architecture 들은 일반적으로 embedding-based 접근 방식 또는 caption-based 접근 방식의 변형을 사용한다. 그러나 이 두 접근 방식의 비교는 그동안 간과되어 왔다. 본 연구의 기여는 이 두 접근 방식을 통제된 환경에서 집중적으로 비교하는 것이다. 추가적으로, 저자는 in-context example 을 선택하는 방법이 각 접근 방식의 prediction 동작에 어떤 영향을 미치는지를 분석하였다.</p><h1>2 Background</h1><p>LLM 은 transformer 기반 model 로서, task-specific training 없이도 text-to-text task 를 수행할 수 있음이 입증되었다. 이는 주어진 context 에 기반하여 질문에 답하도록 prompt 를 받는 extractive question–answering 을 포함한다.</p><p>vision–language 연구에서 VQA 는 근본적인 task 이다. 한 가지 접근 방식은 image caption 의 형태로 시각적 정보를 LLM 에 전달하는 것이다. 이 경우 VQA task 는 extractive QA task 로 재구성된다.</p><p>한편, computer vision 분야에서 Dosovitskiy et al. 은 vision transformer (ViT) 를 제안하였다. </p><ul><li>Radford et al. 은 ViT 를 위한 training objective 를 제시했는데, 이는 인코딩된 이미지와 caption 사이의 contrastive loss 를 최소화하는 것이다. </li><li>이들은 이미지를 일반적인 feature representation 으로 변환할 수 있는 CLIP model 을 배포하였다.</li></ul><p>LLM 과 ViT image encoder 를 통합하는 주요 도전 과제는 image space 와 text space 의 feature 를 정렬(alignment)하는 것이다. </p><ul><li>Tsimpoukelli et al. 은 image encoder 의 출력을 frozen LLM 에 전달하는 방식으로 image encoder 를 finetune 하였다. </li><li>여기서 frozen model 이란 training 과정에서 weight 가 업데이트되지 않는 model 을 의미한다. </li><li>이들은 image–caption training data 를 활용하여 image encoder 의 weight 를 업데이트하였다. </li></ul><p>이후 embedded question 을 image embedding 에 append 하여 LLM 에 입력하는 방식으로 VQA task 를 평가하였다.</p><ul><li>Merullo et al. 은 CLIP image space 와 GPT-J text space 사이의 단순한 linear mapping 만으로도 유사한 결과를 달성하였다. </li><li>Yi-Lin Sung 과 Eichenberg et al. 은 LLM 에 few trainable parameter 를 추가하였으며, 이를 adapter 라고 한다. </li><li>Adapter 는 LLM transformer block 에 삽입되는 trainable MLP network 이다. </li><li>Alayrac et al. 은 LLM 내부에 masked cross-attention layer 를 추가하여, 각 질문이 마지막 image embedding 집합에만 attention 하도록 하였다. </li><li>Li et al. 은 querying transformer network 에 prompt question 을 포함하는 system 을 제안하였다. 이 두 system 은 모두 방대한 training data 를 필요로 한다.</li></ul><p>위의 system 들은 모두 image captioning task 학습으로 시작한다. 따라서 embedding-based VQA 접근 방식을 동일한 system 이 생성한 image caption 을 사용하는 방법과 비교하는 것이 자연스러운 baseline 이 된다. 그러나 앞서 언급한 system 들 중 어느 것도 이러한 결과를 보고하지 않았다.</p><h1>3 Two approaches to VQA</h1><p>여기서는 embedding-based 접근 방식과 caption-based 접근 방식의 VQA 방법을 설명한다. 두 접근 방식을 통제된 조건에서 비교하기 위해 동일한 image feature representation 을 사용한다. 유일한 차이는 이 visual feature representation 을 먼저 image caption 으로 변환할지, 아니면 직접 LLM 에 전달할지 여부이다.</p><p>두 설정 모두 하나의 trained model 을 사용한다. 이 model 은 image encoder 와 LLM 을 연결하는 ‘mapping network’ 이다. Image encoder 와 LLM 은 frozen 상태로 두고, mapping network 의 parameter 는 입력 image 가 주어졌을 때 target caption token sequence <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 의 likelihood 를 최대화하도록 학습된다. Image encoder 가 frozen 이므로, 이 likelihood 는 image encoder feature embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 가 주어졌을 때 target 의 likelihood 와 동일하다. 이 objective 는 autoregressive language model 에 대해 전개하면 Eq. 1 과 같다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>l</mi></munder><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>l</mi></msub><mtext> </mtext><mi mathvariant="normal">∣</mi><mtext> </mtext><mi>x</mi><mo separator="true">,</mo><msub><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\log p(y|x) = \sum_l \log p(y_l \,|\, x, y_{1:l-1}) \tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.3521em;vertical-align:-1.3021em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em"><span style="top:-1.8479em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:2.3521em;vertical-align:-1.3021em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></div><ol><li><p><strong>Image encoder (image to feature embedding)</strong>
Image encoder 는 image pixel value matrix <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>W</mi><mo>×</mo><mi>H</mi><mo>×</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">i \in \mathbb{R}^{W \times H \times 3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6986em;vertical-align:-0.0391em"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">W</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span><span class="mbin mtight">×</span><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span></span></span> 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span></span> 차원의 vector <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 로 인코딩한다.</p></li><li><p><strong>Mapping network (feature embedding to LLM image representation)</strong>
Mapping network 은 이 feature embedding 을 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 개의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span></span>-dimensional embedding 으로 매핑한다. 각각의 embedding 은 단일 textual token embedding 과 동일한 차원을 가진다.</p></li><li><p><strong>LLM (image representation to text output)</strong>
이 embedding 들은 LLM 에 전달되어 output token sequence <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 를 생성하며, 이는 groundtruth caption 과 비교된다.</p></li></ol><p>저자의 두 가지 VQA 접근 방식은 다음과 같다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="embedding-based">Embedding-based<a href="#embedding-based" class="hash-link" aria-label="Direct link to Embedding-based" title="Direct link to Embedding-based">​</a></h4><p>LLM 의 tokenizer 와 embedder 는 질문을 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span> 개의 embedding 으로 변환한다. 이미지는 image encoder 와 mapping network 을 통해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 개의 LLM embedding 으로 변환된다. 이렇게 매핑된 image embedding 과 question embedding 을 concatenate 하여 함께 LLM 에 전달한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="caption-based">Caption-based<a href="#caption-based" class="hash-link" aria-label="Direct link to Caption-based" title="Direct link to Caption-based">​</a></h4><p>Caption-based VQA 에서는 LLM image embedding 만을 LLM 에 전달하여 image caption 을 생성한다. 이렇게 얻은 textual image caption 을 textual question 과 concatenate 하고, 이 prompt 를 LLM 에 전달한다.</p><h1>4 Experimental Setup</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models">Models<a href="#models" class="hash-link" aria-label="Direct link to Models" title="Direct link to Models">​</a></h4><p>저자는 Flan-T5 XL, 즉 3B parameter 를 가진 autoregressive encoder–decoder LLM 을 사용한다. 이 model 은 in-context learning 에서 좋은 성능을 보이는 것으로 실험적으로 입증되었기 때문에 선택되었다. 이미지를 인코딩하기 위해서는 pretrained CLIP ViT-G model 을 사용한다. 이 두 component 는 하나의 hidden layer mapping network 으로 연결되며, 이 network 는 1280-D input, 크기가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>20</mn><mo>×</mo><mn>2048</mn><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">20 \times 2048 / 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">20</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2048/2</span></span></span></span></span> 인 hidden layer, 그리고 20 개의 2048-D output embedding 으로 구성된다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="datasets">Datasets<a href="#datasets" class="hash-link" aria-label="Direct link to Datasets" title="Direct link to Datasets">​</a></h4><p>training data 로는 conceptual captions dataset 을 사용한다. 이 dataset 이 제공하는 3M+ 개의 image URL 중에서 2.7M 개가 active 상태임을 확인하였다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="training">Training<a href="#training" class="hash-link" aria-label="Direct link to Training" title="Direct link to Training">​</a></h4><ul><li>학습 과정에서는 cross-entropy loss 를 최소화한다. </li><li>Frozen LLM 을 통해 loss 를 backpropagation 하고, AdamW optimizer 로 mapping network 를 업데이트한다. </li><li>Optimizer 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mo stretchy="false">(</mo><mn>0.9</mn><mo separator="true">,</mo><mn>0.98</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\beta = (0.9, 0.98)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">0.9</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">0.98</span><span class="mclose">)</span></span></span></span></span>, weight decay = 0.01 을 사용한다. </li><li>Learning rate 는 375 step 동안 선형적으로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2 \times 10^{-4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span></span> 까지 증가한 후, 2 epoch 동안 batch size 32 로 학습하면서 선형적으로 0 까지 감소한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="in-context-example-selection">In-context example selection<a href="#in-context-example-selection" class="hash-link" aria-label="Direct link to In-context example selection" title="Direct link to In-context example selection">​</a></h4><p>Prompt 에 올바른 응답과 함께 입력 예시를 포함하는 것은 성능 향상에 효과적임이 입증되었으며, 이를 in-context learning 이라고 한다. 특히, &quot;shots&quot; 는 포함되는 sample 의 개수를 의미한다.</p><p>가장 유사한 in-context example 을 찾기 위해, train 과 validation VQAv2 image 와 question 의 CLIP embedding 을 추출한다. 저자는 최대 4 개의 in-context example 을 선택하며, 선택 방법은 무작위 선택 (R), question similarity 만 사용 (Q), 그리고 question 과 image similarity 를 50-50 으로 결합한 방법 (Q+I) 을 비교한다. 유사도 측정에는 inner product 를 사용하고, in-context example 은 FAISS index 를 통해 효율적으로 사전 계산한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="metrics">Metrics<a href="#metrics" class="hash-link" aria-label="Direct link to Metrics" title="Direct link to Metrics">​</a></h4><p>시스템은 VQA v2 benchmark 의 few-shot 환경에서 검증된다. 이 benchmark 는 214K 개의 image–question–answer validation triplet 을 포함한다. 평가에는 공식 metric 을 사용하며, 이는 예측된 답과 10 개의 human-annotated 답 사이에서 exact match 의 개수를 계산한다. 예측된 답이 최소 3 개 이상의 human annotated 답과 정확히 일치하면 점수를 획득한다. 저자는 최근 접근 방식과 동일하게 open-ended generation setting 에서 평가를 수행한다.</p><p>본 실험 설정은 Lin and Byrne 의 framework 을 기반으로 구현되었다.</p><p>모든 실험은 단일 random seed 로 한 번씩 실행된다. 저자는 system, competitor, baseline 사이의 차이를 검정하기 위해 양측(two-tailed) paired permutation test 를 수행하며, 이는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>=</mo><mn>10</mn><mo separator="true" lspace="0em" rspace="0em">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">R = 10{,}000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">10</span><span class="mord"><span class="mpunct">,</span></span><span class="mord">000</span></span></span></span></span> 으로 근사하고, 임계값은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.01</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.01</span></span></span></span></span> 로 설정한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="systems-competitors-and-baselines">Systems, competitors and baselines<a href="#systems-competitors-and-baselines" class="hash-link" aria-label="Direct link to Systems, competitors and baselines" title="Direct link to Systems, competitors and baselines">​</a></h4><p>저자는 embedding-based 와 caption-based VQA 접근 방식의 결과를 Frozen, Linear mapping, MAGMA 세 가지와 비교한다. 이 system 들은 data 와 computation 수준이 유사하다. 이러한 competitor 와의 비교는 저자의 experimental setup 의 적절성을 검증하기 위한 sanity check 역할을 한다.</p><h1>5 Results</h1><p>Tab. 1 은 VQAv2 benchmark 결과를 보여준다.</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-69-f9f981fbc03ffd855646bd9ffc5dd9f9.png" width="687" height="478" class="img_ev3q"></p><ul><li>저자의 system 은 sanity check 를 통과함을 확인할 수 있다. </li><li>Few-shot VQA 에 대한 두 접근 방식 각각의 best 결과 (line 6, 9) 는 모든 shot 수에서 세 가지 competitor system (line 1-3) 보다 우수하다.</li><li>저자의 결과 (line 4-6 vs. line 7-9) 는 0-shot 상황에서 caption-based VQA 가 embedding-based VQA 보다 유의미하게 우수함을 보여준다 (+4.5%). <ul><li>이는 동일한 visual representation 을 사용했음에도 나타나는 놀라운 결과이다. </li><li>유일한 차이는 caption-based VQA 에서 embedding 이 먼저 LLM 에 단독으로 전달되어 caption 을 생성한 후, question 과 concatenate 된다는 점이다. </li><li>이 결과는 image–text space mapping 을 image captioning task 로 학습하는 system 에 대해, caption-based 접근 방식이 반드시 비교되어야 함을 명확히 보여준다.</li></ul></li><li>저자의 결과 (line 9) 는 zero-shot 에서 1-shot embedding-based VQA 로 갈 때 큰 성능 향상 (+9.1%) 이 있음을 보여준다. <ul><li>이는 in-context example 하나만 포함되어도 task 가 명확히 제시되며, LLM 의 VQA 능력이 크게 향상된다는 것을 의미한다. </li><li>이는 task-specific training 없이도 VQA task 에 적응할 수 있게 하는 in-context learning 접근 방식이 효과적임을 보여주는 증거이다.</li></ul></li><li>또한, question (line 5, 8) 또는 question + image (line 6, 9) similarity 기반으로 in-context example 을 선택한 모든 결과가 무작위 선택 (line 4, 7) 보다 유의미하게 우수하다. <ul><li>이는 test question/image 와 유사한 example 을 제공하는 것이 단순히 무작위 example 을 제공하는 것보다 낫다는 것을 보여준다. </li><li>이는 in-context learning 접근 방식이 의도대로 작동하고 있다는 추가적인 증거이다.</li></ul></li><li>1-shot 환경에서 embedding-based 접근 방식 (line 9) 은 caption-based 접근 방식 (line 6) 보다 우수하다 (+0.5%). <ul><li>이 추세는 이후에도 이어지며, 결과는 소폭 개선되고 2-shot 과 4-shot 에서는 embedding-based 접근 방식이 caption 기반보다 약간 더 낫다. </li><li>결과는 shot 수가 증가함에 따라 plateau 에 도달하며, 이는 기존 연구와 일치한다.</li></ul></li></ul><p>저자는 caption-based 와 embedding-based 접근 방식의 prediction 동작을 비교하기 위해 다양한 분석을 수행하였다. 그중 하나는 in-context example selection 방법을 변화시키는 것이었다. 주요 발견은 두 접근 방식이 in-context example 에 대한 반응성에서 차이를 보인다는 점이다.</p><ul><li>가장 두드러진 결과는 embedding-based VQA 에서, in-context example 이 question similarity 기준으로 선택될 때 나타난다 (line 8). <ul><li>이 경우 task 를 적절히 보여주기 위해 최소 두 개의 shot/example 이 필요하며, 이때도 성능 향상은 이전보다 작은 수준 (+4.3%) 에 그쳤다. </li></ul></li><li>또한 caption-based 접근 방식은 in-context example 을 question similarity 만으로 선택했을 때, 모든 shot 수에서 embedding-based VQA (line 5 vs. line 8) 보다 항상 우수하였다.</li></ul><p>추가적으로 저자는 color 나 counting 정보와 같은 특정 question category 에서 system 의 정확도를 분석하였다. </p><ul><li>분석 결과, embedding-based 접근 방식의 4-shot VQA 는 caption-based 접근 방식보다 세부적인 visual detail 이 요구되는 질문에서 더 많은 정답을 맞췄다 (e.g., color +4.2%, counting +2.7%). </li><li>반면, 단순한 질문 (e.g., yes/no) 에서는 약간 뒤처졌다 (-0.5%). 복잡한 &quot;why&quot; 질문이나 location &quot;where&quot; 질문처럼 system 이 약한 영역에서는 두 접근 방식 모두 유사한 prediction 동작을 보였다.</li></ul><h1>6 Limitations</h1><p>저자의 발견은 Flan-T5 XL LLM 에 한정되며, 이는 다른 closed-source LLM 만큼 크지 않다. 더 큰 model 에 대해서는 다른 결과가 나왔을 수도 있다. 또한 저자는 Li et al. 이 제시한 방식과 같은 다른 방법이 아닌, 비선형 network 를 사용하여 image–text space 사이의 mapping 을 학습하였다. 대신 저자는 caption-based 와 embedding-based 접근 방식의 few-shot VQA 를 탐구하기 위한 통제되고 집중된 실험 환경을 제시하였다.</p><h1>7 Conclusion</h1><p>본 연구에서는 few-shot VQA 의 두 가지 접근 방식을 집중적으로 비교하였다. 이 두 접근 방식의 비교는 기존 연구에서 간과되어 왔다. 저자는 CLIP 과 Flan-T5 XL embedding space 사이의 비선형 mapping 을 학습하였으며, 이를 통해 동일한 system 에서 image caption 과 embedding-based LLM image representation 을 모두 생성할 수 있었다.</p><p>저자의 결과는 비선형 mapping 과 비교적 작은 frozen LLM 을 사용할 때, visual embedding 을 LLM embedding space 에 직접 연결하는 것이 image caption 을 사용하는 것보다 반드시 성능 향상을 보장하지 않는다는 것을 보여준다. 오히려 관련성 있는 in-context example 의 선택이 훨씬 더 중요한 요인임을 확인하였다.</p><p>결론적으로 본 연구는 embedding-based visual representation 을 LLM 에 전달하는 multimodal system 에서, system-generated caption 을 활용한 결과를 보고하는 것이 반드시 수행되어야 할 중요한 비교임을 보여주었다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">Vision-Language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multimodal">Multimodal</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vqa">VQA</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/few-shot-learning">few-shot learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/frozen">Frozen</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/llm">LLM</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/VQA-IC/Few-shot/2024-03-LLMVQA.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/CoTMT"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.ec4b5b0a.js"></script>
<script src="/assets/js/main.d9f0a85a.js"></script>
</body>
</html>