<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/VQA-IC/Zero-shot/2023-03-DeCap">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">DeCap: Decoding CLIP Latents for Zer-Shot Captioning via Text-Only Training | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Zero-shot/DeCap"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="DeCap: Decoding CLIP Latents for Zer-Shot Captioning via Text-Only Training | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Zero-shot/DeCap"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Zero-shot/DeCap" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/VQA-IC/Zero-shot/DeCap" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.3298527c.js" as="script">
<link rel="preload" href="/assets/js/main.aca6605f.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Reinforce Learning/DPO/RLHF/Self-RLM">Reinforce Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">CLIP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Foundation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">VQA-IC</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">Few-shot</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/ZeroCap">Zero-shot</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/ZeroCap">ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/MAGIC">Language Models Can See: Plugging Visual Controls in Text Generation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/CLOSE">I Can&#x27;t Believe There&#x27;s No Images! Learning Visual Tasks Using only Language Supervision</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/CapDec">Text-Only Training for Image Captioning using Noise-Injected CLIP</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/DeCap">DeCap: Decoding CLIP Latents for Zer-Shot Captioning via Text-Only Training</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/Knight">From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/ZeroGen">ZeroGen: Zero-shot Multimodal Controllable Text Generation with Multiple Oracles</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/ViECap">Transferable Decoding with Visual Entities for Zero-Shot Image Captioning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/MeaCap">MeaCap: Memory-Augmented Zero-shot Image Captioning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/IFCap">IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/MERCap">Zero-Shot Image Captioning with Multi-type Entity Representations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/Patch-ioner">One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework</a></li></ul></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">VQA-IC</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Zero-shot</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">DeCap: Decoding CLIP Latents for Zer-Shot Captioning via Text-Only Training</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>DeCap: Decoding CLIP Latents for Zer-Shot Captioning via Text-Only Training</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2303.03032" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2303.03032</a></p><h1>Abstract</h1><p>Large-scale pre-trained multi-modal model (e.g., CLIP) 은 image classification 등 많은 discriminative task 에서 strong zero-shot transfer capability 를 보여준다. zero-shot image-conditioned text generation task 로의 adaptation 은 점점 더 많은 관심을 받고 있다. </p><p>기존 연구는 zero-shot captioning 을 위해 기존 large language model (e.g., GPT-2) 을 활용하거나, encoder-decoder network 를 end-to-end 방식으로 pre-training 하는 접근을 취한다. 그러나 large language model 은 captioning 과 language modeling 사이의 task discrepancy 때문에 합리적인 description 을 생성하지 못할 수 있고, end-to-end pre-training 은 paired data 와 막대한 computational resource 를 필요로 한다.</p><p>이 논문에서는 zero-shot captioning 을 위한 간단한 framework 인 <strong>DeCap</strong> 을 제안한다. 저자는 lightweight 한 visual-aware language decoder 를 도입한다. 이 decoder 는 data-efficient 하고 computation-efficient 하다.</p><ol><li>training 에 text data 만 필요하여, paired data 수집에 대한 부담을 줄인다.</li><li>end-to-end training 을 요구하지 않는다.</li></ol><p>text-only data 로 training 할 때, decoder 는 off-the-shelf CLIP encoder 에서 추출한 text embedding 을 prefix embedding 으로 사용한다. 도전 과제는 decoder 가 text corpus 로 training 되지만, inference 단계에서는 visual input 에 기반해 caption 을 생성해야 한다는 점이다. CLIP text embedding 과 visual embedding 이 서로 상관관계가 있긴 하지만, multi-modal contrastive model 에서 널리 관찰되는 modality gap 문제로 인해 visual embedding 을 prefix embedding 으로 직접 사용할 수 없다.</p><p>저자는 training-free mechanism 을 제안하여 modality gap 을 줄인다. visual embedding 을 CLIP text embedding space 로 projection 하는데, 이때 projected embedding 은 visual input 의 정보를 유지한다. projected embedding 을 prefix embedding 으로 사용하면, decoder 는 visual input 과 잘 부합하는 high-quality description 을 생성한다.</p><p>실험 결과 DeCap 은 대표적인 image captioning benchmark 인 MSCOCO 와 NoCaps 에서 다른 zero-shot captioning 방법 및 unpaired captioning 방법들을 큰 폭으로 능가한다. 또한 DeCap 을 video captioning 에 적용하여 MSR-VTT 와 ActivityNet-Captions 에서 state-of-the-art zero-shot performance 를 달성한다.</p><h1>1 Introduction</h1><p>image captioning 의 목표는 주어진 image 에 대해 자동으로 description 을 생성하는 것이다. human-annotated image-text pair 로 training 된 model 은 typical image captioning benchmark 에서 impressive 한 결과를 달성했다. 그러나 human-annotated dataset 의 small size 와 제한된 visual concept 때문에, 이러한 model 은 wild 환경의 image 에 대해 generalization 이 낮게 나타난다. 본 논문에서는 human-annotated paired data 에 대한 의존을 줄이고 real-world captioning scenario 에서의 generalization 을 개선하기 위해, training 시 text-only data 만을 요구하는 새로운 zero-shot captioning framework 를 제안한다.</p><p>web-scale noisy paired data 로 pre-training 하는 것은 robust multi-modal representation 을 학습하는 데 효과적임이 입증되어 왔다. Changpinyo et al. 과 Wang et al. 은 web-scale image-text pair 를 사용하여 captioning model 을 training 하고, pretraining-finetuning paradigm 을 통해 MSCOCO 와 NoCaps 에서 큰 향상을 달성했다. 그러나 이러한 model 은 MSCOCO zero-shot captioning 에서는 성능이 낮게 나타나, 여전히 fine-tuning 을 위해 human-annotated paired data 에 의존하고 있음을 보여준다. 또한 web-scale data 에서 captioning objective 로 training 하는 것은 효율적이지 않으며, 예를 들어 Wang et al. 은 ALIGN 과 C4 에서 약 1M step 을 512 TPU v3 chip 으로 training 했다.</p><p>web-scale image-text pair 를 end-to-end 방식으로 captioning model 로 직접 training 하는 것 대신, 또 다른 연구 흐름은 기존 pre-trained model 을 결합하여 zero-shot captioning 을 달성한다. </p><ul><li>구체적으로, pre-trained multi-modal model 인 CLIP 을 사용해 pre-trained language model (PLM) 인 GPT-2 를 guide 하여 주어진 image 와 일치하는 sentence 를 생성한다. </li><li>그러나 이러한 방법은 word 를 하나 생성할 때마다 CLIP text encoder forward 가 필요하므로 inference 속도가 매우 느리다. </li><li>또한 웹에서 수집된 다양한 문서로 pre-training 된 language model 은 image 내 visual concept 과 그 관계를 묘사하는 captioning task 와 잘 맞지 않아 captioning benchmark 에서 성능이 떨어진다.</li></ul><p>본 논문에서는 zero-shot captioning 을 위한 새로운 framework 인 <strong>DeCap</strong> 을 제안한다. </p><ul><li>저자는 CLIP multi-modal embedding space 로부터 sensible 한 visual description 을 decoding 하는 것을 목표로 한다. </li><li>decoder pre-training 에 paired image-text data 를 사용하지 않고 text data 만을 활용한다. <ul><li>이는 image 와 text 의 alignment 가 noisy 해질수록 더 flexible 하고 efficient 하다. </li></ul></li></ul><p>DeCap framework 는 다음과 같다. </p><ul><li>pre-training 동안 text decoder 는 scratch 에서 training 된다. 목적은 CLIP text encoder 를 invert 하는 것이다. <ul><li>즉, sentence 를 CLIP text encoder 가 embedding 으로 encode 하고, text decoder 가 이를 reconstruct 한다. </li></ul></li><li>decoder 는 CLIP text encoder 로부터 얻은 text embedding 을 prefix embedding 으로 사용한다. </li><li>zero-shot inference 동안에는 input image 와 잘 맞고 decoder 가 잘 decode 할 수 있는 prefix embedding 을 얻는 것이 핵심 문제이다. </li><li>multi-modal contrastive model 에서 modality gap phenomenon 이 관찰되며, 이는 visual embedding 을 prefix embedding 으로 직접 사용하는 것을 어렵게 만든다. <ul><li>Ramesh et al. (2022) 은 paired data 로 text embedding 을 대응하는 image embedding 으로 map 하는 model 을 학습했다. </li></ul></li><li>저자는 이러한 학습 대신, training-free mechanism 을 제안하여 image embedding 을 CLIP text embedding space 로 projection 한다. </li><li>text decoder 와 projection mechanism 을 결합해 주어진 image 에 대한 high-quality description 을 생성한다.</li></ul><p>저자의 주요 contribution 은 다음과 같다.</p><ol><li>zero-shot captioning 을 위한 새로운 framework 를 제안한다. DeCap framework 는 pre-trained contrastive model (i.e., CLIP) 과 CLIP embedding 을 input 으로 받는 lightweight visual-aware language decoder 로 구성된다. <ul><li>decoder 는 text corpus 로만 training 되었음에도, CLIP embedding space 에 encode 되어 있는 multi-modal correlation 덕분에 visual embedding 과 text embedding 을 모두 연결할 수 있다.</li></ul></li><li>CLIP multi-modal embedding space 의 modality gap 을 줄이기 위한 training-free projection mechanism 을 제안한다. <ul><li>pre-training 단계에서 text corpus 의 embedding 으로 구성된 simple support memory 를 도입한다. </li><li>visual embedding 을 support memory 를 통해 CLIP text embedding space 로 projection 한다. </li><li>실험 결과 이 mechanism 은 modality gap 을 효과적으로 줄이고 성능을 크게 향상시킨다.</li></ul></li><li>다양한 captioning scenario 에서 DeCap 의 flexibility 를 extensive experiment 로 입증한다. <ul><li>DeCap 은 MSCOCO 와 NoCaps 의 image captioning benchmark 에서 다른 zero-shot captioning 방법을 큰 폭으로 능가한다. </li><li>text-only data 로 training 된 DeCap 은 MSCOCO 와 Flickr30k 에서 다른 unpaired captioning 방법보다 우수하다. </li><li>또한 DeCap 을 video captioning 에 적용하여 MSR-VTT 와 ActivityNet-Captions 에서 state-of-the-art zero-shot 결과를 달성한다.</li></ul></li></ol><h1>2 Related Work</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="clip-in-captioning">CLIP in Captioning<a href="#clip-in-captioning" class="hash-link" aria-label="Direct link to CLIP in Captioning" title="Direct link to CLIP in Captioning">​</a></h4><p>contrastive loss 로 training 된 vision-language model 은 많은 discriminative task 에서 impressive 한 능력을 보인다. 그러나 pre-training 동안 text decoder 가 없기 때문에 captioning 과 같은 generative task 에는 직접 적용할 수 없다. </p><p>기존 연구는 CLIP 을 image captioning task 의 visual encoder 로 사용했다. 그러나 이들은 CLIP text encoder 를 무시하고, CLIP 이 제공하는 aligned multi-modal latent space 를 활용하지 않았다. </p><p>본 연구에서는 text-only data 로 text decoder 를 training 하여 CLIP text encoder 를 invert 한다. CLIP multi-modal latent space 를 활용함으로써, 추가적인 pairwise training 없이 captioning task 에 CLIP 을 적용한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="zero-shot-captioning">Zero-shot Captioning<a href="#zero-shot-captioning" class="hash-link" aria-label="Direct link to Zero-shot Captioning" title="Direct link to Zero-shot Captioning">​</a></h4><p>zero-shot captioning 은 human-annotated data 없이 image/video caption 을 생성하는 것을 목표로 한다. </p><ul><li>Changpinyo et al., Wang et al., Alayrac et al. 은 Web 에서 수집된 noisy paired image-text data 로 vision-language model 을 training 하고, fine-tuning 없이 downstream benchmark 를 평가한다. </li></ul><p>또 다른 연구 흐름은 web-scale pre-trained model 을 결합해 zero-shot captioning 을 구현한다. </p><ul><li>ZeroCap 은 multi-modal model (e.g., CLIP) 과 PLM (e.g., GPT-2) 을 결합한다. </li><li>매 generation step 마다, CLIP 을 사용해 GPT-2 를 desired visual direction 으로 guide 한다. </li><li>Socratic Models 은 pre-trained VLM 을 활용해 GPT-3 의 prompt template 을 생성한 뒤, CILP 를 사용해 candidate 중 image 와 가장 가까운 description 을 retrieval 한다. </li></ul><p>본 연구에서는 CLIP 을 zero-shot captioning 에 활용한다. 위 연구들이 PLM 을 사용하는 것과 달리, 저자는 text-only data 로 decoder 를 scratch 에서 training 한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="text-reconstruction">Text Reconstruction<a href="#text-reconstruction" class="hash-link" aria-label="Direct link to Text Reconstruction" title="Direct link to Text Reconstruction">​</a></h4><p>기존 연구는 unpaired/unsupervised captioning task 에서 decoder 를 training 하기 위해 text reconstruction task 를 사용한다. well-aligned multi-modal latent space 가 없기 때문에, 대부분의 방법은 decoder 와 visual input 을 align 하기 위해 복잡한 pseudo-training 이나 adversarial training 이 필요하다. </p><p>Liu et al. (2021b) 는 visual domain 과 textual domain representation 을 연결하기 위해 knowledge graph 를 구성한다. 그러나 이 방법은 well-defined knowledge graph 와 multi-label classification task 로 graph 를 training 해야 하므로, medical report generation 외 captioning task 에 적용하기 어렵다. </p><p>CLIP 의 이점을 통해, 한편으로 저자의 decoder 는 CLIP 의 aligned cross-modal embedding space 를 활용해 visual input 과 직접 연결될 수 있다. 다른 한편으로, decoder 는 다양한 text data 로 training 될 수 있으며 다양한 captioning task 에 적용될 수 있다.</p><h1>3 Method</h1><p>저자의 framework 는 Fig. 1 에 제시된다. </p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-22-51a1031628d52dcb3322ab5a2a3eb91c.png" width="1214" height="644" class="img_ev3q"></p><p>저자는 CLIP text encoder 를 invert 하기 위해 text decoder 를 학습한다 (Sec. 3.1). 이 text decoder 는 CLIP text embedding 에 기반하여 sentence 를 생성할 수 있게 한다. inference 시에는 image embedding 을 text embedding space 로 projection 하여 text embedding space 와 image embedding space 사이의 modality gap 을 줄이는 training-free mechanism 을 제안한다 (Sec. 3.2.1). 비교를 위해 추가적인 inference strategy 도 도입한다 (Sec. 3.2.2).</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-text-only-decoder-pre-training">3.1 Text-Only Decoder Pre-training<a href="#31-text-only-decoder-pre-training" class="hash-link" aria-label="Direct link to 3.1 Text-Only Decoder Pre-training" title="Direct link to 3.1 Text-Only Decoder Pre-training">​</a></h2><p>이전 접근법은 zero-shot captioning 을 위해 PLM 을 사용하여 다양한 sentence 를 생성한다. 그러나 웹 페이지에서 수집된 다양한 document 로 training 된 PLM 은 주어진 image 의 visual concept 과 그 관계를 설명하는 것을 목표로 하는 captioning task 와 잘 맞지 않는다.</p><p>저자는 PLM 을 사용하는 대신, CLIP text encoder 를 invert 하기 위해 text decoder 를 scratch 에서 training 한다. </p><p>recent work 을 따라, 저자는 prefix language modeling 을 사용해 decoder 를 training 한다. </p><ul><li>구체적으로, 주어진 sentence <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mtext>word</mtext><mn>1</mn></msub><mo separator="true">,</mo><msub><mtext>word</mtext><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mtext>word</mtext><mrow><mi mathvariant="normal">∣</mi><mi>t</mi><mi mathvariant="normal">∣</mi></mrow></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">t = \{\text{word}_1, \text{word}_2, \ldots, \text{word}_{|t|}\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3552em"></span><span class="mopen">{</span><span class="mord"><span class="mord text"><span class="mord">word</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord text"><span class="mord">word</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord text"><span class="mord">word</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight">t</span><span class="mord mtight">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span> 에 대해, prefix language model <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>∗</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex">P*\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span></span> 는 fixed CLIP text encoder 로부터 추출된 text embedding 에 조건부로 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span> 를 reconstruct 하도록 학습된다. </li><li>저자는 text embedding 을 caption 의 prefix 로 간주한다. </li></ul><p>저자의 objective 는 다음과 같이 표현된다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="script">L</mi><mtext>Recons</mtext></msub><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>t</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi mathvariant="normal">∣</mi><mi>t</mi><mi mathvariant="normal">∣</mi></mrow></munderover><mi>log</mi><mo>⁡</mo><msub><mi>P</mi><mi>θ</mi></msub><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><msub><mtext>word</mtext><mi>i</mi></msub><mo>∣</mo><msub><mtext>word</mtext><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo separator="true">,</mo><msub><mi>E</mi><mtext>text</mtext></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{Recons}}(\theta) = - \frac{1}{|t|} \sum_{i=1}^{|t|} \log P_\theta\big(\text{word}_i \mid \text{word}_{&lt;i}, E_{\text{text}}(t)\big),</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">Recons</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.2387em;vertical-align:-1.2777em"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">∣</span><span class="mord mathnormal">t</span><span class="mord">∣</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.961em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.386em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight">t</span><span class="mord mtight">∣</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord text"><span class="mord">word</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mord"><span class="mord text"><span class="mord">word</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">text</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mord"><span class="delimsizing size1">)</span></span><span class="mpunct">,</span></span></span></span></span></div><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mtext>text</mtext></msub><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E_{\text{text}}(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">text</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> 은 sentence 를 CLIP text encoder 를 통해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">ℓ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\ell_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>-normalized embedding space 로 mapping 하는 함수를 의미한다. </li></ul><p>text-only data 로 self-supervised 방식으로 training 된 이 decoder 는 두 가지 이점을 제공한다.</p><ol><li>text-only data 의 source 를 조정함으로써 생성되는 sentence 의 style 을 제어할 수 있다. <ul><li>task-specific 한 descriptive caption 을 생성하기 위해, 저자는 human-annotated image description 과 web 에서 수집된 image caption 으로부터 온 text data 로 decoder 를 training 한다.</li></ul></li><li>이 text decoder 는 CLIP text embedding 을 prefix embedding 으로 사용한다. <ul><li>CLIP text embedding 은 CLIP image embedding 과 상관관계를 갖도록 최적화되어 있기 때문에, 별도의 pairwise training 없이도 text decoder 를 visual input 과 연결하는 것이 가능하다.</li></ul></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-inference-strategies">3.2 Inference Strategies<a href="#32-inference-strategies" class="hash-link" aria-label="Direct link to 3.2 Inference Strategies" title="Direct link to 3.2 Inference Strategies">​</a></h2><p>Sec. 3.1 에서 저자는 CLIP text embedding 을 조건으로 description 을 생성할 수 있는 decoder 를 얻었다. </p><p>inference 단계에서는 CLIP image embedding 이 주어졌을 때, decoder 를 어떻게 활용해 description 을 생성할지 결정해야 한다. CLIP image embedding space 와 text embedding space 사이에는 modality gap 이 존재하므로, CLIP image embedding 을 prefix embedding 으로 직접 사용하는 것은 실용적이지 않다. </p><p>Ramesh et al. (2022) 은 text embedding 을 대응하는 image embedding 으로 map 하는 prior model 을 paired data 로 학습한다. 그러나 이러한 접근은 paired data 가 필요하다. 저자는 image embedding 을 text embedding space 로 projection 하는 training-free mechanism 을 제안한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="321-projection-based-decoding-pd">3.2.1 Projection-based Decoding (PD)<a href="#321-projection-based-decoding-pd" class="hash-link" aria-label="Direct link to 3.2.1 Projection-based Decoding (PD)" title="Direct link to 3.2.1 Projection-based Decoding (PD)">​</a></h3><p>language model <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">P_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 가 주어진 text set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>t</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>t</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">T = \{t_1, t_2, \ldots, t_N\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span> 으로 training 되었다고 가정한다. 여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span></span></span></span></span> 의 size 이다. CLIP text embedding space 를 표현하기 위해 support memory <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>m</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>m</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>m</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">M = \{m_1, m_2, \ldots, m_N\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span> 를 유지하며, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><msub><mi>E</mi><mtext>text</mtext></msub><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">m_i = E_{\text{text}}(t_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">text</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 이다.</p><p>inference 시, 주어진 image <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">I</span></span></span></span></span> 에 대해 caption 을 생성하는 것이 목표이다. support memory <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span></span> 을 활용해 image embedding 을 text embedding space 로 projection 할 수 있다. 구체적으로, image embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><msub><mi>E</mi><mtext>image</mtext></msub><mo stretchy="false">(</mo><mi>I</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v = E_{\text{image}}(I)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">image</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mclose">)</span></span></span></span></span> 가 주어지면, support memory 의 embedding 에 대한 weighted combination 을 통해 text embedding space 내에서의 representation 을 얻는다.</p><p>weight 를 구하기 위해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span> 의 cosine similarity 를 계산하고, temperature parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span></span></span></span></span> 로 scaling 한 뒤 softmax 로 정규화한다. project 된 vector <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mtext>proj</mtext></msub></mrow><annotation encoding="application/x-tex">v_{\text{proj}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">proj</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 는 아래와 같이 계산된다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>v</mi><mtext>proj</mtext></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>w</mi><mi>i</mi></msub><mo>∗</mo><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mo stretchy="false">(</mo><msubsup><mi>m</mi><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><mi>v</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>τ</mi><mo fence="true">)</mo></mrow></mrow><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mo stretchy="false">(</mo><msubsup><mi>m</mi><mi>k</mi><mi mathvariant="normal">⊤</mi></msubsup><mi>v</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>τ</mi><mo fence="true">)</mo></mrow></mrow></mfrac><mo>∗</mo><msub><mi>m</mi><mi>i</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">v_{\text{proj}} = \sum_{i=1}^{N} w_i * m_i = \sum_{i=1}^{N} \frac{ \exp\left((m_i^\top v) / \tau\right) }{ \sum_{k=1}^{N} \exp\left((m_k^\top v) / \tau\right) } * m_i,</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">proj</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.59em"><span style="top:-2.1288em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">exp</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size1">(</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8309em"><span style="top:-2.3987em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span><span style="top:-3.0448em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size1">)</span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.74em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop">exp</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size1">(</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size1">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2212em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span></span></span></span></span></div><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 support memory 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>-th text embedding 의 weight 이다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mtext>proj</mtext></msub></mrow><annotation encoding="application/x-tex">v_{\text{proj}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">proj</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 는 CLIP text embedding 의 combination 으로 구성되며, prefix embedding 으로 사용할 수 있다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P_\theta(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> 를 prefix embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 에 조건부로 sentence 를 auto-regressive 하게 생성하는 process 라 하자. </li></ul><p>final output 은 아래와 같이 생성된다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>θ</mi></msub><mrow><mo fence="true">(</mo><mfrac><msub><mi>v</mi><mtext>proj</mtext></msub><mrow><mi mathvariant="normal">∥</mi><msub><mi>v</mi><mtext>proj</mtext></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub></mrow></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_\theta\left(\frac{v_{\text{proj}}}{\|v_{\text{proj}}\|_2}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8087em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.334em"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">proj</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.5073em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.334em"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">proj</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5423em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size2">)</span></span></span></span></span></span></span>.</p><ul><li>이 projection-based 방법은 추가 training 을 필요로 하지 않는다. 다양한 dataset 에서 잘 작동하고, 매우 flexible 하다. </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mtext>proj</mtext></msub></mrow><annotation encoding="application/x-tex">v_{\text{proj}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">proj</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 는 support memory 내 text embedding 의 정보를 흡수할 수 있어, 다양한 형태의 정확한 description 을 생성한다. </li><li>또한 training 에 사용한 text data 와 support memory 에 저장된 text data 는 서로 다를 수 있다. </li><li>target domain 에 맞는 새로운 text data 로 support memory 를 재구성하면, image embedding 도 새로운 text embedding space 로 projection 되어 retraining 없이 새로운 domain 에 빠르게 generalize 할 수 있다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="322-discussion">3.2.2 Discussion<a href="#322-discussion" class="hash-link" aria-label="Direct link to 3.2.2 Discussion" title="Direct link to 3.2.2 Discussion">​</a></h3><p>저자의 decoder 와 projection-based mechanism 의 영향을 분석하기 위해, 다음 inference strategy 를 비교 대상으로 포함한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-clipre">1) CLIPRe<a href="#1-clipre" class="hash-link" aria-label="Direct link to 1) CLIPRe" title="Direct link to 1) CLIPRe">​</a></h4><p>decoder 가 필요 없는 simple retrieval-based approach 로, Su et al. (2022) 에 언급되어 있다. image <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">I</span></span></span></span></span> 와 text set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span></span></span></span></span> 가 주어지면, CLIPRe 는 CLIP 의 image-text similarity 를 기반으로 가장 관련 높은 text 를 retrieval 한다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">arg max</mi><mo>⁡</mo></mrow><mrow><mi>t</mi><mo>∈</mo><mi>T</mi></mrow></msub><mtext>sim</mtext><mo stretchy="false">(</mo><msub><mi>E</mi><mtext>image</mtext></msub><mo stretchy="false">(</mo><mi>I</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>E</mi><mtext>text</mtext></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\argmax_{t \in T} \text{sim}(E_{\text{image}}(I), E_{\text{text}}(t)).</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mop"><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">arg</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathrm">max</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2342em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2715em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord text"><span class="mord">sim</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">image</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">text</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">))</span><span class="mord">.</span></span></span></span></span></p><ul><li>sim 은 cosine similarity 를 의미한다. </li><li>모든 실험에서 저자는 CLIPRe 를 baseline 으로 사용하며, 이는 decoder 없이 original CLIP 의 zero-shot 성능을 잘 반영한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-visual-decoding-vd">2) Visual Decoding (VD)<a href="#2-visual-decoding-vd" class="hash-link" aria-label="Direct link to 2) Visual Decoding (VD)" title="Direct link to 2) Visual Decoding (VD)">​</a></h4><p>text embedding 과 image embedding 이 correlated 라는 점에 기반한 단순한 접근으로, image embedding 자체를 prefix embedding 으로 사용하는 것이다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>E</mi><mtext>image</mtext></msub><mo stretchy="false">(</mo><mi>I</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P_\theta(E_{\text{image}}(I))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">image</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mclose">))</span></span></span></span></span>.</p><p>그러나 대부분의 실험에서 이 방법은 만족스러운 성능을 내지 못하며, 이는 CLIP image embedding 과 text embedding 사이에 modality gap 이 존재함을 의미한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="3-nearest-neighbor-decoding-nnd">3) Nearest-neighbor Decoding (NND)<a href="#3-nearest-neighbor-decoding-nnd" class="hash-link" aria-label="Direct link to 3) Nearest-neighbor Decoding (NND)" title="Direct link to 3) Nearest-neighbor Decoding (NND)">​</a></h4><p>또 다른 단순한 방법은 가장 가까운 text embedding 을 prefix embedding 으로 사용하는 것이다. image embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mtext>image</mtext></msub><mo stretchy="false">(</mo><mi>I</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E_{\text{image}}(I)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">image</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mclose">)</span></span></span></span></span> 와 support memory <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span></span> 의 text embedding 간 similarity 를 계산한 뒤, 가장 가까운 embedding 을 선택한다: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>θ</mi></msub><mrow><mo fence="true">(</mo><msub><mrow><mi mathvariant="normal">arg max</mi><mo>⁡</mo></mrow><mrow><mi>m</mi><mo>∈</mo><mi>M</mi></mrow></msub><mtext>sim</mtext><mo stretchy="false">(</mo><msub><mi>E</mi><mtext>image</mtext></msub><mo stretchy="false">(</mo><mi>I</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_\theta\left(\argmax_{m \in M} \text{sim}(E_{\text{image}}(I), m)\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">(</span><span class="mop"><span class="mop"><span class="mord mathrm" style="margin-right:0.01389em">arg</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathrm">max</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2342em"><span style="top:-2.4559em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2715em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord text"><span class="mord">sim</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">image</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">m</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">)</span></span></span></span></span></span>.</p><p>이론적으로 NND 와 CLIPRe 는 유사한 성능을 보여야 한다. decoder 가 text embedding 에 조건부로 원래 text 를 잘 복원하도록 학습되었기 때문이다. 흥미롭게도, 실험 결과 NND 는 대부분의 경우 CLIPRe 보다 더 좋은 성능을 보인다. 이는 decoder 가 더 descriptive 한 sentence 를 생성할 수 있음을 시사한다. 또한 decoder 를 사용해 새로운 text corpus 를 reconstruct 하면 성능이 더 향상될 수 있음도 확인했다. 추가 결과와 논의는 Appendix B 에 제시된다.</p><h1>4 Experiments</h1><p>저자는 zero-shot image captioning, unpaired image captioning, video captioning 등 다양한 captioning task 에 대해 extensive experiment 를 수행한다. </p><p>DeCap 은 다양한 설정에서 효율적으로 impressive 한 결과를 달성한다. Sec. 4.1 에서는 어떠한 human annotation 없이 zero-shot image captioning 에 초점을 둔다. Sec. 4.2 에서는 image 와 sentence 를 독립적으로 다루는 unpaired image captioning 에 초점을 둔다. Sec. 4.3 에서는 DeCap 을 video captioning task 에 적용한다. Sec. 4.4 에서는 DeCap 에 대한 detailed ablation study 를 수행한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="implementation-details">Implementation Details<a href="#implementation-details" class="hash-link" aria-label="Direct link to Implementation Details" title="Direct link to Implementation Details">​</a></h4><ul><li>저자는 frozen pre-trained ViT-B/32 CLIP model 을 사용한다. </li><li>language model 로는 4-layer Transformer (4 attention head) 를 adopt 하며, hidden state size 는 768 이다. </li><li>기본적으로, training set 의 모든 text data 를 사용해 naive cross-entropy loss 로 language model 을 scratch 부터 training 한다. </li><li>training corpus 로부터의 모든 text embedding 은 별도 언급되지 않는 한 support memory 에 저장한다. </li><li>inference 시, Eq. 2 의 temperature <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span></span></span></span></span> 는 video captioning 에서는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mn>150</mn></mrow><annotation encoding="application/x-tex">1/150</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">1/150</span></span></span></span></span>, image captioning 에서는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mn>100</mn></mrow><annotation encoding="application/x-tex">1/100</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">1/100</span></span></span></span></span> 으로 설정한다. </li><li>captioning 평가 metric 으로 BLEU@N, METEOR, CIDEr, SPICE 를 사용한다. </li><li>또한 CLIP-SRef 와 CLIP-S 를 사용해 text-text similarity 와 text-image similarity 를 측정한다. beam search 및 constrained beam search 는 사용하지 않는다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-zero-shot-image-captioning">4.1 Zero-Shot Image Captioning<a href="#41-zero-shot-image-captioning" class="hash-link" aria-label="Direct link to 4.1 Zero-Shot Image Captioning" title="Direct link to 4.1 Zero-Shot Image Captioning">​</a></h2><p>본 절에서는 webly-collected corpus 를 사용해 zero-shot image captioning 을 수행한다. 기존 image captioning 방법은 paired human-annotated data 를 사용하지만, 이러한 data 는 얻기 어렵고 규모 및 다양성이 제한적이다. 저자는 DeCap training 을 위해 다음 세 가지 webly-collected corpus 를 고려한다.</p><ol><li><strong>CC3M (CC3M-text)</strong><ul><li>CC3M 은 Web 에서 수집된 300 만 개의 image-description pair 로 구성된 dataset 이다. </li><li>저자는 training 시 text description (CC3M-text) 만 사용한다. support memory 는 300 만 description 중 100 만 개를 random sampling 하여 구성한다.</li></ul></li><li><strong>SS1M</strong><ul><li>SS1M 은 MSCOCO caption 을 위해 특별히 설계된 webly-collected corpus 이다. </li><li>Feng et al. 은 MSCOCO 의 80 object class 이름을 keyword 로 사용해 Shutterstock 에서 description 을 crawling 하여 총 2,322,628 개의 distinct image description 을 수집했다. </li><li>저자는 이 corpus 를 reuse 하되, 길이가 15 단어를 넘는 sentence 를 제거하여 978,662 개의 sentence 를 얻는다.</li></ul></li><li><strong>Book Corpus</strong><ul><li>Book Corpus 는 많은 양의 novel text 로 구성된 대규모 dataset 이며, unsupervised language model pre-training 에 자주 사용된다. </li><li>본 연구에서는 Book Corpus 를 captioning 용 text decoder training 에 활용한다. <ul><li>그러나 Book Corpus 는 매우 크고 많은 sentence 가 visual-related 가 아니므로 training 이 비효율적이다. </li></ul></li><li>저자는 CLIP text embedding 의 norm 이 visual-related 여부를 대략적으로 구분할 수 있음을 발견했다. <ul><li>보통 norm 이 큰 sentence 는 visual-related 가 아니다. training efficiency 를 개선하기 위해, sentence 길이가 15 미만이고 norm 이 10 미만인 sentence 만 유지하여 총 6,217,799 sentence 를 얻는다. </li><li>support memory 는 training data 중 100 만 sentence 를 random sampling 해 구성한다. </li><li>또한 Book Corpus 로 training 한 model 에는 prompt “Attention! There is/are” 를 사용한다. </li><li>반면 CC3M 로 training 한 경우 prompt engineering 이 필요하지 않았다. 추가 prompt, 결과 및 분석은 Appendix F 에 정리되어 있다.</li></ul></li></ul></li></ol><p>다음 zero-shot captioning method 들을 비교 대상으로 포함했다.</p><ul><li>Changpinyo et al. 은 webly-collected paired data 로 captioning model 을 training 한 뒤 fine-tuning 없이 downstream dataset 에 transfer 한다.</li><li>ZeroCap 은 CLIP + GPT-2 를 이용하는 training-free zero-shot captioning 방법이다.</li><li>DeCap 은 CLIP 을 활용하지만, webly-collected corpus 로부터 scratch 부터 decoder 를 training 한다. 기본적으로 projection-based decoding (PD) 을 사용한다.</li><li>Sec. 3.2 의 다른 inference strategy 인 visual decoding (DeCap-VD), nearest-neighbor decoding (DeCap-NND) 도 비교한다.</li></ul><p>모든 방법은 zero-shot image captioning 을 목표로 하며, human-annotated data 를 사용하지 않는다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results">Results<a href="#results" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h4><p>Tab. 1 은 MSCOCO 및 NoCaps 에서의 zero-shot 결과를 보여준다. </p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-23-84d05ead243b654c26ce821f212aae11.png" width="1217" height="481" class="img_ev3q"></p><ul><li>DeCap 은 모든 metric 에서 새로운 state-of-the-art 를 달성했다. </li><li>NoCaps 에서는 webly-collected data 로 pre-training 한 model 들이 out-of-domain 성능이 더 좋다. <ul><li>이는 webly-collected data 에 다양한 visual concept 이 포함되어 있기 때문이다. </li></ul></li><li>MSCOCO 에서, CC3M-text 로 pre-trained DeCap 은 CIDEr 기준 ZeroCap 대비 27.5% 향상한다. </li><li>SS1M 로 pre-trained DeCap 은 ZeroCap 대비 36% 향상한다. </li><li>또한 SS1M 으로 training 한 경우 CC3M 대비 더 높은 성능(CIDEr: 50.6% vs. 42.1%) 을 보이며, 이는 task-specific corpus 가 downstream dataset 성능을 크게 향상시킬 수 있음을 의미한다. </li><li>Book Corpus 로 training 한 경우에도 ZeroCap 보다 높은 성능을 보인다. </li><li>주목할 점은 DeCap-BookCorpus 와 ZeroCap 모두 caption-related data 를 보지 않았다는 것이다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-unparied-image-captioning">4.2 Unparied Image Captioning<a href="#42-unparied-image-captioning" class="hash-link" aria-label="Direct link to 4.2 Unparied Image Captioning" title="Direct link to 4.2 Unparied Image Captioning">​</a></h2><p>DeCap 의 잠재력을 더 다양한 captioning scenario 에서 탐구하기 위해, human-annotated image-sentence pair 를 서로 독립적인 image 와 sentence 로 취급하는 unpaired image captioning setting 을 고려한다. Sec. 4.2.1 에서는 training data 와 test data 가 같은 dataset 에서 오지만, training data 가 paired 가 아닌 in-domain captioning 을 다룬다. Sec. 4.2.2 에서는 training data 와 test data 가 서로 다른 distribution 에서 오는 cross-domain 상황을 고려한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="421-in-domain-captioning">4.2.1 In-Domain Captioning<a href="#421-in-domain-captioning" class="hash-link" aria-label="Direct link to 4.2.1 In-Domain Captioning" title="Direct link to 4.2.1 In-Domain Captioning">​</a></h3><p>저자는 DeCap 을 supervised method 및 다른 unpaired image captioning method 와 비교한다.</p><ol><li><strong>Supervised methods</strong>\
BUTD (Anderson et al., 2018) 는 Faster R-CNN (Ren et al., 2015) 으로 visual feature 를 추출하는 classic 방법이다. CLIPCap (Mokady et al., 2021), CLIP-VL (Shen et al., 2021), Barraco et al. (2022) 는 CLIP 을 visual encoder 로 사용하는 recent approach 이다.</li><li><strong>Unpaired methods</strong>\
Laina et al. (2019) 와 Feng et al. (2019) 은 MSCOCO training set 의 image 와 caption 을 unpaired data 로 취급한다. UVC-VI (Liu et al., 2021a) 는 training 을 위해 image-Chinese pair (Wu et al., 2019) 를 사용한다.</li><li><strong>(CLIP+GPT2)-based methods</strong>\
ZeroCap, Magic, ESPER-Style 은 training set 의 caption 으로 GPT-2 를 finetune 한다.</li><li><strong>ESPER-Free</strong>\
ESPER-Free 는 reinforcement learning 으로 multi-modal input 을 language model generation 에 align 한다.</li><li><strong>CLIPRe</strong>\
CLIPRe 는 retrieval-based baseline 이다.</li><li><strong>우리 방법: DeCap, DeCap-VD, DeCap-NND</strong>
저자의 decoder 는 training set 의 caption 으로 training 되며, 모든 training caption 의 text embedding 은 support memory 에 저장된다.</li></ol><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-1">Results<a href="#results-1" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h4><p>Tab. 2 는 MSCOCO 와 Flickr30K 에서의 결과를 보여준다. 전반적으로 DeCap 은 recent unpaired approach 보다 큰 폭으로 우수한 성능을 보인다. 특히 Flickr30K 에서는 supervised learning method 인 BUTD 와 경쟁 가능한 성능을 달성한다. 여기서 두 가지 결론을 도출할 수 있다.</p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-24-3b7ce4afb8b86b6b90c978cc36d6d8c9.png" width="1207" height="803" class="img_ev3q"></p><ol><li>CLIP 이 captioning task 를 위한 aligned multi-modal representation 을 제공한다.\
visual concept detector 로 multi-modal embedding space 를 구성하는 기존 unpaired method 와 비교했을 때, CLIP 기반 방법들은 text data 만 사용하고도 competitive 한 결과를 달성할 수 있다.</li><li>decoder 와 projection mechanism 이 high performance 에 매우 중요하다.\
CLIPRe 와 비교했을 때, DeCap-NND 는 nearest-neighbor text embedding 을 추가로 decode 하여 더 높은 성능을 보이며, 이는 decoder 가 더 descriptive 한 sentence 를 생성할 수 있음을 보여준다.\
반면 DeCap-VD 는 성능이 떨어지는데, 이는 CLIP image embedding 과 text embedding 사이의 modality gap 이 크다는 점을 보여주며, 저자의 projection mechanism 이 필수적임을 뒷받침한다.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="422-cross-domain-captioning">4.2.2 Cross-Domain Captioning<a href="#422-cross-domain-captioning" class="hash-link" aria-label="Direct link to 4.2.2 Cross-Domain Captioning" title="Direct link to 4.2.2 Cross-Domain Captioning">​</a></h3><p>cross-domain setting 에서, training data 와 testing data 가 서로 다른 dataset 에서 올 때 MSCOCO 와 Flickr30K 에 대해 다음 방법들을 평가한다.</p><ol><li><strong>Zhao et al. (2020)</strong>\
source domain 에서 training 한 retrieval model 을 사용해 target domain 에 대한 pseudo image-text pair 를 생성한다.</li><li><strong>Magic (Su et al., 2022)</strong>\
source domain 의 text data 로 GPT-2 를 finetune 한다.</li><li><strong>CLIPRe-S</strong>\
source domain 의 text data 를 gallery 로 사용하는 CLIP 기반 retrieval 방법이다.</li><li><strong>DeCap</strong>\
decoder 를 source domain 의 text data 로 training 한다.</li><li><strong>DeCap-TT</strong>\
decoder 는 source domain 의 text data 로 training 하고, target domain 의 caption 을 support memory 구성에만 사용한다.</li></ol><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-2">Results<a href="#results-2" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h4><p>Tab. 3 에 결과가 제시된다. </p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-25-aebb63ecdcf7623d29a7ca1394472d98.png" width="1221" height="396" class="img_ev3q"></p><ul><li>paired source domain data 에 의존하고 target domain 에서 추가 training 을 요구하는 traditional cross-domain 방법과 달리, recent CLIP 기반 text-only 방법들은 source domain 의 text-only data 만으로 training 된다. </li><li>DeCap 은 cross-domain 평가에서 Magic, CLIPRe-S 등 다른 text-only 방법보다 상당히 우수한 성능을 보인다.</li><li>더 나아가, target domain 의 text data 를 사용할 수 있는 경우, DeCap-TT 는 추가 training 없이도 captioning 성능을 크게 향상시킨다 (e.g., CIDEr 가 44.4% 에서 63.1% 로 증가). </li><li>이는 단순히 target domain 의 text embedding 을 support memory 로 사용하는 것만으로 달성된다. </li><li>이러한 결과는 cross-domain generalization 에서 DeCap 의 강력한 capability 와, 저자의 projection-based decoding mechanism 의 효과를 보여준다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-video-captioning">4.3 Video Captioning<a href="#43-video-captioning" class="hash-link" aria-label="Direct link to 4.3 Video Captioning" title="Direct link to 4.3 Video Captioning">​</a></h2><p>이 절에서는 DeCap 을 video captioning task 에 적용한다. 저자는 MSR-VTT, Activity-Captions, VATEX 에서 experiment 를 수행한다. 주목할 점은, VATEX public test video 6000 개 중 일부 video 가 더 이상 사용 불가하여, 저자는 raw test video 5182 개만 다운로드했다. Activity-Captions 에서는 Krishna et al. 을 따라 ground-truth proposal 을 사용한다. video captioning 에도 동일한 DeCap 을 그대로 적용한다.</p><p>decoder training 을 위해 세 가지 data source 를 고려한다.</p><ol><li><strong>Generic corpus</strong>\
unsupervised language model 학습에 널리 사용되는 generic corpus 인 Book Corpus 로 decoder 를 training 한다.</li><li><strong>Image captions</strong>\
image captioning task 를 위해 수집 또는 annotation 된 MSCOCO 와 CC3M 의 caption 으로 decoder 를 training 한다.</li><li><strong>Video captions</strong>\
video captioning dataset 의 training set 에 포함된 text annotation 을 추출해 decoder 를 training 한다.</li></ol><p>앞의 두 경우는, training 에 video-related data 를 전혀 사용하지 않는 zero-shot video captioning setting 으로 볼 수 있다.</p><p>inference 시에는 frame-level feature 에 대한 pooling mechanism 을 사용해 video-level feature 를 얻는다. 구체적으로, 각 proposal 에 대해 clip 으로부터 frame <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>f</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>f</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">f_1, f_2, \ldots, f_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 를 random sampling 한다. 그리고 CLIP image encoder 가 추출한 frame-level feature 에 mean pooling 을 적용해 video-level feature 를 얻는다. 모든 experiment 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 는 10 으로 설정한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="results-3">Results<a href="#results-3" class="hash-link" aria-label="Direct link to Results" title="Direct link to Results">​</a></h4><p>Tab. 4 는 결과를 보여준다. </p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-26-a594381d69153942eda3daf1ca6d3182.png" width="1211" height="1011" class="img_ev3q"></p><ul><li>image caption 으로 training 한 DeCap 은 standard captioning metric 에서 recent zero-shot captioning approach 를 능가하며, CLIP-S 와 CLIP-SRef metric 에서도 competitive 한 결과를 보인다. </li><li>특히 다른 방법과 달리, DeCap 은 CLIP visual-text similarity 를 직접 optimization objective 로 사용하지 않는다. </li><li>더 나아가 video caption 으로 training 하면 성능이 추가로 향상된다. </li><li>이러한 결과는 DeCap 이 단순한 random sampling 전략과 temporal mean pooling mechanism 만으로도 video captioning 에 쉽게 적용될 수 있음을 보여준다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="44-ablation-study">4.4 Ablation Study<a href="#44-ablation-study" class="hash-link" aria-label="Direct link to 4.4 Ablation Study" title="Direct link to 4.4 Ablation Study">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-size-of-training-data">The size of training data.<a href="#the-size-of-training-data" class="hash-link" aria-label="Direct link to The size of training data." title="Direct link to The size of training data.">​</a></h4><p>핵심 질문 중 하나는, decoder 를 scratch 에서 training 하기 위해 얼마나 많은 text data 가 필요한가 하는 점이다. 이를 조사하기 위해, MSCOCO 에서 서로 다른 scale 의 data 를 sampling 한다. inference 시에는 모든 experiment 에 대해 동일한 support memory (full training set, 560K caption) 를 사용한다. 결과는 Fig. 2 (left) 에 제시된다.</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-27-0de785673cda0c84b6e8e989578c0397.png" width="1135" height="284" class="img_ev3q"></p><ul><li>전반적으로 DeCap 은 large data size 에서 이득을 본다. </li><li>full set 으로 training 했을 때와 비교하면, data 의 1 % (5.6K caption) 만 사용했을 때 CIDEr score 가 91.2 % 에서 81.5 % 로 감소한다. </li><li>이 결과는 DeCap 이 상당히 data-efficient 하다는 점을 보여준다. </li><li>또한 data-limited scenario 에서 DeCap 을 적용하는 것이 유망한 방향임을 시사한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-size-of-the-support-memory">The size of the support memory.<a href="#the-size-of-the-support-memory" class="hash-link" aria-label="Direct link to The size of the support memory." title="Direct link to The size of the support memory.">​</a></h4><p>support memory size 의 효과를 조사하기 위해, 먼저 language model 을 full training set (560K caption) 으로 training 한다. inference 시에는 서로 다른 비율의 text embedding 을 random sampling 하여 support memory 로 사용한다. 결과는 Fig. 2 (right) 에 제시된다.</p><ul><li>전반적으로 DeCap 과 CLIPRe 는 모두 large support memory 에서 이득을 본다. </li><li>그러나 support memory 로 data 의 1 % 만 사용했을 때에도 성능 감소는 비교적 작다 (CIDEr 기준 3.8 % drop). </li><li>이는 저장 및 computation cost 를 허용 가능한 수준으로 유지하면서도, 상대적으로 작은 support memory 로 competitive 한 결과를 유지할 수 있음을 의미한다.</li></ul><p>추가로, 저자는 support embedding 의 수를 줄이기 위한 filtering strategy 를 Appendix E 에 제시한다. 또한 Appendix G 에서는 support memory 와 projection embedding 을 시각화한다. inference speed 분석은 Appendix D 에 추가한다.</p><h1>5 Conclusion</h1><p>저자는 zero-shot captioning 을 위한 simple framework 를 제안하고, data-efficient 이며 computation-efficient 한 lightweight visual-aware language decoder 를 도입한다. </p><p>저자는 visual embedding 을 text embedding space 로 projection 하는 training-free mechanism 을 제안하여, modality gap 문제를 상당히 줄인다. </p><p>decoder 와 projection mechanism 을 결합함으로써, 저자는 기존 zero-shot 방법을 크게 능가하며 MSCOCO, MSR-VTT, ActivityNet-Captions 에서 새로운 state-of-the-art 를 확립한다. 향후 DeCap framework 는 visual dialog 와 같은 다른 zero-shot text generation 문제에도 적용될 수 있을 것이다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">Vision-Language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/multimodal">Multimodal</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/zero-shot-learning">Zero-shot Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/text-only-training">Text-Only Training</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/clip">CLIP</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/modality-gap-mitigation">Modality Gap Mitigation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/support-memory">Support Memory</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/de-cap">DeCap</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/VQA-IC/Zero-shot/2023-03-DeCap.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/CapDec"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Text-Only Training for Image Captioning using Noise-Injected CLIP</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/VQA-IC/Zero-shot/Knight"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-text-only-decoder-pre-training" class="table-of-contents__link toc-highlight">3.1 Text-Only Decoder Pre-training</a></li><li><a href="#32-inference-strategies" class="table-of-contents__link toc-highlight">3.2 Inference Strategies</a><ul><li><a href="#321-projection-based-decoding-pd" class="table-of-contents__link toc-highlight">3.2.1 Projection-based Decoding (PD)</a></li><li><a href="#322-discussion" class="table-of-contents__link toc-highlight">3.2.2 Discussion</a></li></ul></li><li><a href="#41-zero-shot-image-captioning" class="table-of-contents__link toc-highlight">4.1 Zero-Shot Image Captioning</a></li><li><a href="#42-unparied-image-captioning" class="table-of-contents__link toc-highlight">4.2 Unparied Image Captioning</a><ul><li><a href="#421-in-domain-captioning" class="table-of-contents__link toc-highlight">4.2.1 In-Domain Captioning</a></li><li><a href="#422-cross-domain-captioning" class="table-of-contents__link toc-highlight">4.2.2 Cross-Domain Captioning</a></li></ul></li><li><a href="#43-video-captioning" class="table-of-contents__link toc-highlight">4.3 Video Captioning</a></li><li><a href="#44-ablation-study" class="table-of-contents__link toc-highlight">4.4 Ablation Study</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.3298527c.js"></script>
<script src="/assets/js/main.aca6605f.js"></script>
</body>
</html>