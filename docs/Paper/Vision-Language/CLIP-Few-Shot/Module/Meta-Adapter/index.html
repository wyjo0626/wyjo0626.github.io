<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/CLIP-Few-Shot/Module/2023-11-Meta-Adapter">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Meta-Adapter: An Online Few-shot Learner for Vision-Language Model | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/Meta-Adapter"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Meta-Adapter: An Online Few-shot Learner for Vision-Language Model | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/Meta-Adapter"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/Meta-Adapter" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/Meta-Adapter" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.bb3dcfd4.js" as="script">
<link rel="preload" href="/assets/js/main.3fd1c2fa.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Composition/CLIP-LoRA">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Composition/CLIP-LoRA">CLIP-Few-Shot</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Composition/CLIP-LoRA">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/CLIP-Adapter">Module</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/CLIP-Adapter">CLIP-Adapter: Better Vision-Language Models with Feature Adapters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/Tip-Adapter">Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/TaskRes">Task Residual for Tuning Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/Meta-Adapter">Meta-Adapter: An Online Few-shot Learner for Vision-Language Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/CLAP">A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Multi-Modality/MaPLe">Multi-Modality</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Pixel-Level/CMAR">Prompting</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Foundation</a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">CLIP-Few-Shot</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Meta-Adapter: An Online Few-shot Learner for Vision-Language Model</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Meta-Adapter: An Online Few-shot Learner for Vision-Language Model</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2311.03774v2" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2311.03774v2</a></p><h1>Abstract</h1><p>contrastive vision-language pre-training 인 CLIP 은 open-world visual concept 를 인식하는 데 있어 뛰어난 잠재력을 보이며, 효과적인 zero-shot image recognition 을 가능하게 한다. 그러나 CLIP 기반의 few-shot learning 방법은 일반적으로 few-shot sample 에 대해 parameter 를 offline 으로 fine-tuning 해야 하며, 이는 inference 시간이 길어지고 특정 domain 에서 over-fitting 위험이 발생한다. </p><p>이러한 문제를 해결하기 위해, 저자는 few-shot sample 에 의해 online 방식으로 CLIP feature 를 정제하는 lightweight residual-style adapter 인 <strong>Meta-Adapter</strong> 를 제안한다. few training sample 만으로, 이 방법은 효과적인 few-shot learning 기능을 가능하게 하고, 추가 fine-tuning 없이도 unseen data 나 task 로 일반화할 수 있으며, 경쟁력 있는 성능과 높은 효율성을 달성한다. </p><ul><li>추가적인 복잡한 기법 없이, 제안한 방법은 8 image classification datasets 에서 SOTA online few-shot learning 방법 대비 평균 3.6% 높은 성능을 보이며 inference 속도도 더 빠르다. </li><li>또한, 제안한 model 은 단순하고 유연하여 downstream task 에 바로 적용 가능한 plug-and-play module 로 활용될 수 있다. </li><li>추가 fine-tuning 없이도 Meta-Adapter 는 open-vocabulary object detection 및 segmentation task 에서 성능 향상을 보인다.</li></ul><h1>1 Introduction</h1><p>contrastive vision-language pre-training 인 CLIP 은 open-world visual concept 를 modeling 하는 데 있어 놀라운 잠재력을 보이며, image recognition 과 open-vocabulary perception 을 포함한 다양한 vision task 에 도움이 된다. 이는 large-scale dataset 과 고도화된 pre-learning technique 덕분이다. visual category 에 기반하여 prompt 를 구성함으로써, CLIP 은 효과적인 zero-shot image classification 성능과 unseen data 에 대한 generalization 능력을 보인다. 최근, CLIP 기반의 few-shot learning 이 점점 더 많은 연구 관심을 받고 있다. NLP 분야에서 feature adapter 와 prompt tuning 이 성공한 것에 영감을 받아, CLIP 을 위한 다양한 few-shot 접근법이 제안되고 연구되었다.</p><p>CLIP 의 few-shot learning 방법은 unseen category 의 few-shot sample 에 대해 fine-tuning 이 필요한지 여부에 따라 offline 과 online 방법으로 나눌 수 있다. Offline 방법은 parameter optimization 을 통해 few-shot sample 로부터 지식을 추출한다. 대표적인 예로, </p><ul><li>CoOp 과 CoCoOp 은 CLIP 의 hand-crafted template 을 few-shot sample 로 fine-tuning 하여 learnable continuous token 으로 대체한다. </li><li>또한, CLIP-Adapter 는 few-shot sample 로부터 task-specific knowledge 를 학습하여 feature adapter 를 CLIP 에 도입한다. </li></ul><p>그러나 이러한 추가 구성 요소는 promising few-shot learning 기능을 제공하지만, 추가 training cost 를 발생시키고 특정 data distribution 에서 심각한 over-fitting 문제를 겪는다. 이러한 training cost 를 제거하기 위해, Tip-Adapter 라는 online 방법이 제안되었다. 이 방법은 category embedding 과 few-shot visual embedding 간 비율을 조절하는 hand-crafted modulation function 을 제안하여, fine-tuning 없이 few-shot sample 로부터 knowledge 를 얻고 zero-shot 대비 성능 향상을 보였다. 그러나 복잡한 hyper-parameter search scheme 때문에, Tip-Adapter 는 seen data distribution 에서 여전히 over-fitting 되는 경향이 있어 generalization capability 가 제한된다. 이전의 방법들과 달리, 저자는 새로운 관점으로 <em>meta-learning 을 통한 CLIP 에 대한 online few-shot learner 를 학습한다.</em></p><p>이를 달성하기 위해, 저자는 Tip-Adapter 의 hand-crafted modulation function 과 searching scheme 을 <em>lightweight residual-style network</em> 로 대체한 <strong>Meta-Adapter</strong> 를 제안한다. </p><ul><li>Offline few-shot learning 방법은 unseen category 의 few-shot sample 에 대해 추가 fine-tuning 이 필요하지만, 제안한 접근법은 meta-testing mechanism 을 사용하므로, 학습 시의 category 와 테스트 시의 category 가 달라도 된다. </li><li>제한된 수의 few-shot data 를 사용하여 Meta-Adapter 는 few-shot learning 기능을 학습할 수 있으며, 추가 fine-tuning 없이도 다른 unseen data 로 일반화하여 online 방식으로 few-shot sample 로부터 지식을 추출할 수 있다.</li></ul><p>효율성을 위해, Meta-Adapter 는 <em>gated multi-head attention mechanism</em> 기반의 lightweight network 로 구성되며, 각 category 에 대해 few-shot image feature 와 textual feature 간의 간극을 메운다. 이 과정은 few-shot image 에 의해 category embedding 을 정제하는 learnable filter 로 볼 수 있다. </p><ul><li>Meta-Adapter 는 추가 fine-tuning 이 필요하지 않아 zero-shot 대비 계산 오버헤드가 미미하다.</li><li>Tip-Adapter 대비 over-fitting 문제를 완화하며 dataset 전반에서 더 나은 generalization 을 보인다.</li><li>또한, Meta-Adapter 는 단순하며 다양한 CLIP 기반 방법에 plug-and-play module 로 적용 가능해, 많은 open-vocabulary downstream task 에 유연하게 사용할 수 있다.</li></ul><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-45-de90e6380e6a50d54d3b516c3047a11d.png" width="1377" height="735" class="img_ev3q"></p><p>광범위한 실험을 통해 Meta-Adapter 의 효과와 효율성이 image classification, object detection, segmentation 에서 입증된다. Generalizability 검증을 위해 cross-category generalization, cross-dataset generalization, cross-task generalization 등 일련의 ablation study 를 수행했다. </p><ul><li>Fig. 1(a) 에서 보이듯, base class 데이터로 학습 시 Meta-Adapter 는 16-shot setting 에서 8 개 image classification dataset 의 novel class 에 대해 Tip-Adapter 대비 평균 3.6% 절대 향상을 달성했다. </li><li>더 많은 image shot 을 사용할수록 Tip-Adapter 대비 성능 향상이 증가하며, 이는 Fig. 1(b) 에 나타난다. </li><li>또한, ImageNet pre-trained model 을 다른 7 개 classification dataset 에 직접 평가한 결과, 제안한 방법은 Tip-Adapter 대비 평균 4.9% 성능 향상을 보였다. </li><li>더 나아가, Meta-Adapter 는 open-vocabulary object detection 과 같은 다른 task 개선 가능성을 보이며, object detection 과 instance segmentation 모두에서 일관된 성능 향상을 이끈다. </li><li>ImageNet pre-trained Meta-Adapter 를 open-vocabulary object detection framework 인 ViLD 와 통합했을 때, rare category 의 APr 평균 precision 에서 1.0% 절대 향상을 달성했다.</li></ul><h1>2 Related Work</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-vision-language-pretrained-models">2.1 Vision-Language Pretrained Models<a href="#21-vision-language-pretrained-models" class="hash-link" aria-label="Direct link to 2.1 Vision-Language Pretrained Models" title="Direct link to 2.1 Vision-Language Pretrained Models">​</a></h2><p>CV 와 NLP 분야에서 pre-trained model 이 성공한 데에 영감을 받아, vision 과 language modality 를 모두 처리할 수 있는 large-scale model 을 pre-train 하려는 많은 연구가 제안되었다. 전형적인 vision-language model 은 vision encoder, language encoder, fusion encoder, loss function 의 네 가지 핵심 구성 요소로 이루어진다. </p><p>최근, CV 와 NLP 의 base model 이 성공함에 따라, multi-modal learning 커뮤니티는 이러한 large-scale base model 을 활용하여 성능을 더 높일 수 있게 되었다. VisualBERT, OSCAR, Uniter 는 BERT 를 사용하여 raw text 를 전처리하며, visual question answering (VQA) 과 같은 multi-modal task 에서 뛰어난 성능을 보인다. </p><p>또한, 이러한 방법들은 cross-modal interaction 을 통합하기 위해 잘 설계된 fusion encoder 가 필요하다. 최근에는 CLIP, DeCLIP, ALIGN 이 vision-language contrastive learning 이 transferable feature 를 생성할 수 있으며, vision 과 language embedding 간 dot product 계산만으로도 multi-modal interaction 을 잘 해석할 수 있음을 보였다. 추가적인 self-attention 또는 cross-attention module 없이도 multi-modal embedding 은 사전에 계산하고 저장할 수 있어, 효율적이며 다른 task 에 쉽게 적용할 수 있다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-vision-language-model-adaption">2.2 Vision-Language Model Adaption<a href="#22-vision-language-model-adaption" class="hash-link" aria-label="Direct link to 2.2 Vision-Language Model Adaption" title="Direct link to 2.2 Vision-Language Model Adaption">​</a></h2><p>최근 많은 연구에서는 vision-language model 을 downstream task 에 적응시키기 위한 효과적이고 효율적인 접근 방식을 탐구하고 있으며, 이는 prompt-tuning 방법 (e.g., Context-Optimization (CoOp))과 feature adapter 방법(e.g., Tip-Adapter)로 구분된다. </p><p>Prompt learning 의 성공에서 영감을 받아, </p><ul><li>CoOp 은 hand-crafted template 을 continuous token 으로 대체하고 이를 전통적인 fine-tuning 방식으로 최적화한다. </li><li>또한, CoCoOp 은 심각한 over-fitting 문제를 완화하기 위해 shallow MLP 로 학습된 image-specific token 을 추가로 통합한다. </li><li>수작업으로 설계된 prompt template 과 비교해, CoOp 과 CoCoOp 은 few-shot image classification 에서 뛰어난 성능을 달성했다. </li></ul><p>이러한 prompt tuning 방법과 달리, CLIP-Adapter 와 Tip-Adapter 는 residual feature blending 을 수행하여 few-shot knowledge 를 CLIP 의 zero-shot knowledge 와 결합한다. </p><ul><li>이들은 CLIP 의 모든 parameter 를 고정한 채 소수의 추가 weight 만 fine-tuning 하며, few-shot image classification 에서 좋은 성능을 보인다. </li><li>또한, Tip-Adapter 는 linear weight 를 few-shot knowledge (즉, cache model) 로 초기화함으로써, 학습 없이도 우수한 성능을 내는 방식으로 확장 가능하다. </li></ul><p>그러나 이러한 방법들은 특히 source 와 target dataset 간 domain gap 이 클 때 over-fitting 문제가 발생한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-meta-learning">2.3 Meta-Learning<a href="#23-meta-learning" class="hash-link" aria-label="Direct link to 2.3 Meta-Learning" title="Direct link to 2.3 Meta-Learning">​</a></h2><p>Meta-learning 은 “learning-to-learn” 으로 해석되며, 주어진 task family 에 가장 적합한 algorithm (inductive bias) 을 탐색하여 generalization 을 향상시키는 것을 의미한다. 반면, 전통적인 machine learning algorithm 은 특정 single task 에 대해 더 많은 data 가 주어질수록 성능이 향상되는 것을 기대한다. </p><p>일반적으로, meta-learning 은 task family 에서 sampling 한 learning instance 를 학습하며, 이를 통해 해당 family 에서 sampling 된 new task 에서 잘 작동하는 base learning algorithm 을 시뮬레이션한다. </p><p>또한, 특정한 경우에는 모든 training instance 가 single task 에서 sampling 될 수 있다. Vision-language model 을 downstream task 에 적응시키는 맥락에서, meta-learning 은 서로 다른 task 나 dataset 에서 일관된 성능 향상을 가져오는 일반적인 fine-tuning algorithm 을 학습하는 것으로 볼 수 있다. </p><p>현재의 방법들은 주로 특정 task 나 dataset 의 성능을 향상시키는 데 집중한다. 저자의 알기로, 본 논문은 vision-language model adaption 분야에서 meta-learning 의 가능성을 연구한 최초의 사례이다.</p><h1>3 Method</h1><p>본 절에서는 제안하는 Meta-Adapter 를 소개한다. Sec. 3.1 에서는 먼저 CLIP 과 Tip-Adapter 를 다시 살펴보고, Sec. 3.2 에서는 제안한 Meta-Adapter 의 구현 방법을 상세히 설명한다. Sec. 3.3 에서는 다른 관련 연구와의 차이점을 논의한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-revisiting-clip-and-tip-adapter">3.1 Revisiting CLIP and Tip-Adapter<a href="#31-revisiting-clip-and-tip-adapter" class="hash-link" aria-label="Direct link to 3.1 Revisiting CLIP and Tip-Adapter" title="Direct link to 3.1 Revisiting CLIP and Tip-Adapter">​</a></h2><p>vision-language pre-training model 인 CLIP 은 large-scale noisy image-text pair 를 사용한 contrastive learning 을 통해 open-world visual representation 을 modeling 하는 데 뛰어난 zero-shot learning 가능성을 보여주었다. Zero-shot image classification 을 위해 CLIP 은 image feature 와 각 class 의 textual feature 간 cosine distance 를 계산하여 classification score 를 구한다.</p><p>구체적으로, 주어진 image <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 에 대해, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>D</mi><mo>×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f \in \mathbb{R}^{D \times 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span> 는 query image 의 feature 이고, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>w</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">\{w_i\}_{i=1}^N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>D</mi><mo>×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">w_i \in \mathbb{R}^{D \times 1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6891em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span> 는 text encoder 에 의해 생성된 category embedding 집합이라고 하자. 여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span></span> 는 embedding space 의 차원이고, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> 은 전체 category 의 개수이다. 각 class 의 textual feature <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 hand-crafted template 로부터 생성되며, 전형적인 형태는 “a photo of <!-- -->[CLASS]<!-- -->” 이다. 이때 <!-- -->[CLASS]<!-- --> 토큰은 “Alp” 또는 “Lemon” 과 같은 특정 category 이름으로 대체된다. 주어진 image <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>-th class 에 속할 확률에 대한 predicted logits 는 다음과 같이 정의된다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mtext>logits</mtext><mo stretchy="false">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>=</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msubsup><mi>w</mi><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><mi>f</mi></mrow><mrow><mi mathvariant="normal">∥</mi><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∥</mi><mtext> </mtext><mi mathvariant="normal">∥</mi><mi>f</mi><mi mathvariant="normal">∥</mi></mrow></mfrac><mo separator="true">,</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\text{logits}(y_c = i) = \frac{w_i^\top f}{\|w_i\| \ \|f\|}, \tag{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">logits</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4621em;vertical-align:-0.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∥</span><span class="mspace"> </span><span class="mord">∥</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord">∥</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.4413em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span></span><span class="tag"><span class="strut" style="height:2.4621em;vertical-align:-0.936em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></div><p>Tip-Adapter 는 few-shot sample 로부터 knowledge 를 학습하는 online 방법을 제안한다. 이 방법은 stochastic hyper-parameter search 전략과 함께 간단한 modulation function 을 사용하여 특정 domain 에서 뛰어난 few-shot 성능을 달성한다. 구체적으로, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> 두 개의 hyper-parameter 를 사용하여 dataset 마다 visual feature 와 textual feature 간 비율을 조정한다. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span>-way, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span>-shot 의 support image set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>x</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">x = \{x_i\}_{i=1}^N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span></span></span></span></span> 가 주어졌을 때, Tip-Adapter 의 predicted logits 는 다음과 같이 표현된다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mtext>logits</mtext><mo stretchy="false">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>=</mo><mi>i</mi><mtext> </mtext><mi mathvariant="normal">∣</mi><mtext> </mtext><mi>x</mi><mo separator="true">,</mo><mi>α</mi><mo separator="true">,</mo><mi>β</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msubsup><mi>w</mi><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><mi>f</mi></mrow><mrow><mi mathvariant="normal">∥</mi><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∥</mi><mtext> </mtext><mi mathvariant="normal">∥</mi><mi>f</mi><mi mathvariant="normal">∥</mi></mrow></mfrac><mo>+</mo><mi>α</mi><mo>⋅</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mo>−</mo><mi>β</mi><mrow><mo fence="true">(</mo><mn>1</mn><mo>−</mo><mfrac><mrow><msubsup><mi>F</mi><mi>j</mi><mi mathvariant="normal">⊤</mi></msubsup><mi>f</mi></mrow><mrow><mi mathvariant="normal">∥</mi><msub><mi>F</mi><mi>j</mi></msub><mi mathvariant="normal">∥</mi><mtext> </mtext><mi mathvariant="normal">∥</mi><mi>f</mi><mi mathvariant="normal">∥</mi></mrow></mfrac><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow><msub><mi>L</mi><mi>j</mi></msub><mo separator="true">,</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\text{logits}(y_c = i \ | \ x, \alpha, \beta) = \frac{w_i^\top f}{\|w_i\| \ \|f\|} + \alpha \cdot \exp \left( -\beta \left( 1 - \frac{F_j^\top f}{\|F_j\| \ \|f\|} \right) \right) L_j, \tag{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">logits</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">i</span><span class="mspace"> </span><span class="mord">∣</span><span class="mspace"> </span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4621em;vertical-align:-0.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∥</span><span class="mspace"> </span><span class="mord">∥</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord">∥</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.4413em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em"></span><span class="mop">exp</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size4">(</span></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size4">(</span></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6339em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mord">∥</span><span class="mspace"> </span><span class="mord">∥</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord">∥</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.7848em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9721em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size4">)</span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size4">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mpunct">,</span></span><span class="tag"><span class="strut" style="height:3em;vertical-align:-1.25em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></div><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>D</mi><mo>×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">F_i \in \mathbb{R}^{D \times K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span></span></span></span></span></span></span></span></span></span> 는 few-shot sample 의 support embedding 이고, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">L_i \in \mathbb{R}^{N \times K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span></span></span></span></span></span></span></span></span></span> 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>-th class 의 one-hot label 이다. </li></ul><p>Tip-Adapter 는 few-shot sample 에 대해 추가적인 학습 없이도 우수한 성능을 보인다. 그러나 Tip-Adapter 는 target dataset 에서의 hyper-parameter search 전략에 크게 의존하므로, 특정 data distribution 에 과적합되기 쉽고 out-of-distribution generalization 능력이 제한된다.</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-46-7176ea6f88ba8e3fa92c526ff6b5ab22.png" width="1696" height="581" class="img_ev3q"></p><ul><li>Tab. 1 에 나타나듯이, ImageNet 에서 search 한 hyper-parameter 를 고정한 채 다른 7 개 dataset 에 대해 Tip-Adapter 성능을 직접 평가하면, dataset 간 distribution 이 다를 때 일반화 성능이 저하되는 것을 확인할 수 있다. </li><li>Dataset 마다 개별적으로 search 한 경우에 비해, Tip-Adapter 의 성능이 눈에 띄게 하락한다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-meta-adapter">3.2 Meta-Adapter<a href="#32-meta-adapter" class="hash-link" aria-label="Direct link to 3.2 Meta-Adapter" title="Direct link to 3.2 Meta-Adapter">​</a></h2><p>일반화 성능 저하 문제를 해결하기 위해, 저자는 Tip-Adapter 의 hand-crafted modulation function 과 searching strategy 를 learnable 한 Meta-Adapter 로 대체한다. Tip-Adapter 와 달리, 저자는 few-shot learning 을 visual-language scheme 내에서 few-shot image sample 의 guidance 를 받아 textual feature 에 작용하는 learnable filter 로 모델링하여, 더 구별력 있는 category embedding 을 얻는다. 이는 computer vision 의 non-local filter 에서 영감을 받아, <em>gated multi-head attention mechanism</em> 기반의 Meta-Adapter 로 구현된다.</p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-47-7b8c65934a47591ef8ae9c1ffc202275.png" width="1369" height="789" class="img_ev3q"></p><ul><li>Fig. 2 에서 보이듯, CLIP encoder 를 통해 먼저 few-shot image 의 support embedding 과 category embedding 을 추출한다. </li><li>Meta-Adapter 는 이후 visual feature 로부터 few-shot knowledge 를 추출하여 textual feature 로 전달함으로써, 정제된 category embedding 을 얻는다. </li><li>구체적으로, original category embedding 을 query 로, support embedding 을 key 와 value 로 사용하여 multi-head attention block 에 입력한다. </li><li>standard transformer encoder 와 달리, 저자의 접근법은 query 와 key 에 대해서만 Multilayer Perceptron (MLP) layer 를 적용한다. 이는 value 에 대해 feature 변환을 수행하지 않으므로, training 후에도 zero-shot 성능이 일반적으로 변하지 않게 하는 중요한 전략이다.</li></ul><p>제안한 Meta-Adapter 의 predicted logits 는 다음과 같이 표현된다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mtext>logits</mtext><mo stretchy="false">(</mo><msub><mi>y</mi><mi>c</mi></msub><mo>=</mo><mi>i</mi><mtext> </mtext><mi mathvariant="normal">∣</mi><mtext> </mtext><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msubsup><mover accent="true"><mi>w</mi><mo>^</mo></mover><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><mi>f</mi></mrow><mrow><mi mathvariant="normal">∥</mi><msub><mover accent="true"><mi>w</mi><mo>^</mo></mover><mi>i</mi></msub><mi mathvariant="normal">∥</mi><mtext> </mtext><mi mathvariant="normal">∥</mi><mi>f</mi><mi mathvariant="normal">∥</mi></mrow></mfrac><mo separator="true">,</mo><mspace width="1em"></mspace><mtext>where</mtext><mspace width="1em"></mspace><mover accent="true"><mi>w</mi><mo>^</mo></mover><mo>=</mo><mtext>MetaAdapter</mtext><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>F</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(3)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\text{logits}(y_c = i \ | \ x) = \frac{\hat{w}_i^\top f}{\|\hat{w}_i\| \ \|f\|}, \quad \text{where} \quad \hat{w} = \text{MetaAdapter}(w, F). \tag{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">logits</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">i</span><span class="mspace"> </span><span class="mord">∣</span><span class="mspace"> </span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4621em;vertical-align:-0.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">∥</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∥</span><span class="mspace"> </span><span class="mord">∥</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mord">∥</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.4413em;margin-left:-0.0269em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord text"><span class="mord">where</span></span><span class="mspace" style="margin-right:1em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">MetaAdapter</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="mclose">)</span><span class="mord">.</span></span><span class="tag"><span class="strut" style="height:2.4621em;vertical-align:-0.936em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">3</span></span><span class="mord">)</span></span></span></span></span></span></div><p>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>w</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{w}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span> 는 정제된 category embedding 이다. </p><p>Fig. 2 오른쪽에서 보듯, 제안한 방법은 category 와 few-shot image 간의 affinity 에 따라 support embedding 을 적응적으로 집계한다. 이 과정은 cross-attention mechanism 으로 구현될 수 있다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mover accent="true"><mi>F</mi><mo>^</mo></mover><mo>=</mo><msup><mi>F</mi><mi mathvariant="normal">⊤</mi></msup><mtext> </mtext><mi>σ</mi><mrow><mo fence="true">(</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>F</mi><msubsup><mi>W</mi><mn>1</mn><mi mathvariant="normal">⊤</mi></msubsup><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>w</mi><msubsup><mi>W</mi><mn>2</mn><mi mathvariant="normal">⊤</mi></msubsup><msup><mo stretchy="false">)</mo><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><mi>D</mi></msqrt></mfrac><mo fence="true">)</mo></mrow><mo separator="true">,</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(4)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\hat{F} = F^\top \ \sigma \left( \frac{(F W_1^\top) (w W_2^\top)^\top}{\sqrt{D}} \right), \tag{4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9468em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span><span style="top:-3.2523em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4761em;vertical-align:-0.95em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.1833em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9267em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:0.833em"><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span><span style="top:-2.8867em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1133em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.4519em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.4519em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span></span><span class="tag"><span class="strut" style="height:2.4761em;vertical-align:-0.95em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">4</span></span><span class="mord">)</span></span></span></span></span></span></div><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">W_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 과 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">W_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 MLP layer 의 weight 를 나타내며, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span></span></span> 는 Softmax function 이고, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>F</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{F}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9468em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span><span style="top:-3.2523em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span> 는 집계된 support feature 를 의미한다. </li></ul><p>직관적으로, non-local filter 와 유사하게 Meta-Adapter 는 category 설명과 더 관련 있는 sample 에 집중하면서 outlier sample 은 무시할 수 있어, 더욱 견고한 feature representation 을 제공한다.</p><p>또한, few-shot learning 에서 textual feature 와 visual feature 의 중요도는 data distribution 에 따라 다르다. 따라서, 저자는 modulation scalar 를 생성하는 learnable gating block <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> 을 제안하여, category embedding 과 집계된 support embedding 간 비율을 적응적으로 제어한다. 이에 따라, 정제된 category embedding 은 다음과 같이 구할 수 있다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mover accent="true"><mi>w</mi><mo>^</mo></mover><mo>=</mo><mi>w</mi><mo>+</mo><mi>g</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>⊙</mo><mover accent="true"><mi>F</mi><mo>^</mo></mover><mo separator="true">,</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(5)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\hat{w} = w + g(w) \odot \hat{F}, \tag{5}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.1412em;vertical-align:-0.1944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span><span style="top:-3.2523em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1667em"><span class="mord">^</span></span></span></span></span></span></span><span class="mpunct">,</span></span><span class="tag"><span class="strut" style="height:1.1968em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">5</span></span><span class="mord">)</span></span></span></span></span></span></div><p>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊙</mo></mrow><annotation encoding="application/x-tex">\odot</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord">⊙</span></span></span></span></span> 는 Hadamard product 를 나타낸다. Few-shot sample 학습을 통해 gating block 은 category description 에 따라 비율을 조정할 수 있으며, 이를 통해 few-shot knowledge 와 zero-shot knowledge 를 효과적으로 통합할 수 있다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-comparison-with-counterparts">3.3 Comparison with Counterparts<a href="#33-comparison-with-counterparts" class="hash-link" aria-label="Direct link to 3.3 Comparison with Counterparts" title="Direct link to 3.3 Comparison with Counterparts">​</a></h2><p>CLIP-Adapter 나 CoOp 같은 offline 방법과 비교하면, </p><ul><li>제안한 Meta-Adapter 는 target sample 에 대한 추가 fine-tuning 이 필요하지 않아 inference 시 계산 비용을 크게 줄인다. </li><li>또한, Tip-Adapter 와 같은 online 방법과 비교하면, 제안 기법은 handcrafted hyper-parameter search 과정을 support sample 기반의 learnable network 로 대체한다. </li><li>Tab. 1 에서 보이듯, Meta-Adapter 는 over-fitting 문제를 더 효과적으로 완화하고, 추가 fine-tuning 없이도 dataset 간 generalization 을 보여준다. 더 나아가, Meta-Adapter 는 textual embedding feature 의 차원을 변경하지 않고 직접 정제하기 때문에, CLIP 기반의 다양한 downstream task 에 자연스럽게 적용될 수 있다.</li></ul><h1>4 Experiments</h1><p>training set 과 testing set 의 distribution 은 동일할 수도, 다를 수도 있으며, Meta-Adapter 가 두 상황 모두에서 잘 작동하는 것이 중요하다. 또한, Meta-Adapter 의 downstream task 에서의 잠재력도 매우 중요하다. 저자는 이 세 가지 상황을 각각 “<em>cross-category generalization</em>”, “<em>cross-dataset generalization</em>”, “<em>cross-task generalization</em>” 이라 부른다.</p><p>구체적으로, “<em>cross-category generalization</em>” 을 위해 각 dataset 의 전체 category 를 Zero-shot CLIP 이 예측한 per-category accuracy 에 따라 base set 과 novel set 으로 분할한다. 이때 base set 은 easy sample, novel set 은 hard sample 을 포함한다. 이러한 dataset 분할 전략은 dataset-irrelevant 접근 방식을 학습할 수 있는지를 검증하기 위해 특히 어려운 상황을 시뮬레이션하며, hard sample 에 대한 성능을 중점적으로 확인한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="datasets">Datasets<a href="#datasets" class="hash-link" aria-label="Direct link to Datasets" title="Direct link to Datasets">​</a></h4><p>cross-category generalization 실험에는 8 개의 대표적인 image classification dataset 을 사용한다: ImageNet, FGVCAircraft, OxfordPets, SUN397, UCF101, Caltech101, DTD, EuroSAT. 이들은 다양한 classification task 를 포함한다.
cross-dataset generalization 실험에서는 ImageNet 을 source dataset 으로 사용하고, ImageNet-A, ImageNet-R, ImageNet-Sketch 의 세 가지 변형을 target dataset 으로 사용한다. 또한, Meta-Adapter 의 open-vocabulary detection 잠재력을 탐구하기 위해 LVIS dataset 에서 실험을 수행한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="baselines">Baselines<a href="#baselines" class="hash-link" aria-label="Direct link to Baselines" title="Direct link to Baselines">​</a></h4><p>Meta-Adapter 는 training-free 방법인 Zero-shot CLIP 과 Tip-Adapter 와 비교한다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="training-details">Training Details<a href="#training-details" class="hash-link" aria-label="Direct link to Training Details" title="Direct link to Training Details">​</a></h4><p>CLIP backbone 으로는 대부분의 실험에서 visual encoder 로 ResNet50, textual encoder 로 transformer 를 사용한다. Prompt ensemble 전략을 채택하여, 7 개의 template 을 CLIP textual encoder 에 입력한 뒤 평균하여 최종 prompt embedding 으로 사용한다. Meta-Adapter 는 base set 에 대해 batch size 64 로 학습하며, AdamW optimizer 와 learning rate 0.0001, cosine scheduler 로 5 epoch 동안 학습한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-cross-category-generalization">4.1 Cross-Category Generalization<a href="#41-cross-category-generalization" class="hash-link" aria-label="Direct link to 4.1 Cross-Category Generalization" title="Direct link to 4.1 Cross-Category Generalization">​</a></h2><p>경험적 연구에 따르면, Tip-Adapter 는 특정 dataset 에 적용될 때 일반적으로 큰 hyper-parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>α</mi><mo separator="true">,</mo><mi>β</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\alpha, \beta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mclose">)</span></span></span></span></span> 값을 필요로 한다. 이 hyper-parameter 는 classification distribution 을 smoothing 하고 few-shot knowledge 에 큰 가중치를 부여하는 데 사용된다. 이는 Tip-Adapter 가 ImageNet 과 같이 비교적 일반적인 dataset 에 대해서도 few-shot knowledge 에 크게 의존함을 의미한다. 그 결과, overfitting 문제가 발생하여 일반화 능력이 저하된다.</p><ul><li>Tab. 6 에 따르면, UCF101 과 Caltech101 같은 dataset 에서 Tip-Adapter 는 training set 에서 Meta-Adapter 보다 약간 높은 classification accuracy 를 보인다. </li><li>그러나 novel sample 에 대해서는 Tip-Adapter 가 Meta-Adapter 보다 크게 뒤처진다 (e.g., UCF101 에서 40.26% vs 47.72%). 이는 Tip-Adapter 가 과도한 hyper-parameter search 전략 때문에 특정 localized distribution 에 지나치게 맞춰지기 때문임을 시사한다.</li><li>반면, Meta-Adapter 는 설계상 일반화 가능한 ensemble 접근 방식을 사용하므로 base set 에서도 유사한 성능을 유지하면서 novel set 에서 우수한 성능을 보인다. </li><li>Fig. 1(b) 에서 보이듯, ImageNet dataset 에서 Meta-Adapter 는 다른 방법들보다 우수하다. </li><li>Zero-shot CLIP 대비, 모든 few-shot 설정에서 일관되게 더 높은 성능을 보인다. </li><li>Tip-Adapter 와 비교하면, 두 방법 모두 shot 수가 증가함에 따라 성능이 향상되지만, shot 수가 4 미만일 때 Tip-Adapter 가 소폭 우세하다. 이는 두 가지 이유 때문이다.<ol><li>Tip-Adapter 는 few-shot feature 와 해당 one-hot label 로부터 직접 classification logits 를 계산하므로, Meta-Adapter 의 일반적 ensemble 접근 방식보다 더 단기적인 해결책이 될 수 있다.</li><li>Tip-Adapter 는 특정 dataset 에서 최고 accuracy 를 찾기 위한 hyper-parameter search 전략을 사용하여 few-shot knowledge 의 잠재력을 적극적으로 활용한다.</li></ol></li><li>그러나 shot 수가 증가하면, Meta-Adapter 는 Tip-Adapter 를 명확히 앞서며, classification accuracy 가 꾸준히 상승하는 반면, Tip-Adapter 는 shot 수가 32 일 때 성능이 하락하여 성능 한계 가능성을 시사한다. </li><li>Fig. 1(a) 에서 보이듯, 16-shot 설정에서 다른 7 개 dataset 에 대한 정량적 비교에서 Meta-Adapter 는 Zero-shot CLIP 대비 큰 폭의 accuracy 향상을 보이며, Tip-Adapter 대비 최대 +7.46% 까지 성능 향상을 달성한다. </li></ul><p><img loading="lazy" alt="Table 2" src="/assets/images/image-48-78326628a8a27fb3320177e45d63c7e9.png" width="1696" height="449" class="img_ev3q"></p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-49-13c89b381372c99cbccac83a7101effc.png" width="1639" height="333" class="img_ev3q"></p><p>Meta-Adapter 의 효과성을 추가로 검증하기 위해, 모든 방법에 대해 서로 다른 visual encoder 를 적용하고 ImageNet 에서 실험을 수행했다. </p><ul><li>Tab. 3 의 정량적 비교 결과, visual encoder 선택과 관계없이 Meta-Adapter 는 Tip-Adapter 대비 우위를 유지한다. </li><li>더 고급 backbone (e.g., ViT-B/16) 을 사용하면 Meta-Adapter 의 classification accuracy 는 더욱 향상된다. <ul><li>이는 더 강력한 vision-language model 을 사용하면 Meta-Adapter 의 학습 잠재력이 향상될 수 있음을 시사한다.</li></ul></li></ul><p>요약하면, 기존 training-free 방법과 비교했을 때 Meta-Adapter 는 over-fitting 문제를 효과적으로 완화하면서도 우수한 일반화 능력을 유지하며, novel set 에서 SOTA classification accuracy 를 달성한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-cross-dataset-generalization">4.2 Cross-Dataset Generalization<a href="#42-cross-dataset-generalization" class="hash-link" aria-label="Direct link to 4.2 Cross-Dataset Generalization" title="Direct link to 4.2 Cross-Dataset Generalization">​</a></h2><p>학습된 classifier 가 서로 다른 distribution 을 가진 다양한 dataset 을 다룰 때도 유사한 성능을 유지하는 것은 매우 중요하다. 이는 dataset 간 appearance 와 shape 이 완전히 다를 수 있기 때문에 (e.g., ImageNet 의 object recognition 과 DTD 의 texture classification) 더 어려운 문제다. 또한, Meta-Adapter 가 dataset-irrelevant 한 구별력 있는 ensemble 접근 방식을 학습할 수 있는지도 확인하고자 한다.</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-50-03242093ea60b2b079865159e9c924be.png" width="1361" height="693" class="img_ev3q"></p><p>이를 위해, Tip-Adapter 의 경우 source dataset 에서 search 한 최적 hyper-parameter 를 target dataset 에 그대로 적용하고, Meta-Adapter 의 경우 source dataset(base set) 에서 학습한 뒤 모든 learnable parameter 를 freeze 한 상태로 target dataset(novel set) 에서 평가한다. 두 실험 모두 16-shot 설정에서 수행된다. Fig. 3(a) 에는 ImageNet(전체 category 사용)에서 다른 7 개 dataset 으로 transfer 했을 때의 상대 accuracy 향상을 보고한다. </p><ul><li>Fig. 3(a) 에서 Tip-Adapter 의 base 결과를 baseline 으로 설정했으므로, 해당 상대 accuracy 향상 값은 항상 1.0 이다.</li><li>ImageNet 이 다양한 class(e.g., 여러 종류의 동물, 차량 종류)를 포함하므로, 두 모델 모두 counterpart 와 비교해 유사한 accuracy 를 유지하는 것은 놀랍지 않다. </li><li>그러나 Tip-Adapter 는 Meta-Adapter 대비 명확한 성능 저하를 보인다. <ul><li>특히, ImageNet 에서 SUN397, UCF101, EuroSAT 으로 transfer 할 경우, target dataset 에 직접 학습한 baseline 을 초과하는 성능을 보이며, </li><li>이는 Meta-Adapter 의 학습 잠재력이 일반화된 dataset 으로부터 이익을 얻을 수 있음을 시사한다.</li></ul></li><li>또한 Fig. 3(b) 에는 OxfordPets, Caltech101 등 7 개의 소규모 classification dataset (전체 category 사용)에서 ImageNet 으로 transfer 했을 때의 상대 accuracy 향상을 보고한다. </li><li>놀랍게도, Meta-Adapter 는 안정적인 성능을 유지하지만, Tip-Adapter 는 특히 OxfordPets 와 Caltech101 에서 명확한 성능 저하를 보인다. <ul><li>따라서 Meta-Adapter 가 다양한 distribution 과 domain shift 상황에서 더 나은 transferability 를 유지한다고 결론지을 수 있다.</li></ul></li></ul><p>추가로, CoCoOp 와 동일하게 domain generalization 실험을 수행한다. 사람은 자연스럽게 out-of-distribution data 에 일반화할 수 있는 능력을 갖고 있으며, Meta-Adapter 가 동일한 장점을 가질 수 있는지가 궁금하다. 이를 위해, Tip-Adapter 의 경우 ImageNet 에서 search 한 최적 hyper-parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>α</mi><mo separator="true">,</mo><mi>β</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\alpha, \beta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mclose">)</span></span></span></span></span> 를, Meta-Adapter 의 경우 ImageNet 에서 search 및 최적화한 weight 를 각각 세 가지 변형 dataset (ImageNet-A, ImageNet-R, ImageNet-Sketch) 에 그대로 transfer 한다. 또한, Zero-shot CLIP 의 세 dataset 에 대한 성능을 baseline 으로 보고한다.</p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-51-83d4d16d5d79b294191e4e055d96c5e2.png" width="1363" height="499" class="img_ev3q"></p><ul><li>Tab. 4 의 정량적 결과에서 보듯, ImageNet 에서 search 한 Tip-Adapter 의 최적 hyper-parameter 를 변형 dataset 에 직접 적용하면 성능이 떨어지며, Zero-shot CLIP 보다도 낮아진다. <ul><li>이는 Tip-Adapter 가 hyper-parameter 설정에 민감하고, 심각한 over-fitting 문제를 겪는다는 이전 분석과 일치한다. </li></ul></li><li>반면, Meta-Adapter 는 domain shift 에 더 잘 적응하며, Zero-shot CLIP 과 유사한 성능을 보여준다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-cross-task-generalization">4.3 Cross-Task Generalization<a href="#43-cross-task-generalization" class="hash-link" aria-label="Direct link to 4.3 Cross-Task Generalization" title="Direct link to 4.3 Cross-Task Generalization">​</a></h2><p>few-shot learning 방법이 downstream task 에서도 이점을 제공할 수 있는지는 중요한 문제다. 이를 위해, 저자는 Meta-Adapter 와 Tip-Adapter 를 open-vocabulary object detection 방법인 ViLD 와 통합한다. ViLD 는 CLIP 을 활용하여 open-vocabulary 가능성을 탐구하며, object detection framework 의 일반적인 classifier 를 CLIP 의 text encoder 로 생성된 textual feature 로 대체하고, knowledge distillation 을 통해 textual feature 와 ROI feature 를 정렬한다.</p><p>먼저 LVIS 의 annotation 을 기반으로 pre-processed region feature 를 few-shot sample 로 생성한다. ViLD 의 방식에 따라, LVIS dataset 의 object category 는 training set 내 빈도에 따라 “frequent”, “common”, “rare” 로 구분된다. 이 중 866 개 frequent 와 common category 를 base set 으로, 337 개 rare category 를 novel set 으로 사용한다. Tip-Adapter 의 경우, ViLD 의 prediction logits 에 추가적인 few-shot term 을 더해 수정한다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mtext>logits</mtext><mo stretchy="false">(</mo><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo stretchy="false">)</mo><msup><mi>W</mi><mi mathvariant="normal">⊤</mi></msup><mo>+</mo><mi>α</mi><mo>⋅</mo><mi>ϕ</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo stretchy="false">)</mo><msup><mi>F</mi><mi mathvariant="normal">⊤</mi></msup><mo stretchy="false">)</mo><mi>L</mi><mo separator="true">,</mo><mspace width="1em"></mspace><mi>ϕ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>β</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(6)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\text{logits}(\hat{r}) = f(\hat{r}) W^\top + \alpha \cdot \phi(f(\hat{r})F^\top) L, \quad \phi(x) = \exp(-\beta(1 - x)) \tag{6}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">logits</span></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.1491em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1.1491em;vertical-align:-0.25em"></span><span class="mord mathnormal">ϕ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal">L</span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">ϕ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">x</span><span class="mclose">))</span></span><span class="tag"><span class="strut" style="height:1.1491em;vertical-align:-0.25em"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">6</span></span><span class="mord">)</span></span></span></span></span></span></div><ul><li>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mover accent="true"><mi>r</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\hat{r})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 는 proposal <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>r</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.1944em"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span> 의 region feature, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\phi(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">ϕ</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> 는 Tip-Adapter 에서 사용된 modulation function, </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span></span></span></span> 와 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span></span></span></span></span> 은 각각 few-shot region feature 와 해당 one-hot label 을 나타낸다. </li><li>Eq. 6 은 ViLD 에서 소개된 ViLD-text 와 ViLD-image 모두에 적용된다.</li></ul><p>Tip-Adapter 의 hyper-parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>α</mi><mo separator="true">,</mo><mi>β</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\alpha, \beta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mclose">)</span></span></span></span></span> 설정과 관련하여, ImageNet 에서 search 한 값을 그대로 사용하면 ViLD 가 collapse 되는 현상이 나타났다. 따라서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.05</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.05</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\beta = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span> 로 설정하여, ViLD 가 collapse 를 피하고 CLIP knowledge 에 더 의존하도록 한다. Meta-Adapter 의 경우, ImageNet 으로 pre-train 한 모델을 사용하여 LVIS few-shot knowledge 를 원본 textual feature 에 통합한다. ViLD 는 DetPro 에서 제안한 재구현 버전을 사용하며, pre-trained ResNet-50 을 self-supervised pre-trained SoCo 로 대체한다.</p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-52-5083844241a4c920233d62fcc3bd1ae5.png" width="1037" height="346" class="img_ev3q"></p><ul><li>Tab. 5 의 평균 precision 결과에서, Meta-Adapter 는 rare category 의 detection 성능을 명확하게 향상시키지만, Tip-Adapter 는 poor transferability 와 ViLD 의 원래 prediction score 를 수정해야 하는 구조적 특성 때문에 detection 성능을 저하시킨다. </li><li>또한, LVIS annotation 의 일부는 크기가 약 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">10 \times 10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">10</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">10</span></span></span></span></span> 픽셀인 작은 bounding box 를 포함하고 있어, LVIS few-shot dataset 의 품질이 image classification counterpart 만큼 좋지 않을 수 있다. </li><li>앞서 언급했듯, Tip-Adapter 는 few-shot knowledge 에 과도하게 의존하므로 open-vocabulary object detection 에서 성능 저하의 주요 원인이 될 수 있다. </li><li>반면, Meta-Adapter 는 일반적인 ensemble 접근 방식을 학습하며, prediction score 공식을 변경하지 않고도 open-vocabulary object detection 방법에 쉽게 통합될 수 있다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="44-comparison-with-offline-methods">4.4 Comparison with Offline Methods<a href="#44-comparison-with-offline-methods" class="hash-link" aria-label="Direct link to 4.4 Comparison with Offline Methods" title="Direct link to 4.4 Comparison with Offline Methods">​</a></h2><p><img loading="lazy" alt="Table 6" src="/assets/images/image-53-ec85f8ba45b6eef23cb21d98bd74e2ea.png" width="1357" height="468" class="img_ev3q"></p><ul><li>Tab. 6 에서 제안한 Meta-Adapter 와 다른 offline 방법(CoCoOp, CoOp, CLIP-Adapter) 간의 ablation study 를 제공한다. </li><li>공정한 비교를 위해, CoCoOp 과 유사하게 base-to-novel generalization 설정을 사용한다. 결과적으로, 제안한 방법은 novel class 일반화에서 뚜렷한 장점을 보이며, 예를 들어 ImageNet 에서 CoCoOp 대비 6.9% 향상된다.</li><li>또한, 이전 연구와 동일하게 base 와 novel class 의 전체 성능을 측정하기 위해 Harmonic Mean 을 도입한다. </li><li>결과에서 제안한 방법이 전반적인 성능 측면에서도 명확한 우위를 보임을 확인할 수 있다. </li><li>더 중요한 점은, 제안한 방법은 새로운 dataset 이나 task 에 적용할 때 추가 fine-tuning 이 필요하지 않다는 것이다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="45-more-ablation-studies-of-the-meta-adapter">4.5 More Ablation Studies of the Meta-Adapter<a href="#45-more-ablation-studies-of-the-meta-adapter" class="hash-link" aria-label="Direct link to 4.5 More Ablation Studies of the Meta-Adapter" title="Direct link to 4.5 More Ablation Studies of the Meta-Adapter">​</a></h2><p><img loading="lazy" alt="Table 7" src="/assets/images/image-54-ca5eceb8f774c43ed5d92ada9e59ba0e.png" width="1356" height="365" class="img_ev3q"></p><ul><li>Tab. 7 의 결과에서 보듯, multi-head attention 이 accuracy 향상에 가장 크게 기여한다. </li><li>제안한 learnable gating block 은 성능을 추가로 향상시키지만, value projection 을 도입하면 generalization 능력이 감소한다. </li></ul><p><img loading="lazy" alt="Table 8" src="/assets/images/image-55-5c93409f37d9c13787a4dd1c821eeb15.png" width="1020" height="364" class="img_ev3q"></p><ul><li>또한, Tab. 8 에서는 projection layer 의 폭을 넓히는 방법(Wider)과 여러 모듈을 연속적으로 쌓는 방법 (Deeper)으로 meta-adapter 의 model scale 을 증가시켰다. </li><li>결과적으로, 모듈 수를 늘리면 parameter 수와 accuracy 가 소폭 향상되지만, 효율성이 크게 떨어진다.</li></ul><h1>5 Limitations and Conclusion</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="limitations">Limitations<a href="#limitations" class="hash-link" aria-label="Direct link to Limitations" title="Direct link to Limitations">​</a></h4><p>Tab. 6 의 결과에 따르면, Zero-shot CLIP 의 classification accuracy 가 높은 경우(UCF101, Caltech101)에는 Meta-Adapter 의 학습 잠재력이 제한될 수 있음을 확인할 수 있다. 이는 image-image similarity score 와 text-image similarity score 의 결합이 few-shot learning 의 잠재력을 저해할 수 있기 때문이다. 다시 말해, 이러한 상황에서 Meta-Adapter 는 few-shot knowledge 보다 zero-shot knowledge 를 선호하게 되어 두 지식 간 균형이 무너질 수 있다.</p><p>또한, 경험적 연구에 따르면, Meta-Adapter 는 high-quality few-shot dataset 확보가 어려운 open-vocabulary semantic segmentation task 에 적용하는 데 어려움을 겪는다. 외부 데이터를 통합하는 방식이 이러한 문제를 완화하는 데 도움이 될 가능성이 있다. 이러한 도전 과제는 향후 연구 과제로 남겨둔다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h4><p>본 논문은 CLIP 을 위한 새로운 few-shot learning 방법인 Meta-Adapter 의 잠재력을 입증했다. Meta-Adapter 는 이전 방법들의 일반화 능력 저하와 낮은 효율성 문제를 해결하도록 설계되었으며, meta-testing mechanism 과 lightweight residual-style network 를 활용해 few-shot sample 로부터 추가 fine-tuning 없이 지식을 추출한다. 이를 통해 over-fitting 문제를 완화하면서도 높은 효율성을 유지한다.</p><p>Meta-Adapter 는 image classification, object detection, segmentation 을 포함한 다양한 task 에서 우수한 성능을 보였으며, dataset 및 task 전반에서 뛰어난 generalization 능력을 입증했다. 향후 연구에서는 Meta-Adapter 를 더욱 정교화하고, 다른 vision task 에서의 잠재적 활용 가능성을 탐구함으로써 visual concept modeling 분야의 few-shot learning 기법 발전을 도모할 수 있다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/meta-adapter">Meta-Adapter</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/adapter">Adapter</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/offile-tuning">Offile Tuning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/residual">residual</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/2023-11-Meta-Adapter.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/TaskRes"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Task Residual for Tuning Vision-Language Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/CLIP-Few-Shot/Module/CLAP"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-vision-language-pretrained-models" class="table-of-contents__link toc-highlight">2.1 Vision-Language Pretrained Models</a></li><li><a href="#22-vision-language-model-adaption" class="table-of-contents__link toc-highlight">2.2 Vision-Language Model Adaption</a></li><li><a href="#23-meta-learning" class="table-of-contents__link toc-highlight">2.3 Meta-Learning</a></li><li><a href="#31-revisiting-clip-and-tip-adapter" class="table-of-contents__link toc-highlight">3.1 Revisiting CLIP and Tip-Adapter</a></li><li><a href="#32-meta-adapter" class="table-of-contents__link toc-highlight">3.2 Meta-Adapter</a></li><li><a href="#33-comparison-with-counterparts" class="table-of-contents__link toc-highlight">3.3 Comparison with Counterparts</a></li><li><a href="#41-cross-category-generalization" class="table-of-contents__link toc-highlight">4.1 Cross-Category Generalization</a></li><li><a href="#42-cross-dataset-generalization" class="table-of-contents__link toc-highlight">4.2 Cross-Dataset Generalization</a></li><li><a href="#43-cross-task-generalization" class="table-of-contents__link toc-highlight">4.3 Cross-Task Generalization</a></li><li><a href="#44-comparison-with-offline-methods" class="table-of-contents__link toc-highlight">4.4 Comparison with Offline Methods</a></li><li><a href="#45-more-ablation-studies-of-the-meta-adapter" class="table-of-contents__link toc-highlight">4.5 More Ablation Studies of the Meta-Adapter</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.bb3dcfd4.js"></script>
<script src="/assets/js/main.3fd1c2fa.js"></script>
</body>
</html>