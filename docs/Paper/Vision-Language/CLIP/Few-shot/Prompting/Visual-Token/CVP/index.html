<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2023-03-CVP">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Convolutional Visual Prompt for Robust Visual Perception | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/CVP"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Convolutional Visual Prompt for Robust Visual Perception | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/CVP"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/CVP" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/CVP" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.24800c52.js" as="script">
<link rel="preload" href="/assets/js/main.4c1744f1.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Multi-Modal/PEFT/Composition/Moka">Multi-Modal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">CLIP</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Few-shot</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Module/CLIP-Adapter">Module</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/MaPLe">Multi-Modality</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/CMAR">Prompting</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-6 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/CMAR">Pixel-Level</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-6 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/CoOp">Textual-Token</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-6 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/VPT">Visual-Token</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-7 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/VPT">Visual Prompt Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-7 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/DePT">Visual Prompt Tuning For Test-time Domain Adaptation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-7 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/LPT">LPT: Long-Tailed Prompt Tuning For Image Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-7 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/CVP">Convolutional Visual Prompt for Robust Visual Perception</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-7 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/E2VPT">E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-7 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/SA2VP">SA^2VPT: Spatially Aligned-and-Adapted Visual Prompt</a></li></ul></li></ul></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN">Foundation</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot">VQA-IC</a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">CLIP</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Few-shot</span><meta itemprop="position" content="4"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Prompting</span><meta itemprop="position" content="5"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Visual-Token</span><meta itemprop="position" content="6"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Convolutional Visual Prompt for Robust Visual Perception</span><meta itemprop="position" content="7"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Convolutional Visual Prompt for Robust Visual Perception</h1></header><p>논문 및 이미지 출처 : <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/58be158bf831a706b1a66cffbc401cac-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">https://proceedings.neurips.cc/paper_files/paper/2023/file/58be158bf831a706b1a66cffbc401cac-Paper-Conference.pdf</a></p><h1>Abstract</h1><ul><li>Vision models 는 종종 out-of-distribution (OOD) samples 에 취약(vulnerable)하며, 이를 수정하기 위해 adaptation 이 필요.</li><li>visual prompts 는 large-scale vision models 에 input space adaptation 의 lightweight method 를 제공하지만, high-dimensional additive vectors 와 labeled data 에 의존</li><li>이는 label 이 없는 self-supervised test-time setting 에서 model adapting 할 때 overfitting 을 초래</li></ul><hr><ul><li>저자는 robust visual perception 을 위해 label-free test-time adaptation 을 위해 convolutional visual prompt (CVP)를 소개</li><li>CVP 의 structured nature 은 standard visual prompts 에 비해 1% 미만의 fewer trainable parameters 를 요구하여 overfitting 을 방지</li><li>다양한 OOD visual perception tasks 에 대한 광범위한 실험과 분석을 통해 저자의 접근 방식이 효과적이며 여러 large-scale models 에서 최대 5.87% 까지 robustness 를 향상시킴</li></ul><h1>1. Introduction</h1><p>Deep model 은 in-distribution data 에서 test 할 때 인간을 능가하지만, unforeseen out-of-distribution (OOD) data, 예로, unexpected corruptions 및 shiftings 을 만날 때 성능이 급락.</p><ul><li>이러한 취약성은 특히 safety-critical applications 및 high-stakes tasks 에서 이러한 model 이 배포될 때 심각한 위험을 초래</li><li>이전 연구들은 training time 에 OOD data 에 대한 generalization 향상 방법을 연구했으나, test time 에 OOD data 에 model apdating  연구는 거의 없었으며, 대부분은 model weight 수정을 필요로 함</li></ul><hr><p>Visual prompting 은 test time 시 model 을 수정하지 않고 adapting 하는 효율적이고 경량화된 방법으로 등장 (adversarial reprogramming)</p><ul><li>이는 model 전체 weight 를 fine-tuning 하는 것과 달리, input space 에서 context 를 제공하여 model 의 original task 를 수정할 수 있음</li><li>fewer OOD samples 를 요구하고, 실용적인 applications 에서 model version management 를 단순화</li><li>하지만 OOD samples 는 label 있어야 하므로, 이러한 방법은 예상치 못한 distribution shift 에 대처할 수 없음.</li></ul><hr><p>최근 연구는 test time 시 self-supervised loss 를 minimizing 함으로써 &quot;reversal vectors&quot; 라는 high-dimensional prompt vectors 를 input 에 직접 추가하며 adversarial input 을 복구하여 unseen attacks 에 방어한다.</p><p>하지만, (given bound 내에) 임의의 양 의 pixel 에 damage 를 입히는 adversarial attacks 과 달리, structured changes 는 unstructured high dimensional vector prompts 로는 효과적으로 처리되지 않는다.</p><p>self-supervised objective 는 종종 shortcut 과 trivial solutions 를 가지므로, right structures 가 없는 prompt 는 성능 향상을 최소화하는 경우가 많음.</p><hr><p>이 논문은 test time 시 visual OOD samples 에 adapting 하기 위한 inductive bias 로 convolutional structure 를 사용하는 <strong>Convolutional Visual Prompt (CVP)</strong> 를 제시</p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-23-7f7a0b75cb8845a704af0e5e5888836f.png" width="1347" height="494" class="img_ev3q"></p><ul><li>이전 연구는 convolutional operations 가 local motifs 가 있는 structured data 를 처리하는 데 효과적임을 보여줌.</li><li>이에 영감을 받아 CVP 는 small tunable parameters 를 가진 convolutional kernels 로, 전형적인 unstructured visual prompts 의 1% 미만의 parameter 만 사용하여 매우 경량화되고 효율적 (Fig. 1)</li></ul><p>저자의 실험은 self-supervised adaptation 을 위한 structured inductive bias 의 중요성을 보여줌: high-dimensional free-form visual prompts 와 비교하여, low-rank structured prompts 는 견고성을 3.38% 향상시키며, CIFAR-10-C 에서 convolutional structure 를 가진 prompt 는 low-rank prompt 보다 2.94% 더 우수</p><p>ImageNet-Rendition, ImageNet-Sketch, 및 CIFAR-10 과 ImageNet 의 15 types unforeseen corruptions 에서 CVP 는 인기 있는 large-scale visual models (ResNet50, WideResNet18, CLIP 등)에서 견고성을 5.87% 향상</p><p>저자의 방법은 input space 를 수정하므로, established test-time model weight adaptation 방법과 보완할 수 있으며, multiple self-supervised objectives 에 일반화할 수 있다.</p><h1>2. Related Work</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="domain-generalization">Domain Generalization<a href="#domain-generalization" class="hash-link" aria-label="Direct link to Domain Generalization" title="Direct link to Domain Generalization">​</a></h4><ul><li>OOD data 는 machine learning models 의 성능을 크게 저하시킬 수 있음.</li><li>Domain generalization (DG)는 training 시 target domain data 를 알지 못한 채로 OOD samples 로 model 을 adapting 하는 것을 목표로 함</li><li>OOD data 에 model apdating 하는 것은 또한 견고성을 향상시킴.</li><li>Test-time adaptation 은 distribution shift 에 대한 robustness 를 위한 새로운 패러다임으로, 주로 deep models 의 weight 를 업데이트</li><li>BN : batch normalization statistics 를 사용하여 model 을 업데이트하고, TENT 는 각 batch 의 conditional entropy 를 minimizing 하여 model weight 를 adapting</li><li>TTT : rotation prediction 을 위한 auxiliary self-supervision model 로 training 하고 SSL loss 를 이용하여 model 을 adapting 하는 시도</li><li>MEMO : single sample 을 augment 하고 augmented samples 의 marginal entropy model 을 adapting</li><li>Test time transformation ensembling (TTE) : fixed transformations set 으로 image 를 augment 하고 averaging 을 통해 outputs 를 ensembling</li><li>model 을 update 하지 않는 두 task 는 <!-- -->[Adversarial attacks are reversible with natural supervision., Robust perception through equivariance]<!-- --> 로, OOD sample 이 아닌 adversarial sample 의 pixel 을 수정하여 self-supervised objectives 를 minimizing</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="visual-prompting">Visual Prompting<a href="#visual-prompting" class="hash-link" aria-label="Direct link to Visual Prompting" title="Direct link to Visual Prompting">​</a></h4><p>prompting 은 NLP 분야에 specific task 에 맞게 model 을 adapting 하기 위해 context 를 제공하기 위해 제안.</p><ul><li>이 아이디어를 활용하여, visual prompt 는 vision task 와 foundation model 을 위한 input space 에서 small trainable parameters 로 model 을 adapting</li><li>다른 연구들은 target classification task 를 위해 model repurpose 를 로 adversarial perturbations sample 을 prompt 하는 것을 제안<ul><li>이를 <em>Adversarial Reprogramming</em> 이라 함</li></ul></li><li>Black-box adversarial reprogramming : limited data 로 downstream classification task 를 위해 black-box model 을 reprogramming</li><li>V2S : time-series data classification task 를 위해 speech recognition model 을 reprogramming</li><li>Robust visual prompts : attack 하에 model 의 adversarial robustness 를 향상시키기 위해 training 에 tuning<ul><li>그러나 이 연구는 distribution 이  분포가 자연적으로 변동되는 domain generalization 에 아직 적용되지 않음.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="self-supervised-learning-ssl">Self-supervised learning (SSL)<a href="#self-supervised-learning-ssl" class="hash-link" aria-label="Direct link to Self-supervised learning (SSL)" title="Direct link to Self-supervised learning (SSL)">​</a></h4><p>SSL 은 annotations 없이 image 를 통해 효과적인 representation 학습</p><ul><li>이전 연구들은 다양한 pretext tasks (jigsaw puzzles, rotation prediction, image colorization 및 deep clustering) 에서 학습된 representation 이 image classification, object detection 및 test-time domain adaptation 등 여러 downstream task 에 활용될 수 있음을 보여줌</li><li>SSL 의 또 다른 잘 알려진 분파는 contrastive learning 으로, sample transformation 에 대한 관련 feature 를 그룹화하고 dataset 에서 다른 비슷하지 않은 sample 과는 거리를 두는 것을 목표로 함</li><li>일부 방법들은 outlier detection 을 위해 SSL 을 사용하며, 이는 일반화 가능한 OOD features 를 학습하고 test-time 에 이를 거부하는 것을 목표로 함.</li><li>training phase 에서 target domain distribution 에 대한 정보를 요구하는 이러한 방법들과 달리, 저자의 방법은 test phase 에서 예기치 않은 domain 에 대한 정보를 요구하지 않고 model adapting 가능</li></ul><h1>3. Test-Time Convolutional Visual Prompting</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-learning-intrinsic-structure-with-self-supervision-task">3.1 Learning Intrinsic Structure with Self-Supervision Task<a href="#31-learning-intrinsic-structure-with-self-supervision-task" class="hash-link" aria-label="Direct link to 3.1 Learning Intrinsic Structure with Self-Supervision Task" title="Direct link to 3.1 Learning Intrinsic Structure with Self-Supervision Task">​</a></h2><p>OOD data 에 대한 model robustness 를 향상시키는 표준 방법은 training robust 이다.</p><ul><li>여기서 training algorithm 은 inference time 에 발생할 수 있는 corruption 과 distribution shifts 를 예상하고 이에 대해 training 한다<ul><li>그러나 test-time shifting 을 예상하는 것은 real world 에서 종종 비현실적인 강한 가정</li><li>따라서 저자는 unforeseen corruption 및 unknown shifts 에 동적으로 적응하여 test time 에 robustness 를 향상</li></ul></li><li>inference time 에 model adapting 의 이상적인 경우는, target task 의 ground truth label 을 아는 것이지만, test data 는 label 이 없기 때문에 이는 불가능<ul><li>unlabeled 에 대한 downstream classification tasks 의 성능을 크게 향상시키기 위해서는 target classification task 와 rich information 을 공유하는 적절한 self-supervision task 를 선택해야 함.</li></ul></li><li>training time 에 representation learning 을 위한 good self-supervision tasks 에 대한 많은 연구가 있음<ul><li>예로, jigsaw puzzles, rotation prediction, image colorization 및 deep clustering 은 image classification, object detection 및 test-time domain adaptation 같은 여러 downstream task 에 적용될 수 있음</li></ul></li><li>visual recognition 의 경우, 인기 있는 self-supervised objective 는 contrastive learning 으로, same image transformation  의 feature 을 nearby place 에 mapping 하는 representation 을 학습, 이는 다음과 같이 공식화:</li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi mathvariant="script">L</mi><mi>s</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mrow><mo fence="true">[</mo><msubsup><mi>y</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>s</mi></msubsup><mi>log</mi><mo>⁡</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>z</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>τ</mi><mo stretchy="false">)</mo></mrow><mrow><munder><mo>∑</mo><mi>k</mi></munder><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>z</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>τ</mi><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">]</mo></mrow><mo separator="true">,</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \mathcal{L}_s(x) = -\mathbb{E}_{i,j} \left[ y^s_{i,j} \log \frac{\exp(\cos(z_i, z_j)/\tau)}{\sum_k \exp(\cos(z_i, z_k)/\tau)} \right], \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4357em;vertical-align:-0.9679em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4679em"><span style="top:-3.4679em"><span class="pstrut" style="height:3.45em"></span><span class="mord"><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord">−</span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7144em"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mop">cos</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mop">cos</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9857em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">]</span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9679em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4679em"><span style="top:-3.4679em"><span class="pstrut" style="height:3.45em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9679em"><span></span></span></span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span> : pre-trained backbone 의 extrected <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> contrastive features</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>s</mi></msubsup></mrow><annotation encoding="application/x-tex">y^s_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0592em;vertical-align:-0.3948em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em"><span></span></span></span></span></span></span></span></span></span></span> : positive pairs 및 negative pairs 를 나타내는 0-1 vector<ul><li>만약 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>y</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>s</mi></msubsup></mrow><annotation encoding="application/x-tex">y^s_{i,j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0592em;vertical-align:-0.3948em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3948em"><span></span></span></span></span></span></span></span></span></span></span> 가 1 이라면, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>-th feature <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 및 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span>-th feature <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">z_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 는 모두 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> sample 에서 나온 것이며, 그렇지 않으면, 다른 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 에서 나온 것이다.</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo separator="true">⋅</mo><mo separator="true">,</mo><mo separator="true">⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\cos(·, ·)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mpunct">⋅,⋅</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mclose">)</span></span></span></span></span> : cosine similarity</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span></span></span></span></span> : temperature scaling value</li></ul></li><li>저자는 contrastive loss 를 사용하여 SSL model <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">C</mi></mrow><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.05834em">C</span></span></span></span></span> 의 parameter 를 optimizing<ul><li>training 을 위한 objective function 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>min</mi><mo>⁡</mo></mrow><msub><mi>θ</mi><mi mathvariant="script">C</mi></msub></msub><msub><mi mathvariant="double-struck">E</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>∼</mo><msub><mi mathvariant="script">X</mi><mi>s</mi></msub></mrow></msub><mo stretchy="false">[</mo><msub><mi mathvariant="script">L</mi><mi>s</mi></msub><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\min_{\theta_\mathcal{C}} \mathbb{E}_{(x) \sim \mathcal{X}_s}[\mathcal{L}_s(\cdot)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3552em"></span><span class="mop"><span class="mop">min</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.3567em;margin-left:-0.0278em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathcal mtight" style="margin-right:0.05834em">C</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2503em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:0.14643em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em"><span style="top:-2.357em;margin-left:-0.1464em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)]</span></span></span></span></span> 로 정의</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">X</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{X}_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14643em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.1464em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> : training 을 위한 source domain data</li><li>저자는 non-corruption dataset 에서 가져온 clean samples 에서만 training</li><li>저자는 이 task 를 ablation study 에서 other self-supervised tasks 와 비교</li></ul></li></ul><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-24-4c4252b4979e23aeaa40ee02c956ae66.png" width="1250" height="490" class="img_ev3q"></p><p>training time 에 self-supervised task 를 training 하기 때문에, self-supervised task 는 training 과 동일한 distribution 의 test data 에서 높은 성능을 발휘한다. 저자의 결과는 test 시 distribution shift 에서 self-supervised task 의 성능이 크게 떨어지는 것을 발견 (Fig. 2)</p><ul><li>이는 self-supervised task 에 유용한 정보가 classification performance 하락 외에도 부수적으로 corrupt 된다는 것을 시사</li></ul><p>이전 연구들은 self-supervised tasks 와 perception task 간의 상당한 mutual ionformation 을 보여주며, self-supervised task 로 pre-training 하면 recognition task 의 성능이 향상됨을 입증.</p><p>저자는 inference 시 self-supervised loss 를 minimizing 하도록 model 을 adapting 하는 것을 제안하며, self-supervised task 는 recognition task 에서 많은 정보를 capture 하는 proxy. self-supervised task 의 information 을 복구하면서, distribution shifts 로 인한 corrupted perception tasks 의 정보를 복구할 수 있다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-test-time-adaptation-for-vision-models">3.2 Test-time Adaptation for Vision Models<a href="#32-test-time-adaptation-for-vision-models" class="hash-link" aria-label="Direct link to 3.2 Test-time Adaptation for Vision Models" title="Direct link to 3.2 Test-time Adaptation for Vision Models">​</a></h2><ul><li>inference time adaptation 은 model 이 new distribution 의 고유 특성에 online 으로 adapting 할 수 있도록 한다.<ul><li>하지만 핵심 문제는 저자가 model adapting 을 위해 사용하는 SSL objective 가 proxy 로, 종종 loss 를 줄이지만 model 을 잘못된 방식으로 adapting 시키는 trivial solution 이 있다는 것</li><li>foundation models 를 포함해 vision model 을 adapting 하기 위한 몇 가지 확립된 방법이 존재</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="finetuning-ft">Finetuning (FT)<a href="#finetuning-ft" class="hash-link" aria-label="Direct link to Finetuning (FT)" title="Direct link to Finetuning (FT)">​</a></h4><p>FT 는 deep model을 adapting 하는 standard way.</p><ul><li>이는 종종 deep models 의 all parameters 또는 partially 를 optimizing 하며, 이는 무거우며 모델을 re-initializing 및 updating 하기 위해 large space 가 필요</li><li>이전 연구는 supervised task 에서 fine-tuning 될 때 이 방법이 효과적임을 보여주지만, 몇 가지 예에서 self-supervised task 로 adapting 하는 것에는 충분한 탐구가 되지 않았음</li><li>저자는 self-supervised task 를 사용한 finetunig 방법을 논의</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="partial-finetuning-pft">Partial Finetuning (PFT)<a href="#partial-finetuning-pft" class="hash-link" aria-label="Direct link to Partial Finetuning (PFT)" title="Direct link to Partial Finetuning (PFT)">​</a></h4><p>PFT 는 batch-normalization layer 의 statistics 만 변경하여 inference 시 model 을 adapting 하는 다른 방법</p><ul><li>이 방법은 test data 와 mean 과 standard deviation 에서 distribution 가 drift 하며 test-time adaptation 을 통해 이를 회복할 수 있다 가정.</li><li>가장 가까운 기존 작업은 BN, Tent 및 MEMO.</li><li>Tent : BN statistics 를 업데이트하지만 same distribution 에서 계속 training 해야 함</li><li>MEMO : single test data point 만 필요하지만, 전체 model update 와 heavy augmentations 로 인해 알고리즘이 느림</li></ul><p>여기서 저자는 제안된 contrastive learning-based self-supervised loss 를 통해 batch normalization 을 adapting</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="visual-prompts-vp">Visual Prompts (VP)<a href="#visual-prompts-vp" class="hash-link" aria-label="Direct link to Visual Prompts (VP)" title="Direct link to Visual Prompts (VP)">​</a></h4><p>VP 는 pre-trained models 를 adapting 하기 위한 lightweight way 로 등장</p><ul><li>vision model 에 visual prompt 를 적용하는 두 가지 방법 존재<ul><li>image <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 로 두고, input image 에 vector <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span></span> 를 추가: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">x = x + v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span></span></li><li>low-rank visual prompt 의 경우, optimizing 중 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span></span> 에서 low-rank matrix 사용</li></ul></li><li>대부분 visual prompt 는 training setting 에 연구가 되어, inference time 에는 탐구되지 않음.</li><li>[Adversarial attacks are reversible with natural supervision., Robust perception through equivariance]<!-- --> : adversarial perturbation 을 수정하고 model reobustness 향상을 위해 image 에 추가 visual prompt 를 optimizing</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-adapting-via-convolutional-visual-prompts">3.3 Adapting via Convolutional Visual Prompts<a href="#33-adapting-via-convolutional-visual-prompts" class="hash-link" aria-label="Direct link to 3.3 Adapting via Convolutional Visual Prompts" title="Direct link to 3.3 Adapting via Convolutional Visual Prompts">​</a></h2><p>self-supervised objective 에 대한 trivial solution 을 피하기 위해 right structure 을 추가하는 것이 효과적인 방법.</p><p>저자는 이제 test distribution 에 적응하도록 deep model 을 안내하는 Convolution Visual Prompts (CVP) 를 소개</p><ul><li>convolution 은 visual task 에 성공적인 inductive bias 로 증명되었으며, sample efficient.</li><li>저자의 가정은 image data 에서 large family distribution shift 가 visually structure 되어 있으며, 이를 convolution 으로 modeling 할 수 있다는 것.</li><li>저자의 prompt 는 간단하며 다음과 같이 정의:</li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>x</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>λ</mi><mtext>conv</mtext><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} x = x + \lambda \text{conv}(x, k) \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">λ</span><span class="mord text"><span class="mord">conv</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-2.85em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span></span></div><ul><li>convolutional prompt 의 주요 장점 중 하나는 prompt 의 parameter 수가 다른 prompt (e.g., patch prompt 및 padding prompt) 보다 상당히 적다는 것 (1%)</li><li>full model weight adapting 과 비교하여, 저자의 방법은 visual distribution shifts 의 구조를 활용하여 경량화되고 빠름.</li><li>저자의 lightweight adjustment 는 과도한 계산 및 메모리 오버헤드 없이 model 이 novel data points 에 빠르게 업데이트할 수 있도록 함.</li><li>vision model 이 엣지 장치에 지속적으로 배포됨에 따라 제한된 컴퓨팅 자원을 고려할 때 이는 매우 중요.</li></ul><p><img loading="lazy" alt="Algorithm 1" src="/assets/images/image-25-f49d848bf90c3f1b1a4a0df66b6e8f92.png" width="1250" height="873" class="img_ev3q"></p><p><img loading="lazy" alt="Algorithm 2" src="/assets/images/image-26-1919351432aabaddb674e7f082f623e9.png" width="1250" height="873" class="img_ev3q"></p><h1>4. Experiments</h1><p>이 섹션에선 실험 설정의 세부 사항을 설명하고 기존 VP 및 기존 test-time approach 와 비교하여 CVP 의 성능 평가</p><p>CVP 에 대한 포괄적인 연구를 수행하며, 여기에는 다양한 prompt 설계, kernel vs structure analysis, 여러 SSL task, batch size 및 adapt iteration 에 대한 sensitivity analysis, GradCam visualization 및 CVP 의 optimization cost 포함</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-experiment-setting">4.1 Experiment Setting<a href="#41-experiment-setting" class="hash-link" aria-label="Direct link to 4.1 Experiment Setting" title="Direct link to 4.1 Experiment Setting">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dataset">Dataset.<a href="#dataset" class="hash-link" aria-label="Direct link to Dataset." title="Direct link to Dataset.">​</a></h4><p>우리 방법은 5 OOD dataset 에서 평가: CIFAR-10-C, ImageNet-C, ImageNet-R, ImageNet-Sketch, 그리고 ImageNet-A.</p><ul><li><strong>Synthesis OOD Data</strong>: corruption data 는 real-world corruption 을 시뮬레이션하기 위해 다양한 유형의 변환(e.g., snow, brightness, contrast)으로 합성<ul><li>dataset 에는 CIFAR-10-C 와 ImageNet-C 가 포함</li><li>둘 다 original dataset 의 corruption version 으로, 15 corruption types 와 5 severity levels 가 포함</li><li>larger severity level 은 데이터에 more corruption 이 추가됨을 의미</li><li>저자의 방법을 잘 평가하기 위해, 각 15 corruption types 에 대해 공식 GitHub 코드를 기반으로 5 severities samples 생성</li></ul></li><li><strong>Natural OOD Data</strong>: ImageNet-Rendition 은 Flickr 에서 수집된 ImageNet 의 200 object classes 의 특정 유형이 포함된 30,000 images 포함<ul><li>ImageNet-Sketch dataset 은 50,000 sketch images 포함</li><li>ImageNet-Adversarial 은 natural world 에서 수집된 7,500 images 가 포함된 natural adversarial shifting dataset</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="model">Model.<a href="#model" class="hash-link" aria-label="Direct link to Model." title="Direct link to Model.">​</a></h4><p>backbone model architecture 는 CIFAR-10-C 를 위해 WideResNet18 과 ResNet26 에 pre-training 됨</p><ul><li>ImageNet-C, Rendition, Sketch 를 위해서는 ResNet50 에 pre-training 됨</li><li>저자는 SSL model training 을 위해 backbone model 의 fully connected layer 전에 logit features 를 추출</li><li>SSL model 은 contrastive learning task 를 위해 one-dimensional features 을 추출하는 단순한 MLP</li><li>저자는 또한 prompt 방법을 foundation CLIP model 로 확장하여, vision encoder 만 prompting</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="baseline-details">Baseline Details.<a href="#baseline-details" class="hash-link" aria-label="Direct link to Baseline Details." title="Direct link to Baseline Details.">​</a></h4><p>저자는 CVP 와 여러 test-time adaptation benchmarks 를 비교</p><ul><li><strong>Standard</strong>: baseline 은 adaptation 없이 pre-trained model 을 사용<ul><li>CIFAR-10-C 의 경우, baseline 은 WideResNet18 과 ResNet 에서 50,000 clean CIFAR-10 training dataset 으로 훈련됨</li><li>ImageNet1K-C 의 경우, baseline 은 ResNet50 에서 약 1.2M clean ImageNet training dataset 으로 훈련</li></ul></li><li><strong>Finetune (FT)</strong>: 저자는 self-supervised loss 로 inference time 동안 다가오는 모든 batch 를 위해, full model weight 를 조정<ul><li>실험에서, one-batch fine-tuning 후, model 은 initial weight status 로 복원되고 new type 의 corruption sample 을 받음</li></ul></li><li><strong>Partial Finetune (PFT)</strong>: partial fine-tuning 은 inference time 마다 batch normalizationl ayers 만 조정하여 model 에 sample batch 를 adapting 시킴<ul><li>finetuning baseline 과 마찬가지로, model 은 one-batch adaptation 후 initial weight status 로 복원</li></ul></li><li><strong>SVP</strong>: contrastive loss 를 통해 optimizing 된 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">ℓ</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">\ell_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span>-norm perturbation 으로 adversarial sample 을 수정하여 adversarial attacks 을 역전시키는 prompt method<ul><li>저자는 patch 와 padding 의 두 prompt setting 으로 이 방법 확장</li><li>patch setting 의 경우, input 에 perturbation 의 전체 크기 patch 를 직접 추가</li><li>padding setting 의 경우, perturbation 의 프레임을 input outside 에 포함</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="design-of-convolutional-visual-prompts-cvp">Design of Convolutional Visual Prompts (CVP)<a href="#design-of-convolutional-visual-prompts-cvp" class="hash-link" aria-label="Direct link to Design of Convolutional Visual Prompts (CVP)" title="Direct link to Design of Convolutional Visual Prompts (CVP)">​</a></h4><p>저자는 convolutional kernels 를 추가해 input samples 를 prompting 함.</p><ul><li>여기서 1.) fixed or random kernel initialization, 2.) 3<!-- -->*<!-- -->3 or 5<!-- -->*<!-- -->5  kernel sizes 포함</li><li>실험 결과 all kernel setups 평가를 보여줌<ul><li>initialization 의 경우, 저자는 kernel size <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 를 uniform distribution or fixed values 로 초기화 가능</li><li>fixed initialization 의 경우, sharpness kernel 에서 시작하는 것이 효과적임을 경험적으로 발견</li></ul></li><li>저자는 projected gradient descent 의 1 to 5 interations 로 optimizing</li><li>원래 구조를 보존하기 위해, 저자는 input 과 convolved output 을 learnable parameters λ 로 결합</li><li>저자는 convolutional kernel <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> 와 λ 를 self-supervised loss <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">L</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{L}_s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 로 jointly optimizing</li><li>λ 의 범위는 predefine 되어 있으며 경험적으로 고정된 범위로 설정<ul><li>CIFAR-10-C 의 경우, 범위는 <!-- -->[0.5, 3]<!-- --> 로 설정되고, </li><li>ImageNet-C 의 경우 <!-- -->[0.5, 1]<!-- --> 로 설정</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-experimental-results">4.2 Experimental Results<a href="#42-experimental-results" class="hash-link" aria-label="Direct link to 4.2 Experimental Results" title="Direct link to 4.2 Experimental Results">​</a></h2><p><img loading="lazy" alt="Table 1" src="/assets/images/image-27-de9972336bc6407ce1afbb8693877b59.png" width="1411" height="372" class="img_ev3q"></p><p>Tab. 1 은 CIFAR-10-C 에서 CVP 의 평가 결과를 보여줌</p><ul><li>저자는 standard, VP (patch/padding), fine-tuning (FT), partially fine-tuning (PFT) 과 같은 5 baselines 와 2 models 를 사용하여 CVP 를 비교</li><li>또한, CVP 의 다양한 커널 설정 (fixed/random initialization 및 different kernel sizes) 을 탐색</li><li>결과에 따르면, WideResNet18 에서 CVP 는 random initialized 3<!-- -->*<!-- -->3 kernel 을 업데이트할 때 average error rate 가 5.87% 로 가장 크게 감소</li><li>ResNet-26 의 경우, CVP 는 일관되게 error rate 2.8% 줄임.</li></ul><p><img loading="lazy" alt="Table 2" src="/assets/images/image-28-c9a2dac65443f7c4fd4e9ed99813ad63.png" width="1411" height="662" class="img_ev3q"></p><p>Tab. 2 는 ImageNet-C, Rendition, Sketch, 및 Adversarial 에 대한 결과</p><ul><li>standard baseline 은 ResNet50 에 pre-trained</li><li>ImageNet-C 의 경우, ResNet50 과 표준 AlexNet 의 성능 비율을 기준으로 average corruption error (mCE) 를 report</li><li>ImageNet dataset 의 larger dimension (224) 때문에, larger kernel size 가 input 에서 더 많은 정보를 효과적으로 capture</li><li>따라서, 커널 크기를 3<em>3 및 5</em>5로 설정했습니다. 우리의 결과는 CVP가 랜덤 초기화된 5*5 커널로 업데이트할 때, ImageNet-R과 A에서 각각 0.77% 및 1.6%로 가장 높은 오류율 감소를 달성한다는 것을 나타냅니다. CVP는 ImageNet-C와 Sketch에 대해서도 모든 기준선을 일관되게 능가합니다. VP(패치)와 VP(패딩)는 대부분의 데이터셋에서 성능을 저하시켜, 더 많은 학습 가능한 파라미터를 가진 비구조적 섭동이 일반적인 OOD 데이터의 자연적 이동에 적응하는 데 비효과적임을 시사합니다.</li></ul><p><img loading="lazy" alt="Table 3" src="/assets/images/image-29-c70dea295bb533a686148dcd8c0dd984.png" width="1230" height="419" class="img_ev3q"></p><p><strong>테이블 3</strong>에서는 대규모 기초 모델인 CLIP <!-- -->[52]<!-- -->에서 CVP의 성능을 추가로 평가합니다. 기준선 VP(패딩) 및 VP(패치)와 비교할 때, CVP는 랜덤 초기화된 5*5 커널 크기로 업데이트할 때 모든 데이터셋에서 최고의 성능을 달성합니다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="cvp-complements-other-test-time-adaptation-methods">CVP Complements Other Test-Time Adaptation Methods<a href="#cvp-complements-other-test-time-adaptation-methods" class="hash-link" aria-label="Direct link to CVP Complements Other Test-Time Adaptation Methods" title="Direct link to CVP Complements Other Test-Time Adaptation Methods">​</a></h4><p>저자의 방법은 input space 를 수정하고 adapted samples 의 representation 을 original manifold 에 정렬하기 때문에, model weight 를 조정하는 기존의 test-time adaptation approach 도 보완. 따라서, 저자는 CVP 를 MEMO, BN, TENT 와 같은 여러 기존 방법과 결합. </p><ul><li>공정한 비교를 위해 모든 실험에서 batch size 를 16 으로 설정</li><li>parameter setting 의 경우, CIFAR-10-C 에 대해 3<!-- -->*<!-- -->3 kernel size 로, ImageNet-C, R, S, A 에 대해서는 5<!-- -->*<!-- -->5로 설정</li><li>adaptation iteration 는 모두 5</li></ul><p><img loading="lazy" alt="Table 4" src="/assets/images/image-30-d4773b5f8d1f342d43f176083dad0f46.png" width="1230" height="568" class="img_ev3q"></p><p>Tab. 4 는 5 benchmarks 에 대한 결과를 보여줌</p><ul><li>CVP 는 TENT 를 1.83 points 향상시키고 표준 방법과 비교하여 error rate 21.55% 감소</li><li>other datasets 에서도, CVP 는 TENT 방법 위에서 lowest error rate 달성</li><li>그러나, BN 방법은 small batch settings 성능 저하</li></ul><h1>5. Ablation Study</h1><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ssl-objective-와-shortcut-회피를-위한-low-rank">SSL Objective 와 Shortcut 회피를 위한 Low-Rank<a href="#ssl-objective-와-shortcut-회피를-위한-low-rank" class="hash-link" aria-label="Direct link to SSL Objective 와 Shortcut 회피를 위한 Low-Rank" title="Direct link to SSL Objective 와 Shortcut 회피를 위한 Low-Rank">​</a></h4><p>SSL (Self-Supervised Learning) objective 는 visual recognition task 의 proxy 이기 때문에, SSL objective 만을 minimizing 하는 것은 overfitted prompt 를 생성할 수 있으며, visual recognition 의 향상에는 크게 기여하지 않을 수 있다. </p><p>이러한 suboptimal 을 피하기 위해서는 적절한 inductive bias 을 추가하는 것이 일반적인 방법이다.</p><p>highly structured visual data 의 경우, adaptation 중 shortcut 방지를 위해 추가할 수 있는 best inductive bias 연구</p><hr><p>convolution 외에도, 저자는 SVD (singular value decomposition)를 통한 low-rank structure visual prompt (LVP) 조사.</p><p>low-rank matrices 를 optimizing 하는 알고리즘(algorithm 2) 을 사용하여 LVP 를 생성하고, natural corruption 을 reverse 하는 능력에 대해 low-rank prompt 와 convolution 을 비교</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-31-2fb811b562a8bf2896ae31cebd105056.png" width="1230" height="568" class="img_ev3q"></p><p><img loading="lazy" alt="Table 6" src="/assets/images/image-32-19155b1861f7629854735b6758eab939.png" width="1230" height="365" class="img_ev3q"></p><p>Tab. 6 및 Fig. 3 에서, LVP 와 CVP 모두에서 prompt rank 가 증가할수록 corruption 을 reverse 하여 clean image 를 생성하는 성능이 저하되며, real shifting 과 approximated shifting 간의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">ℓ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\ell_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> norm distance 가 증가하는 것을 확인</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="cvp-in-supervised-learning">CVP in Supervised Learning<a href="#cvp-in-supervised-learning" class="hash-link" aria-label="Direct link to CVP in Supervised Learning" title="Direct link to CVP in Supervised Learning">​</a></h4><p>SSL 은 final task 의 proxy 이기 때문에 test-time adaptation 을 위해 CVP 가 필요.</p><p>SSL 을 최소화하지만, final task 의 성능이 suboptimal.</p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-33-90b5e9456bb25efa0723f478a108f17c.png" width="1040" height="221" class="img_ev3q"></p><p>Tab. 5 에서 supervised learning setting (Sup-VP 및 Sup-CVP) 과 SSL setting (SSL-VP 및 SSL-CVP) 에서 VP 와 CVP 를 비교</p><ul><li>모든 실험에서 Tab. 4 와 동일한 optimization setting 및 parameters 를 사용했으며, supervised setting 에선 각 batch 에 prompt 를 적용하고 ground truth 로 update</li><li>ground truth 로 학습했기 때문에, VP 와 CVP 모두 SSL 보다 높은 정확도를 달성했으며, VP 가 CVP 보다 높은 성능을 보임</li><li>이는 label 이 없는 경우, structured convolutional prompt 가 성능 향상에 필요함을 보여줌</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="generalize-cvp-to-other-ssl-tasks">Generalize CVP to other SSL tasks.<a href="#generalize-cvp-to-other-ssl-tasks" class="hash-link" aria-label="Direct link to Generalize CVP to other SSL tasks." title="Direct link to Generalize CVP to other SSL tasks.">​</a></h4><p>CVP 는 다른 self-supervision task 로 일반화될 수 있음을 보여줌. </p><p>Tab. 7 에서는 contrastive learning, rotation prediction 및 masked autoencoder (MAE) 등 3 SSL task 에서 CVP 의 성능을 비교</p><ul><li>모든 SSL task 에서 CVP 가 ImageNet-C 의 모든 severity level (s1~s5) 에서 robust accuracy 가 향상시킴을 확인.</li><li>contrastive learning 이 rotation prediction 보다 더 나은 성능을 보였고, MAE 가 가장 좋은 성능을 보임.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="batch-size-및-adaptation-iteration-수의-영향">Batch Size 및 Adaptation Iteration 수의 영향<a href="#batch-size-및-adaptation-iteration-수의-영향" class="hash-link" aria-label="Direct link to Batch Size 및 Adaptation Iteration 수의 영향" title="Direct link to Batch Size 및 Adaptation Iteration 수의 영향">​</a></h4><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-34-e01a416f4553a8a42a507612f7608c29.png" width="1040" height="548" class="img_ev3q"></p><p>Figure 4(a) 에서는 batch size 가 다양한 prompt 방법의 성능에 미치는 영향을 실험적으로 보여줌.</p><ul><li>batch size 를 2, 4, 8, 16, 32 로 설정하고 Finetune, VP (patch), VP (patch) 와 CVP 의 정확도를 비교</li><li>batch size 가 2일 때, Finetune 의 성능은 36.95% 로 CVP (71.53%) 보다 낮았음.</li><li>batch size 를 4와 8 로 늘리면 Finetune 과 VP 의 성능이 약간 향상되지만 여전히 CVP 보다 낮음.</li><li>전반적으로 CVP 는 small batch setting 에서 adaptation 에 우위를 보임.</li></ul><p>Figure 4(b) 에서는 adaptation iteration 횟수에 따른 성능을 보여줌.</p><ul><li>iteration 횟수가 증가할수록 CVP 는 self-supervised objective 에 overfitting 될 위험이 낮아짐을 실험적으로 확인</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="saliency-map-의-visualization">Saliency Map 의 Visualization<a href="#saliency-map-의-visualization" class="hash-link" aria-label="Direct link to Saliency Map 의 Visualization" title="Direct link to Saliency Map 의 Visualization">​</a></h4><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-35-4d2cd39c305500340ead7e41a634e7dc.png" width="1040" height="428" class="img_ev3q"></p><p>CVP 가 corrupted input 에 adapting 하는 방식을 더 잘 이해하기 위해 다양한 유형의 손상에 대한 saliency map 을 시각화.</p><p>Fig. 5 에서 왼쪽에서 오른쪽으로 첫 번째 행은 original, corrupted, adapted samples; 두 번째 행은 predicted labels 에 대한 Grad-CAM.</p><ul><li>Grad-CAM 의 빨간 영역은 모델이 타겟 입력에 집중하는 부분을 강조.</li><li>corrupted sample 에서 heap map 이 target object 에서 초점을 잃는 것을 실험적으로 발견</li><li>CVP 이후, adapted sample 의 heap map 의 빨간 영역이 원본 이미지와 유사한 부분에 다시 초점을 맞추며, self-supervised visual prompt 가 input adaptation 을 개선하고 모델이 올바른 영역에 다시 집중하게 만듦</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="training-cost-vs-different-kernel-size">Training Cost v.s. Different Kernel Size<a href="#training-cost-vs-different-kernel-size" class="hash-link" aria-label="Direct link to Training Cost v.s. Different Kernel Size" title="Direct link to Training Cost v.s. Different Kernel Size">​</a></h4><p><img loading="lazy" alt="Table 8" src="/assets/images/image-36-4dced544911a9c3ffaa53dcab443405f.png" width="589" height="270" class="img_ev3q"></p><p>Tab. 8 에서는 CVP 의 다양한 kernel size 를 평가하고, kernel size 를 적절하게 증가시키면 성능이 약간 향상될 수 있음을 실험적으로 확인</p><ul><li>one corruption-type impulse noise 를 선택하여 결과를 보여줌.</li><li>kernel size 를 증가시키면 optimization cost 가 증가</li><li>impulse noise 의 경우, kernel size 7<!-- -->*<!-- -->7 이 가장 높은 robust accuracy 를 달성했지만 optimization cost 가 훨씬 높음.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="training-time-vs-number-of-adapt-iteration">Training Time v.s. Number of Adapt Iteration<a href="#training-time-vs-number-of-adapt-iteration" class="hash-link" aria-label="Direct link to Training Time v.s. Number of Adapt Iteration" title="Direct link to Training Time v.s. Number of Adapt Iteration">​</a></h4><p>Fig. 4(b) 에서는 adaptation iteration 횟수에 따른 CVP 의 성능을 보여줌.</p><ul><li>adaptation iteration 횟수가 증가할수록 training time 이 증가.</li></ul><p><img loading="lazy" alt="Table 9" src="/assets/images/image-37-01ba138268e4134c826c1b1b168abd3f.png" width="715" height="159" class="img_ev3q"></p><p>Tab. 9 에서는 CIFAR-10-C 의 gaussian noise type with severity 1 에 대한 결과를 보여줌</p><ul><li>adaptation iteration 횟수 (0 to 20) 에 따른 accuracy 와 batch 정확도와 per batch training time 비교</li><li>CVP 는 몇 epoch (1 epoch) 만으로도 VP 보다 더 큰 성능 향상</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="cvp-가-corrupted-images-를-normal-one-으로-reverse-하는가">CVP 가 Corrupted Images 를 Normal One 으로 Reverse 하는가?<a href="#cvp-가-corrupted-images-를-normal-one-으로-reverse-하는가" class="hash-link" aria-label="Direct link to CVP 가 Corrupted Images 를 Normal One 으로 Reverse 하는가?" title="Direct link to CVP 가 Corrupted Images 를 Normal One 으로 Reverse 하는가?">​</a></h4><p><img loading="lazy" alt="Table 10" src="/assets/images/image-38-c057c4988719232224987549fdc7ba1a.png" width="875" height="232" class="img_ev3q"></p><p><img loading="lazy" alt="Figure 6" src="/assets/images/image-39-fe41572c2401b57a29e55260a1ac0553.png" width="974" height="385" class="img_ev3q"></p><p>Sliced Wasserstein Distance (SWD) 및 structural similarity index measure (SSIM) 를 통해 distribution distance 측정.</p><p>저자는 두 input distributions 간의 distance 를 측정: source domain distribution 및 target domain distribution (before/after CVP adaptation)</p><p>SWD 를 통한 두 input distribution 간의 거리를 계산하기 위해, high dimensional probability distribution 을 linear projection 을 통해 group 의 marginal distribution 을 얻은 다음, 해당 marginal distribution 에 대해 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span></span>-Wasserstein Distance 를 계산</p><p>Tab. 10 과 Fig. 6 에서 severity 1 의 CIFAR-10-C 에 대한 SWD 결과를 보여줌.</p><ul><li>평균적으로 CVP 는 adaptation 후 더 낮은 SWD 를 달성하여, adaptation 후 target distribution 이 source distribution 에 더 가까워짐을 의미</li><li>prompting 후 average SWD 는 0.7% 감소</li></ul><h1>6. Conclusion</h1><p>Self-supervised convolutional visual prompt (CVP) 는 OOD samples 의 test-time adaptation 을 위한 새로운 방법.</p><ul><li>기존 label 을 사용해 visual prompt 를 훈련하는 방식과 달리, CVP 는 label 이 필요없으며 경량화 됨.</li><li>이전 visual prompt 의 1% 미만의 learnable parameters 를 사용하여, test time  ㅔ self-supervised objective 를 adapting 할 때 overfitting 의 위험을 피할 수 있음</li><li>5 SOTA benchmark 의 결과는 CVP 가 model 의 robustness 를 5.87% 향상시키며, 기존의 weight-adaptation 방법을 보완함을 보여줌.</li><li>광범위한 ablation study 에 따르면 distribution shifts 는 실제로 구조화되어 있으며, 따라서 CVP 는 adapting 중에 VP 보다 구조를 더 잘 포착<ul><li>이는 visual prompt 기술과 test time 에 adapting 에 새로운 통찰력을 제공</li></ul></li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/cvp">CVP</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/prompt">Prompt</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/visual-prompt">Visual Prompt</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/convolutional-visual-prompts">Convolutional Visual Prompts</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2023-03-CVP.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/LPT"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">LPT: Long-Tailed Prompt Tuning For Image Classification</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/E2VPT"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-learning-intrinsic-structure-with-self-supervision-task" class="table-of-contents__link toc-highlight">3.1 Learning Intrinsic Structure with Self-Supervision Task</a></li><li><a href="#32-test-time-adaptation-for-vision-models" class="table-of-contents__link toc-highlight">3.2 Test-time Adaptation for Vision Models</a></li><li><a href="#33-adapting-via-convolutional-visual-prompts" class="table-of-contents__link toc-highlight">3.3 Adapting via Convolutional Visual Prompts</a></li><li><a href="#41-experiment-setting" class="table-of-contents__link toc-highlight">4.1 Experiment Setting</a></li><li><a href="#42-experimental-results" class="table-of-contents__link toc-highlight">4.2 Experimental Results</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.24800c52.js"></script>
<script src="/assets/js/main.4c1744f1.js"></script>
</body>
</html>