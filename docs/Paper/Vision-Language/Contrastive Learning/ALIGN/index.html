<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/Contrastive Learning/2021-02-ALIGN">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/Contrastive Learning/ALIGN"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision | WYJLab"><meta data-rh="true" name="description" content="논문 및 image  출처 :"><meta data-rh="true" property="og:description" content="논문 및 image  출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/Contrastive Learning/ALIGN"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/Contrastive Learning/ALIGN" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/Contrastive Learning/ALIGN" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.331f850c.js" as="script">
<link rel="preload" href="/assets/js/main.2ba55cd9.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Unsupervised Prompt Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Prismer">Prismer: A Vision-Language Model with An Esemble of Experts</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/Contrastive Learning/ALIGN">Contrastive Learning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/Contrastive Learning/ALIGN">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Contrastive Learning/CLIP">Learning Transferable Visual Models From Natural Language Supervision</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Composition/CLIP-LoRA">PEFT</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Single-Stream/VLBERT">Single-Stream</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Two-Stream/LXMERT">Two-Stream</a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Contrastive Learning</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</h1></header><p>논문 및 image  출처 : <a href="https://arxiv.org/pdf/2102.05918" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2102.05918</a></p><h1>Abstract</h1><ul><li>pre-trained representations 는 NLP 및 perception task 에 중요. NLP 는 훈련에 human annotation 이 필요 없지만, visual/vision-language representation 는 여전히 의존.</li><li>ImageNet 이나 OpenImage 같은 explit class label 이 포함된 dataset 에서 대부분의 representation 학습. </li><li>반면 vision-language 는 Conceptual Captions, MSCOCO 같은 dataset 이 모두 non-trivial data collection 과정이 필요해서 비용이 크고 선별 과정은 dataset size 를 제한하고, 훈련 모델 확장을 방해한다.</li></ul><p>본 논문은 Conceptual Captions dataset 에 비용 및 후처리 단계 없이 1B 넘는 image alt-text pair 의 noisy dataset 을 활용함.</p><ul><li>simple dual-encoder architecture 는 contrastive loss 로 image 및 text pair 의 visual 및 language representation 를 align 을 학습</li><li>corpus scale 은 noisy 를 상쇄할 수 있으며, 간단한 학습 방법으로 SOTA 달성</li><li>ImageNet 및 VTAB 같은 classification task 로 전환 시 강력한 성능 발휘</li><li>aligned visual 및 language representation 은 zero-shot image classification 을 가능케 하며, Flickr30K 및 MSCOCO image-text retrieval benchmark 에서도 cross-modality model 과 비교해 SOTA 달성</li><li>이 representation 은 compex text 및 text + image queries 와의 cross-modality search 를 가능케 함</li></ul><h1>Introduction</h1><p>기존 visual 및 vision-language representation learning 대부분 다른 training data sources 를 사용해 별도로 연구함.</p><p>vision domain 에선 ImageNet, OpenImages, JFT-300M 같은 supervision dataset 에 대한 pre-training 이 transfer learning 을 통한 downstream task 성능 향상에 중요한 것을 입증. 이런 pre-training datasets 의 curation 에는 data 수집, sampling, 인간 주석이 필요해 확장이 어려움</p><p>vision-language modeling 도 pre-training 접근법을 사용하지만 Conceptual Captions, Visual Genome Dense Captions, ImageBERT 같은 vision-language pre-training dataset 은 인간 주석, semantic parsing, cleaning 및 balancing 잡기에 더 많은 작업 필요함. 이런 dataset scale 은 ~10M examples 수준에 불과하며 이는 vision domain 의 couterpars dataset 보다 한 자릿수 작고 NLP pre-training 을 위한 인터넷의 large-scale corpora text 보다 훨씬 작음.</p><hr><p>본 논문에서 1B noisy image 가 있는 image alt-text pairs 를 활용해 visual 및 vision-language representation learning 의 scale 을 확장.</p><ul><li>Conceptual Captions dataset 절차를 따라 large-scale noisy dataset 을 만듦. 그러나 (Fixing the train-test resolution discrepancy.) 에서 제안한 complex filtering 및 post-processing steps 을 적용하는 대신, simple frequency-based filtering 만 적용<ul><li>얻어진 데이터셋은 noisy 가 있지만 Conceptual Captions dataset 보다 두 자릿수 더 큼</li></ul></li><li>저자는 이 exascale dataset 으로 pre-training 된 visual 및 vision-language representation 이 다양한 task 에 매우 강력한 성능을 달성한다는 것을 보여줌</li></ul><hr><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-25-2081079b59e4bdd63751f45a3e19aaaf.png" width="1609" height="869" class="img_ev3q"></p><ul><li>모델 훈련을 위해 simple dual encoder 를 사용해 visual 및 language representation 을 shared latent embedding 에 align 하는 objective 사용</li><li>이와 유사한 objective 는 visual-semantic embeddings (VSE) learning 에 적용되어 옴<ul><li>저자는 이 모델을 ALIGN (<strong>A</strong> <strong>L</strong>arge-scale <strong>I</strong>ma<strong>G</strong>e and <strong>N</strong>oisy-text embedding)이라 명명</li></ul></li><li>image 및 text encoder 는 contrastive loss (normalized softmax 형태) 를 통해 학습되는데, 이는 matched image-text pair 의 embedding 을 가까이 당기고 non-matched image-text pair 는 멀리 밀어냄<ul><li>이는 self-supervised 및 supervised representation learning 에서 가장 효과적인 loss function 중 하나</li></ul></li><li>text 를 image 의 fine-grained labels 로 간주하면 image-to-text contrastive loss 는 기존 label-based classification objective 와 유사하며, 핵심 차이는 text encoder 가 &quot;label&quot; weights 를 생성한다는 것임</li></ul><hr><p>aligned image 및 text representation 은 cross-modality matching/retrieval task 에 적합하며, benchmark 에 SOTA 달성.</p><ul><li>ALIGN은 Flickr30K 및 MSCOCO 의 대부분의 zero-shot 및 fine-tuning R@1 metrics 에서 이전 SOTA 보다 7% 이상 우수한 성능을 보임</li><li>또한 이러한 cross-modality matching 은 text encoder 에 classnames 를 입력해 zero-shot image classification 을 가능케 하며, ImageNet 의 training samples 를 사용하지 않고도 76.4% 의 top-1 accuracy 달성</li><li>image representation 자체도 다양한 downstream visual tasks 에 우수한 성능을 보임<ul><li>예로, ALIGN 은 ImageNet 에서 88.64% 의 top-1 accuracy 달성</li><li>Figure 1 bottom 은 ALIGN 으로 구축된 real retrieval system 에서 가져온 cross-modality retrieval example 을 보여줌</li></ul></li></ul><h1>2. Related Work</h1><p>high-quality visual representation 을 classification 또는 retrieval 에 사용하기 위해 보통 large-scale labeled dataset 을 사용함. 최근엔 self-supervised 및 semi-supervised learning 이 대안으로 연구되고 있음</p><p>근데 이 방법으론 trained model 을 downstream task 로의 transferability 이 제안적임.</p><hr><p>image 및 natural language captions 의 활용은 learning visual representation 의 또 다른 방향이며 (Joulin et al. (2015); Li et al. (2017); Desai &amp; Johnson (2020); Sariyildiz et al. (2020); Zhang et al. (2020)) 은 image 로부터 caption 을 예측하여 우수한 visual representation 학습을 보여주며, 이는 저자에게 영감을 줌</p><p>그러나 이런 연구들은 Flickr 와 COCO Captions 같은 small dataset 에 국한되며, 결과적으로 cross-modality retrieval 같은 task 에 필요한 vision-language representation 을 생성하지 못함</p><hr><p>vision-language representation learning domain 에서 visual-semantic embeddings (VSE) 및 향상된 visions (예, obvject detectors, dense feature maps, 또는 multi-attention layer 를 활용) 을 (Socher et al., 2014; Karpathy et al., 2014; Kiros et al.; Nam et al., 2017; Li et al., 2019; Messina et al., 2020; Chen et al., 2020a) 에서 제안</p><p>최근엔, cross-modal attention layer (Liu et al., 2019a; Lu et al., 2019; Chen et al., 2020c; Huang et al., 2020b) 을 갖춘 진보된 모델들이 등장하며 image-text matching task 에서 우수한 성능을 보였다.</p><p>하지만 이 모델들은 속도가 매우 느려 real world 의 image-text retrieval system 에 실용적이지 못한 반면, 저자의 모델은 가장 단순한 VSE 형태를 유지하면서도 image-text matching benchmark 에서 이전의 모든 cross-attention models 를 능가한다.</p><hr><p>저자의 연구와 밀접한 CLIP 은 유사한 contrastive learning 에서 language supervision 을 통한 vision representation learning 을 제안한다. </p><p>다른 vision 및 language encoder architecture 를 사용하는 것 외에 주요 차이점은 training dataset 에 있다: ALIGN 은 raw alt-text data 의 natrual distribution 를 따르는 반면, CLIP 은 english wikipedia 의 high-frequency visual concepts 의 allowlist 로 구성하여 dataset 을 수집한다.</p><p>저자는 전문가의 지식이 필요하지 않은 dataset 으로도 강력한 visual 및 vision-language representation 을 학습할 수 있음을 보여줌</p><h1>3. A Large-Scale Noisy Image-Text Dataset</h1><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-26-28fae2dbbb1ca53e603bc9acbe8d180a.png" width="1028" height="745" class="img_ev3q"></p><p>연구 초점은 visual 및 vision-language representation learning 을 확장하는 것.</p><p>이를 위해 기존 보다 훨씬 큰 dataset 사용.</p><ul><li>구체적으로 Conceptual Captions dataset 을 구축하는 방법론을 따라 raw English alt-text data (image 및 alt-text pairs) 의 버전을 얻음</li><li>Conceptual Captions dataset 은 철저한 filtering 및 post-processing 을 통해 정리</li><li>저자의 확장은 원본 작업에서 대부분 cleaning  steps 로 완화하여 quality 를 trade 하고 scale 을 늘림<ul><li>minimal frequency-based filtering 적용</li><li>결과, 훨씬 더 크지만 (1.8B image-text pairs) 더 많은 noisy 가 포함</li><li>Fig. 2 는 dataset 에서 sample 한 image-text pairs 보여줌</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="image-based-filtering">Image-based filtering<a href="#image-based-filtering" class="hash-link" aria-label="Direct link to Image-based filtering" title="Direct link to Image-based filtering">​</a></h4><ul><li>Sharma et al. (2018) 을 따르며, pornographic images 를 제거하고 shorter dimension 의 길이가 200 pixels 보다 크고 가로세로 비율이 3 보다 작은 image 들만 유지.</li><li>1000 개 이상의  alt-text 가 연결된 image 는 제외</li><li>test images 에서 훈련되지 않도록 하기 위해, 모든 downstream evaluation datasets (e.g., ILSVRC-2012, Flickr30K, MSCOCO) 에서 test image 와 중복되거나 거의 중복인 image  제거</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="text-based-filtering">Text-based filtering<a href="#text-based-filtering" class="hash-link" aria-label="Direct link to Text-based filtering" title="Direct link to Text-based filtering">​</a></h4><ul><li>10 개 이상의 image 와 공유되는 alt-text 제외</li><li>이러한 alt-text 는 종종 image 의 내용과 관련없음 (e.g., &quot;1920x1080&quot;, &quot;alt img&quot;, &quot;cristina&quot;). 또한, dataset 내에 100M 의 가장 빈번한 unigrams 및 bigrams 에 속하지 않는 rare token 을 포함한 alt-text 와 너무 짧은(&lt; 3 unigrams) 또는 너무 긴(&gt; 20 unigrams) alt-text 도 제외</li><li>이 과정은 &quot;image tid 25&amp;id mggqpuweqdpd&amp;cache 0&amp;lan code 0&quot; 같은 noisy 가 많은 text 나 너무 일반적이어서 유용하지 않은 text 를 제거</li></ul><h1>4. Pre-training and Task Transfer</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-pre-training-on-noisy-image-text-pairs">4.1 Pre-training on Noisy Image-Text Pairs<a href="#41-pre-training-on-noisy-image-text-pairs" class="hash-link" aria-label="Direct link to 4.1 Pre-training on Noisy Image-Text Pairs" title="Direct link to 4.1 Pre-training on Noisy Image-Text Pairs">​</a></h2><p>ALIGN 은 dual-encoder architecture 사용하여 pre-training.</p><ul><li>모델는 image 및 text encoder pair 로 구성되며 top 에 cosine-similarity combination function 이 함께 있음</li><li>image encoder 는 global pooling (classification head 의 1x1 conv layer 는 학습하지 않음) 이 있는 EfficientNet 을 사용</li><li>text encoder 는 <!-- -->[CLS]<!-- --> token embedding 을 사용하는 BERT 사용 (training dataset 에서 생성된 100K wordpiece vocabulary 사용)<ul><li>BERT encoder top 에 image tower 의 dimension 과 일치시키기 위해 linear activation 을 가진 fully-connected layer 를 추가</li></ul></li><li>image 및 text encoder 는 scratch training 됨</li></ul><p>image 및 text encoder 는 normalized softmax loss (Zhai &amp; Wu, 2019)을 통해 optimizing</p><p>training 에서, matched image-text pairs 를 positive 로, training batch 에서 형성될 수 있는 other random image-text pairs 를 negative 로 간주</p><p>저자는 image-to-text classification 을 위해서 image-to-text classification 하나:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>L</mi><mrow><mi>i</mi><mn>2</mn><mi>t</mi></mrow></msub><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mi>i</mi><mi>N</mi></munderover><mi>log</mi><mo>⁡</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msubsup><mi>x</mi><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi>y</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><mi>σ</mi><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msubsup><mi>x</mi><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi>y</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><mi>σ</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} L_{i2t} = -\frac{1}{N} \sum^N_i \log \frac{\exp (x_i^\top y_i/\sigma)}{\sum^N_{j=1} \exp (x_i^\top y_i/\sigma)} \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.1354em;vertical-align:-1.3177em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8177em"><span style="top:-3.8177em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.1288em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8309em"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.0448em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.307em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3177em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8177em"><span style="top:-3.8177em"><span class="pstrut" style="height:3.8283em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3177em"><span></span></span></span></span></span></span></span></span></div><p>다른 하나는 text-to-image classification:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>L</mi><mrow><mi>t</mi><mn>2</mn><mi>i</mi></mrow></msub><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mi>i</mi><mi>N</mi></munderover><mi>log</mi><mo>⁡</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msubsup><mi>y</mi><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><mi>σ</mi><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msubsup><mi>y</mi><mi>i</mi><mi mathvariant="normal">⊤</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">/</mi><mi>σ</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} L_{t2i} = -\frac{1}{N} \sum^N_i \log \frac{\exp (y_i^\top x_i/\sigma)}{\sum^N_{j=1} \exp (y_i^\top x_i/\sigma)} \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.1354em;vertical-align:-1.3177em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8177em"><span style="top:-3.8177em"><span class="pstrut" style="height:3.8283em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em"><span style="top:-1.8723em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.05em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">lo<span style="margin-right:0.01389em">g</span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.1288em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8309em"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.0448em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">⊤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.307em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3177em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8177em"><span style="top:-3.8177em"><span class="pstrut" style="height:3.8283em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3177em"><span></span></span></span></span></span></span></span></span></div><p>두 losses 의 합을 minimizing</p><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 및 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 는 각각 image 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>-th 및 text 의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span>-th pair 의 normalized embedding</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> : batch size</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">σ</span></span></span></span></span> : logits scale 의 temperature </li></ul><p>in-batch negatives 가 더 효과적이도록 하기 위해, 모든 computing cores 의 embeddings 를 연결하여 훨씬 더 큰 batch 를 형성.</p><p>temperature variable 은 image 및 text embeddings 이 L2-normalize 되기 때문에 중요함.</p><p>optimal temperature 를 수동으로 탐색하는 대신, 저자는 모든 other parameters 와 함께 효과적으로 학습될 수 있음을 발견.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-transferring-to-image-text-matching--retrieval">4.2 Transferring to Image-Text Matching &amp; Retrieval<a href="#42-transferring-to-image-text-matching--retrieval" class="hash-link" aria-label="Direct link to 4.2 Transferring to Image-Text Matching &amp; Retrieval" title="Direct link to 4.2 Transferring to Image-Text Matching &amp; Retrieval">​</a></h2><p>저자는 ALIGN 을 image-to-text 및 text-to-image retrieval task 를 fine-tuning 없이 평가, 두 dataset betchmark : Flickr30K 및 MSCOCO</p><p>또한 MSCOCO 에 caption-caption, image-image 및 image-caption pairs 에 대한 human semantic similarity judgments 를 추가로 MSCOCO 의 확장인 Crisscrossed Captions (CxC) 도 평가</p><p>확장된 CxC 는 image-to-text, text-to-image, text-to-text 및 image-to-image retrieval, semantic textual similarity (STS), semantic image similarity (SIS) 및 semantic image-text similarity (SITS) 를 포함한 semantic similarity tasks 로 four intra- 및 inter-modal retrival task 포함</p><p>training set 은 original MSCOCO 와 동일하여, MSCOCO fine-tuned ALIGN model 을 CxC annotations 에 직접 평가 가능</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-transferring-to-visual-classification">4.3 Transferring to Visual Classification<a href="#43-transferring-to-visual-classification" class="hash-link" aria-label="Direct link to 4.3 Transferring to Visual Classification" title="Direct link to 4.3 Transferring to Visual Classification">​</a></h2><p>먼저 ALIGN 의 zero-shot transfer 을 ImageNet ILSVRC-2012 와 그 변형들에 대해 visual classification tasks 에 적용. </p><ul><li>변형에는 ImageNet-R (endition) (예술, 만화, 스케치와 같은 비자연적 image ), ImageNet-A (dversarial) (머신러닝 모델에 더 어려운 image ), ImageNet-V2 가 포함</li><li>이 변형들은 모두 동일한 (또는 부분적인) ImageNet 클래스를 따르지만, ImageNet-R 과 ImageNet-A 의 image 는 ImageNet 과는 매우 다른 distribution에서 샘플링</li></ul><p>또한 image encoder 를 downstream visual classification tasks 으로 transfer</p><ul><li>이를 위해 ImageNet 과 함께 Oxford Flowers-102, Oxford-IIIT Pets, Stanford Cars, Food101 와 같은 smaller fine-grained classification dataset 을 사용</li><li>ImageNet 의 경우, 두 가지 setting 에서 결과를 report : top classification layer 만 training (with frozen ALIGN image encoder) 과 full fine-tuning 설정<ul><li>fine-grained classification benchmark 에선 후자 설정만 report</li></ul></li><li>Kolesnikov et al. (2020) 을 따라, 저자는 Visual Task Adaptation Benchmark (VTAB) 에서 모델의 robustness 를 평가함<ul><li>VTAB 은 각각 1000 training samples 를 가진 19 diverse (자연, 특수 및 구조화된 image  classification task을 다루는) visual classification tasks 로 구성</li></ul></li></ul><h1>5. Experiments and Results</h1><p>ALIGN model 을 scratch training 을 위해, EfficientNet 의 open source implementation 을 image encoder 로, BERT 를 text encoder 로 사용.</p><p>분석 연구를 제외하고는, image encoder 로 EfficientNet-L2, text encoder 로 BERT-Large 를 사용하는 ALIGN 결과를 사용한다.</p><ul><li>모든 EfficientNet 변형에서 image encoder 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>289</mn><mo>×</mo><mn>289</mn></mrow><annotation encoding="application/x-tex">289 × 289</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">289</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">289</span></span></span></span></span> pixels resolution 로 학습된다. </li><li>input image 는 먼저 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>346</mn><mo>×</mo><mn>346</mn></mrow><annotation encoding="application/x-tex">346 × 346</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">346</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">346</span></span></span></span></span> resolution 로 resizing 한 후, training 중엔 random crop(additional random horizontal flip 포함)하고, evaluation 시엔 center crop 을 수행한다.</li><li>BERT 에선 최대 64 tokens 로 이루어진 wordpiece sequence 를 사용하며, input text 는 20 미안의 unigram 으로 구성된다.</li><li>softmax temperature value 는 1.0 으로 초기화되며(이 temperature value 는 image-to-text loss 와 text-to-image loss 간에 공유됨), softmax loss 에서 label smoothing parameter 로 0.1 을 사용한다.</li><li>저자는 LAMB optimization 와 weight decay ratio 1e-5 를 사용한다.</li><li>learning rate 는 10k step 에서 0 to 1e-3 까지 선형적으로 증가하며, 이후 1.2M step(약 12 epoch) 동안 선형적으로 0 으로 감소.</li><li>model 은 1024개 Cloud TPUv3 코어에서 16 positive paris 를 각 코어에서 학습한다. </li><li>따라서 총 유효 batch size 16384</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51--image-text-matching--retrieval">5.1.  Image-Text Matching &amp; Retrieval<a href="#51--image-text-matching--retrieval" class="hash-link" aria-label="Direct link to 5.1.  Image-Text Matching &amp; Retrieval" title="Direct link to 5.1.  Image-Text Matching &amp; Retrieval">​</a></h2><p>저자는 ALIGN 을 Flickr30K 와 MSCOCO cross-modal retrieval benchmark 에서 zero-shot 와 fully fine-tuning 설정에서 평가</p><p>(Karpathy &amp; Fei-Fei, 2015) 및 대부분의 기존 연구을 따라 train/test split 을 얻는다.</p><ul><li>Flickr30K 의 경우, standard 1K test set 에서 평가하고 30K training set 에서 fine-tuning</li><li>MSCOCO 의 경우, 5K test set 에서 평가하며, 82K training images 와 5K validation 또는 5K test set 에 포함되지 않은 30K additional validation images 에서 fine-tuining</li></ul><p>fine-tuning 중에도 동일한 loss function 을 사용한다. 그러나 batch size 가 training sample 수와 유사할 때 false negatives 가 발생할 수 있다.</p><ul><li>따라서 global batch size 를 16384 에서 2048로 줄인다.</li><li>또한 initial learning rate 를 1e-5 로 줄이고, Flickr30K 와 MSCOCO 에서 각각 3K 와 6K steps 동안 학습한다 (linear decay). </li><li>다른 hyper-parameters 는 pre-training 과 동일하게 유지한다.</li></ul><p><img loading="lazy" alt="Table 1" src="/assets/images/image-27-7f4c01303fc04d440f70ea16c195e2ba.png" width="1043" height="394" class="img_ev3q"></p><p>Tab. 1 은 ALIGN 이 이전 연구과 비교하여 Flickr30K 와 MSCOCO benchmark 의 all metrics 에서 SOTA 결과를 달성했음을 보여준다.</p><ul><li>zero-shot 설정에서 ALIGN 은 이전 SOTA 인 CLIP 대비 image retrieval task 에서 7% 이상의 개선을 보였다. </li><li>fine-tuning 을 통해 ALIGN 은 ImageBERT, UNITER, ERNIE-ViL, VILLA, Oscar 같은 더 복잡한 cross-modal attention layer 를 사용하는 방법을 포함한 모든 기존 방법을 큰 폭으로 능가했다.</li></ul><p><img loading="lazy" alt="Table 2" src="/assets/images/image-28-43d2a0ef5d6e1f21bbd4f7ef62444e52.png" width="1036" height="243" class="img_ev3q"></p><p>Tab. 2 는 Crisscrossed Captions (CxC) retrieval task 에서 ALIGN 의 성능을 보고한다.</p><ul><li>다시 한번 ALIGN 은 all metrics 에서 SOTA 결과를 달성했으며, 특히 image-to-text (+22.2% R@1) 및 text-to-image (+20.1% R@1) task 에서 큰 격차를 보였다.</li></ul><p><img loading="lazy" alt="Table 3" src="/assets/images/image-29-2768963c4b33167cbcddade303b6e118.png" width="513" height="283" class="img_ev3q"></p><p>Tab. 3 은 ALIGN 이 SITS task 에서도 5.7% 의 개선으로 이전 SOTA를 능가했음을 보여준다.</p><ul><li>흥미로운 관찰 중 하나는, ALIGN 이 inter-modal task 에서 훨씬 뛰어난 성능을 보였음에도 불구하고, intra-modal task 에서는 인상적이지 않다는 것이다.<ul><li>예로, text-to-text 및 image-to-image retrieval task(특히 전자)에서의 개선은 image-to-text 및 text-to-image task 에서의 개선에 비해 덜 두드러진다. </li><li>STS 및 SIS task 에서의 성능도 VSE++ 및 DEI2T 보다 약간 떨어진다. </li><li>저자는 ALIGN 의 training objective 가 intra-modal matching 대신 cross-modal matching (image-to-text matching)에 중점을 두기 때문이라고 추측한다.</li><li>Parekh et al.(2021) 은 multi-task learning 이 더 균형 잡힌 표현을 만들어낼 수 있다고 제안한다. (이는 향후 연구)</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-zero-shot-visual-classification">5.2. Zero-shot Visual Classification<a href="#52-zero-shot-visual-classification" class="hash-link" aria-label="Direct link to 5.2. Zero-shot Visual Classification" title="Direct link to 5.2. Zero-shot Visual Classification">​</a></h2><p>classname test 를 text encoder 에 직접 입력하면, ALIGN 은 image-to-text retrieval 을 통해 image 를 candidate class 에 분류할 수 있다.</p><p>Tab. 4 는 Imagenet 및 그 variants 에서 ALIGN 과 CLIP 을 비교한 것이다.</p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-30-5a9f74c4bf8a5c407e5ececf462994aa.png" width="522" height="159" class="img_ev3q"></p><ul><li>CLIP 과 유사하게, ALIGN 은 다양한 image  distribution 에서 classification task 에 대해 뛰어난 robustness 를 보여준다.</li><li>공정한 비교를 위해, CLIP 에서 정의한 prompt ensembling 방법을 동일하게 사용한다.</li><li>각 classname 은 &quot;A photo of a {classname}&quot; 와 같은 CLIP 에서 정의한 prompt template set 으로 확장된다.</li><li>class embedding 은 all template 의 embedding 을 평균한 후 L<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4511em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>-normalization 을 통해 계산된다.</li><li>이러한 ensembling 은 ImageNet 의 top-1 accuracy 를 2.9% 개선하는 것으로 나타났다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="53-visual-classification-w-image-encoder-only">5.3. Visual Classification w/ Image Encoder Only<a href="#53-visual-classification-w-image-encoder-only" class="hash-link" aria-label="Direct link to 5.3. Visual Classification w/ Image Encoder Only" title="Direct link to 5.3. Visual Classification w/ Image Encoder Only">​</a></h2><p>ImageNet benchmark 에서 저자는 learned visual features 를 frozen 상태로 classification head 만을 훈련시키고, 이후에 all layers 를 fine-tuning 했다.</p><ul><li>data augmentation 은 Szegedy et al. (2015)와 동일하게 random crop 및 horizontal flip 을 포함하며, 평가 시에는 비율 0.875 의 single center crop 을 적용한다.</li><li>Touvron et al. (2019)을 따라 train 및 eval 간 resolution 불일치를 완화하기 위해 0.8 scale ratio 를 사용한다. <ul><li>구체적으로, visual features 가 고정된 경우 train/eval resolution 는 289/360 이고, 모든 values 를 fine-tuning 할 때는 475/600 이다.</li></ul></li><li>training 의 두 stages 모두에서 global batch size 는 1024이며, momentum 0.9 를 가진 SGD optimization 를 사용하고, learningrate 는 30 epochs 마다 0.2 비율로 감소하며 총 100 epochs 동안 학습한다.</li><li>weight decay 는 0 으로 설정된다.</li><li>visual features 가 고정된 경우 initial learning rate 는 0.1 이다.</li><li>all layers 를 fine-tuning 할 때는 initial learning rate 를 0.01 로 설정하고, backbone network 에선 classification head 에 비해 10배 작은 learning rate 를 사용한다.</li></ul><p><img loading="lazy" alt="Table 5" src="/assets/images/image-31-551fe699c9e88d645a4662dbef124429.png" width="1035" height="277" class="img_ev3q"></p><p>Tab. 5 는 ImageNet benchmark 에서 ALIGN 과 이전 방법들을 비교한다. </p><ul><li>visual features 가 고정된 경우 ALIGN 은 CLIP 보다 약간 우수하여 85.5% 의 top-1 accuracy 로 SOTA 결과를 달성했다.</li><li>all layers 를 fine-tuning 한 후 ALIGN 은 BiT 와 ViT 보다 높은 정확도를 달성했으며, ImageNet training 과 large-scale unlabeled data 간의 깊은 상호작용이 필요한 Meta Pseudo Labels 만큼은 높지 않았다.</li><li>EfficientNet-L2 를 사용하는 NoisyStudent 및 Meta-Pseudo-Labels 와 비교하여 ALIGN 은 smaller test resolution (800 대신 600)를 사용하여 44% 의 FLOPS 를 절약했다.</li></ul><hr><p><img loading="lazy" alt="Table 6" src="/assets/images/image-32-772471f902e7914946db6fbb1faba508.png" width="520" height="135" class="img_ev3q"></p><ul><li>VTAB eval 에서는 (Zhai et al., 2019) 의 hyper-parameter sweep 을 따라 각 task 에 대해 50 trials.</li><li>각 task 는 800 image 로 학습되고, hyper-parameter 는 200 image 로 구성된 validation set 을 사용하여 선택된다. </li><li>sweep 후 선택된 hyper-parameter 는 각 task 에 대해 1000 images 로 구성된 train/eval set 을 결합한 데이터에서 학습하는 데 사용된다.</li><li>Tab. 6 은 3 fine-tuning runs 에서 표준 편차를 포함한 평균 정확도(각 subgroups 의 세부 결과 포함)를 보고하며, ALIGN 이 비슷한 hyper-parameter 선택 방법을 적용한 BiT-L 을 능가했음을 보여준다.</li></ul><hr><p>smaller fine-grained classification benchmark 를 평가하기 위해, 저자는 all tasks 에 대해 간단한 fine-tuning 전략을 채택했다.</p><ul><li>ImageNet fine-tuning 과 동일한 data augmentationb 및 optimizer 를 사용한다.</li><li>마찬가지로, 먼저 classification head 를 학습한 다음, batch norm statistics 는 frozen 상태로 all layers 를 fine-tuning 한다.</li><li>train/eval resolution 는 289/360 으로 고정된다.</li><li>batch size 는 256 이고 weight decay 는 1e-5 이다. </li><li>initial learning rate 는 각각 1e-2 와 1e-3 으로 설정되며, 20k steps 에서 cosine learning rate decay 를 사용한다.</li></ul><p><img loading="lazy" alt="Table 7" src="/assets/images/image-33-9110b71330028c3f286a84ef703ad0f1.png" width="516" height="263" class="img_ev3q"></p><p>Tab. 7 은 ALIGN 과 BiT-L 및 all tasks 에 대해 동일한 fine-tuning hyper-parameters 를 적용한 SAM 을 비교한다.</p><ul><li>이러한 small tasks 에선 fine-tuning 의 세부 사항이 중요한다. 따라서 optimization 알고리즘을 사용하지 않고 SAM optimization 을 사용하지 않은 상태의 기준 결과를 (Foret et al., 2021) 에 나열하여 공정한 비교를 한다.</li><li>저자의 결과(average 3 runs)는 optimization 알고리즘에 대한 세부 조정 없이 SOTA 결과와 비슷하다.</li></ul><h1>6. Ablation Stduy</h1><p>ablation study 에선 MSCOCO zero-shot retrieval 및 ImageNet K-Nearest-neighbor (KNN) tasks 에서 모델 성능을 주로 비교한다.</p><p>이 two metrics 가 대표적이며, 위 섹션에서 보고된 other metrics 와 잘 상관관계가 있음을 발견했다. </p><p>언급되지 않은 경우, analysis factor 외의 hyper-parameter 는 기본 모델과 동일하게 유지된다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="61-model-architectures">6.1. Model Architectures<a href="#61-model-architectures" class="hash-link" aria-label="Direct link to 6.1. Model Architectures" title="Direct link to 6.1. Model Architectures">​</a></h2><p>저자는 먼저 다른 image 와 text backbone 을 사용하는 ALIGN model 의 성능을 연구한다. </p><p>image encoder 로 EfficientNet 을 B1에서 L2 까지, text encoder 로 BERT-Mini 에서 BERT-Large 까지 학습시킨다.</p><p>B1, B3, B5 및 L2 의 globally-pooled features 에 추가적인 linear activation 이 있는 fully-connected layer 를 추가하여 B7 의 output dimension (640)을 맞춘다.</p><p>all text encoder 에도 유사한 linear layer 를 추가한다. </p><p>similar linear layer 를 절약하기 위해 분석에서는 training steps 를 1M 으로 줄인다.</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-34-a23324e1c6b9d3962bd4783a240d7155.png" width="1417" height="413" class="img_ev3q"></p><p>Fig. 3 은 MSCOCO zero-shot retrieval 및 ImageNet KNN 결과를 다양한 image 및 text backbone combinations 를 보여준다.</p><ul><li>model quality 는 larger backbone 에서 확실히 개선되지만, BERT-Base 에서 BERT-Large 까지의 ImageNet KNN metric 은 EfficientNet-B7 및 EfficientNet-L2 와 함께 포화 상태에 이르렀다.</li><li>예상대로 image encoder 용량을 확장하는 것이 vision task 에 더 중요하다(e.g., BERT-Mini text tower 를 사용하더라도 L2 는 BERT-Large 와 함께 B7 보다 더 나은 성능을 보인다).</li><li>image-to-text retrieval task 에서는 image 및 text encoder 용량이 동일하게 중요하다.</li><li>Fig. 3 에 나타난 nice scaling property 를 기반으로, Sec. 5 에서 보고된 대로 EfficientNet-L2 + BERT-Large 를 사용하여 모델을 fine-tuning 한다.</li></ul><hr><p>이후 embedding dimension, batch 내 random negatives 및 softmax temperature 같은 주요 아키텍처 hyper-parameter 를 연구한다.</p><p><img loading="lazy" alt="Table 8" src="/assets/images/image-35-b4e9da342ba22d5185cfa8f1fd0579da.png" width="797" height="492" class="img_ev3q"></p><p>Tab. 8 은 BERT-Base text encoder, embedding dimension 640, batch 내 random negatives 및 trainable softmax temperature 를 사용하여 학습된 EfficientNet-B5 image encoder 를 baseline model (first row)과 여러 model variants 를 비교한다.</p><ul><li>Tab. 8 의 Rows 2~4 는 higher embedding dimensions 가 성능을 개선한다는 것을 보여준다. 따라서 저자는 diemsion 이 더 큰 EfficientNet backbone(L2 사용 시 1376)으로 scaling 한다</li><li>Tab. 8 의 Rows 5, 6 은 softmax loss 에서 batch 내 random negatives 수를 줄이는 것이(50% 및 25%) 성능을 저하시킨다는 것을 보여준다.</li><li>Tab. 8 의 Rows 7~9 은 softmax loss 의 temperature parameter 의 영향을 연구한다.<ul><li>temperature parameter 를 학습하는 baseline model (약 1/64 로 수렴됨)과 비교하여, 일부 hand-selected, fixed temperature 가 약간 더 나을 수 있다.</li><li>그러나 learnable temperature 를 사용하는 것이 경쟁력 있고 학습을 더 쉽게 만들어주기 때문에 이 방법을 선택한다.</li><li>또한 temperature 는 보통 첫 100k step 에서 converged value 의 약 1.2배로 빠르게 감소한 후 학습이 끝날 때까지 서서히 수렴한다는 것을 확인했다.</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="62-pre-training-datasets">6.2. Pre-training Datasets<a href="#62-pre-training-datasets" class="hash-link" aria-label="Direct link to 6.2. Pre-training Datasets" title="Direct link to 6.2. Pre-training Datasets">​</a></h2><p>모델이 다양한 크기의 dataset 으로 훈련되었을 때 어떻게 성능이 달라지는지를 이해하는 것도 중요하다.</p><ul><li>이를 위해, 저자는 두 가지 모델(EfficientNet-B7 + BERT-base와 EfficientNet-B3 + BERT-mini)을 3 (full ALIGN training data, randomly sampled 10% 의 ALIGN training data, 그리고 Conceptual Captions(CC-3M, 약 3M images))으로 훈련시켰다.</li><li>CC-3M 은 samller dataset 이기 때문에, 모델을 default steps 의 1/10 로 훈련한다.</li><li>모든 모델은 scratch training 한다.</li></ul><p><img loading="lazy" alt="Table 9" src="/assets/images/image-36-a4e3640bf0757e808e4c7ee77088413e.png" width="787" height="396" class="img_ev3q"></p><p>Tab. 9 에서 볼 수 있듯이, </p><ul><li>large-scale training set 은 모델을 확장하고 더 나은 성능을 달성하는 데 필수적이다.<ul><li>예로, ALIGN data 로 훈련된 모델은 CC-3M data 로 훈련된 모델보다 명확히 우수하다.</li><li>CC-3M 에선 B7+BERT-base 가 overfit 되기 시작하여 B3+BERT-mini 보다도 성능이 떨어진다.</li></ul></li><li>반대로, larger dataset 을 완전히 활용하려면 더 큰 모델이 필요하다.<ul><li>small B3+BERT-mini model 은 ALIGN data 의 10% 에서 거의 포화 상태에 도달하는 반면, larger B7+BERT-base model 은 full ALIGN data 를 사용할 때 명확히 개선된다.</li></ul></li></ul><hr><p><img loading="lazy" alt="Table 10" src="/assets/images/image-37-fbc8a2d1c938858c61b8a0807fbd10bb.png" width="766" height="293" class="img_ev3q"></p><p>data size scaling 이 increased noise 를 이겨내는 방식을 더 잘 이해하기 위해, 저자는 ALIGN training data 3M, 6M, 12M 을 randomly sampling 하고, 이들을 B7+BERT-base model 에서 CC-3M data 와 비교했다. </p><ul><li>Tab. 10 은 ALIGN data 가 동일한 크기(3M)로 CC data 보다 훨씬 열악하게 수행되지만, 6M 및 12M ALIGN data 로 훈련된 모델의 품질이 빠르게 따라잡는다는 것을 보여준다.</li><li>noise 가 있음에도 불구하고, ALIGN data 는 CC data 보다 4배 큰 크기만으로 이를 능가한다.</li></ul><h1>7. Analysis of Learned Embeddings</h1><p>저자는 ALIGN 으로 훈련된 embedding 의 동작을 연구하기 위해 간단한 image retrieval system 을 구축했다.</p><p>시연 목적으로, training set 과는 별도로 160M CC-BY 라이선스 이미지를 포함하는 인덱스를 사용했다.</p><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-38-8666c1873567f1b1c12fe96e9b7f4d9e.png" width="810" height="707" class="img_ev3q"></p><p>Fig. 4 는 training data 에 존재하지 않는 일부 text query 에 대한 text-to-image retrieval 의 top-1 결과를 보여준다.</p><ul><li>ALIGN 은 장면의 세부 묘사나 랜드마크 및 예술 작품과 같은 세밀한 개념을 정확하게 검색할 수 있다.</li><li>이 예들은 ALIGN 모델이 similar semantic 을 가진 image 및 text 를 align 할 수 있으며, 복잡한 새로운 개념에 generalizing 할 수 있음을 보여줍니다.</li></ul><hr><p>이전에 word2vec 는 word vector 가 문장과 단락에서 인접 단어를 예측하도록 훈련되면서 word vector 간 linear relationships 가 형성된다고 보여줬다.</p><p>저자는 ALIGN 에서도 image 와 text embedding 간의 linear relationships 가 형성된다는 것을 보여준다.</p><p>image+text query 로 image retrieval 를 수행했다. 구체적으로, query image 와 text string 이 주어진 상태에서 그들의 ALIGN embedding 을 더하고 이를 사용해 관련 이미지를 검색한다.</p><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-39-312a29ac0572bc6d4878b1d71c47f722.png" width="572" height="944" class="img_ev3q"></p><p>Fig. 5 는 다양한 iomage+text query 에 대한 결과를 보여준다.</p><ul><li>이 예들은 ALIGN embedding 이 vision 및 language domain 간의 뛰어난 조합성을 보여줄 뿐만 아니라, text 또는 image query 만으로는 어렵던 &quot;multi-modal query retrieval&quot; 의 새로운 패러다임이 가능함을 보여준다.</li><li>예로, 이제는 &quot;“Australia”&quot; 또는 &quot;Madagascar&quot; 와 같은 팬더를 찾거나 검은 신발 쌍을 &quot;beige&quot; 같은 색으로 동일하게 보이도록 바꿀 수 있다.</li><li>마지막으로 Fig. 5 의 마지막 3 row 에서 보여지듯, embedding space 에서 빼기를 수행하여 장면에서 objects/attributes 를 제거하는 것도 가능하다.</li></ul><h1>8. Multilingual ALIGN Model</h1><p>ALIGN 의 한 가지 장점은 모델이 simple filter 를 사용해 noisy web image text data 로 훈련된다는 점이며, filter 중 어느 것도 특정 언어에 한정되지 않는다는 점이다.</p><p>이에 따라, conceptual caption data processing pipeline 의 언어 제한을 해제하여 dataset 을 multilingual (100+languages) 로 확장하고, english dataset (1.8B image-text pair)과 크기를 일치시켰다.</p><p>이 데이터로 multilingual model ALIGNmling 을 훈련시켰다.</p><p>all languages 를 포괄하기 위해 크기 250k 의 new multilingual wordpiece vocabulary 를 생성했다.</p><hr><p>저자는 multilingual image text retrieval dataset 인 Multi30k 에서 multilingual model 을 test 했다.</p><p>이 데이터셋은 Flickr30K 를 독일어(de), 프랑스어(fr), 체코어(cs) 로 확장한 것이다.</p><p>dataset 은 영어와 독일어로 이미지당 5 captions, 프랑스어와 체코어로는 이미지당 1 caption 이 포함된 31,783 images 로 구성되어 있다.</p><p>train/eval/test 분할은 Young et al. (2014) 에 정의되어 있다. 저자는 ALIGN 의 zero-shot model 성능을 평가하고 이를 M3P 및 UC2 와 비교했다.</p><p>evaluation metric 은 image-to-text retrieval 및 text-to-image retrieval task 에서 Recall@1, Recall@5, Recall@10 의 평균 점수를 계산하는 평균 리콜(mR)이다.</p><p><img loading="lazy" alt="Table 11" src="/assets/images/image-40-51fe712e8d93b7eb2320eabf0bab4fe4.png" width="716" height="368" class="img_ev3q"></p><ul><li>Tab. 11 은 ALIGNmling 의 zero-shot 성능이 모든 언어에서 M3P 를 큰 차이로 능가하며, 프랑스어에서 가장 큰 57.8 점의 절대 mR 개선을 보여준다.</li><li>ALIGNmling 의 zero-shot 성능은 체코어를 제외하고, fine-tuned M3P 및 UC2 와 비교할 만한 수준이다.</li><li>영어에서는 ALIGNmling 이 EN 전용 데이터로 훈련된 ALIGNEN 보다 약간 낮은 성능을 보인다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="9-결론">9. 결론<a href="#9-결론" class="hash-link" aria-label="Direct link to 9. 결론" title="Direct link to 9. 결론">​</a></h3><p>저자는 large-scale noisy image-text data 를 활용하여 vision 및 vision-language representation learning 을 확장하는 간단한 방법을 제시한다.</p><p>이 방법은 data curation 및 annotation task 를 크게 줄이고, minimal frequency-based cleaning 만을 필요로 한다.</p><p>이 dataset 에서 저자는 contrastive loss 을 사용하여 간단한 dual-encoder model 을 훈련시켰다.</p><p>결과적으로, ALIGN model 은 cross-modal retrieval 이 가능하며 SOTA VSE 및 cross-attention vision-language model 을 크게 능가한다.</p><p>vision-only downstream task 에서 ALIGN 은 large-scale labeled data 를 사용해 SOTA 모델과도 비교 가능하거나 이를 능가하는 성능을 보인다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">Vision-Language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/align">ALIGN</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/contrastive-learning">contrastive learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/zero-shot-transfer">zero-shot transfer</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/Contrastive Learning/2021-02-ALIGN.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/Prismer"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Prismer: A Vision-Language Model with An Esemble of Experts</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/Contrastive Learning/CLIP"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Learning Transferable Visual Models From Natural Language Supervision</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#41-pre-training-on-noisy-image-text-pairs" class="table-of-contents__link toc-highlight">4.1 Pre-training on Noisy Image-Text Pairs</a></li><li><a href="#42-transferring-to-image-text-matching--retrieval" class="table-of-contents__link toc-highlight">4.2 Transferring to Image-Text Matching &amp; Retrieval</a></li><li><a href="#43-transferring-to-visual-classification" class="table-of-contents__link toc-highlight">4.3 Transferring to Visual Classification</a></li><li><a href="#51--image-text-matching--retrieval" class="table-of-contents__link toc-highlight">5.1.  Image-Text Matching &amp; Retrieval</a></li><li><a href="#52-zero-shot-visual-classification" class="table-of-contents__link toc-highlight">5.2. Zero-shot Visual Classification</a></li><li><a href="#53-visual-classification-w-image-encoder-only" class="table-of-contents__link toc-highlight">5.3. Visual Classification w/ Image Encoder Only</a></li><li><a href="#61-model-architectures" class="table-of-contents__link toc-highlight">6.1. Model Architectures</a></li><li><a href="#62-pre-training-datasets" class="table-of-contents__link toc-highlight">6.2. Pre-training Datasets</a><ul><li><a href="#9-결론" class="table-of-contents__link toc-highlight">9. 결론</a></li></ul></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.331f850c.js"></script>
<script src="/assets/js/main.2ba55cd9.js"></script>
</body>
</html>