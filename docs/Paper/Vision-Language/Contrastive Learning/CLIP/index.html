<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/Contrastive Learning/2021-03-CLIP">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Learning Transferable Visual Models From Natural Language Supervision | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://whdnjsdyd111.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://whdnjsdyd111.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://whdnjsdyd111.github.io/docs/Paper/Vision-Language/Contrastive Learning/CLIP"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Learning Transferable Visual Models From Natural Language Supervision | My Site"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://whdnjsdyd111.github.io/docs/Paper/Vision-Language/Contrastive Learning/CLIP"><link data-rh="true" rel="alternate" href="https://whdnjsdyd111.github.io/docs/Paper/Vision-Language/Contrastive Learning/CLIP" hreflang="en"><link data-rh="true" rel="alternate" href="https://whdnjsdyd111.github.io/docs/Paper/Vision-Language/Contrastive Learning/CLIP" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.d2ad26d0.css">
<link rel="preload" href="/assets/js/runtime~main.082cf41b.js" as="script">
<link rel="preload" href="/assets/js/main.26b4fb6b.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/whdnjsdyd111/whdnjsdyd111.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Unsupervised Prompt Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Prismer">Prismer: A Vision-Language Model with An Esemble of Experts</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/Contrastive Learning/ALIGN">Contrastive Learning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Contrastive Learning/ALIGN">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/Contrastive Learning/CLIP">Learning Transferable Visual Models From Natural Language Supervision</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Composition/CLIP-LoRA">PEFT</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Single-Stream/VLBERT">Single-Stream</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Two-Stream/LXMERT">Two-Stream</a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Contrastive Learning</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Learning Transferable Visual Models From Natural Language Supervision</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Learning Transferable Visual Models From Natural Language Supervision</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2103.00020.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2103.00020.pdf</a></p><h1>Abstract</h1><p>SOTA vision system 은 fixed predetermined object categories set 을 예측하도록 훈련된다. 이는 label 확장이 필요하여 일반성과 사용성이 제한된다.</p><ul><li>저자는 simple pre-training task 인 caption 과 image 의 관련성을 예측하는 것이 400M dataset (image, text) 에서 SOTA image representation 을 효과적이고 확장 가능한 방식으로 scratch learning 함을 보여줌</li><li>pre-training 후 natural language 로 learned visual concepts (or describtion new one) 을 참조하거나 model 을 downstream task 로 zero-shot transfer</li><li>이 방법으로 OCR, action recognition video, geo-localization 및 여러 유형의 fine-grained object classification 등 30개 이상의 vision dataset 에 연구</li><li>비교적 대부분의 task 에 쉽게 transfer 되며, 특정 dataset 은 훈련 없이 fully supervised baseline 과 경쟁력 있다.<ul><li>예로, ImageNet zero-shot 에서 기존 ResNet-50 의 accuracy 와 일치하지만, ResNet-50 이 훈련한 1.28M training example 이 필요가 없음</li></ul></li></ul><h1>1. Introduction and Motivating Work</h1><ul><li>BERT, GPT 등이 NLP 에서 raw text 를 직접 학습하는 pre-training 으로 성과를 이루었다. </li><li>auto-regressive 및 masked language modeling 같은 task-agnostic objective 는 모델 및 데이터에 여러 단계로 확장하여 꾸준히 향상시켰다.</li><li>T5, GPT 는 input-output interface 개발로 task-agnostic architecture 가 downstream task 에 zero-shot transfer</li><li>GPT-3 는 specific training data 필요 없이도 경쟁력을 가짐</li></ul><p>이는 web-scale 의 pre-training 이 제공하는 aggregate supervision 이 고품질의 crowd-labeled NLP dataset 보다 뛰어남을 시사. 하지만 vision 분야는 ImageNet 같은 crowd-labeled dataset 이 표준적이며, web-scale 에서 직접 학습하는 확장 가능한 pre-training 이 vision 에서도 가져올지 의문.</p><ul><li>Mori et al. (1999) : image-text pair 의 document 에서 명사와 형용사를 예측하여 컨텐츠 기반 이미지 검색을 개선</li><li>Quattoni et al. (2007) : image caption 에서 단어를 예측하는 classifier 의 weight space 에서 manifold learning 을 통해 더 효율적인 data representation 을 학습할 수 있다는 것을 입증 </li><li>Srivastava &amp; Salakhutdinov (2012) : low-level image 및 text tag 기능 위에 multimodal Deep Boltzmann 을 훈련시킴으로써 deep representation learning 탐구</li><li>Joulin et al. (2016) : 이러한 연구를 현대화하고 image caption 에서 단어를 예측하는 데 훈련된 CNN 이 유용한 image representation 을 학습 <ul><li>YFCC100M dataset(Thomee et al., 2016)의 이미지 제목, 설명 및 해시태그 메타데이터를 bag-of-word multi-label classification task 로 변환</li></ul></li><li>AlexNet : 이러한 label 을 예측하도록 pre-training 하여 이러한 label 로 표현된 것이 ImageNet-based pre-training on trasfer tasks</li><li>Li et al. (2017) : 이 접근 방식을 확장하여 개별 단어 외에 구문 n-gram 도 예측하고 visual n-grams 및 highest score 하나를 예측하는 것으로 다른 image classification dataset 으로의 zero-shot system 의 능력을 입증</li></ul><p>VirTex, ICMLM 및 ConVIRT 같은 최신 architecture 및 pre-training approach 를 채택하여, language modeling, masked language modeling 및 contrastive objective 를 사용해 text 로부터 image representation 을 잠재력을 시연</p><p>이런 image prepresentation learning 을 위해 natural supervision 사용이 드문데, 성능이 대안법보다 훨씬 낮기 때문</p><ul><li>Li et al (2017) : zero-shot setting 에서 ImageNet 정확도가 11.5% 에 불과<ul><li>이는 88.4% 정확도 (당시 SOTA)보다 훨씬 낮음</li><li>고전적인 접근법의 50% 보다도 낮음</li><li>대신, 범위를 좁게 설정하지만 well-targeted weak supervision 사용이 성능을 향상 </li></ul></li><li>Mahajan et al (2018) : Instagram image 에서 ImageNet 관련 hashtag 예측이 효과적인 pre-training task 임을 보여줌<ul><li>pre-training model 이 ImageNet 에 맞게 조정면 정확도가 5% 이상 향상</li></ul></li><li>Kolesnikov et al (2019), Dosovitskiy et al (2020) : noisy 가 있는 JFT-300M dataset 의 class 예측하기 위해 모델을 pre-training 하여 보다 넓은 범위의 transfer benchmark 에서 큰 이득</li></ul><p>이런 연구는 제한된 양의 supervision &quot;gold-labels&quot; 로 학습하는 것과 실제로 제한되지 않은 raw text 로부터 학습하는 것 사이의 중간 지점이지만, 타협되지 않음</p><ul><li>두 방식 모두 supervision 을 1000 및 18291 class 로 각각 설계하고 제한</li><li>natural language 는 일반성을 통해 훨씬 넓은 visual concepts 를 표현하고, 따라서 supervise 가능</li><li>두 방식 모두 static softmax classifier 를 사용하여 예측하며, dynamic output mechanism 이 없음</li><li>이는 그들의 유연성을 심각하게 제한하고 &quot;zero-shot&quot; 기능을 제한</li></ul><p>이러한 weakly supervised models 과 최근의 자연어에서 image representation 을 학습하는 탐구의 주요 차이점은 규모다. </p><ul><li>Mahajan 등 (2018), Kolesnikov 등 (2019) : 1M 에서 1B images 에 대해 accelerator years 동안 모델을 훈련시켰지만, VirTex, ICMLM 및 ConVIRT 는 accelerator days 에 해당하는 시간에 1 ~ 200K images 이미지에 대해 훈련</li><li>이 연구는 large-scale natural language spervision 하에 훈련된 image classifier 의 행동을 연구하여 이 차이를 해소</li></ul><p>web 의 public large-scale dataset 으로  우리는 400M dataset (image, text) 을 생성하고, 이를 <strong>CLIP</strong> (<strong>C</strong>ontrastive <strong>L</strong>anguage-<strong>I</strong>mage <strong>P</strong>re-training)라는 ConVIRT 의 단순화된 버전을 scratch learning 하여 natural supervision 으로 학습하는 효율적인 방법임을 입증</p><ul><li>CLIP 의 확장 가능성을 연구하고 transfer 성능이 계산량의 smoothly predicable function 임을 관찰</li><li>CLIP 이 GPT 와 유사하게 pre-training 중 OCR, geo-localization, action recognition 등 많은 task 를 수행하는 것 발견</li><li>30 dataset 에 대한 zero-shot transfer 성능을 평가하여 이전 연구인 task-specific supervised models 와 경쟁력이 있음을 확인</li><li>또한 linear-probe representation 분석으로 결과를 확인하고, CLIP 이 ImageNet 모델들보다 우수한 성능을 보여주면서도 계산적으로 효율적임을 보여줌</li><li>zero-shot CLIP 이 동등한 정확도의 supervision ImageNet 모델들보다 훨씬 더 robust 함을 발견하며, 이는 task-agnostic model 의 zero-shot 평가가 모델의 능력을 훨씬 더 잘 대표한다는 것을 시사</li></ul><h1>2. Approach</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-natural-language-supervision">2.1 Natural Language Supervision<a href="#21-natural-language-supervision" class="hash-link" aria-label="Direct link to 2.1 Natural Language Supervision" title="Direct link to 2.1 Natural Language Supervision">​</a></h2><p>접근법의 핵심은 <em>natural language supervision</em> 으로 perception learning 이다.</p><ul><li>새로운 아이디어는 아니며, Zhang et al (2020), Gomez et al (2017), Joulin et al (2016) 및 Desai &amp; Johnson (2020) 는 image 와 text 에서 visual representation 을 학습하는 법을 소개하며, 각각 이들 방법을 unsupervision, self-supervision, weakly supervised 및 supervised 라 설명</li><li>natural language 를 training signals 로 인식하는 것이 위 연구 라인의 공통점이라는 점을 강조</li><li>이런 방식은 natural language supervision 으로부터 학습하며, 이는 다른 방법에 비해 여러 장점 존재<ul><li>image classification 을 위한 crowd-labeled 보다 훨씬 쉬운데, &quot;gold-label&quot; 이 필요하지 않기 때문</li><li>web 의 large-scale text 에 포함된 supervision 을 수동으로 학습</li><li>unsupervised 또는 self-supervised learning 과 비교하여 장점이 있는데, 단훈히 representation learning 이 아니라 representation 을 language 에 연결하여 유연한 zero-shot learning 가능</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-creating-a-sufficiently-large-dataset">2.2 Creating a Sufficiently Large Dataset<a href="#22-creating-a-sufficiently-large-dataset" class="hash-link" aria-label="Direct link to 2.2 Creating a Sufficiently Large Dataset" title="Direct link to 2.2 Creating a Sufficiently Large Dataset">​</a></h2><p>기존 연구는 3 dataset: MS-COCO, Visual Genome 및 YFCC100M 를 사용</p><ul><li>MS-COCO 및 Visual Genome 은 현대 기준에 비해 작으며 각각 약 100,000 training image 존재</li><li>다른 vision system 은 최대 3.5B 인스타그램 사진에서 훈련(Mahajan et al, 2018)</li><li>YFCC100M 은 100M images 로 대안이 될 수 있지만, 각 image 의 metadata 는 희소하며 품질이 다름</li></ul><hr><p>natural language supervision 동기 중 하나는 web large-scale dataset 이 공개 사용 가능하다는 것.</p><ul><li>기존 dataset 은 이러한 가능성을 반영하지 않아 잠재력을 과소 평가할 수 있음. 이에 대응해 web public large-scale source 400M pairs (image, text) dataset 을 구축</li><li>visual concepts 포괄을 위해 500,000 queries 중 하나를 포함하는 pairs (image, text)를 구성하는 것이 목표</li><li>query 당 20,000 pairs (image, text) 을 포함하여 대략적으로 class balance 맞춤</li><li>GPT-2 훈련에 사용된 WebText dataset 과 유사한 total word count</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-selecting-an-efficient-pre-training-method">2.3 Selecting an Efficient Pre-Training Method<a href="#23-selecting-an-efficient-pre-training-method" class="hash-link" aria-label="Direct link to 2.3 Selecting an Efficient Pre-Training Method" title="Direct link to 2.3 Selecting an Efficient Pre-Training Method">​</a></h2><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-c5d2bfb5cec159dfdb40fe20931f1a06.png" width="2472" height="1159" class="img_ev3q"></p><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-1-9aa315ca64ac315d667d1ca9d4235f94.png" width="1431" height="1490" class="img_ev3q"></p><p>natural language 에서 visual concepts learning 은 어려울 수 있지만, 저자는 training efficiency 이 natural language supervision 의 성공적인 확장에 필수적임을 발견하여, 이 metric 을 final pre-training method 로 선택</p><ul><li>초기 방식은 VirText 와 유사하게 image CNN 과 text transformer 를 함께 scratch training 하여 image caption 을 예측하는 것<ul><li>이를 효율적으로 확장하는데 어려움이 있음</li><li>Fig. 2 처럼 ResNet-50 image encoder 의 2배 계산을 사용하는 63M parameter transformer language model 을 사용하여, ImageNet class 인식에 simpler baseline 인 bag-of-words encodings 보다 3배 느리게 학습</li></ul></li></ul><hr><ul><li>image 에 대한 contrastive representation learning 최신 연구에서 contrastive objectives 가 동일한 predictive objective 들보다 representations 을 더 잘 학습함을 발견</li><li>다른 연구에선 image generative model 이 high quality image representation 을 학습할 순 있지만 동일한 성능을 내려면 contrastive model 보다 더 많은 계산이 필요함을 발견</li></ul><p>이를 고려해, text 의 exact word 를 예측하는 대신, 어느 text 가 어느 image 와 pair 를 이루는지만 예측하는 더욱 쉬운 proxy task 를 해결하는 시스템으로 훈련하는 것을 연구.</p><p>Fig. 2 같이 baseline 을 시작으로 contrastive object 로 predictive object 를 바꾸고, ImageNet 으로의 zero-shot trasnfer rate 에서 4x efficiency 향상을 관찰</p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-2-0cba2524b21bb91781d4e525b7c0857b.png" width="1431" height="1490" class="img_ev3q"></p><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> (image, text) pair batch 가 주어지면, batch 전역에서 발생한 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N \times N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> possible (image, text) pair 중 어떤 것인지 예측하도록 훈련</li><li>이를 위해, batch 내의 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> real pairs 의 image 및 text embeddings 의 cosine similarity 를 maximizing 하고 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>−</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N^2 - N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span> incorrect pairs 의 embeddings 의 cosine similarity 는 minimizing 하여 multi-modal embedding space 를 학습</li><li>이러한 similarity scores 에 대해 symmetric cross entropy loss 를 optimizing</li><li>Fig. 3 은 CLIP 의 핵심 구현의 의사 코드를 포함</li><li>이러한 batch construction 및 objective 는 <em>multi-class <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span></span>-pair loss</em> 같은 deep metric learning Sohn (2016) 에서 도입</li><li>Oord et al (2018)에 의해 contrastive representation learning 이 인기를 얻었으며, 최근에는 Zhang et al (2020)의 medical imaging domain 에서 contrastive (text, image) representation learning 으로 적용</li></ul><hr><ul><li>pre-training dataset 이 매우 커서 overfitting 은 고려사항이 아니며, training 세부 사항은 Zhang et al (2020)의 구현과 비교하여 단순화</li><li>저자는 image encoder 를 ImageNet weight 로 초기화하거나 text encoder 를 pre-trained weight 로 초기화하지 않고 CLIP 을 scratch training</li><li>representation 과 contrastive embedding space 간의 non-linear projections 을 사용하지 않음<ul><li>이는 Bachman et al (2019)에 의해 도입되었고 Chen et al (2020b)에 의해 보급</li><li>저자는 각 encoder 의 representation 을 multi-modal embedding space 로 mapping 하기 위해 linear projection 만 사용</li><li>두 버전 간에 training efficiency 차이를 관찰하지 못했으며 non-linear projection 이 self-supervised representation learning 에서 현재 이미지의 세부 사항과 함께 상호 적응될 수 있음을 추측</li></ul></li><li>또한 많은 (image, text) pair 가 CLIP 의 pre-training dataset 에서 single sentence 일 때 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">t_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 가 샘플링하는 텍스트에서 균일하게 single sentence 를 사용하므로 Zhang et al (2020)의 텍스트 변형 기능을 제거</li><li>또한 image transformation function <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">t_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 를 단순화</li><li>조정된 이미지의 random square crop 이 training 중에 사용되는 유일한 data augmentation</li><li>마지막으로, softmax 의 logits 범위를 제어하는 temperature parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span></span></span></span></span> 는 hyper-parameter 로서 변환되지 않도록 직접 training 중에 optimizing</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="24-choosing-and-scaling-a-model">2.4 Choosing and Scaling a Model<a href="#24-choosing-and-scaling-a-model" class="hash-link" aria-label="Direct link to 2.4 Choosing and Scaling a Model" title="Direct link to 2.4 Choosing and Scaling a Model">​</a></h2><p>image encoder 는 두 architecture 를 고려</p><ol><li>ResNet-50<ul><li>He et al (2019) 의 ResNetD 개선과 Zhang (2019) 의 antialiased rect-2 blur pooling 으로, 기존 버전을 여러 가지 수정</li><li>global average pooling layer 를 attention pooling mechanism 으로 대체</li><li>attention pooling 은 image 의 global average-pooled representation 에 conditioning 되는 query 로 구현</li></ul></li><li>Vision Transformer (ViT)<ul><li>조금 다른 초기화를 사용하여 combined patch 및 position embeddings 이전에 추가적인 layer normalization 을 제외하고 이들의 구현을 따름</li><li></li></ul></li></ol><p>text encoder 는 GPT 같이 수정된 Transformer</p><ul><li>63M parameter 12-layer 512-wide model with 8 attention heads 사용</li><li>Transformer 는 text 의 byte pair encoding (BPE) representation 에서 작동하며 49,152 vocab size 가짐</li><li>computational efficiency 를 위해 max sequence length 76 으로 제한</li><li>text sequence 는 <!-- -->[SOS]<!-- --> 및 <!-- -->[EOS]<!-- --> token 으로 표시되며, transformer 의 highest layer 의 activation 은 <!-- -->[EOS]<!-- --> token 에서 text representation 으로 취급되며, layer normalizaing 되고, 이후 multi-modal embedding space 로 linear projection</li><li>text encoder 에선 masked self-attention 을 사용하여 pre-trained language model 을 초기화하거나 language modeling 을 auxiliary objective 로 추가할 수 있도록 보존</li></ul><hr><p>이전 vision 연구에서 model width or depth 만 증가 시키지만, ResNet image encoder 의 경우 Tan &amp; Le (2019) 의 방식 채택</p><ul><li>이는 model width, depth 및 resolusion 모두에 추가 계산을 할당하는 것이 model dimension 중 하나에만 할당하는 것보다 우수한 것을 발견</li><li>Tan &amp; Le (2019) : EfficientNet 을 위해 각 차원에 할당된 계산 비율을 조정</li><li>저자는 추가 계산을 model width, depth 및 resolusion 모두 증가시킨 것만 baseline 으로 사용</li></ul><p>text encoder 의 경우, text encoder 의 용량에 대한 CLIP 의 성능이 덜 민감한 것을 발견하여 model width 만 증가시키고 depth 는 전혀 증가시키지 않음</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="25-training">2.5 Training<a href="#25-training" class="hash-link" aria-label="Direct link to 2.5 Training" title="Direct link to 2.5 Training">​</a></h2><p>5 ResNet 및 3 ViT 훈련</p><ul><li>ResNet 의 경우, ResNet-50, ResNet-101 을 훈련하고 EfficientNet 스타일의 model scaling 을 따르는 3 개의 model 추가 훈련</li><li>각각 ResNet-50의 약 4x, 16x, 64x 계산 사용<ul><li>이들 각각 RN50x4, RN50x16, RN50x64 로 표시</li></ul></li><li>Vision Transformer 의 경우 ViT-B/32, ViT-B/16, ViT-L/14 를 훈련</li><li>모든 모델을 32 epoch 동안 훈련</li><li>Adam optimizer 를 사용하고 모든 가중치에 대해 적용되는 decoupled weight decay regularization 를 사용하여 learning rate 를 cosine schedule 에 따라 감소</li><li>initial hyper-parameter 는 ResNet50 모델을 1 epoch 동안 훈련할 때 greedy search, random search 및 manual tuning  의 조합을 사용하여 설정</li><li>계산 제약으로 인해 hyper-parameter 는 이후 더 큰 모델에 대해 휴리스틱하게 조정</li><li>learnable temperature parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span></span></span></span></span> 는 Wu et al, 2018 와 동등한 값인 0.07 로 초기화</li><li>training instability 방지를 위해, logits 100배 이상 확장하지 않도록 clipping</li><li>very large minibatch size 32,768 사용</li><li>Mixed-precision 를 사용하여 training 을 가속화하고 메모리 절약</li><li>추가 메모리 절약을 위해, gradient checkpointing, half-precision Adam statistics 및 half-precision stochastically rounded text encoder weights 사용</li><li>embedding similarity 계산은 각 GPU 가 local batch 에 필요한 pairwise similarities subset 만을 계산하도록 sharding</li><li>largest ResNet RN50x64 는 592 V100 GPU 에서 18 days training, largest Vision Transformer 는 256 V100 GPU 에서 12 days training</li><li>ViT-L/14 의 경우 성능 향상을 위해, 추가로 336 pixel resolution 에서 한 epoch 동안 pre-training<ul><li>이 모델을 ViT-L/14@336px 로 표시</li></ul></li><li>그 외의 경우 &quot;CLIP&quot; 으로 보고된 모든 결과는 가장 우수한 성능을 보여준 이 모델을 사용</li></ul><h1>3. Experiments</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-zero-shot-transfer">3.1 Zero-Shot Transfer<a href="#31-zero-shot-transfer" class="hash-link" aria-label="Direct link to 3.1 Zero-Shot Transfer" title="Direct link to 3.1 Zero-Shot Transfer">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="311-motivation">3.1.1 Motivation<a href="#311-motivation" class="hash-link" aria-label="Direct link to 3.1.1 Motivation" title="Direct link to 3.1.1 Motivation">​</a></h3><p>vision 에서의 zero-shot learning 은 unseen object 에 대한 generalizing 연구를 의미.</p><p>저자는 더 넓은 의미로 unseen dataset 으로의 generalizing 연구를 한다.</p><ul><li>Larochelle et al. (2008) 의 zero-data learning 논문에서 추구하는 것처럼 unseen task 수행을 위한 대리로써의 역할</li><li>unsupervised learning 의 많은 연구는 <em>representation learning</em> 능력에 중점을 두지만, 저자는 zero-shot transfer 을 연구함으로써 <em>task-learning</em> 능력을 측정하는 것을 동기부여</li><li>이 관점에서 dataset 은 specific distribution 의 성능을 평가<ul><li>하지만 많은 vision dataset 은 image classification 을 지원하기 위해 연구 공동체에 의해 주로 만들어졌음</li><li>SVHN : Google Street View 사진의 분포에서 street number task 를 측정</li><li>CIFAR-10 :어떤 &quot;real&quot; task 를 측정하는지는 명확하지 않지만, 추출된 분포는 명확 - TinyImages</li></ul></li><li>이런 dataset 에서 zero-shot transfer 은 CLIP 의 distribution shift 및 domain generalization 에 대한 robustness 평가가 task generalization 보다 더 중요</li></ul><p>저자는 zero-shot transfer 을 NLP 의 task learning 에 영감을 받음</p><ul><li>language model 로 wikipedia document 를 생성하는 학습을 진행할 때 이름을 다른 언어로 번역하는 능력을 갖추게 되는 &quot;unexpected side-effect&quot; 으로 task learning 을 처음 식별</li><li>GPT-1 은 supervised fine-tuning 개선을 위한 transfer learning 을 집중했지만, 아무런 supervised adaption 없이도 네 가지 heuristic zer-shot transfer methods 의 성능이 지속적으로 향상되는 것을 보여주는 연구를 포함한 실험을 수행</li><li>이 분석은 task learning 을 전적으로 연구하는 데 초점을 맞춘 GPT-2 의 기초</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="312-using-clip-for-zero-shot-transfer">3.1.2 Using CLIP For Zero-Shot Transfer<a href="#312-using-clip-for-zero-shot-transfer" class="hash-link" aria-label="Direct link to 3.1.2 Using CLIP For Zero-Shot Transfer" title="Direct link to 3.1.2 Using CLIP For Zero-Shot Transfer">​</a></h3><p>CLIP 은 dataset 에서 image 와 text snippet 이 함께 연결되어 있는지 예측하도록 pre-training</p><ul><li>zero-shot classification 수행을 위해, 위 능력을 재사용</li><li>각 dataset 의 모든 class name 을 potential text pair set 으로 사용하고 CLIP 에 따라 most probable (image, text) pair 를 예측 (e.g. &quot;A photo of a {class}.&quot;)<ul><li>먼저 각 image 와 possible texts 의 feature embedding 을 해당 encoder 로 계산</li><li>이러한 embedding 의 cosine similarity 는 temperature parameter <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span></span></span></span></span> 로 scaling 되고 softmax 를 통해 probability distribution 으로 normalizing</li><li>이 prediction layer 는 L2-normalized inputs, L2-normalized weight, no bias 및 temperature scaling 인 logistic regression  classifier</li></ul></li><li>이렇게 해석하면 image encoder 는 image 의 feature representation 을 계산하는 vision backbone 이고, text encoder 는 class 가 나타내는 visual concepts 를 지정하는 text 에 기반한 linear classifier 의 weight 를 생성하는 hypernetwork</li><li>이 해석을 계속 적용하면 CLIP pre-training 의 모든 단계를 포함하여 무작위로 생성된 proxy 의 성능을 최적화하고 한 예제당 클래스를 포함하고 natural language descriptions 을 통해 정의된 총 32,768 total classes 를 포함하는 vision dataset 에 대한 CLIP 의 성능을 볼 수 있음</li><li>zero-shot 평가는 text-encoder 에 의해 계산된 zero-shot classifier 를 캐싱하고 이를 dataset 의 모든 subsequent predictions 에 재사용합. 이를 통해 생성 비용을 dataset 의 모든 예측에 걸쳐 분할 가능</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="313-initial-comparison-to-visual-n-grams">3.1.3 Initial Comparison to Visual N-Grams<a href="#313-initial-comparison-to-visual-n-grams" class="hash-link" aria-label="Direct link to 3.1.3 Initial Comparison to Visual N-Grams" title="Direct link to 3.1.3 Initial Comparison to Visual N-Grams">​</a></h3><p>Tab. 1 에서 Visual N-Grams 와 CLIP 비교</p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-3-8473fb72de6bb23de7f70746ec422475.png" width="1360" height="704" class="img_ev3q"></p><ul><li>best CLIP 은 ImageNet 의 proof concept 의 11.5% to 76.2% 로 accuracy 향상<ul><li>1.28M crowd-labeled training examples 를 사용하지 않음에도 original ResNet-50 의 성능과 일치</li></ul></li><li>CLIP 의 top-5 accuracy 는 top-1 accuracy 보다 뚜렷하게 높으며, 95% top-5 accuracy 를 가지며 Inception-V4 와 일치<ul><li>강력한 fully supervised baselines 을 zero-shot setting 에서 일치시키는 능력은 CLIP 가 유연하고 실용적인 zero-shot vision classifier 로의 중요한 발전임을 시사</li></ul></li><li>Visual N-Grams 와의 비교는 CLIP 의 contextualizing 하기 위한 것이며, CLIP 와 Visual N-Grams 사이의 직접적인 비교로 해석되어서는 안됨<ul><li>이 두 시스템 사이의 많은 성능 관련 차이점이 제어되지 않았기 때문</li><li>예로, 저자는 10x larger dataset 에서 훈련하고, 예측 당 거의 100x computing 필요로하는 vision model 을 사용했으며, 아도 training computing 량은 Visual N-Grams 의 1000x 이상일 것</li><li>그리고 Visual N-Grams 가 발표될 때는 존재하지 않았던 Transformer 기반 모델을 사용</li></ul></li><li>가까운 비교로, Visual N-Grams 가 훈련된 YFCC100M dataset 에서 CLIP ResNet-50 을 훈련시키고 이 모델이 보고된 ImageNet 성능과 일치하는 것을 발견<ul><li>이 기준 성능은 Visual N-Grams 처럼 pre-trained ImageNet weight 로 초기화된 것이 아니라 scratch training</li></ul></li><li>CLIP 은 다른 2 reported dataset 에서도 Visual N-Grams 를 능가<ul><li>Yahoo 에서 CLIP 는 error number 를 95% 줄이고, SUN 에선 Visual N-Grams 의 accuracy 두 배 이상 늘림</li></ul></li><li>더 포괄적인 분석과 스트레스 테스트를 수행을 위해, 30 dataset 을 포함하고, 50 vision systems 와 비교</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="314-prompt-engineering-and-ensembling">3.1.4 Prompt Engineering And Ensembling<a href="#314-prompt-engineering-and-ensembling" class="hash-link" aria-label="Direct link to 3.1.4 Prompt Engineering And Ensembling" title="Direct link to 3.1.4 Prompt Engineering And Ensembling">​</a></h3><p>대부분의 standard image classification dataset 은 natural language based zero-shot transfer 을 가능케하는 class name 을 지정하거나 설명하는 정보를 후속처리한다.</p><ul><li>대부분 image 를 numeric id 만으로 annotation 처리하고 이 id 를 english name 으로 다시 매핑하는 파일 포함<ul><li>Flowers102 와 GTSRB 등은 zero-shot transfer 을 완전히 방지하여 이런 매핑이 없다.</li><li>많은 데이터셋에서 이러한 label 은 다소 무작위로 선택되어 zero-shot transfer 과 관련된 문제를 예상하지 않음</li></ul></li></ul><hr><p><img loading="lazy" alt="Table 2" src="/assets/images/image-19-e79ec4abf991b3e6eaad32870661873a.png" width="883" height="633" class="img_ev3q"></p><ul><li>일반적인 문제는 다의성 (polysemy) 이다. class name 이 CLIP 의 text encoder 에 제공된 유일한 정보일 때, context 부재로 어떤 단어 의미를 지칭하는지 구분할 수 없음<ul><li>경우에 따라 동일한 단어의 여러 의미가 동일한 데이터셋의 서로 다른 클래스로 포함될 수 있음</li><li>이는 건설 크레인과 비행하는 크레인을 모두 포함하는 ImageNet 에서 발생</li><li>다른 예로는 Oxford-IIIT Pet 의 클래스. 문맥에서 명백하게 개종의 품종을 지칭하지만, 문맥이 없는 text encoder 에게는 개종이라는 단어가 그냥 종류의 스포츠 선수일 수도 있음</li></ul></li><li>다른 문제는 pre-training dataset 에서 image 와 함께 연결된 text 가 일반적으로 single word 가 아니라는 것<ul><li>일반적으로, text 는 어떤 방식으로든 이미지를 설명하는 전체 문장. 이 distribution gap 을 줄이기 위해 &quot;A photo of a {label}&quot; 같은 prompt template 를 사용하는 것이 이미지의 내용에 대한 text 임을 명시하는 good default 임을 발견</li><li>이렇게 하면 label text 만 사용하는 baseline 보다 ImageNet 의 정확도가 1.3% 향상</li></ul></li><li>&quot;prompt engineering&quot; 에 대해서는 GPT3 처럼 각 task 에 대한 prompt text 를 사용자 정의하여 zero-shot 성능을 크게 향상시킬 수 있다는 것을 발견<ul><li>image classification dataset 에서 카테고리 지정이 도움이 됨<ul><li>예로, Oxford-IIIT Pets 에서 &quot;A photo of a {label}, a type of pet.&quot; 을 사용하여 context 를 제공하는 것이 잘 작동</li><li>Food101 에선 음식 유형을 지정하고, FGVC Aircraft 에선 항공기 유형을 지정하는 것이 도움</li><li>OCR 에선 텍스트나 숫자 주위에 따옴표를 넣는 것이 성능을 향상</li><li>마지막으로, 위성 이미지 분류 데이터셋에서 이미지가 이러한 형식의 것임을 명시하는 것이 도움이 되었으며 &quot;a satellite photo of a {label}&quot; 의 변형을 사용</li></ul></li></ul></li><li>또한 성능을 향상시키는 다른 방법으로 여러 zero-shot classifier 를 ensemble 을 시도<ul><li>이러한 classfier 는 &quot;A photo of a big {label}&quot; 과 &quot;A photo of a small {label}&quot; 같은 다른 context prompt 를 사용하여 계산</li><li>probability space 가 아닌 embedding space 에서 ensemble 구성<ul><li>이를 통해 computing cost 를 많은 예측에 걸쳐 분할할 때 앙상블의 계산 비용이 single classifier 사용하는 것과 동일해짐</li></ul></li><li>저자는 many generated zero-shot classifier 를 거친 앙상블을 통해 신뢰할 수 있는 성능 향상을 관찰했으며 대부분의 데이터셋에 대해 이를 사용<ul><li>ImageNet 에선 80 different context prompts 를 사용하여 앙상블을 구성하며, 위 설명인 single default prompt 보다 3.5% 추가 성능 향상</li></ul></li><li>종합적으로, 프롬프트 엔지니어링과 앙상블링은 ImageNet 의 정확도를 거의 5% 향상</li><li>Fig. 4 에서 프롬프트 엔지니어링과 앙상블링이 컨텍스트 없는 baseline 에 비해 CLIP 모델의 일련의 성능을 어떻게 변화시키는지를 시각화</li></ul></li></ul><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-4-3ffd600ad134bd9a4eec40d45b8dbe12.png" width="1407" height="1781" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="315-analysis-of-zero-shot-clip-performance">3.1.5 Analysis of Zero-Shot CLIP Performance<a href="#315-analysis-of-zero-shot-clip-performance" class="hash-link" aria-label="Direct link to 3.1.5 Analysis of Zero-Shot CLIP Performance" title="Direct link to 3.1.5 Analysis of Zero-Shot CLIP Performance">​</a></h3><p>CLIP zero-shot classifier 의 여러 특성을 연구</p><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-5-df879b2b006e48fa9bbb4c87976fbf86.png" width="1407" height="1781" class="img_ev3q"></p><ol><li>zero-shot classifier 의 성능을 살펴봄<ul><li>simple off-the-shelf baseline: ResNet-50 의 feature 에 fitting fully supervised, regularized, logistic regression classifier 에 교차 검증</li><li>Fig. 5 에서 27 dataset 에서 비교</li><li>Zero-shot CLIP 이 baseline 보다 더 자주 성능이 우수. 16 dataset 에서 우승</li><li>individual datasets 를 살펴보면 몇 가지 흥미로운 동작이 관찰<ul><li>fine-grained classification tasks 에서 성능에 큰 차이가 있음을 알 수 있음</li><li>Stanford Cars 및 Food101 에서 zero-shot CLIP 은 ResNet-50 특성의 logisitc regression 보다 20% 이상 우수한 반면, 다른 2 dataset 인 Flowers102 및 FGVCAircraft 에선 10% 이상 성능이 떨어짐</li><li>OxfordPets 및 Birdsnap 에선 성능은 훨씬 비슷</li><li>이런 차이는 WIT 와 ImageNet 간의 다양한 수준의 task supervision 이 주된 이유로 추측</li><li>&quot;general&quot; opbject classification dataset 인 ImageNet, CIFAR10/100, STL10 및 PascalVOC2007 의 성능은 모두 비교적 유사하며 모든 경우에서 zero-shot CLIP 에 약간의 이점이 있음</li></ul></li><li>STL10 에선 CLIP 은 전체적으로 99.3% 정확도를 달성하여 훈련 예제를 사용하지 않은 SOTA 수준인 것으로 보임</li><li>zero-shot CLIP 는 비디오에서 행동 인식을 측정하는 2 dataset 에서 ResNet-50 보다 높은 성능을 보임</li><li>Kinetics700 에서 CLIP 는 ResNet-50 보다 14.5% 우수한 성능, UCF101 에서 ResNet-50 보다 7.7% 우수한 성능<ul><li>이는 명사 중심의 object supervision 과 비교하여 동사를 포함하는 visual concept 자연어가 넓은 supervision 을 제공하므로 추정</li></ul></li><li>zero-shot CLIP 가 뚜렷하게 성능이 떨어지는 영역을 살펴보자<ul><li>zero-shot CLIP 가 위성 이미지 분류 (EuroSAT 및 RESISC45), 림프 결절 종양 감지 (PatchCamelyon), 합성 장면에서 물체 수 계산 (CLEVRCounts), 자율 주행 관련 작업 (GTSRB), 가장 가까운 자동차까지의 거리 인식 (KITTI Distance)과 같이 특화된, 복잡한 또는 추상적인 task 에 상당히 약함</li><li>이 결과는 더 복잡한 작업에 대한 zero-shot CLIP 의 능력이 부족함을 강조</li><li>대조적으로, non-expert humans 는 몇 가지 작업, 예를 들어 계산, 위성 이미지 분류 및 교통 신호 인식과 같은 작업을 강력하게 수행할 수 있으므로 향후 개선의 많은 여지가 있다는 것을 시사</li><li>그러나 우리는 거의 모든 인간 (및 가능성 있는 CLIP)에게 사전 경험이 없는 림프 결절 종양 분류와 같은 어려운 task 에 대한 zero-shot transfer 을 측정하는 것이 의미가 있는지 여부는 명확하지 않음</li></ul></li></ul></li></ol><p><img loading="lazy" alt="Figure 6" src="/assets/images/image-6-ec4fda5046c5b294c24f168e3e3889b3.png" width="1407" height="1851" class="img_ev3q"></p><ol start="2"><li>1 에서 zero-shot 을 fully supervised model 과 비교하는 한편, 2 에선 few-shot 비교<ul><li>Fig. 6 에서 zero-shot CLIP 가 best ImageNet model, self-supervised learning 및 CLIP 자체를 포함한 여러 image model 의 feature 에 대한 few-shot logistic regression 과 비교한 결과를 시각화</li><li>zero-shot 이 1-shot 보다 성능이 떨어질 것으로 직관적으로 기대되지만, 실제로 4-shot logistic regression 성능과 일치</li><li>이는 zero-shot 과 few-shot 방식 사이의 주요 차이 때문<ul><li>먼저, CLIP 의 zero-shot classifier 는 visual concepts 를 직접 지정할 수 있는(““communicated””) natural language 를 통해 생성</li><li>반면, &quot;normal&quot; supervised learning 은 training example 에서 visual concepts 를 간접적으로 추론해야 함</li><li>context-less example-based learning 은 많은 다른 가설이 데이터와 일관될 수 있으며, 특히 1-shot 의 경우는 더 그럼</li><li>single image 는 종종 많은 다른 visual concept 를 포함</li><li>capable learner 는 visual cues 와 heuristics 를 활용할 수 있지만, 이미지에서 시연되는 개념이 주요 객체임을 가정하는 등의 보장은 없음</li></ul></li><li>이런 zero-shot 과 few-shot 성능의 차이를 해소하는 잠재적인 방법은 few-shot classifier 의 weights 에 대한 CLIP 의 zero-shot classifier 를 사전 정보로 사용하는 것<ul><li>직접적인 구현으로는 생성된 가중치에 대한 L2 penalty 를 추가하는 것이지만, 이 regularizer 값이 너무 큰 경우 few-shot classifier 는 zero-shot classifier 가 될 수 있다는 것을 발견</li><li>다른 모델의 특성에서 zero-shot CLIP 과 few-shot logistic regression 을 비교할 때, CLIP 는 최고의 성능을 발휘하는 16-shot classifier 와 거의 같은 성능을 보임</li></ul></li></ul></li></ol><p><img loading="lazy" alt="Figure 7" src="/assets/images/image-7-89c736b73d7ee9043c143cec86c68c6c.png" width="1407" height="1783" class="img_ev3q"></p><ol start="3"><li>개별 데이터셋에서의 성능도 조사<ul><li>Fig. 7 에서 zero-shot CLIP 성능을 맞추기 위해 동일한 feature space 에서 logistic regression classifier 가 필요로하는 클래스 당 labeled example 수의 추정치를 보여줌</li><li>zero-shot CLIP 도 linear classifier 이므로, 이런 setting 에서 zero-shot transfer 의 효율성을 추정<ul><li>수천 개의 linear classifier training 을 피하기 위해 1, 2, 4, 8, 16-shot, 그리고 각 데이터셋에서 훈련된 fully supervised linear classifier 의 성능을 log-linear interpolation 기반으로 추정</li><li>zero-shot transfer 는 dataset 마다 매우 다양한 효율성을 나타낼 수 있으며, class 당 1 labeled example 부터 184 까지 범위가 있음<ul><li>Flowers102 및 EuroSAT 은 1-shot model 을 보다 뒤쳐짐</li><li>dataset 절반은 class 당 less 5 example 이 필요하며, 평균은 5.4</li><li>그러나 mean estimated data efficiency 는 class 당 평균 20.8 example</li><li>이는 supervised classifier 가 많은 labeled example 을 필요로하는 데이터셋의 20% 때문</li></ul></li><li>ImageNet 에선 zero-shot CLIP 가 16-shot linear classifier trained on same feature space 와 일치</li></ul></li></ul></li></ol><p><img loading="lazy" alt="Figure 8" src="/assets/images/image-8-038af04854a718622cc353b9958293dd.png" width="1472" height="1779" class="img_ev3q"></p><ol start="4"><li>evaluate dataset 이 linear classifier parameter 로 잘 추정된다 가정하면, CLIP 의 zero-shot classifier 도 linear classifier 성능으로 setting 하고 있으므로, fully supervised classifier 성능은 zero-shot transfer 이 달성 가능한 상한을 대략 설정한다.<ul><li>이에 Fig. 8 에서 CLIP 의 zero-shot 성능을 dataset 별로 fully supervised linear classifier 와 비교</li><li>dosh line <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y = x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> 은 &quot;optimial&quot; zero-shot classifier 을 나타냄</li><li>대부분의 dataset 에서 zero-shot classifier 의 성능은 여전히 fully supervised classifier 보다 10% ~ 25% 정도 떨어지므로, CLIP 의 task-learning 및 zero-shot transfer 능력을 개선할 여지가 많이 남아 있다는 것을 시사</li><li>zero-shot 성능과 fully supervised 성능 사이에는 0.82 의 positive correlation 이 있으며 (p-value &lt; <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">10^{-6}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">6</span></span></span></span></span></span></span></span></span></span></span></span></span>), 이는 CLIP 가 underlying representation 과 task-learning 을 zero-shot transfer 에 상대적으로 일관되게 연결하는 경향이 있음을 시사</li><li>그러나 zero-shot CLIP 은 5 dataset 에서만 fully supervised 성능에 접근: STL10, CIFAR10, Food101, OxfordPets 및 Caltech101<ul><li>이 dataset 에서 zero-shot accuracy 와 fully supervised accuracy 모두 90% 이상</li><li>이는 CLIP 가 underlying representation 이 high quality task 에 대해 zero-shot transfer 이 더 효과적일 수 있다는 것을 시사</li><li>zero-shot 성능의 증가에 대한 linear regression model 의 slope 을 계산하면, fully supervised 성능을 기준으로 zero-shot 성능이 1.28% 개선</li><li>그러나 95th-percentile confidence intervals 는 1 미만의 값도 포함(0.93-1.79)</li></ul></li></ul></li></ol><p><img loading="lazy" alt="Figure 9" src="/assets/images/image-9-1c44db1086557d2fa26cec83c21ce558.png" width="1514" height="1561" class="img_ev3q"></p><ol><li>몇 년간 dataset 크기와 양에 대한 성능 예측이 가능. GPT 계열 모델은 지금까지 training computing cost 1000x 증가에 따라 zero-shot 성능이 일관되게 향상<ul><li>Fig. 9 에서 CLIP 의 zero-shot 성능이 비슷한 비율로 증가하는지 확인</li><li>39 evaluation 을 걸쳐 36 dataset 에서 5개의 ResNet CLIP 모델의 average error rate 를 plot 하고, CLIP 이 44x 증가하는 model computing 에 대해 비슷한 log-log linear scaling 경향이 유지되는 것을 발견</li><li>전반적인 경향은 부드럽지만, 개별 평가의 성능은 훨씬 더 불안정<ul><li>이는 individual training runs on sub-tasks 간의 high variance 에 의한 것인지 꾸준히 향상되는 경향을 가려주는 것인지 또는 성능이 실제로 일부 task 에서 computing function 으로서 non-monotonic 일 수 있음</li></ul></li></ul></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-representation-learning">3.2 Representation Learning<a href="#32-representation-learning" class="hash-link" aria-label="Direct link to 3.2 Representation Learning" title="Direct link to 3.2 Representation Learning">​</a></h2><p>zero-shot 분석을 했지만 representation 능력 연구가 더 일반적이다.</p><p>&quot;ideal&quot; representation 이 가져야 할 속성에 대한 의견이 다르며, representation 의 quality 를 평가하는 많은 방법이 존재</p><ul><li>representation 을 linear classification 에 fitting 하고 다양한 dataset 에 성능을 측정하는 것이 일반적인 접근</li><li>대안으로 모델의 end-to-end fine-tuning 성능을 측정<ul><li>이는 유연성을 높이며, 이전 연구에서 fine-tuning 이 대부분의 image classification dataset 에서 linear classification 보다 우수한 성능을 보여줌</li></ul></li></ul><p>fine-tuning 의 높은 성능은 실용적이지만, 저자는 linear classifier 기반 평가.</p><p>저자는 high-performing task 및 dataset 에 중립적인 pre-trainng 방식에 중점을 둔다.</p><p>fine-tuning 은 각 dataset 에 representation 을 적응시켜 일반적이고 견고한 prepresentation 을 학습에 실패할 수 있는 한편, linear classifier 는 유연성이 제한되어 이런 실패를 개발 중에 피드백 가능</p><ul><li>CLIP 의 경우 supervised linear classifier training 이 zero-shot classifier 에 사용된 방법과 매우 유사하기 때문에 광범위한 비교와 분석 가능</li><li>마지막으로, 많은 task 에 걸쳐 CLIP 를 다양한 baseline model 과 비교하기를 목표</li><li>27 dataset 에서 66 models 를 연구하는 것은 1782가지 다른 평가를 조율하는 것을 요구</li><li>fine-tuning 의 largeer design 및 hyperparameter 로서 공정한 평가가 어려운 반면, linear classifier 는 최소한의 hyperparameter tuning 만 필요하여 표준화된 구현과 평가가 있음</li></ul><p><img loading="lazy" alt="Figure 10" src="/assets/images/image-10-86bc480e4e24a1fb0ceb861a15e237c3.png" width="2044" height="1590" class="img_ev3q"></p><p>Fig. 10 은 저자의 발견이며, confirmation 또는 reporting bias 의 우려를 최소화하기 위해 12 dataset 에서 성능 연구</p><ul><li>ResNet-50 및 ResNet-101 같은 small CLIP 은 다른 ResNet (BiT-S 및 original) trained on ImageNet-1K 보다 우수한 성능을 발휘하지만, ResNet (BiT-S) trained on ImageNet-21K (BiT-M) 보다는 성능이 낮다<ul><li>이런 small CLIP 은 유사한 계산 요구 사항을 갖는 EfficientNet family 보다도 성능이 낮다</li><li>그러나 CLIP 로 훈련된 모델은 매우 잘 확장되며, 우리가 훈련한 가장 큰 모델 (ResNet-50x64)은 총 점수 및 컴퓨팅 효율성 측면에서 최상의 기존 모델 (Noisy Student EfficientNet-L2)을 약간 능가</li><li>또한, CLIP vision transformer 는 CLIP ResNet 보다 약 3배 더 컴퓨팅 효율적이며, 이는 우리의 컴퓨팅 예산 내에서 더 높은 전반적인 성능을 달성</li><li>best model 은 1 additional epoch 동안 dataset 을 336 pixels fine-tuned ViT-L/14 이다. </li><li>이 모델은 이 evaluation suite 전반에 걸쳐 기존 모델의 평균적인 성능을 2.6% 향상</li></ul></li><li>Fig. 21 에서 품질적으로 보면, CLIP 은 이전에 random initialization 에서 end-to-end trained single vision model 에서 보여진 것보다 더 넓은 task set 을 학습<ul><li>이런 task 에는 지리적 위치, 광학 문자 인식, 얼굴 감정 인식, 동작 인식이 포함<ul><li>이는 Kornblith et al. (2019)의 연구가 ImageNet 과 겹치는 task 으로 편향되었다고 주장하여, 이를 해결하기 위해 우리는 더 넓은 27 dataset evaluation suite 에서 성능 측정</li></ul></li><li>넓은 evaluation suite 에서 CLIP 의 장점이 더욱 명확<ul><li>규모에 관계없이 모든 CLIP 은 컴퓨팅 효율성 측면에서 모든 평가를 능가</li><li>best model 의 평균 점수 향상은 이전 시스템보다 2.6% 에서 5% 로 증가</li><li>또한, self-supervised system 이 저자의 evaluation suite 에서 뚜렷하게 더 잘 수행<ul><li>예를 들어, Kornblith et al. (2019)의 12 dataset 에서 여전히 SimCLRv2 가 평균적으로 BiT-M 보다 성능이 낮지만, SimCLRv2 는 저자의 27 dataset evaluation suite 에서 BiT-M 을 능가</li><li>이러한 결과는 &quot;general&quot; 성능을 더 잘 이해하기 위해 task 다양성과 범위를 계속 확대하는 것이 유용할 것으로 보임</li><li>저자는 VTAB 와 유사한 추가적인 평가 노력이 가치 있다고 의심</li></ul></li></ul></li></ul></li></ul><p><img loading="lazy" alt="Figure 11" src="/assets/images/image-11-8d28fbb60c1575f127ec3b07f99f9a5b.png" width="1276" height="1672" class="img_ev3q"></p><p>Fig. 11 에서 27 dataset evaluation suite 에서의 CLIP 과 best model 간의 차이 시각화</p><ul><li>CLIP 은 27 dataset 중 21 에서 Noisy Student EfficientNet-L2 를 능가</li><li>CLIP 은 OCR(SST2 및 HatefulMemes), 지리적 위치 및 장면 인식(Country211, SUN397) 및 비디오에서의 활동 인식(Kinetics700 및 UCF101)과 같은 task 에서 가장 크게 성능을 개선</li><li>또한, CLIP 은 Stanford Cars 및 GTSRB 에서 세밀한 자동차 및 교통 표지판 인식에서 훨씬 더 잘 수행<ul><li>이는 ImageNet 에서 narrow supervision 문제일 수 있음</li></ul></li><li>GTSRB 에서의 14.7% 향상과 같은 결과는 모든 교통 및 도로 표지판에 대해 single label 만 있는 ImageNet-1K 의 문제를 나타낼 수 있음<ul><li>이는 supervised representation 이 class 내 세부 사항을 축소하고 fine-grained downstream task 에서 정확도를 저해할 수 있음</li></ul></li><li>언급했듯, CLIP 은 여전히 몇몇 dataset 에서 EfficientNet 보다 성능이 낮음</li><li>놀랍게도, EfficientNet 이 CLIP 에 비해 가장 잘 수행하는 dataset 은 훈련된 데이터셋인 ImageNet</li><li>EfficientNet 은 CIFAR10 및 CIFAR100 과 같은 낮은 해상도 데이터셋에서도 CLIP 보다 약간 더 잘 수행<ul><li>이는 CLIP 에 규모별 데이터 증강이 부족한 것이 일부 원인일 것</li></ul></li><li>EfficientNet 은 PatchCamelyon 및 CLEVRCounts 에서도 약간 더 잘 수행하는데, 이는 두 접근 방식 모두에서 전반적인 성능이 여전히 낮기 때문</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-robustness-to-natural-distribution-shift">3.3 Robustness to Natural Distribution Shift<a href="#33-robustness-to-natural-distribution-shift" class="hash-link" aria-label="Direct link to 3.3 Robustness to Natural Distribution Shift" title="Direct link to 3.3 Robustness to Natural Distribution Shift">​</a></h2><p>2015년, ImageNet 에서 인간을 능가하지만, 여전히 단순한 실수를 한다. 이러한 불일치에 대한 여러 아이디어가 제안되고 있다.</p><p>공통 주제는 딥 러닝이 training dataset 에 유지되는 correlation 및 pattern 발견에 뛰어나며, in-distribution 성능을 향상시킨다. 그러나 이러한 correlation 과 pattern 중 많은 것들이 실제로 의미 없으며, 다른 distribution 에서 유지되지 않아 다른 dataset 에서 성능이 크게 저하된다.</p><p>이런 연구는 ImageNet 에서 한정적으로 평가했다. 이는 일반화 실수가 있을 수 있으며, 이런 실패가 딥러닝과 ImageNet 의 결합이 어느 정도일까? very large dataset 에서의 natural language supervision 을 통해 훈련되고 high zero-shot 성능을 발휘할 수 있는 CLIP 모델은 이러한 정도를 조사한다.</p><p>Taori et al. (2020) 는 ImageNet model 의 이런 행동을 양적으로 이해하기 위해 나아가는 포괄적인 연구이며 성능이 <em>natural distribution shifts</em> 를 평가할 때 어떻게 변하는지 연구</p><ul><li>그들은 7 distribution shifts set 성능을 측정 : ImageNetV2, ImageNet Sketch, Youtube-BB 및 ImageNet-Vid, ObjectNet, ImageNet Adversarial 및 ImageNet Rendition</li><li>dataset 을 새로운 image 로 구성하여 다양한 소스에서 수집</li><li>ImageNet-C, Stylized ImageNet 또는 존재하는 이미지를 다양한 방법으로 왜곡하여 만든 adversarial attacks 과 같은 synthetic distribution shifts</li><li>이를 제안한 이유 중 하나는 여러 기법이 synthetic distribution shifts 성능을 향상시키는 것이 증명되었지만, 이러한 기법들이 natural distribution 에서 일관된 개선을 가져오지 못하는 경우가 많다고 발견하기 때문</li></ul><hr><p><img loading="lazy" alt="Figure 12" src="/assets/images/image-12-482cecce99b9f1e82f1f4829919fc1ef.png" width="1935" height="1392" class="img_ev3q"></p><p>수집된 데이터셋 전역에서, ImageNet model 의 정확도는 ImageNet validation set 에 기대치 이하로 떨어진다. </p><ul><li>all 7 natural distribution shift datasets 의 평균 정확도와 ImageNet class subset 에서의 평균 정확도를 보고하며, 특별히 명시되지 않은 경우에는 ImageNet 과 관련된 class subset 을 사용</li><li>또한 Youtube-BB 및 ImageNet-Vid 의 경우 두 가지 다른 평가 설정이 있으므로 pm-0 및 pm-10 정확도의 평균을 사용</li><li>ResNet-101 은 이러한 natural distribution shift 에서 평가될 때 ImageNet validation set 과 비교하여 5배 많은 실수를 저지른다.</li><li>그러나 Taori et al. (2020)는 distribution shift 하에서의 정확도가 ImageNet 정확도와 예측 가능하게 증가하고, logit-transformed 정확도의 로그에 대한 linear function 으로 잘 모델링됨을 발견</li><li>Taori et al. (2020)는 이 결과를 사용하여 견고성 분석이 effective 및 relative robustness 를 구별해야 한다고 제안<ul><li>효과적인 견고성은 distribution shift 하에서의 정확도가 in-distribution 과 out-of-distribution 정확도 사이의 문서화된 관계를 예측하는 것보다 개선되는 정도를 측정</li><li>relative robustness 는 out-of-distribution 정확도의 개선을 포착</li><li>Taori et al. (2020)는 견고성 기법이 <em>effective</em> 및 <em>relative</em> robustness 를 모두 향상시키도록 목표로 해야 한다고 주장</li></ul></li></ul><hr><p>Taori et al. (2020)에서 연구된 거의 모든 모델은 ImageNet 에서 training 되거나 fine-tuning 한다.</p><p>ImageNet dataset distribution 에 대한 training 또는 adapting 이 observsed robustness gap 원인일까?</p><p>직관적으로, zero-shot model 은 해당 distribution 에서만 유지되는 특정 correlation 이나 pattern 을 활용할 수 없어야 한다. 따라서 zero-shot model 이 higher effective robustness 을 가지고 있을 것으로 기대하는 것은 합리적이다. </p><p><img loading="lazy" alt="Figure 13" src="/assets/images/image-13-0adac16a7492aeef41b31a375d819b7c.png" width="1935" height="1047" class="img_ev3q"></p><p>Fig. 13 에서 natural distribution shift 에서 zero-shot CLIP 의 성능을 기존 ImageNet 모델과 비교</p><ul><li>모든 zero-shot CLIP 은 effective robustness 를 크게 향상시키고, ImageNet 정확도와 distribution shift 하에서의 정확도 간의 격차를 최대 75% 까지 줄임</li><li>이런 결과는 zero-shot model 이 훨씬 더 견고할 수 있지만, supervised learning on ImageNet 이 robustness gap 을 일으키는 것은 아님을 반드시 의미하지는 않는다</li><li>CLIP 의 다른 세부 사항, 예로 large 및 diverse pre-training dataset 또는 natural language supervision 사용은 image feature 를 훨씬 더 견고하게 만들 수 있다</li><li>zero-shot 또는 fine-tuning 에 관계없이, 이를 좁히기 위한 초기 실험으로, ImageNet distribution 에 적응하기 위해 CLIP 모델의 feature 에 L2 regularized logistic regression classifier 를 adapting 후 CLIP 의 성능이 어떻게 변하는지 측정</li></ul><p><img loading="lazy" alt="Figure 14" src="/assets/images/image-14-2de28ac7ffe3ae4aab52b3222b0ca570.png" width="1935" height="1302" class="img_ev3q"></p><p>우리는 Fig. 14 에서 zero-shot classifier 에서의 성능 변화를 시각화</p><ul><li>CLIP 을 ImageNet distribution 에 adapting 하려면 전반적으로 ImageNet 정확도가 9.2% 증가하여 85.4% 로, 이는 Mahajan et al. (2018)의 2018년 SOTA 와 정확도가 동일. 그러나 distribution shift 하에서의 평균 정확도는 약간 감소</li><li>ImageNet dataset 에서 정확도가 9.2% 증가하는 것은 distribution shift 하에서의 성능에는 거의나 전혀 영향을 주지 않으면서도 의미적으로 중요<ul><li>이러한 이득은 주로 &quot;exploiting spurious correlations&quot; 에서 오는 것일가? 이 행동이 CLIP, ImageNet 및 distribution shifts studied 또는 general phenomena 인지, 그리고 linear classifier 같은 end-to-end fine-tuning 같은 경우도 마찬가지인지 알 수 없음</li></ul></li><li>또한 flexible zero-shot natural-language-based image classifiers 로 가능한 또 다른 robustness 개입을 조사<ul><li>7 transfer dataset 간의 target class 가 항상 ImageNet 의 것과 완벽하게 일치하지는 않음</li><li>Youtube-BB 와 ImageNet-Vid 두 dataset 은 ImageNet 의 super-class 로 구성</li><li>ImageNet model 의 fixed 1000-way classifier 를 사용하여 예측하려고 할 때 문제가 발생</li><li>Taori et al. (2020)은 ImageNet class 계층 구조에 따라 모든 sub-classes 에 대해 예측을 최대화함으로써 이를 처리. 때로는 이 매핑이 완벽 이하일 수 있음<ul><li>Youtube-BB 의 person 의 경우, 예측은 야구 선수, 신부, 스쿠버 다이버의 ImageNet class 를 pooling 하는 것</li></ul></li><li>CLIP 을 사용하면 각 dataset 에 대한 class name 을 기반으로 custom zero-shot classifier 를 직접 생성 가능</li></ul></li><li>Fig. 14 에선 이것이 평균 effective robustness 를 5% 향상시키지만 큰 개선이 몇몇 dataset 에만 집중되어 있음을 볼 수 있음<ul><li>흥미롭게도, ObjectNet의 정확도도 2.3% 증가</li><li>이 dataset 은 ImageNet class 와 근접하게 겹치도록 설계되었지만, ObjectNet 의 각 class name 을 사용하는 것은 필요한 경우 ImageNet class name 을 사용하고 예측을 pooling 하는 것보다 약간 도움이 됨</li></ul></li></ul><p>zero-shot CLIP 은 effective robustness 를 향상시키지만, Fig. 14 처럼 이 혜택은 fully supervised setting 에선 거의 사라진다. zero-shot 에서 fully supervised 까지의 연속에서 effective robustness 가 어떻게 변하는지 조사.</p><p>Fig. 15 에서 best CLIP 의 feature 에 대한 zero-shot, 1-shot, 2-shot, 4-shot, ..., 128-shot 및 fully supervised logistic regression classifier 의 성능을 시각화</p><p><img loading="lazy" alt="Figure 15" src="/assets/images/image-15-61451f90a6a8b66efa45719e37a57e34.png" width="975" height="1257" class="img_ev3q"></p><ul><li>적은 양의 데이터로 학습된 모델도 baseline model 보다 higher effective robustness 를 보이지만, 이 혜택은 in-distribution 성능이 더 많은 training data 와 함께 증가함에 따라 사라지고 대부분의 경우에는 완전히 없어진다</li></ul><p>이러한 결과를 종합하면, large-scale task 및 dataset agnostic pre-training 과 broad evaluation suites 에 대한 zero-shot 및 few-shot 벤치마킹으로의 reorientation towards 가 더 견고한 시스템의 개발을 촉진하고 성능을 보다 정확하게 평가한다. </p><h1>4. Comparison to Human Performance</h1><p>CLIP 과 인간 성능 및 인간 학습 비교를 위해 CLIP 의 evaluation setting 과 유사하게 인간들 평가</p><ul><li><p>이는 인간이 얼마나 강력한 zero-shot 성능을 보이는지, 성능이 한 두개의 image sample 을 보여줄 때 얼마나 향상되는지 알아보고자 함</p></li><li><p>이를 통해 인간과 CLIP 간의 task 난이도를 비교하고, 이들간의 correlation 과 difference 파악</p><p>Oxford IIT Pets dataset 의 test set 3669 images 를 각각 5명에게 제공하고 비슷한 37 cat 및 dog 를 선택하도록 함.</p></li><li><p>zero-shot 상황에서 인간은 각 품종 예시를 받지 않은채 이미지를 라벨링하도록 요청.</p></li><li><p>1-shot 에서는 각 품종의 한장 샘플 이미지를 받고, 2-shot 에서는 두 장의 샘플 이미지를 받음</p></li></ul><p><img loading="lazy" alt="Figure 16" src="/assets/images/image-16-b08144cece35076f50475d99ad5cc139.png" width="1440" height="1546" class="img_ev3q"></p><ul><li>인간은 class 당 1 training example 의 평균 성능이 54% to 76% 로 향상<ul><li>추가 훈련 예제에 대한 한계적인 이득은 거의 없음</li></ul></li><li>zero-shot to 1-shot 의 과정에서 인간은 불확실한 이미지에 대한 정확도가 향상</li><li>하나의 예제를 통해 불확실한 이미지에 대한 확률을 업데이트가 가능함을 시사</li><li>이를 고려해, CLIP 은 zero-shot 성능을 위한 유망한 training 전략이지만, few-shot 과 차이가 크다는 것을 알 수 있음</li></ul><p>이 결과는 아직 기계과 인간의 샘플 효율성 간의 격차를 줄이기 위한 알고리즘 개선이 여전히 필요하다는 것을 시사</p><ul><li>few-shot 평가에서 CLIP 은 사전 지식을 효과적으로 활용하지 않으며, 인간은 그렇기 때문에 우리는 사전 지식을 효과적으로 퓨샷 학습에 통합하는 방법을 찾는 것이 CLIP의 알고리즘 개선에 중요한 단계라고 추측</li><li>우리의 지식에 따르면, 고품질 사전 훈련 모델의 특성 위에 linear classifier 사용이 few-shot learning 에 대한 거의 최신 기술 수준<ul><li>이는 최고의 few-shot learning 방법과 인간의 few-shot learning 사이에 격차가 있다는 것을 시사</li></ul></li><li>만약 인간 정확도 대 CLIP 의 zero-shot 정확도를 그래프로 그려보면 (Fig. 16), CLIP 에 대한 가장 어려운 문제가 인간에게도 어려운 것임을 확인. <ul><li>오차가 일관되는 한, 저자의 가설은 dataset noisy (잘못 라벨링된 이미지 포함) 및 인간과 모델 모두에게 난제인 분포 밖 이미지 때문일 것이라고 생각</li></ul></li></ul><h1>5. Data Overlap Analysis</h1><p>pre-training on large internet dataset 의 우려 사항은 downstream evals 와 의도하지 않은 overlap 이다.</p><ul><li>최악의 경우, evaluation dataset 의 complete copy 가 pre-training dataset 으로 유출되어 일반화의 의미 있는 테스트로서의 평가를 무효화할 수 있기 때문에 조사하는 것이 중요</li><li>이를 방지하기 위한 한 가지 옵션은 모델을 훈련하기 전에 모든 중복을 식별하고 제거하는 것<ul><li>이는 진정한 hold-out 성능을 보고하기는 보장하지만, 모델이 사전에 평가될 수 있는 모든 가능한 데이터를 미리 알아야 한다는 것을 요구</li><li>이것은 벤치마킹과 분석의 범위를 제한하는 단점 존재</li><li>새로운 평가를 추가하기 위해서는 비용이 많이 드는 re-training 이 필요하거나 중첩으로 인한 미정량적 이점을 보고할 위험이 있다</li></ul></li></ul><p>대신, 저자는 중첩이 얼마나 발생하는지와 이러한 중첩으로 인해 성능이 어떻게 변하는지 문서화. 이를 위해 다음 절차를 사용한다.</p><ol><li>각 evaluation dataset 에 대해, 그 예제에 대한 duplicate detector 실행.<ul><li>이후 수동으로 찾은 nearest neighbors 를 검사하고 높은 정밀도를 유지하면서 재현율을 극대화하기 위한 dataset 당 threshold 설정</li><li>이 임계값을 사용하여 Overlap과 Clean 두 개의 새로운 subset 생성</li><li>Overlap 에는 임계값 이상의 유사성을 가진 모든 예제가 포함되고, Clean 에는 이 임계값 이하의 모든 예제가 포함</li><li>변경되지 않은 전체 데이터셋은 All로 표기</li><li>이를 통해 데이터 오염 정도를 모든 예제 중 Overlap 의 수와 All 의 크기의 비율로 기록</li></ul></li><li>이후 CLIP RN50x64 의 zero-shot 정확도를 계산하고 세 분할에 대한 All - Clean 을 주요 지표로 보고<ul><li>이는 오염으로 인한 정확도 변화</li><li>이 값이 양수이면 overlapping data 로 인해 전체적으로 보고된 정확도가 얼마나 over-fitting 되었는지에 대한 추정값</li></ul></li><li>overlap 량은 종종 작기 때문에 Overlap 하위 집합의 일평균 (더 큰) p-value 을 계산하고 일변량 유의성 검정을 실행<ul><li>또한 99.5% 의 Clopper-Pearson confidenec intervals 를 Dirty 에 대해 계산하여 또 다른 확인을 진행</li></ul></li></ol><p><img loading="lazy" alt="Figure 17" src="/assets/images/image-17-9e63be3355195fc414ceefff16f4660c.png" width="2039" height="1070" class="img_ev3q"></p><p>이 분석의 요약은 Fig. 17 에 제시.</p><ul><li>35 dataset 중 9 dataset 에는 중복디 감지되지 않음<ul><li>일부는 합성 또는 전문화된 dataset 이라 인터넷에 게시될 가능성 적음 (e.g. MNIST, CLEVR 및 GTSRB)</li><li>또는 저자의 dataset 에는 생성된 후 만들어진 novel data 를 포함하여 중복이 없음을 보장 (ObjectNet 및 Hateful Memes)</li></ul></li><li>이는 저자의 detector 가 low-false positive rate 를 가짐을 보여줌<ul><li>false positive 는 분석에서 오염 효과를 under-estimate 할 수 있어서 중요함</li></ul></li><li>median overlap 2.2%, average oberlap 3.2%<ul><li>소량의 중복이 전체적인 정확도가 0.1% 이상으로 변경되는 경우는 드물다.</li><li>이 중 임계값을 초과하는 7 dataset 만 있다.</li><li>이 중 2개만 보정된 Bonferroni 수정 후에 통계적으로 유의함</li><li>최대 감지된 개선은 overlap rate 가 12.1% 인 Birdsnap 에서 0.6% 만큼이다.</li><li>largest overlap 은 21.5% 인 Country211 에서 발생<ul><li>이는 저자의 pre-training dataset 이 filtered sub-set 을 포함하고 있는 YFCC100M 에서 구성되었기 때문</li><li>그러나 이러한 큰 중복에도 불구하고 Country211 의 정확도는 0.2% 만 증가</li><li>이는 중복되는 예제의 training text 가 downstream eval measure 가 specific task 와 관련이 없는 경우가 많기 때문일 수 있음</li></ul></li><li>Country211 은 지리적 위치 파악 능력을 측정하지만, 이러한 중복에 대한 training text 를 검사하면 이미지의 위치를 언급하지 않는 것으로 나타남</li></ul></li></ul><hr><p>저자의 분석엔 two potential concerns 를 인식.</p><ol><li>detector 는 완벽하지 않음<ul><li>detector 는 proxy training task 에 거의 100% 정확도를 달성하고, 수동 검사 및 threshold tuning 을 통해 발견된 nearest-neighbors 사이에 높은 정밀도와 재현율을 가짐</li><li>하지만 400M examples 를 통한 재현율을 추적적으로 확인은 불가능</li></ul></li><li>Overlap 과 Clean subset 간의 data distribution 이 변할 수 있다는 점<ul><li>Kinetics-700 에서 많은 &quot;overlap&quot; 이 실제로 모두 black transition frames<ul><li>이것이 Kinetics-700 이 Overlap 에서 20% 의 정확도 하락을 보이는 이유<ul><li>보다 subtle distribution shifts 가 있을 수 있다고 의심</li></ul></li><li>CIFAR-100 의 경우, 해상도가 매우 낮기 때문에 많은 중복이 새의 새나 비행기와 같은 작은 객체의 false positives 로 나타남</li><li>정확도의 변화는 overlap class distribution 이나 difficulty 변화 때문일 수 있음</li><li>불행히도, 이런 distribution 및 difficulty 변화는 over-fitting 효과를 가리기도 함</li></ul></li></ul></li></ol><p>그러나 위 결과는 large scale pre-training 에 관한 이전 연구에서의 유사한 duplicate analysis 결과를 따른다.</p><p>Mahajan et al. (2018) 및 Kolesnikov et al. (2019)는 유사한 overlap rate 를 검출하고 전반적인 성능에는 최소한의 변화만 있었다. 중요한 점은 Kolesnikov et al. (2019)가 논문 introduction 에서 논의한 alternative de-duplication 전략과 우리가 결정한 접근 방식 사이의 차이가 거의 없었다는 것</p><h1>6. Limitations</h1><p>CLIP 에는 많은 제한 존재.</p><ul><li>training splits 을 포함한 dataset 의 경우, zero-shot CLIP 성능은 평균적으로 ResNet-50 features 의 top 에 linear classifier 의 simple supervised baseline 과 경쟁력 있음<ul><li>이런 dataset 대부분에서, 이 baseline 의 성능은 전반적인 SOTA 보다 훨씬 낮음</li><li>CLIP 의 task learning 및 transfer 능력 개선을 위해 많은 연구가 필요</li><li>scaling 은 성능을 꾸준히 향상시키며, 계속해서 개선할 수 있는 경로임을 제시</li><li>하지만 zero-shot CLIP 이 SOTA 달성을 위해선 computing 량을 1000x  증가해야 한다고 추정</li></ul></li><li>CLIP zero-shot 성능은 여러 task 에선 여전히 약함<ul><li>task-specific model 과 비교하여, 자동차 모델, 꽃 종류 및 항공기 등 세부 분류 유형에서 약함</li><li>또한 이미지 객체 수를 세는 것 같은 추상적이고 체계적인 task 도 어려움</li><li>CLIP 의 pre-training dataset 에 포함되지 않은, 생략된 novel task 에 대해선 CLIP 의 성능이 거의 무작위에 가까움</li></ul></li><li>zero-shot CLIP 은 많은 natural image distribution 에 generalizing 하지만, out-of-distribution 에는 여전히 generalizing 하지 못함<ul><li>이는 OCR task 에서 명확히 나타남</li><li>CLIP 은 MNIST 손글씨 숫자에선 88% 정확도만 달성</li><li>raw pixel 에 대한 logistic regression 의 simple baseline 이 zero-shot CLIP 보다 뛰어남</li><li>의미론적 및 near-duplicate nearest-neighbor retrival 결과, MNIST 는 pre-training dataset 에 없음을 확인했으며, 이는 CLIP 이 취약한 일반화 해결에는 거의 기여하지 않음을 시사</li><li>대신 이를 우회하려고 하면, large scale 이고 diverse dataset 으로 훈련하여 effectively in-distribution 으로 포함될 것으로 기대</li></ul></li><li>넓고 다양한 task 및 dataset 에 flexible zero-shot classifier 를 생성할 수 있지만, 주어진 zero-shot classifier 의 concept 중에서만 선택 가능<ul><li>novel output 을 생성하는 image captioning 같은 flexible approach 와 비교하면 상당히 제한적</li><li>image caption baseline 의 계산 효율성은 CLIP 보다 훨씬 낮음. 이는 CLIP 의 효율성을 결합한 접근 방식을 시도할 가치가 있음. 대안으로 inference 시 natural language explanations 의 많은 retrieval 수행을 할 수 있음</li></ul></li><li>deep learning 의 data efficiency 해결하지 않음<ul><li>대신, 400M training examples 로 확장할 수 있는 supervision 을 사용하여 보상</li><li>training 중에 본 image 가 초당 하나씩 제시되면, 32 epochs 동안 12.8B 의 image 를 반복하는데 405년 걸릴 것</li><li>CLIP 을 self-supervision 및 self-training 과 결합하는 것은 standard supervision learning 보다 data efficiency 를 향상시킬 수 있는 능력을 보여주어 유망하다</li></ul></li><li>저자의 초점은 zero-shot transfer 임에도 불구하고, 반복적인 CLIP 개발 안내를 위해 full validation set 성능을 쿼리<ul><li>validation set 에는 수천개 예제가 있어, 실제로는 zero-shot 에는 현실적이지 않음</li></ul></li><li>잠재적인 문제는 validation set 선택<ul><li>12 dataset set 에 대한 결과를 보고하며, 무작위의 27 dataset set 을 사용</li><li>이 set 은 명백히 CLIP 개발 및 기능과 함께 적응된 것으로, 광범위한 zero-shot 평가를 위해 명시적으로 설계된 작업의 새로운 벤치마크를 만드는 것이 이러한 문제를 해결하는 데 도움이 될 것</li></ul></li><li>CLIP 은 인터넷에서 image 와 text 를 결합하여 훈련<ul><li>이러한 image-text 쌍은 필터링되지 않고 선별되지 않았으며, CLIP 이 많은 사회적 편향을 학습하게 된다.</li><li>image caption model 에 대해 이전에 증명</li></ul></li><li>이 연구에서 image classification 을 natural language 를 통해 명시하는 것이 유연하고 일반적인 인터페이스임을 강조했지만, 이것에는 제한 사항 존재<ul><li>많은 complex task 와 visual concepts 는 text 만으로 지정하기가 어려울 수 있음</li><li>실제 training examples 는 명백하게 유용하지만, CLIP 는 few-shot 성능을 직접 최적화하지는 않음</li></ul></li><li>저자의 연구에선 CLIP 의 feature top 에 linear classifier 를 맞추게 됨<ul><li>이는 zero-shot setting 에서 few-shot setting 으로 전환할 때 성능이 반대로 감소하는 이상한 현상을 초래</li><li>이는 인간의 성능과는 크게 다르다<ul><li>인간의 성능은 zero-shot setting 에서 1-shot setting 으로 전환함으로써 크게 증가</li></ul></li></ul></li></ul><h1>7. Broader Impacts</h1><p>CLIP 의 성능과 목적에 대한 평가가 필요하며, 이를 위한 더 넓은 영향을 분석</p><p>또한 CLIP 은 추가 re-training 없이 자신만의 classifier 를 만들 수 있는 능력 소개</p><ul><li>이 능력은 GPT-3 와 유사한 large-scale generative models 의 특성을 복합하며, 이러한 모델은 비교적 non-trivial zero-shot(또는 few-shot) generalization 능력을 보여서 다양한 능력을 가질 수 있다.</li><li>저자의 zero-shot setting 에서의 CLIP 연구는 image retrieval or search 같은 widely-applicable tasks 에 대한 유망성을 보여줌</li><li>이전 30 dataset 에 추가하여 FairFace 에서 평가하고 bias probes 실시</li><li>이후, downstream task surveillance 에서 비교 분석</li><li>또한 model 의 social biases 를 특성화하기 위해 노력<ul><li>bias test 는 다양한 시나리오에서의 반응을 조사하는 초기 노력이며, 범위는 한정</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="71-bias">7.1 Bias<a href="#71-bias" class="hash-link" aria-label="Direct link to 7.1 Bias" title="Direct link to 7.1 Bias">​</a></h2><p>Buolamwini &amp; Gebru (2018) 및 Karkkainen &amp; Joo (2019)에서 제시된 bias probes 에 영감을 받아 CLIP 의 일부 bias 를 예비적으로 분석하고, Solaiman et al. (2019)가 실시한 것과 유사한 specific bias examples 를 찾기 위한 탐색적 bias search 수행</p><p>FairFace dataset 에서 zero-shot CLIP 의 성능을 분석하여 initial bias probes 로 시작하고, class design 을 포함한 additional biases 및 biases sources 를 더 탐구</p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-18-b4fc5cfd89ff7cf0e82b5ea66d9c83dc.png" width="886" height="411" class="img_ev3q"></p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-20-8d2913b0d32543e1dcf39e5254277080.png" width="884" height="503" class="img_ev3q"></p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-21-9af0671dcc471b3b6c88b4a7425753a1.png" width="1485" height="681" class="img_ev3q"></p><ul><li>FairFace dataset 에서 두 버전의 CLIP 평가<ul><li>zero-shot CLIP (ZS CLIP) 과 CLIP featre 의 top 을 FairFace 에 fitting 한 logistic regression classifier (LR CLIP)<ul><li>LR CLIP 이 classification test 에서 ResNext-101 32x48d Instagram (Linear Probe Instagram) 및 FairFace 자체 모델보다 높은 정확도 달성</li><li>ZS CLIP 성능은 카테고리별로 다르며 일부 카테고리에 대해 FairFace 보다 성능이 나빠지고 다른 카테고리는 나은 결과를 보임 (Tab. 3, Tab. 4)</li></ul></li><li>또한 intersectional race 및 gener 를 기준으로 LR 및 ZS CLIP 테스트<ul><li>모든 race 에 대해 gender 성능이 95% 이상임을 발견 (Tab. 5)</li></ul></li></ul></li><li>LR CLIP 이 gener, race 및 age 에 따른 classification 의 정확도가 Linear Probe Instagram 보다 높은 성능을 달성했지만, bias 에 대한 정확도는 real world 에서의 공정성에 대한 의미있는 척도로서는 종종 실패<ul><li>sub-groups 에서 성능 차이가 낮고 정확도가 높아진다 해도, 이는 bias 가 낮아질 것이라는 것을 의미하진 않음<ul><li>예로, underrepresented groups 의 높은 성능은 회사가 얼굴 인식을 사용하여 group 간에 비례하지 않는 방식으로 사용할 수 있으며, 결과적으로 race, age 및 gener 에 따른 영향이 다르게 발생할 수 있음</li><li>facial classification 사용은 문제 없는 task 라는 것을 시사하거나 배포된 맥락에서 race, age 또는 gener classification 의 사용을 지지하는 것이 아님을 의미</li></ul></li></ul></li></ul><p><img loading="lazy" alt="Table 6" src="/assets/images/image-22-cd3e38bc2fc3e217ec7e42d2ac2abfb4.png" width="1772" height="381" class="img_ev3q"></p><p><img loading="lazy" alt="Table 7" src="/assets/images/image-23-c52576ccbc7257f1ea5486de622e4e03.png" width="1783" height="403" class="img_ev3q"></p><ul><li>또한 Crawford (2017)에서 설명한 표현적 피해를 유발할 가능성이 높은 classification tmers 를 사용하여 모델을 검사<ul><li>ZS CLIP 이 FairFace 의 10,000 images 를 분류하도록 요구하는 실험을 진행</li><li>FairFace classes 외에도 &#x27;animal&#x27;, &#x27;gorilla&#x27;, &#x27;chimpanzee&#x27;, &#x27;orangutan&#x27;, &#x27;thief&#x27;, &#x27;criminal&#x27;, &#x27;suspicius person&#x27; 추가</li><li>이 실험의 목표는 모델이 어떤 demographic subgroups 에 대해 편견이나 피해를 더 많이 줄 수 있는지 확인하는 것</li></ul></li><li>이미지의 4.9% (confidence intervals 는 4.6% ~ 5.4% 사이)가 저자의 probes 에서 사용한 non-human classes(&#x27;animal&#x27;, &#x27;chimpanzee&#x27;, &#x27;gorilla&#x27;, &#x27;orangutan&#x27;) 중 하나로 잘못 분류되었음을 발견<ul><li>이 중 &#x27;Black&#x27; image 의 misclassification rate 가 가장 높았으며(약 14%; confidence intervals 는 <!-- -->[12.6% ~ 16.4%]<!-- -->), 다른 모든 race 는 8% 미만의 misclassification rate 를 보임</li><li>0-20세의 사람들 중 14% 가 이 카테고리에 분류</li></ul></li><li>또한, 남성 이미지의 16.5% 가 범죄와 관련된 class(&#x27;thief&#x27;, &#x27;suspicious person&#x27;, &#x27;criminal&#x27;)로 missclassify 되었으며, 여성 이미지의 9.8% 가 이에 해당<ul><li>흥미롭게, 0-20세 사람들은 범죄 관련 클래스에 속할 확률이 다른 연령대 이미지보다 높았음(약 18%)</li><li>race 간에 범죄 관련 용어에 대한 분류의 차이가 있었음 (Tab. 6)</li></ul></li><li>20세 미만인 사람들이 범죄 관련 및 non-human animal 카테고리에 가장 자주 분류되었음을 관찰했기 때문에, 동일한 클래스를 사용하여 동일한 카테고리로 이미지를 분류하는 실험을 진행<ul><li>목표는 이러한 classification 의 age 에 따른 피해의 분포가 어떻게 변경되는지 확인하는 것</li><li>이로 인해 20세 미만 사람들의 수가 범죄 관련 카테고리나 non-human animal 카테고리에 분류된 이미지 수가 크게 줄어든 것을 발견 (Tab. 7)</li><li>이는 class design 이 모델의 성능뿐 아니라 모델이 표현하는 원치 않는 bias 이나 behavior 를 결정하는 주요 요인이 될 수 있음을 나타냄</li></ul></li></ul><p>이러한 probe results 는 포함되는 클래스 카테고리와 각 클래스를 설명하는 특정 언어에 따라 변경될 수 있다. Poor class design 은 실제 성능이 나빠질 수 있으며, 이러한 우려는 특히 CLIP 같은 모델에게 중요하다. 개발자가 자신의 클래스를 쉽게 디자인할 수 있기 때문이다.</p><p>Schwemmer et al. (2020)에서 제시한 것과 유사한 실험을 수행하여 Members of Congress images 를 사용하여 CLIP 이 남성과 여성 이미지를 다르게 처리하는 방식을 테스트.</p><p>저자는 세 가지를 테스트 한다. gener classification 의 정확도와 two different label sets 간에 어떻게 분산되는지 테스트하는데, first label set 은 300 label set 을 사용하였으며, second label set 은 Google Cloud Vision, Amazon Rekognition 및 Microsoft Azure Computer Vision 이 모든 이미지에 대해 반환한 레이블을 결합한 것.</p><ol><li>Members of Congress images 를 사용해 모델의 gender prediction 성능을 살펴봄<ul><li>official setting/position of power 에 있는 사람의 image 를 보고 gener 를 올바르게 인식하는지 확인하기 위함</li><li>모델이 100% 정확도를 달성</li><li>이는 FairFace dataset 성능이 우수한 것으로 나타나며, Members of Congress dataset 의 모든 이미지가 FairFace 와는 다르게 high-quality 이고 clear 하게 중앙 정렬되어 있기 때문으로 추측.</li></ul></li><li>label probability 에 대한 threshold 를 설정하여 반환된 label 의 bias 가 어떻게 달라지는지 실험<ul><li>threshold 0.5% 및 4.0% 로 설정하여 실험</li><li>lower threshold 가 lower quality 의 label 을 유도하는 경향을 발견<ul><li>이런 threshold 하에 label distribution 의 차이도 bias signal 을 갖을 수 있다.</li><li>예로, 0.5% threshold 하에선 &quot;nanny&quot; 나 &quot;housekeeper&quot; 같은 label 은 여성에게, &quot;prisoner&quot; 및 &quot;mobster&quot; 같은 label 은 남성에게 나타나는 것을 발견</li></ul></li></ul></li></ol><ul><li>higher 4% threshold 에선 both gender 에서 highest probabiliy 를 가진 label 에는 &quot;lowmaker&quot;, &quot;legislator&quot; 및 &quot;congressman&quot; 이 포함<ul><li>이런 bias 는 lower probability labels 에서도 나타나면서 &quot;sufficiently&quot; safe behavior 이 무엇인지 제기</li></ul></li></ul><ol start="3"><li>Google Cloud Vision (GCV), Amazon Rekognition 및 Microsoft 가 반환한 label 의 결합셋을 사용할 때, GCV system 에서 발견된 bias 가 저자의 시스템에도 유사하게 나타남<ul><li>여성에게 더 많이 &#x27;brown hair&#x27;, &#x27;blonde&#x27; 및 &#x27;blond&#x27; 같은 label 이 부여되는 경향이 있음<ul><li>게다가 CLIP 는 &#x27;executive&#x27; 나 &#x27;doctor&#x27; 같은 고위 직책을 남성에게 불균형하게 자주 부여</li><li>여성에게 더 자주 부여된 네 가지 직업 가운데 세 가지는 &#x27;newscaster&#x27;, &#x27;television presenter&#x27;, &#x27;newsreader&#x27; 이고 네 번째는 &#x27;Judge&#x27; 였음</li><li>이는 GCV 에서 발견된 편향과 유사하며, 역사적인 성별 차별을 보여줌</li></ul></li><li>label set 에 threshold 0.5% 로 낮추었을 때, 남성을 더 자세히 묘사하는 label 도 &#x27;suit&#x27;, &#x27;tie&#x27; 및 &#x27;necktie&#x27; 같은 외모 지향적인 단어로 이동하는 것을 발견</li><li>여성 이미지에 사용되지 않은 직업 지향 단어가 남성과 여성 모두에게 사용되었음</li></ul></li></ol><p>이러한 실험은 Design decision 이 어떻게 biases 가 나타나는지에 영향을 미치며, 특히 CLIP 의 유연성을 고려할 때 중요하다.</p><p>training data 와 model architecture 뿐만 아니라 class design 및 threshold 같은 요소에 대한 결정은 모델이 출력하는 label 을 변경시킬 수 있으며, 결과적으로 Crawford(2017)에 설명된 특정 유형의 피해를 높이거나 낮출 수 있습니다. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="72-surveillance">7.2 Surveillance<a href="#72-surveillance" class="hash-link" aria-label="Direct link to 7.2 Surveillance" title="Direct link to 7.2 Surveillance">​</a></h2><p>저자는 사회적으로 민감한 downstream task : surveillance 와 관련하여 모델을 특정한다.</p><p>저자의 분석 목표는 characterization approach 를 더 잘 반영하고 규범과 점검을 지원하는 것을 목표로 한다.</p><p>surveillance 포함은 이 domain 에 대한 관심이 아니며, surveillance 가 사회적 영향을 고려할 때 시도해야할 중요한 domain 이라 생각</p><p>저자는 model 성능을 CCTV camera image 의 classification 측정 및 zero-shot celebrity identification 진행</p><p>먼저, low-resolusion images captured from  surveillance cameras (e.g. CCTV) 에서 test 진행</p><ul><li>VIRAT dataset 과 Varadarajan &amp; Odobez (2009) dataset 을 사용했는데, 이들은 모두 real world outdoor scenes 와 non-actors 포함<ul><li>CLIP 의 유연한 클래스 구성을 고려해 12 different video sequences 에서 캡처한 515 surveillance images 를 대상으로 self-constructed general class 를 대상으로 세밀하고 정교한 분류를 테스트 진행</li><li>Coarse classification 에선 모델이 이미지의 주요 대상을 올바르게 식별해야 함 (i.g., 빈 주차장, 학교 캠퍼스 등의 사진인지 여부를 결정)<ul><li>이미지 내용을 자체적으로 caption 으로 지어서 class 를 구성했으며, 모델이 선택할 수 있는 옵션이 항상  least 6 options 이상이다</li><li>게다가, &#x27;stress test&#x27; 를 실시했는데, 이때 class set 에 이미지와 &#x27;가까운&#x27; 항목을 설명하는 caption 중 하나 이상이 포함 (예: &#x27;흰색 차가 있는 주차장&#x27; 대 &#x27;빨간색 차가 있는 주차장&#x27;). 우리는 초기 평가에서 CCTV 이미지에 대한 모델의 상위 1위 정확도가 91.8%였음을 발견했습니다. 두 번째 평가에서 정확도는 크게 51.1%로 떨어졌으며, 모델은 &#x27;가까운&#x27; 답변을 40.7%의 비율로 잘못 선택했습니다.</li></ul></li><li>Fine-grained classification 에선 모델이 이미지의 작은 특징 (e.g. 구석에 서 있는 사람)의 존재 또는 부재를 판단하기 위해 구성된 두 가지 옵션 중에서 선택해야 함<ul><li>세밀한 감지에서는 제로샷 모델이 성능이 낮았으며, 결과가 거의 무작위로 나타났습니다. 유의할 점은 이 실험이 이미지 시퀀스에서 작은 객체의 존재 또는 부재를 감지하기 위해서만 대상으로 했다는 것입니다.</li></ul></li></ul></li></ul><p><img loading="lazy" alt="Table 8" src="/assets/images/image-24-5fa1bafc736c27089669555cbf1b2b39.png" width="899" height="378" class="img_ev3q"></p><ul><li>또한 CelebA dataset 을 사용하여 &#x27;in the wild&#x27; 의 신원 감지에 대한 CLIP zero-shot 을 테스트<ul><li>이를 통해 모델이 pre-training 된 public data 만 사용하여 신원 감지의 성능을 얼마나 잘 하는지를 평가</li><li>celebrity images dataset 에서 이를 테스트했지만, 모델이 이름과 얼굴을 연결하기 위해 필요한 pre-training data 양이 계속 감소할 것으로 가정 (Tab. 8)</li><li>이는 중요한 사회적 영향을 미친다. 최근 NLP 분야의 최근 발전과 유사하며, 이러한 LLM 은 인터넷 데이터를 기반으로 꽤 작은 공개 인물에 관한 정보를 제공하는 놀라운 능력을 가지고 있다</li></ul></li><li>저자는 &#x27;in the wild&#x27; 8k celebrity images 에 대해 100 possible classes 중 59.2% top-1 accuracy 를 갖는 모델을 발견.<ul><li>그러나 이 성능은 class size 를 1k celebrity names 로 확장했을 때 43.3% 로 떨어짐<ul><li>이 성능은 Google 의 Celebrity Recognition 과 같은 생산 수준의 모델과 비교하면 경쟁력이 없다</li><li>그러나 이러한 결과를 주목할 만한 점은 이 분석이 pre-training data 에서 추론된 names 를 기반으로 한 zero-shot identification 능력만을 사용하여 수행되었다는 것 - additional task-specific dataset 을 사용하지 않았으므로 (상대적으로) 강력한 결과는 이러한 시스템을 주어진 문맥과 도메인에서 조심스럽게 연구해야 할 필요성을 더욱 강조</li></ul></li></ul></li><li>CLIP 은 제로샷 능력을 가지고 있기 때문에 상대적으로 적은 데이터를 사용하는 task 에 대해 상당한 이점을 제공<ul><li>그러나 많은 수요가 있는 surveillance task 에 대해 large datasets 과 고성능 supervised model 이 이미 존재하기 때문에, CLIP 의 이러한 용도에 대한 비교적 매력은 낮다</li><li>게다가, CLIP 는 object detection 과 semantic segmentation 같은 일반적인 surveillance-relevant tasks 에 대해 설계되지 않았다<ul><li>이는 이러한 용도로 설계된 모델인 Detectron2 같은 모델이 널리 사용 가능한 경우에는 특정 surveillance tasks 에 대해 제한적으로 사용되는 것을 의미</li></ul></li></ul></li><li>그러나 CLIP 은 training data 가 필요하지 않은 측면의 사용성을 제공하기 때문에 특정 사용자 정의 및 특수한 surveillance case 를 가능케 함<ul><li>따라서 CLIP 및 유사한 모델은 특별히 맞춤화된 모델이나 데이터셋이 없는 작은 수요 감시 사용 사례를 가능하게 할 수 있으며, 이는 이러한 애플리케이션을 구축하는 데 필요한 기술 요구 사항을 낮출 수 있다. </li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="73-future-work">7.3 Future Work<a href="#73-future-work" class="hash-link" aria-label="Direct link to 7.3 Future Work" title="Direct link to 7.3 Future Work">​</a></h2><p>초기 분석은 일반적인 vision model 의 도전과 bias 영향을 보았고, 단점 및 bias 를 특성화하는 미래 연구를 촉진</p><p>CLIP 의 특성화하고, 유망한 성능을 발휘하는 것이 중요.</p><p>저자는 다음의 이유로 유익할 가능성이 높다고 본다.</p><ul><li>연구 과정 초기에 모델의 잠재적으로 유익한 downstream task 사용을 식별함으로써 응용 분야를 고려</li><li>사회적 이해관계자의 많은 집합과 중요한 민감성을 가진 작업을 부각시킴으로써, 정책 결정자의 개입을 요구할 수 있는 작업을 도출</li><li>모델의 bias 을 더 잘 특성화하여 우려되는 영역과 개입이 필요한 영역을 알림</li><li>CLIP 같은 시스템을 평가하기 위한 test suite 을 생성함으로써, 개발 주기 초기에 모델 능력을 더 잘 특성화함</li><li>잠재적인 실패 모드와 추가 작업 영역을 식별함</li></ul><h1>8. Related Work</h1><p>일반적으로 human language 를 training signal 로 활용하는 경우, supervision 을 사용한다.</p><p>NLP task 에서 description, feedback, instruction 및 device 같은 형태의 natural language supervision 을 활용하여 창의적이고 고급스러운 방법으로 탐구된다</p><ul><li>CLIP 은 language 외의 domain 에서 training signal 로 natural language 를 사용<ul><li>이는 natural language supervision 이 video event understanding task 성능 향상을 위해 natural language description 을 supervision source 로 사용 가능함을 보여준 Ramanathan et al (2013)의 task 이다</li><li>다른 초기 연구는 image 와 related tag (not natural language)를 사용하여 semantic segmentation task 에 활용했다.</li><li>더 최근에는 natural language description 을 fine-grained visual classification birds 개선에 사용되거나 visual representation 및 classifier 개선에 연구를 하기도 한다</li><li>마지막으로, natural language 를 supervision 으로 사용하는 기술과 보강 학습 환경을 결합하는 기술도 있다.</li></ul></li><li>CLIP 의 pre-training task 는 text-image retrieval 을 최적화</li><li>natural language supervision 을 image 외의 domain 에 활용하는 다른 연구도 존재<ul><li>Stroud et al (2020)은 image 대신 video 에 description text 를 결합하여 large scale representation learning 탐구</li><li>이러한 연구들은 large-scale natural language supervision 을 여러 domain 에 대한 high-quality recognition system learning 에 대한 유망한 방법임을 시사</li><li>Alayrac et al (2020)은 이러한 연구를 음원 오디오를 추가적인 supervision source 로 추가함으로써 이 라인의 연구을 확장</li></ul></li><li>CLIP 연구의 일환으로 우리는 새로운 image-text pair dataset 구축<ul><li>현대의 image-text retrieval task 는 Pascal1K, Flickr8K 및 Flickr30K 같은 crowd-based sentence-level image caption evaluation dataset 에 의존</li><li>그러나 이러한 dataset 은 여전히 비교적 작고 실현 가능한 성능을 제한</li><li>Mithun et al (2018)은 인터넷에서 수집한 (image, text) pair 의 추가 집합이 검색 성능을 향상시킬 수 있다는 것을 보여줌</li><li>이런 dataset 은 여전히 큰 filtering 을 사용하거나 OCR 같은 task-specific 을 위해 설계되어, WIT 같은 very large dataset 보다 훨씬 작다</li></ul></li><li>CLIP와 관련된 아이디어 중 하나는 Webly supervised learning<ul><li>이 연구 라인은 image retrieval 를 query 하여 용어에 대한 image dataset 을 구축하고 query 를 반환된 image label 로 사용</li><li>이러한 방식으로 훈련된 classifier 는 smaller carefully labeled dataset 에 훈련된 classifier 와 경쟁</li><li>이러한 image-query pair 는 종종 additional training data 로 사용되어 standard dataset 의 성능을 향상시키기도 함</li><li>CLIP 은 dataset 생성 과정의 일환으로 retrieval query 를 사용</li><li>그러나 CLIP 은 single word 나 short n-gram 인 경우가 많은 query 가 아닌 image 와 함께 발생하는 전체 text sequence 를 supervision 으로 사용</li><li>또한 CLIP 에선 이 단계를 문자열 일치를 query 하는 text 만으로 제한</li><li>대부분의 Webly supervised learning 연구는 복잡한 검색 및 필터링 파이프라인을 사용한다. 이러한 라인의 연구 중에서도 &quot;Learning Everything about Anything: Webly-Supervised Visual Concept Learning&quot; 는 CLIP 과 유사한 목표가 있음</li></ul></li><li>마지막으로, CLIP 은 최근에 활발한 활동과 관련<ul><li>이 연구 라인은 vidual question answering, visual commonsense reasoning 또는 multimodal entailment 같은 complex downstream task 해결을 위해 vision-language 의 jointly model 을 학습하는 데 중점을 둔다.</li><li>이러한 시스템은 image-text pair 에 대한 다양한 training objective 를 통합적으로 tuning 하고 이전에 언급된 연구들에 적용하여 인상적인 결과를 달성</li><li>CLIP 은 대신, natural language supervision 을 통해 visual model 을 scratch training 하는 데 중점을 두며 image 와 text domain 을 densely connection 하지 않음</li><li>CLIP 모델에서 image 와 text domain 간의 유일한 상호 작용은 learned joint embedding space 에서의 single dot product 이다</li></ul></li></ul><h1>9. Conclusion</h1><p>저자는 NLP 의 task-agnostic web-scale pre-training 을 다른 domain 으로 transfer 이 가능한지 조사</p><ul><li>이런 공식을 채택하면 vision 분야에서 유사한 행동이 나타나며, 이 연구 라인의 사회적 함의에 대해 논의</li><li>CLIP 은 pre-training 과정에 다양한 task 를 수행하는 것을 배우기 때문에 training objective 를 optimizing 허기 위해 natural prompting 을 통해 이러한 task learning 을 활용</li><li>충분한 규모로 이러한 접근 방식의 성능은 task-specific supervised models 와 경쟁력을 갖을 수 있으나, 여전히 많은 개선 여지가 있음</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">Vision-Language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/clip">CLIP</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/contrastive-learning">contrastive learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/zero-shot-transfer">zero-shot transfer</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/whdnjsdyd111/whdnjsdyd111.github.io/tree/master/docs/docs/Paper/Vision-Language/Contrastive Learning/2021-03-CLIP.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/Contrastive Learning/ALIGN"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/PEFT/Composition/CLIP-LoRA"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Low-Rank Few-Shot Adaptation of Vision-Language Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-natural-language-supervision" class="table-of-contents__link toc-highlight">2.1 Natural Language Supervision</a></li><li><a href="#22-creating-a-sufficiently-large-dataset" class="table-of-contents__link toc-highlight">2.2 Creating a Sufficiently Large Dataset</a></li><li><a href="#23-selecting-an-efficient-pre-training-method" class="table-of-contents__link toc-highlight">2.3 Selecting an Efficient Pre-Training Method</a></li><li><a href="#24-choosing-and-scaling-a-model" class="table-of-contents__link toc-highlight">2.4 Choosing and Scaling a Model</a></li><li><a href="#25-training" class="table-of-contents__link toc-highlight">2.5 Training</a></li><li><a href="#31-zero-shot-transfer" class="table-of-contents__link toc-highlight">3.1 Zero-Shot Transfer</a><ul><li><a href="#311-motivation" class="table-of-contents__link toc-highlight">3.1.1 Motivation</a></li><li><a href="#312-using-clip-for-zero-shot-transfer" class="table-of-contents__link toc-highlight">3.1.2 Using CLIP For Zero-Shot Transfer</a></li><li><a href="#313-initial-comparison-to-visual-n-grams" class="table-of-contents__link toc-highlight">3.1.3 Initial Comparison to Visual N-Grams</a></li><li><a href="#314-prompt-engineering-and-ensembling" class="table-of-contents__link toc-highlight">3.1.4 Prompt Engineering And Ensembling</a></li><li><a href="#315-analysis-of-zero-shot-clip-performance" class="table-of-contents__link toc-highlight">3.1.5 Analysis of Zero-Shot CLIP Performance</a></li></ul></li><li><a href="#32-representation-learning" class="table-of-contents__link toc-highlight">3.2 Representation Learning</a></li><li><a href="#33-robustness-to-natural-distribution-shift" class="table-of-contents__link toc-highlight">3.3 Robustness to Natural Distribution Shift</a></li><li><a href="#71-bias" class="table-of-contents__link toc-highlight">7.1 Bias</a></li><li><a href="#72-surveillance" class="table-of-contents__link toc-highlight">7.2 Surveillance</a></li><li><a href="#73-future-work" class="table-of-contents__link toc-highlight">7.3 Future Work</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/whdnjsdyd111/whdnjsdyd111.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 WonYongLab, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.082cf41b.js"></script>
<script src="/assets/js/main.26b4fb6b.js"></script>
</body>
</html>