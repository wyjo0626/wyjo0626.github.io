<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-03-VP">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Exploring Visual Prompts for Adapting Large-Scale Models | WYJLab</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/VP"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Exploring Visual Prompts for Adapting Large-Scale Models | WYJLab"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/WYJLab.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/VP"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/VP" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/VP" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="WYJLab RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="WYJLab Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.9ed678b7.css">
<link rel="preload" href="/assets/js/runtime~main.0dfd96f9.js" as="script">
<link rel="preload" href="/assets/js/main.2a692bef.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/WYJLab.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Unsupervised Prompt Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Prismer">Prismer: A Vision-Language Model with An Esemble of Experts</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Contrastive Learning/ALIGN">Contrastive Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Composition/CLIP-LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Composition/CLIP-LoRA">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Module/CLIP-Adapter">Module</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Multi-Modality/MaPLe">Multi-Modality</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/CMAR">Prompting</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/CMAR">Pixel-Level</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/CMAR">Cross-modal Adversarial Reprogramming</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/VP">Exploring Visual Prompts for Adapting Large-Scale Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/Watermarking">Watermarking for Out-of-distribution Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/ILM-VP">Understanding and Improving Visual Prompting: A Label-Mapping Perspective</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/EVP">Unleashing the Power of Visual Prompting At the Pixel Level</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/DAM-VP">Diversity-Aware Meta Visual Prompting</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/EVP-L">Explicit Visual Prompting for Low-Level Structure Segmentations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/HinTAug">Hint-Aug: Drawing Hints from Foundation Vision Transformers towards Boosted Few-shot Parameter-Efficient Tuning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/AutoVP">AutoVP: An Automated Visual Prompting Framework and Benchmark</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/A2XP">A2XP: Towards Private Domain Generalization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/InMeMo">Instruct Me More! Random Prompting for Visual In-Context Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/LaViP">LaViP: Language-Grounded Visual Prompts</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/AdaViPro">AdaViPro: Region-Based Adaptive Visual Prompt For Large-Scale Models Adapting</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/TVP">Exploring the Transferability of Visual Prompting for Multimodal Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-6 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/SMM">Sample-specific Masks for Visual Reprogramming-based Prompting</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Textual-Token/CoOp">Textual-Token</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/VPT">Visual-Token</a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Single-Stream/VLBERT">Single-Stream</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Two-Stream/LXMERT">Two-Stream</a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Prompting</span><meta itemprop="position" content="4"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Pixel-Level</span><meta itemprop="position" content="5"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Exploring Visual Prompts for Adapting Large-Scale Models</span><meta itemprop="position" content="6"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Exploring Visual Prompts for Adapting Large-Scale Models</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2203.17274" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2203.17274</a></p><h1>Abstract</h1><p>lage-scale models 을 vision 에 적응시키기 위해 <em>visual prompt</em> 효과를 조사</p><ul><li>prompt tuning 과 adversarial reprogramming 의 최근 방법을 따라, frozen model 이 perturbation 을 통해 new task 를 수행하도록 single image perturbation 을 학습.</li><li>포괄적인 실험으로 visual prompt 가 CLIP 에 특히 효과적이며, distribution shift 에 강하고 standard linear probes 와 competitive performance 에 달성함을 입증.</li><li>저자는 downstream dataset 의 속성, prompt design 및 output transformation 이 adaptation performance 에 미치는 영향을 추가로 분석</li><li>visual prompt 의 놀라운 효과는 vision 분야의 pre-trained model 을 적응시키는 새로운 관점을 제공</li></ul><h1>1. Introduction</h1><p>new task 학습 시, current knowledge 에서 이를 확장하는 경향이 있다. 예로, 문장에 대해 이해하는 능력을 키우는 것으로, &quot;학교 버스를 놓쳤어요&quot; 라는 문장은 특정한 감정을 전달하며, 그 뒤에 &quot;그래서 나는 <!-- -->[MASK]<!-- --> 느꼈어요&quot; 라는 문장이 오면, 적절한 감정 단어를 제공할 수 있다. </p><p>이 패러다임은 <em>prompting</em> 으로, 최근 NLP 에서 large-scale language models 를 new task 에 적응시키기 위해 downstream dataset 을 pre-training task 의 형식으로 변환하는 방식이 인기다.</p><p>parameters 를 업데이트하지 않고도 모델에 제공된 prompt 의 mask 를 채우기 위해 기존 knowledge base 를 사용하여 new task 의 expert 가 된다.</p><p>prompting 은 NLP 에 특화되어, 다음 목적을 가지고 있다: <em>data space</em> 를 수정하여 frozen pre-trained model 를 적응시키는 것이다. 이 일반성을 고려할 때, <em>pixel</em> 혙애를 고려하여, pixel space 를 수정하여 frozen visual models 가 new task 를 해결할 수 있는지 본다.</p><hr><p>adversarial reprogramming 은 input perturbations 를 통해 model 은 adversary 로 선택한 task 를 수행하도록 재목적화하는 일종의 adversarial attack 이다.</p><p>서로 다른 용어와 동기를 가지고 있지만, 이러한 input perturbations 은 본질적으로 visual prompt 역할을 하여 pixel 을 수정하여 모델을 new task 에 적응시킨다.</p><p>기존의 방법들은 adversarial goals 에 집중하거나 small-scale 과 model 에 제한적으로 적용되었다.</p><p>서로 다른 커뮤니티에서 기원한 adversarial reprogramming 과 prompting 은 input (i.e., prompt enginerring) and/or output (i.e., answer engineering) transforming 을 통해 <em>data-space adaptation</em> 을 수행한다는 일반적인 아이디어를 공유한다.</p><hr><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-6c1d29bcbacf6bf3dc76b09b6ae5d39b.png" width="1015" height="715" class="img_ev3q"></p><p>NLP prompting 에 영감을 받아, 저자는 large-scale model 을 vision 분야에 적응시키기 위해 <em>visual prompting</em> 의 효과을 조사하고자 한다</p><ul><li>pixel space 가 본질적으로 continuous 하기 때문에, prompt 는 continuous task-specific vector 로 취급하는 최근 접근 방식을 따른다</li><li>model parameter 를 freezing 한 상태에서 single image perturbation(i.e., &quot;soft prompt&quot;)를 backpropagation 을 통해 학습</li><li>저자는 CLIP 을 위해 discrete text prompt 를 사용하고 vision model 을 위해 hard-coded mapping 을 사용하여 model output 을 downstream label 에 mapping 한다</li></ul><hr><p>visual prompting 은 기존 adaptation 과 다른 것은, 현재 vision 분야의 standard adaptation 방법은 fine-tuning 과 linear brobe 이다.</p><p>두 방식 모두 모델에 어느 정도 접근이 필요: fine-tuning 의 경우 entire parameters, linear probe 의 경우 model output(보통 penulimate layer 의 activation).</p><p>반면, visual prompting 은 model input 을 적응시킨다. </p><ul><li>visual prompting 획득 후, test 시 model 접근이 필요 없음<ul><li>이는 시스템의 최종 사용자가 통제할 수 있는 고유 application 을 가능케 함</li><li>예로, 보행자는 visual prompting 을 착용하여 자동차에 대한 가시성을 높일 수 있으며, 자동차 자체나 그 vision system 에 접근할 필요가 없다</li></ul></li></ul><p>저자는 4 pre-trained models 및 15 image classification dataset 을 통해 포괄적 실험 수행.</p><p>visual prompting 이 CLIP 에 대해 놀라운 효과를 보이며 distribution shift 에 robust 하며, standard linear probe 와 competitive performance 를 달성함을 입증</p><p>저자는 downstream dataset 의 속성, prompt design 및 output transformation 이 성능에 미치는 영향을 추가로 분석</p><p>목표는 specific task 에 SOTA 달성이 아닌, vision 분야에서 pre-trained model 을 적응시키는 새로운 패러다임을 널리 탐구하는 것</p><p>visual prompting 의 놀라운 효과는 vision 에서 pre-trained model 을 적용시키고 사용하는 새로운 관점을 제공</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-related-work">2. Related Work<a href="#2-related-work" class="hash-link" aria-label="Direct link to 2. Related Work" title="Direct link to 2. Related Work">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-natural-language-prompting">2.1 Natural Language Prompting<a href="#21-natural-language-prompting" class="hash-link" aria-label="Direct link to 2.1 Natural Language Prompting" title="Direct link to 2.1 Natural Language Prompting">​</a></h2><p>NLP prompting 에 영감을 받았으며 NLP 의 prompting 은 downstream dataset 을 (masked) language modeling 문제로 재구성하여, frozen language model 이 parameters 를 업데이트하지 않고도 new task 에 직접 적응할 수 있도록 함. </p><p>prompt 는 task-specific <em>template</em> (e.g., &quot;나는 너무 <!-- -->[MASK]<!-- -->을 느꼈다&quot;)과 label word(e.g., &quot;행복/무서움&quot;)를 구성하여 공백을 채우는 것을 포함한다.</p><p>그러나 적절한 prompt 를 만드는 것은 domain expert 와 상당한 노력이 필요하다.</p><hr><p>prefix tuning 또는 prompt tuning 은 model parameters 를 freezing 한 상태에서 backpropagation 을 통해 &quot;soft prompt&quot; 를 학습하여 문제를 완화한다.</p><p>prefix tuning 은 language model 이 다양한 generation task 에 적응할 수 있도록 task-specific continuous vector (i.e. prefix) 를 학습한다.</p><p>prefix tuning 은 이 prefix 를 각 encoder layer 에 처리하는 반면, prompt tuning 은 tunable token 을 input 에만 처리하여 단순화한다.</p><p>Billion 급 parameters 를 가진 large-scale model 에 적용될 때, 적절히 최적화된 prompt 는 model full fine-tuning 과 competitive performance 를 달성하면서도 메모리 사용량과 작업별 저장소를 크게 줄인다.</p><p>pixel space 의 prompt 는 본질적으로 continuous 하여, 저자는 이 작업의 연장선상에서 pixel 을 직접 최적화한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-prompting-with-images">2.2 Prompting with Images<a href="#22-prompting-with-images" class="hash-link" aria-label="Direct link to 2.2 Prompting with Images" title="Direct link to 2.2 Prompting with Images">​</a></h2><p>image prompting 을 시도한 초기 방법이 있다. prefix tuning 과 유사하게, </p><p>Frozen 은 vision encoder 를 학습시켜 frozen language model 로부터 gradient 를 받아 image-conditional prompt 를 만든다. image 는 vision encoder 에서 continuous embedding 으로 표현되어, frozen language model 이 multimodal task 를 수행할 수 있도록 <em>visual prefix</em> 로 사용된다. </p><p>CPT 는 color block 과 color-based textual prompts 를 사용하여 visual prompt 를 만들고, visual grounding 을 빈칸 채우기 문제로 변환한다. </p><p>그러나 이 방법들은 language-based model 의 능력을 확장하는 데 초점을 맞추고 있다. 반면, 저자는 visual representation 과 image classification dataset 에 대해 prompting 의 효율성을 조사하는데 중점을 둔다.</p><p>즉, pre-trained model 이 vision encoder 로 구성되어 있다 가정하고 image dataset 을 재구성하는데 집중한다.</p><p>Visual prompt tuning 은 Vision Transformers 에 특화된 visual prompt 를 제안하는 동시적 연구다. </p><p>이 방법은 각 Transformer encoder layer 에 tunable parameters set 을 전처리하여 deep prompt tuning 을 사용한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-adversarial-reprogramming-and-unadversarial-examples">2.3 Adversarial Reprogramming and Unadversarial Examples<a href="#23-adversarial-reprogramming-and-unadversarial-examples" class="hash-link" aria-label="Direct link to 2.3 Adversarial Reprogramming and Unadversarial Examples" title="Direct link to 2.3 Adversarial Reprogramming and Unadversarial Examples">​</a></h2><p> Adversarial reprogramming 은 single, class-agnostic perturbation 이 모델을 attacker 로 선택한 new task 를 수행하도록 reprogram 하는 adversarial attack 의 일종이다. adversarial goal 에도 불구하고, 이 framework 는 본질적으로 prompting 과 동일한 목적을 제공한다: downstream dataset 의 input and/or output 을 수정하고 frozen model 을 new task 에 적응시키는 것이다. </p><p>그러나 vision 의 기존 방법은 adversarial goal 을 달성하거나 small-scale vision model 과 simple dataset 에 대한 제한된 응용을 보여준다. 마찬가지로, unadversarial examples 는 (pre-)trained task 의 성능을 향상시키는 것을 목표로 함.</p><p>이는 specific class(i.e., classs-conditional)에 대한 성능을 향상시키는 image perturbation 을 학습한다.</p><p>저자의 연구에선 adversarial reprogramming 을 visual prompt 의 한 형태로 재조명하고, large-scale vision model 을 적응시키는데 효율성을 조사한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="24-adapting-pre-trained-models-in-vision">2.4 Adapting Pre-trained Models in Vision<a href="#24-adapting-pre-trained-models-in-vision" class="hash-link" aria-label="Direct link to 2.4 Adapting Pre-trained Models in Vision" title="Direct link to 2.4 Adapting Pre-trained Models in Vision">​</a></h2><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-1-855527055e9d028a360a0c9276b226d5.png" width="893" height="620" class="img_ev3q"></p><p>Fig. 2 는 pre-trained model 을 적응시키는 방법을 요약</p><p>fine-tuning 및 linear probing 은 new domain 이나 다른 output 의미를 가진 new task 에 적응시키는데 flexible 하게 사용 가능</p><p>하지만 이 방법들은 어느 정도의 모델 접근이 필요: fine-tuning 에는 parameters, linear probing 의 경우 model output(일반적으로 penultimate layer 이전의 activations).</p><p>domain adaptation 은 model adaptation 에 대한 흥미로운 대안으로, image-to-image translation 와 같은 기술을 사용하여 model 의 input 만 수정한다.</p><p>domain adaptation 과 마찬가지로 visual prompting 도 모델의 입력을 수정한다. 따라서 사용자가 visual prompt 를 발견한 후 test 시 모델 자체에 대한 제어가 필요하지 않다.</p><p>이는 고유한 응용 프로그램을 열어준다. 예로, 사용자는 input 만으로 조작할 수 있는 online API 에 domain adapted image 를 제공할 수 있다.</p><p>domain adaptation 은 source domain 을 target domain 처럼 보이도록 adapting 하는데 중점을 두며, source 와 target dataset 을 모두 사용할 수 있어야 한다.</p><p>반면, 저자는 visual prompting 이 모델을 더 임의 조정이 가능함을 보여준다. 예로, 한 classification task 를 수행하는 model 이 input pixel 을 perturbate 하는 것 만으로도 완전히 다른 classification task 를 수행하도록 적응할 수 있다.</p><p>또한 domain adaptation 방법은 일반적으로 input-conditional 인 반면, 이 논문에서 탐구하는 visual prompt 는 NLP 와 같이 frozen (i.e., input-agnostic) prompt 가 전체 dataset 에 걸쳐 추가된다.</p><h1>3. Methods</h1><p>prompting 과 adversarial reprogramming 이 같은 목적을 달성: data-space adaptation</p><p>일반적으로 두 단계로 구성한다.</p><ul><li>input transformation (prompt engineering) : input 에 적용될 task 를 지정하는 적절한 prompt 설계</li><li>output transformation (answer engineering) : model 의 output/answer 을 target label 에 mapping</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-pre-trained-models">3.1 Pre-trained Models<a href="#31-pre-trained-models" class="hash-link" aria-label="Direct link to 3.1 Pre-trained Models" title="Direct link to 3.1 Pre-trained Models">​</a></h2><p>pixel prompt 는 모든 visual representation 에 적용 가능.</p><p>저자는 세 가지 vision model 과 하나의 vision-language 를 선택한다</p><ul><li>Instagram 에서 pre-training 한 ResNetXt (Instagram)</li><li>Big Transformer (BiT-M)</li><li>ImageNet-1K 에서 pre-training 한 ResNet (RN50)</li><li>CLIP</li></ul><p>vision model 은 fixed set 의 pre-determined classes 를 예측하도록 훈련되어, unseen classes 를 예측하기 위해 별도의 layer 를 학습해야하는 반면, CLIP 은 text prompt 로 unseen class 에 유연하게 zero-shot trasnfer 을 수행할 수 있는 vision-language model 이다.</p><p>Instagram 에서 pre-training 한 ResNeXt 의 경우, ImageNet-1k 에서 추가로 fine-tuned model 을 사용.</p><p>저자는 다음과 같이 요약하며, 다양한 input modality, pre-trained dataset size, model architecture 등.</p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-2-a6092cb0d3f8364b3bc7716e70a4e316.png" width="1781" height="484" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-input-transformation">3.2 Input Transformation<a href="#32-input-transformation" class="hash-link" aria-label="Direct link to 3.2 Input Transformation" title="Direct link to 3.2 Input Transformation">​</a></h2><p>visual prompt 설계엔 여러 가지가 있을 수 있다.</p><p>pixel space 는 자연어보다 덜 discrte 하기 때문에, 손수 만드는 것이 어렵다.</p><p>실제로 각 downstream task 에 유용한 visual context 가 어떤 것인지도 명확하지 않다.</p><p>직관적으로, visual prompt 는 반드시 interpretability 할 필요는 없다; 이는 기계 학습 모델의 결정을 돕는 시각적 단서임.</p><p>따라서 모델이 visual context 를 최적화하도록 한다.</p><p>저자는 visual prompt 를 backpropagation 으로 직접 최적화하는 간단한 gradient-based approach 를 따른다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="321-prompt-tuning">3.2.1 Prompt Tuning<a href="#321-prompt-tuning" class="hash-link" aria-label="Direct link to 3.2.1 Prompt Tuning" title="Direct link to 3.2.1 Prompt Tuning">​</a></h3><p>frozen pre-trained model <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span></span></span></span> 및 downstream task dataset <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">D</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>m</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{D} = \{ (x_1, y_1), \dots, (x_m, y_m) \}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathcal" style="margin-right:0.02778em">D</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)}</span></span></span></span></span> 이 주어졌을 때, 목표는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ϕ</span></span></span></span></span> 로 parameterize 된 single, task-specific visual prompt <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>ϕ</mi></msub></mrow><annotation encoding="application/x-tex">v_\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 를 학습하는 것이다.</p><p>prompt 는 input image 에 추가되어 prompted image <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>+</mo><msub><mi>v</mi><mi>ϕ</mi></msub></mrow><annotation encoding="application/x-tex">x + v_\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span></span> 를 형성한다.</p><p>training 중, model 은 correct label <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 의 likelihood 를 maximizing 한다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>ϕ</mi></munder><msub><mi>P</mi><mrow><mi>θ</mi><mo separator="true">;</mo><mi>ϕ</mi></mrow></msub><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>+</mo><msub><mi>v</mi><mi>ϕ</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} \max_{\phi} P_{\theta;\phi}(y|x + v_\phi), \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.7282em;vertical-align:-0.6141em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1141em"><span style="top:-3.2741em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em"><span style="top:-2.3479em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8882em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em">θ</span><span class="mpunct mtight">;</span><span class="mord mathnormal mtight">ϕ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6141em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1141em"><span style="top:-3.1141em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6141em"><span></span></span></span></span></span></span></span></span></div><ul><li>이때 gradient update 는 prompt parameters <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal">ϕ</span></span></span></span></span> 에만 적용되며 model parameters <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span></span> 는 freezing</li></ul><p>evaluation 중, optimized prompt 는 all test-time images 에 추가된다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>X</mi><mtext>test</mtext></msub><mo>=</mo><mo stretchy="false">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>v</mi><mi>ϕ</mi></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo>+</mo><msub><mi>v</mi><mi>ϕ</mi></msub><mo stretchy="false">}</mo><mo separator="true">,</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} X_{\text{test}} = \{x_1 + v_\phi, ... , x_n + v_\phi\}, \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">test</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">}</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-2.85em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span></span></div><ul><li>그리고 이는 frozen model <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span></span></span></span> 를 통해 처리</li></ul><p>목표는 visual prompt 를 실용적인 적응 방법으로 탐색하는 것.</p><p>따라서 perturbations 를 감지할 수 없게 하는 어떠한 adversarial constraint 도 필요하지 않다. 또한 adversarial reprogramming 은 downstream dataset 이 pre-trained dataset 보다 lower-resolution 을 갖도록 가정하여, input perturbations 가 downstream dataset 주위에 padding 된다.</p><p>실제 applications 는 downstream dataset 의 resolution 이 다양할 수 있어, 모든 dataset 을 pre-trained model 의 input size 로 resizing 하고 prompt 를 input 영역에 직접 추가한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="322-prompt-design">3.2.2 Prompt Design<a href="#322-prompt-design" class="hash-link" aria-label="Direct link to 3.2.2 Prompt Design" title="Direct link to 3.2.2 Prompt Design">​</a></h3><p>visual prompt 를 template 및 size 측면에서 여러 설계 방법이 있음</p><p>세 가지 visual template 을 다룸</p><ul><li>random location 의 pixel patch</li><li>fixed location 의 pixel patch</li><li>padding</li></ul><p>various prompt sizes <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span></span></span></span></span> 를 탐구하며, 실제 parameter 수는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><msup><mi>p</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">Cp^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>, padding 의 경우 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>C</mi><mi>p</mi><mo stretchy="false">(</mo><mi>H</mi><mo>+</mo><mi>W</mi><mo>−</mo><mn>2</mn><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">2Cp(H+W-2p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2</span><span class="mord mathnormal">Cp</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">2</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></span> 이다.</p><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">C, H, W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.13889em">W</span></span></span></span></span> 는 각각 image channel, height 및 width</li><li>section 6.2 에서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">p = 30</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">30</span></span></span></span></span> padding 이 다른 설계에 비해 best performance 를 보여준다.<ul><li>저자는 이를 모든 실험의 기본값으로 설정</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-output-transformation">3.3 Output Transformation<a href="#33-output-transformation" class="hash-link" aria-label="Direct link to 3.3 Output Transformation" title="Direct link to 3.3 Output Transformation">​</a></h2><p>model output 을 target label 로 mapping 하기 위해 vision model 및 CLIP 은 다른 방식 사용</p><ul><li>standard vision model 은 image class 를 number ID 로 취급 (e.g., &quot;cat&quot; 은 &quot;index 1&quot;)<ul><li>hard-coded mapping 을 사용하여 downstream class index 를 pre-trained class index 에 임의로 mapping 하고, unassigned indeces 는 loss computation 에서 제외</li></ul></li><li>CLIP 같은 vision-language model 의 경우, text prompt 를 output transformation function 으로 사용<ul><li>image class 는 text 로 표현되며(e.g., &quot;cat&quot;), downstream task 의 context 를 지정하기 위해 prompt 추가(e.g., &quot;a photo of a <!-- -->[object]<!-- -->&quot;). </li><li>single, fixed text prompt 를 사용하고 visual prompt 만 최적화</li><li>CLIP zero-shot transfer protocol 을 따르고 각 class 에 대한 embedding cosine similarity 를  계산하며, 이는 softmax 를 통해 probability distribution 으로 normalizing</li><li>가장 높은 확률을 가진 class 가 model output 으로 선택</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-implementation-details">4. Implementation Details<a href="#4-implementation-details" class="hash-link" aria-label="Direct link to 4. Implementation Details" title="Direct link to 4. Implementation Details">​</a></h2><p>visual prompt 학습을 위한 CLIP 의 objective function 은 evaluation setting 과 동일.</p><ul><li>즉, prompted text string 이 text encoder 를 통해 처리되어 linear classifier 의 weight 를 생성하며, 저자는 image 에 대해 cross entroy loss 만 계산한다.</li><li>vision model 의 경우, new class indices 에 대해 cross entropy loss 계산</li></ul><p>모든 실험에서,</p><ul><li>prompt size 30 의 padding template 사용</li><li>모든 image 는 pre-trained model 의 input size 와 일치하도록 224 × 224 조정</li><li>각 모델의 evaluation setting 과 동일하게 전처리</li><li>pre-trained model 의 evaluation setting 을 면밀히 따르는 것이 good prompt 학습에 중요함을 발견</li><li>all visual prompt 는 1,000 epochs 동안 훈련</li><li>learning rate 40 의 SGD<ul><li>이는 cosine schedule 을 사용하여 감쇠</li></ul></li><li>batch size 는 CLIP 의 경우 256, BiT-M 과 RN50 의 경우 128, Instagram 의 경우 32</li></ul><h1>4. Experimental Setup</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-datasets">4.1 Datasets<a href="#41-datasets" class="hash-link" aria-label="Direct link to 4.1 Datasets" title="Direct link to 4.1 Datasets">​</a></h2><p>visual prompt 가 new tasks 에 model 을 얼마나 잘 적응시키는지 평가하기 위해 12 dataset 에서 측정</p><ul><li>CIFAR100, CIFAR10, Flowers102, Food101, EuroSAT, SUN397, DTD, UCF101, SVHN, OxfordPets, Resisc45, 및 CLEVR</li></ul><p>또한, distributtion shift 에 대한 robustness 측정을 위해, 즉 training distribution 및 test distribution 이 다를 때, WILDS 의 세 가지 image classification dataset 에 평가</p><ul><li>Camelyon17, FMoW, 및 iWildCAM<ul><li>Camelyon17의 경우, training 및 test set 은 다른 병원에서 가져온 조직 패치로 구성</li><li>FMoW 의 경우, training 및 test set 은 다른 지역 및 년도로부터 가져옴</li><li>iWildCAM 은 서로 다른 set 의 camera trap 에서 찍은 사진으로 구성</li><li>training set 에서 visual prompt 를 학습하고 test set 에서 성능 평가</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-baseline-models">4.2 Baseline Models<a href="#42-baseline-models" class="hash-link" aria-label="Direct link to 4.2 Baseline Models" title="Direct link to 4.2 Baseline Models">​</a></h2><p>visual prompting 적응 방법과 비교하여 성능이 얼마나 좋은지 측정을 위해, fine-tuning, linear probe 및 text prompt (i.e., zero-shot transfer) 과 비교.</p><ul><li>fine-tuning 및 linear probe 는 vision 의 standard adaptation method<ul><li>fine-tuning : adaptation 중 full model parameters 업데이트</li><li>linear probe : model parameters 를 freezing 한 채 linear layer 를 학습하여 model output (보통 penultimate layer 의activations) 을 adapting</li><li>text prompting : &quot;This is a photo of a <!-- -->[LABEL]<!-- -->&quot; 을 default <ul><li>CLEVR 의 경우 &quot;This is a photo of <!-- -->[LABEL]<!-- --> objects&quot; 를 사용하며, class label 은 &quot;three&quot; 부터 &quot;ten&quot; 까지</li><li>Camelyon17 의 경우, &quot;a tissue region <!-- -->[LABEL]<!-- --> tumor&quot; 를 사용하며, class label 은 &quot;containing&quot; 과 &quot;not containing&quot;</li></ul></li></ul></li></ul><h1>5. Results</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-effectiveness-of-clip">5.1 Effectiveness of CLIp<a href="#51-effectiveness-of-clip" class="hash-link" aria-label="Direct link to 5.1 Effectiveness of CLIp" title="Direct link to 5.1 Effectiveness of CLIp">​</a></h2><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-3-6cf313ae9275074c8f4a5c956705a6b3.png" width="1154" height="1207" class="img_ev3q"></p><p>먼저 prompting 을 lightweight adaptation 인 linear probe 와 비교.</p><p>Fig. 3 은 pre-trained model 에 대한 12 datasets 에서 average test accuracy 를 보여줌.</p><ul><li>vision model 또는 adversarial reprogramming 을 사용한 Prompting 은 standard linear probe 에 비해 상당한 성능 차이 (+40%) 를 보임.</li><li>반면, prompting 은 CLIP 에서 놀라운 효과를 보이며 linear probe 와 competitive performance 를 보여줌</li></ul><p><img loading="lazy" alt="Table 1" src="/assets/images/image-4-3776ff46e7b011dd5c6594a7d2968fcf.png" width="1708" height="541" class="img_ev3q"></p><ul><li>Prompting 은 EuroSAT, SVHN 및 CLEVR 에서 linear probe 를 각각 1.1%, 23%, 15.4% 초과</li><li>평균적으로, visual prompt 학습은 text prompt 사용 (i.e., zero-shot transfer)에 비해 24% 성능 향상</li></ul><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-5-9a6a818b62354e541f2340ba50381b32.png" width="1331" height="1299" class="img_ev3q"></p><p>흥미로운 점은, visual prompt 성능은 dataset 에 따라 달랐음</p><p>이 현상에 대해, dataset 특성이 성능에 미치는가 section 6.1 에서 다룸.</p><p>12 dataset 에 대한 전체 결과는 다음과 같다.</p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-6-ab408e437deeb9a736332c22b99b0475.png" width="2014" height="768" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-robustness-to-distribution-shift">5.2 Robustness to Distribution Shift<a href="#52-robustness-to-distribution-shift" class="hash-link" aria-label="Direct link to 5.2 Robustness to Distribution Shift" title="Direct link to 5.2 Robustness to Distribution Shift">​</a></h2><p>model parameters 는 freezing 된 상태로 유지되어, prompting 은 pre-trained model 의 general knowledge 수정을 방지함</p><p>이는 downstream dataset 의 spurious correlations 에 overfitting 될 가능성을 줄이며, distribution shift 에 대한 robustness 를 향상시킴.</p><ul><li>WILDS benchmark 를 사용하여, particular domain 의 image 를 포함하는 training set 에서 visual prompt 를 학습하고, different domains (e.g., hospitals, regions, yaers, cameras 의 images) 가 포함된 test set 으로 transfer 하는 방법을 봄</li></ul><p><img loading="lazy" alt="Table 2" src="/assets/images/image-7-c88b26da5ae6e90000f519b393144695.png" width="1283" height="607" class="img_ev3q"></p><p>Table 2 는 </p><ul><li>Linear Probe 및 Fine Tuning 과 비교한 average performance gap 이 각각 4.5% 및 3.5% 로 더 감소했음을 보여줌</li><li>Camelyon17 에서는 Visual Prompt 방식이 Linear Probe 와 Fine Tuning 을 각각 4.9% 및 6.5% outperform</li><li>이는 다양한 범위의 domain shift 가 발생하는 real-world deployment 에서 prompting 이 실용적 유용성을 시사</li></ul><p>vision model 에 대한 robustness results 는 다음과 같다.</p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-8-14a632c17bcd55b2f12eb2f70ba20c01.png" width="1624" height="631" class="img_ev3q"></p><h1>6. Understanding Visual Prompts</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="61-downstream-dataset">6.1 Downstream Dataset<a href="#61-downstream-dataset" class="hash-link" aria-label="Direct link to 6.1 Downstream Dataset" title="Direct link to 6.1 Downstream Dataset">​</a></h2><p>저자는 visual prompting 성능이 downstream dataset 에 따라 다양한 것을 발견</p><p>Fig. 4 에서 보이듯,</p><ul><li>가장 성능 좋은 dataset 은 83.2% 의 accuracy 향상을 달성한 반면,</li><li>성능이 가장 저조한 dataset 은 1% 의 accuracy loss 를 보여줌</li></ul><p>위 현상 설명을 위해, 저자는 visual prompting 이 <em>unfamiliar</em> downstream dataset 을 pre-trained dataset 과 더 유사하게 변환함으로써 distribution gap 을 해소한다고 가정</p><p>이 가설에 따르면, visual prompt 는 pre-traine distribution 내에 있는 dataset 엔 도움이 되진 않지만, out-of-distribution 의 dataset 에는 도움이 될 수 있다</p><p>CLIP 의 pre-trained dataset 은 공개되지 않았지만, ImageNet 의 SOTA zero-shot 성능 달성을 위해 excessively tuning 됨</p><p>따라서 저자는 ImageNet 을 proxy 로 사용</p><p>저자는 FID score 를 사용하여 ImageNet 과 downstream dataset 간의 distribution similarity 을 측정함으로써 가설을 검증.</p><p>이러한 score 를 visual prompting 의 accuracy 향상과 비교. </p><p>computation limitation 으로 인해, ImageNet-1k training set 에서 100k images 를 randomly sampling 하여 metric 계산</p><p><img loading="lazy" alt="Figure 5" src="/assets/images/image-9-e0a3d9a6199ba7f0115b4840eed036f0.png" width="1300" height="1319" class="img_ev3q"></p><p>Figure 5 에서 </p><ul><li>downstream dataset 이 ImageNet 과 더 out-of-distribution 일수록 일반적인 성능 향상이 관찰된다. (e.g. CLEVR, SVHN)</li></ul><hr><p>다른 가설은 각 dataset 당 single prompt 를 학습하는 것과 관련 있다.</p><p>low perceptual diversity 를 가진 dataset 에는 충분할 수 있지만, diversity 증가에 따라 single visual prompt 는 full distribution 을 capture 하는데 실패할 수 있다.</p><p>저자는 LPIPS 를 사용하여 perceptual diversity 를 측정한다.</p><p>각 dataset 에 대해, two randomly sampled image pairs 간의 LPIPS 를 측정하고 average score 를 report</p><p>Fig. 5 에서</p><ul><li>perceptual diversity 가 낮은 dataset 에 대해 sigle prompt 학습이 더 나은 향상을 보여줌</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="62-prompt-design">6.2 Prompt Design<a href="#62-prompt-design" class="hash-link" aria-label="Direct link to 6.2 Prompt Design" title="Direct link to 6.2 Prompt Design">​</a></h2><p>right prompt 설계 선택 (i.e. template 및 size)는 성능에 큰 영향을 미칠 수 있다.</p><p>세 가지 template: random location 의 pixel patch, fixed location 의 pixel patch 및 padding 에 대해 prompt size <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">p = 1, \dots, 224</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">224</span></span></span></span></span> 에 대한 ablation sdudy 수행</p><p>frozen CLIP 을 사용항 EuroSAT dataset 의 accuracy 를 측정</p><p><img loading="lazy" alt="Figure 6" src="/assets/images/image-10-ea978eacdc67d70545b88b9ea921d447.png" width="1116" height="1090" class="img_ev3q"></p><p><img loading="lazy" alt="Figure 7" src="/assets/images/image-11-bf54dc64bb796e52bf1057c3b5168ac6.png" width="839" height="985" class="img_ev3q"></p><p>Figure 6.2 는 fixed-location template (i.e., padding, fixed patch)를 사용하는 것이 더 나은 성능을 보임</p><ul><li>fixed-location template 의 경우, prompt size 증가에 따라 (i.e., more trainable parameters) 성능이 향상되다가 +70k parameters 에서 성능이 하락하기 시작</li><li>놀랍게도, simplest approach - <em>single</em>-pixel prompt 추가 - 는 text-prompted CLIP 보다 3% 향상된 성능을 낼 수 있다는 것을 발견 (그림 7)</li><li>전반적으로, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">p = 30</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">30</span></span></span></span></span> 의 padding 이 실험에서 가장 좋은 성능을 보여줌<ul><li>이는 응용 범위가 image classification 이며, interset tends object 가 image center 에 위치하였기 때문이라고 생각</li></ul></li><li>other visual tasks 에는 상당히 different design choices 가 필요할 수 있다고 생각된다</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="63-output-transformation">6.3 Output Transformation<a href="#63-output-transformation" class="hash-link" aria-label="Direct link to 6.3 Output Transformation" title="Direct link to 6.3 Output Transformation">​</a></h2><p>output transformation 을 어떻게 설계하냐에 따라 prompting performance 조사</p><p>vision model 의 경우, hard-coded mapping 을 사용.</p><p>downstream task indices 는 pre-trained class indices 에 임의로 할당된다.</p><p>이 mapping 이 downstream performance 에 어떤 영향을 미치는지 분석한다.</p><ul><li>OxfordPets 의 downstream subset 을 사용하여 dogs 및 cats classifying 의 simple toy dataset 을 구성.</li><li>ImageNet-1k 에서 pre-training 된 ResNet(RN50) 을 사용하여 두 경우를 비교: </li></ul><p>1) downstream class 가 유사한 의미를 가진 pre-trained class 에 할당되는 경우 (unseen &quot;dog&quot; 가 pre-trained &quot;chihuahua&quot; index 에 할당됨)</p><p>2) indices 를 swap (cat 이 dog index 에 할당되고 그 반대의 경우). (1) 은 100% 를 달성했고 (2) 는 62.5% 를 달성했다; class indices 간의 similar semantics 가 성능에 중요하다. 이는 vision model 과 CLIP 간의 성능 격차를 설명 가능</p><p>CLIP 의 경우, output transformation 에 text prompt 사용</p><p>backpropagation 을 통해 visual prompts 를 학습하므로, learning signal 은 저자가 사용한 text prompt 에 따라 달라짐.</p><p>CLIP 의 zero-shot accuracy 는 더 나은 text prompt 를 사용하여 크게 향상된다 보고된 바 있음</p><p>따라서, text prompt 의 품질이 visual prompt 의 성능에 영향을 미친다고 가정한다. </p><p>EuroSAT 에서, CLIP 의 zero-shot performance 에 따라 text prompt 품질을 측정한다. </p><p><img loading="lazy" alt="Figure 8" src="/assets/images/image-12-6efb9de3fa1170da7e3d4ede2e6658e7.png" width="1174" height="1262" class="img_ev3q"></p><p>Fig. 8 은 visual prompt 의 성능 향상이 zero-shot performance 가 low zero-shot performance 의 text prompt 에서 더 높다는 것을 보여줌. 즉, visual prompt 는 low-quality text prompt 를 보완할 수 있다.</p><p>best text prompt 를 수동으로 찾는 것은 매우 번거로운 일이기 때문에, 이 결과는 visual prompt 의 유용성을 강조</p><h1>7. Discussion</h1><p>본 논문은 pre-trained model 의 input transformation 으로 classification accuracy 개선 방법을 조사.</p><p>visual prompting 의 넓은 해석은 input space 를 수정하여 pre-trained model 을 <em>any</em> direction 으로 tuning 하는 방법을 생각할 수 있다.</p><p>예로, image-to-image model 에 대한 visual prompt 는 input visual style 을 변경하는데 사용할 수 있다.</p><p>이 연구는 &quot;universal&quot; visual prompt 탐구이지만 (i.e., all input image 에 적용되는 single prompt), prompt 는 input-conditional 로 만들 수 있으며 이는 less universal 이지만 더 정확할 수 있다.</p><p>(a) input-specific or input-agnostic, (b) accuracy 향상 또는 감소, (c) pre-trained model 의 유형과 같은, 구체적인 설계 선택 사항들은 prompt 의 미래 응용 프로그램을 창출하기 위해 수정될 수 있다.</p><p>저자가 전달하고자 하는 자연스러운 질문은, 어떤 상황에서 visual prompting 을 선호할 수 있는지 이다.</p><p>Fine-tuning 은 model 수정이 가능하다 가정하지만 항상 그런 것은 아닐 수 잇다. (e.g., model 이 제 3자가 소유한 API 로 노출될 경우)</p><p>prompting 은 몇몇 경우에 linear probe 보다 성능이 낮을 수 있지만, 이 연구의 목표는 여러 dataset 과 pre-trained model 에서 작동하는 &quot;pixel space&quot; 의 prompting mechanism 의 존재를 보여주고, vision model 이 효과적으로 적응될 수 있는 새로운 방법을 제시하는 데 있음을 강조하고자 한다.</p><p>초점은 SOTA 능가가 아닌, 더 나은 성능을 위해 여러 prompt ensembling, Linear Probe 나 Fine-tuning 과 함께 사용하는 Prompting, 또는 pre-trained model 확장 (e.g., CLIP 의 ViT-L/14, 이는 공개되어 있지 않음)과 같은 접근 방법들을 향후 연구로 남겨 둠.</p><h1>8. Conclusion</h1><p>vision 에서의 standard adaptation 은 주로 separate task-specific head 를 도입하고 model parameters 나 activation 을 적응시키는데 집중하 반면, 저자는 visual prompting 을 실용적인 적응 방법으로 조사. </p><p>저자는 gradient-based method 를 사용하여 single, input-agnostic 한 왜곡을 학습하였으며, 이를 통해 얼려진 모델을 downstream task 를 수행할 수 있도록 재활용할 수 있었다. </p><p>다양한 pre-trained model 과 dataset 을 거쳐 여러 실험을 통해, 우리는 CLIP 이 visual prompting 에 특히 적합하며 linear probe 와 competitive 한 결과를 달성할 수 있음을 입증한다.</p><p>저자의 독특한 발견이 심도 있는 연구를 촉발하며, 다음과 같은 분야로의 추가 연구를 기대</p><p>(1) pixel space adaptation 의 효과적인 사용 시기와 그 이유에 대한 더 나은 이해
(2) vision system 의 flexible 및 adaptable mechanism 에 기여하는 더 나은 visual prompt 개발</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vp">VP</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/visual-prompt">Visual Prompt</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/pixel-level">Pixel-Level</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-03-VP.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/CMAR"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Cross-modal Adversarial Reprogramming</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/Watermarking"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Watermarking for Out-of-distribution Detection</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#2-related-work" class="table-of-contents__link toc-highlight">2. Related Work</a></li><li><a href="#21-natural-language-prompting" class="table-of-contents__link toc-highlight">2.1 Natural Language Prompting</a></li><li><a href="#22-prompting-with-images" class="table-of-contents__link toc-highlight">2.2 Prompting with Images</a></li><li><a href="#23-adversarial-reprogramming-and-unadversarial-examples" class="table-of-contents__link toc-highlight">2.3 Adversarial Reprogramming and Unadversarial Examples</a></li><li><a href="#24-adapting-pre-trained-models-in-vision" class="table-of-contents__link toc-highlight">2.4 Adapting Pre-trained Models in Vision</a></li><li><a href="#31-pre-trained-models" class="table-of-contents__link toc-highlight">3.1 Pre-trained Models</a></li><li><a href="#32-input-transformation" class="table-of-contents__link toc-highlight">3.2 Input Transformation</a><ul><li><a href="#321-prompt-tuning" class="table-of-contents__link toc-highlight">3.2.1 Prompt Tuning</a></li><li><a href="#322-prompt-design" class="table-of-contents__link toc-highlight">3.2.2 Prompt Design</a></li></ul></li><li><a href="#33-output-transformation" class="table-of-contents__link toc-highlight">3.3 Output Transformation</a></li><li><a href="#4-implementation-details" class="table-of-contents__link toc-highlight">4. Implementation Details</a></li><li><a href="#41-datasets" class="table-of-contents__link toc-highlight">4.1 Datasets</a></li><li><a href="#42-baseline-models" class="table-of-contents__link toc-highlight">4.2 Baseline Models</a></li><li><a href="#51-effectiveness-of-clip" class="table-of-contents__link toc-highlight">5.1 Effectiveness of CLIp</a></li><li><a href="#52-robustness-to-distribution-shift" class="table-of-contents__link toc-highlight">5.2 Robustness to Distribution Shift</a></li><li><a href="#61-downstream-dataset" class="table-of-contents__link toc-highlight">6.1 Downstream Dataset</a></li><li><a href="#62-prompt-design" class="table-of-contents__link toc-highlight">6.2 Prompt Design</a></li><li><a href="#63-output-transformation" class="table-of-contents__link toc-highlight">6.3 Output Transformation</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.0dfd96f9.js"></script>
<script src="/assets/js/main.2a692bef.js"></script>
</body>
</html>