<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/PEFT/Module/2022-11-TaskRes">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Task Residual for Tuning Vision-Language Models | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/PEFT/Module/TaskRes"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Task Residual for Tuning Vision-Language Models | My Site"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/PEFT/Module/TaskRes"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/PEFT/Module/TaskRes" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/PEFT/Module/TaskRes" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.d2ad26d0.css">
<link rel="preload" href="/assets/js/runtime~main.b3655ab3.js" as="script">
<link rel="preload" href="/assets/js/main.30f56a58.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Unsupervised Prompt Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Prismer">Prismer: A Vision-Language Model with An Esemble of Experts</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Contrastive Learning/ALIGN">Contrastive Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Composition/CLIP-LoRA">PEFT</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Composition/CLIP-LoRA">Composition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Module/CLIP-Adapter">Module</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Module/CLIP-Adapter">CLIP-Adapter: Better Vision-Language Models with Feature Adapters</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Module/Tip-Adapter">Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Module/TaskRes">Task Residual for Tuning Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Module/CLAP">A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Multi-Modality/MaPLe">Multi-Modality</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/CMAR">Prompting</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Single-Stream/VLBERT">Single-Stream</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Two-Stream/LXMERT">Two-Stream</a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">PEFT</span><meta itemprop="position" content="3"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module</span><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Task Residual for Tuning Vision-Language Models</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Task Residual for Tuning Vision-Language Models</h1></header><p>논문 및 이미지 출처 : <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.pdf" target="_blank" rel="noopener noreferrer">https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.pdf</a></p><h1>Abstract</h1><p>large-scale vision-language models (VLMs)은 billion-level data 를 pre-training 하면서 일반적인 visual representation 과 광범위한 visual concepts 를 학습했다. </p><p>VLMs 의 well-trained knowledge structure 는 limited data 를 가진 downstream tasks 로 transfer 될 때 적절하게 계승되어야 한다. </p><p>그러나 대부분의 기존 efficient transfer learning(ETL) 은 VLMs 의 pre-trained knowledge 를 손상시키거나 지나친 biased towards 가 있다. </p><p>예로, prompt tuning(PT)은 pre-trained text-based classifier 를 폐기하고 new classifier 를 구축하는 반면, adapter-stype tuning(AT)은 pre-trained features 에 전적으로 의존한다. </p><p>이를 해결하기 위해, 저자는 Task Residual Tuning(TaskRes) 라는 new efficient tuning approach 를 제안한다. </p><ul><li>이는 text-based classifier 에서 직접 수행되며, pre-trained model 의 prior knowledge 와 target task 에 대한 new knowledge 를 명확하게 분리한다. </li><li>구체적으로, TaskRes 는 VLMs 의 original classifier weights 를 동결하고, original weights 에 독립적인 parameter set 을 residual tuning 하여 new target task 에 대한 classifier 를 얻는다. </li><li>이를 통해 prior knowledge 의 reliable preservation 와 task-specific new knowledge exploration 이 가능해진다. </li><li>TaskRes 는 간단하지만 효과적이며, 구현에 최소한의 노력이 필요하면서도 11 benchmark dataset 에서 이전의 ETL 방법들(e.g., PT 및 AT)보다 크게 우수한 성능을 보인다.</li></ul><h1>1. Introduction</h1><p>DNN-based recognition models 는 큰 성공을 거두어, 이런 SOTA models 는  large image 및 discrete label pairs 로 학습되는 경우가 많다. </p><p>discrete label 은 &quot;American curl cat&quot; 같은 detailed textual description 을 간단한 scalar 로 변환하여 생성되며, 이는 loss function 의 계산을 상당히 용이하게 만든다. </p><p>그러나 이로 인해 두 가지 명백한 한계가 발생한다: (i) textual description 에 담긴 rich semantics 가 충분히 활용되지 않으며, (ii) trained models 는 closed-set classes 만 인식할 수 있다는 점이다.</p><hr><ul><li>최근 large-scale vision-language models(VLM) pre-training 은 textual supervision 을 통해 visual representation 을 학습함으로써 이러한 한계를 극복한다. <ul><li>예로, texts 와 images 는 pre-training 동안 contrastive loss 를 통해 unified space 로 encoding 및 mapping 된다.</li></ul></li><li>pre-trained text encoder 는 대응하는 natural language description 을 기반으로 image recognition 을 위한 text-based classifier 를 생성하는 데 사용될 수 있다(Fig. 2 (a)). <ul><li>이러한 pre-trained VLM 들은 다양한 downstream tasks 에서 zero-shot 방식으로 strong transferability 를 보여주었다. </li></ul></li><li>그러나 이런 모델들의 효과는 large-scale architecture 와 training dataset 에 크게 의존한다. <ul><li>예로, CLIP 은 428M parameters 를 가지고 있으며 0.4B text-image pairs 로 학습되었고, Flamingo 는 80B parameters 를 가지고 있으며 2.1B pairs 로 학습되었다. </li><li>이는 low-data regime 에서 downstream tasks 에 대해 모델을 fully fine-tuning 은 비현실적이라는 것을 의미한다.</li></ul></li></ul><p>이러한 이유로, pre-trained VLM 에 대한 efficient transfer learning(ETL) 이 인기를 얻고 있다. </p><p>ETL 은 parameter- 및 data-efficiency 를 모두 충족하는 방식으로 downstream tasks 에 transfer learning 을 수행한다. </p><p>ETL 의 핵심은 두 가지로 요약된다: (i) 이미 전이 가능한 VLM 의 well-trained knowledge structure 를 적절히 계승하는 것, (ii) limited data 를 가지고 task-specific knowledge 을 효과적으로 탐구하는 것. </p><p><img loading="lazy" alt="Figure 1" src="/assets/images/image-34-0239c350930abc8bdddb0e1aa42be864.png" width="1563" height="1272" class="img_ev3q"></p><ul><li>그러나 대부분의 기존 ETL 접근법, 즉 prompt tuning(PT) 과 adapter-stype tuning(AT) 은 VLM 의 prior knowledge 를 손상시키거나, task 의 new knowledge 를 부적절하거나 불충분하게 학습한다. <ul><li>예로, CoOp (Fig. 2 (b))은 pre-trained text-based classifier 를 사용하는 대신 continuous prompt 를 학습하여 완전히 new classifier 를 생성하는데, 이는 필연적으로 prior knowledge 의 손실을 초래한다. </li><li>그 결과, CoOp 은 1-/2-shot learning 에서 zero-shot CLIP 보다 ImageNet 에서 각각 1.03%/0.37% 낮은 성능을 보인다 (Fig. 1). </li></ul></li><li>반면에, CLIP-Adapter 는 pre-trained classifier 를 유지하지만, new task 를 학습할 때 지나치게 prior knowledge 에 편향되어 있어 (Fig. 2) new knowledge exploration 이 부족하고 결과적으로 더 낮은 정확도를 나타낸다(Fig. 1 참조).</li></ul><p><img loading="lazy" alt="Figure 2" src="/assets/images/image-35-2454ad19da2b6a50b70e006bc88c14e2.png" width="3161" height="1378" class="img_ev3q"></p><p>pre-trained VLM 에 대해 더 나은 ETL 을 위해, 저자는 Task Residual Tuning(TaskRes)이라는 new efficient tuning approach 를 제안한다. </p><ul><li>TaskRes 는 text-based classifier 에서 직접 수행되며, pre-trained model 의 prior knowledge 와 target task 에 대한 new knowledge 를 명확하게 분리한다. </li><li>이 분리는 VLM 에서 prior knowledge 를 더 잘 계승하고 task-specific knowledge 을 더 유연하게 탐구할 수 있게 해준다. </li><li>구체적으로, original classifier weights 를 동결하고, weights 에 더해지는 <em>prior-independent</em> parameter set 을 도입한다. <ul><li>이러한 additive parameters 는 target task 에 대한 adaptation 을 위해 tuning 되며, 이를 &quot;task residual&quot;이라고 부른다.</li></ul></li></ul><p>TaskRes 에 대한 통찰을 얻기 위해, 저자는 11 benchmark dataset 에 걸쳐 광범위한 실험을 수행하고, learned task residual 에 대한 조사를 수행했다. </p><ul><li>실험 결과, task residual 을 도입하면 transfer 성능이 크게 향상될 수 있음을 보여주었다. </li><li>저자는 learned task residual 의 크기와 pre-trained model 을 downstream tasks 로 transfer 하는 데 어려움 사이의 상관관계를 시각화했고, 그 크기가 transfer 의 어려움에 따라 증가한다는 것을 관찰했다. </li><li>이는 residual 이 task 에 맞게 자동으로 적응하여 new knowledge 를 충분히 탐구할 수 있음을 시사하며, 이를 통해 11 dataset 에서 SOTA 를 달성할 수 있었다. </li><li>더 나아가, 이 방법은 구현이 매우 간단하며, 코드 한 줄만 추가하면 된다.</li></ul><p>저자의 기여는 다음과 같이 요약된다:</p><ul><li>pre-trained VLM 에서 downstream tasks 로의 proper knowledge inheritance 의 필요성을 처음으로 강조하고, 기존 tuning paradigms 의 함정을 밝혀내며, 기존 pre-trained knowledge 와 new task-specific knowledge 을 분리하는 것이 핵심임을 심층 분석을 통해 나타낸다.</li><li>저자는 Task Residual Tuning(TaskRes)이라는 new efficient tuning approach 를 제안하며, 이를 통해 VLM 의 prior knowledge 를 더 잘 계승하고, task-specific knowledge 을 더 유연하게 탐구할 수 있다.</li><li>TaskRes 는 few tuning parameters 만 필요하며, 구현이 매우 용이하다.</li></ul><h1>2. Related Work</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-vision-language-models">2.1. Vision-Language Models<a href="#21-vision-language-models" class="hash-link" aria-label="Direct link to 2.1. Vision-Language Models" title="Direct link to 2.1. Vision-Language Models">​</a></h2><p>주로 language-driven visual representation learning(LDVRL) 에 관한 vision-language models(VLMs)의 문헌을 검토한다. </p><p>LDVRL 의 핵심은 texts(language)와 images 를 common space 에 mapping 하여 text representations 를 visual classification 에 사용할 수 있도록 하는 것이다. </p><p>이를 위해 각각의 texts 와 images 를 위한 two encoders 및 regularization 로서 specific loss function 가 필요하다.</p><p>초기 연구들은 text embedding 을 위해 unsupervised trained models 이나 skip-gram text modeling 을 탐구했으며, visual encoding 을 위해 sparse coding 및 vector quantization 또는 Classeme features 를 사용했다. </p><p>training objective 로는 MSE, self-supervised topic probabilities matching 또는 multi-class logistic loss 가 채택되었다. </p><p>그러나 이 방법들은 small dataset 과 약한 대표성을 가진 encoding backbones 에 제한되어 있어 transfer learning 능력을 크게 저해했다.</p><p>반면에, 최근 연구들은 web-scale billion-level data 의 image-text pairs 와 strong neural network, 예로 Transformers 를 활용하여 이전 연구들을 발전시켰다.</p><p>large-scale VLMs 는 contrastive loss 를 사용한 end-to-end pre-training 을 통해 다양한 downstream tasks 에서 zero-shot evaluation 방식으로 뛰어난 transfer learning 능력을 보여주었다. </p><p>저자는 large-scale vision-language pre-training 의 strong transferability 을 기반으로 efficient transfer learning 의 잠재력을 추가로 탐구한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-efficient-transfer-learning">2.2. Efficient Transfer Learning<a href="#22-efficient-transfer-learning" class="hash-link" aria-label="Direct link to 2.2. Efficient Transfer Learning" title="Direct link to 2.2. Efficient Transfer Learning">​</a></h2><p>large-scale dataset (e.g., ImageNet 및 WebImageText) 에서 neural network 을 pre-training 한 후 downstream tasks 에 대해 fine-tuning 하는 것은 transfer learning 의 일반적인 단계였다. </p><p>여기선 pre-trained VLMs 에 대한 efficient transfer learning(ETL)에 주로 초점을 맞춘다. </p><p>ETL 은 small parameters 를 tuning 하고 small-scale dataset 을 활용하는 parameter- 및 data-efficient transfer learning 을 의미한다. </p><p>기존의 ETL 연구들은 prompt tuning 과 adapter-stype tuning(AT) 으로 나눌 수 있다. </p><p>구체적으로, prompt engineering 은 처음에는 downstream tasks 을 위한 proper discrete prompt 를 생성하기 위해 탐구되었다. </p><p>이후 new task 에 더 잘 적응할 수 있는 continuous prompt 를 학습하는 것이 더 발전된 성능을 보여주었다. </p><p>그러나 이러한 방법들은 두 가지 문제에 직면한다: (i) pre-trained text encoder 가 whole training 에 참여해야 하므로 scalability 가 제한되고 computational overhead 가 증가한다; (ii) pre-trained text-based classifier 를 버리고 new classifier 를 생성하므로 VLMs 의 prior knowledge 을 잃게 된다.</p><p>AT-based methods 는 text-based classifier 를 한 번 생성한 후 text/image feature 만을 적응시키는 방식으로 위의 문제를 해결한다. </p><p>이 간단한 설계는 더 나은 성능을 얻을 수 있지만, prior knowledge/pre-trained features 에 크게 의존하여 new knowledge 탐색이 부족하게 된다. </p><p>이를 해결하기 위해, 저자는 더 나은 그리고 더 유연한 task-specific knowledge 학습을 위해 <em>prior-independent</em> &quot;task residual&quot;을 사용하는 new efficient tuning approach, Task Residual Tuning (TaskRes)을 제안한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-few-shot-learning">2.3. Few-Shot Learning<a href="#23-few-shot-learning" class="hash-link" aria-label="Direct link to 2.3. Few-Shot Learning" title="Direct link to 2.3. Few-Shot Learning">​</a></h2><p>Few-shot learning(FSL)은 handful labeled examples 를 사용하여 모델을 new tasks/classes 에 적응시키는 것을 목표로 한다. </p><p>기존의 FSL 방법들은 종종 base classes 의 abundant data 를 meta-learning 하여 적응 능력을 키운다. </p><p>그러나 base dataset 에서 학습해야 하는 요구사항은 확장성을 제한한다. </p><p>최근 VLM pre-training 연구들은 base dataset 을 필요로 하지 않는 효과적인 대안을 제공한다. </p><p>이들은 pre-trained model 이 많은 downstream tasks 에서 zero-shot 방식으로 놀라운 성능을 이미 달성할 수 있음을 보여준다. ETL 은 성능을 더욱 향상시킬 수 있다. </p><p>이 연구에서는 VLMs 를 downstream tasks 에 적응시키기 위한 새로운 ETL 방법을 제안하고, 이를 Few-shot tasks 에서 평가한다.</p><h1>3. Preliminaries</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-contrastive-language-image-pre-training">3.1. Contrastive Language-Image Pre-training<a href="#31-contrastive-language-image-pre-training" class="hash-link" aria-label="Direct link to 3.1. Contrastive Language-Image Pre-training" title="Direct link to 3.1. Contrastive Language-Image Pre-training">​</a></h2><p>CLIP 은 natural language supervision 을 통해 visual representation 을 얻기 위해 설계되었다. </p><ul><li>이는 4M image-text pairs 으로 학습되었으며, image encoder 에서 얻은 image features 와 text encoder 에서 얻은 text features 가 통합된 embedding space 내에서 contrastive learning loss 를 사용해 정렬된다. </li><li>이를 통해 CLIP 은 폭넓은 visual concepts 를 효과적으로 포착하고 general visual representation 을 학습할 수 있게 된다. </li><li>testing 시, CLIP 은 query image 가 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> possible categories 중 하나로 분류될 수 있도록 한다. </li><li>이는 image encoder 로부터 projection 을 통해 얻어진 query image embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span> 와 text branch 에 texts (e.g., “a photo of a {class}”) 를 입력하여 얻어진 text embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi>i</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding="application/x-tex">\{t_i\}_{i=1}^{K}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span></span></span></span></span> 간의 cosine similarity 를 계산함으로써 이루어진다. </li><li>class <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>에 대한 예측 확률은 다음과 같이 공식화된다:</li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mi>i</mi><mi mathvariant="normal">∣</mi><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy="false">(</mo><mi>z</mi><mo separator="true">,</mo><mi>t</mi><mi>i</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>τ</mi><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy="false">(</mo><mi>z</mi><mo separator="true">,</mo><mi>t</mi><mi>j</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>τ</mi><mo stretchy="false">)</mo></mrow></mfrac><mo separator="true">,</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} p(y = i|z) = \frac{\exp(sim(z, ti)/τ)}{\sum_{j=1}^{K} \exp(sim(z, tj)/τ)}, \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.734em;vertical-align:-1.117em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.617em"><span style="top:-3.617em"><span class="pstrut" style="height:3.427em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">i</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.1288em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord mathnormal">im</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.05724em">j</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mord mathnormal">im</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.307em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.117em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.617em"><span style="top:-3.617em"><span class="pstrut" style="height:3.427em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.117em"><span></span></span></span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy="false">(</mo><mo separator="true">⋅</mo><mo separator="true">,</mo><mo separator="true">⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">sim(·, ·)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">im</span><span class="mopen">(</span><span class="mpunct">⋅,⋅</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mclose">)</span></span></span></span></span> : cosine similarity </li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">τ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span></span></span></span></span> : CLIP 이 학습한 temperature</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-revisiting-previous-tuning-paradigms">3.2. Revisiting Previous Tuning Paradigms<a href="#32-revisiting-previous-tuning-paradigms" class="hash-link" aria-label="Direct link to 3.2. Revisiting Previous Tuning Paradigms" title="Direct link to 3.2. Revisiting Previous Tuning Paradigms">​</a></h2><p>NLP 에서 성공한 ETL 접근법, 예로 prompt tuning 과 adapter 에 영감을 받아, 최근의 발전 (e.g., CoOp 및 CLIP-Adapter)은 VLMs 에 대한 ETL 에 이 아이디어를 차용했다.</p><ul><li>CoOp 은 처음으로 VLMs 에 prompt tuning 을 도입했다.<ul><li>&quot;a photo of a&quot; 와 같은 fixed text prompts context 를 사용하는 대신, CoOp 은 task-specific template 으로서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span></span></span></span></span> learnable context vectors <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>v</mi><mi>m</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup></mrow><annotation encoding="application/x-tex">\{v_m\}_{m=1}^{M}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em"><span></span></span></span></span></span></span></span></span></span></span> 를 사용하도록 제안한다. </li><li>text encoder 에 주어진 prompts 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>v</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>v</mi><mn>2</mn></msub><mo separator="true">,</mo><mo separator="true">⋅</mo><mo separator="true">⋅</mo><mo separator="true">⋅</mo><mo separator="true">,</mo><msub><mi>v</mi><mi>M</mi></msub><mo separator="true">,</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{v_1, v_2, · · · , v_M, c_i\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,⋅⋅⋅,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span>가 되며, <ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> : class <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span> 의 embedding</li></ul></li><li>whole training process 동안 CoOp 은 pre-trained VLMs 의 parameters 를 freezing 하고, learnable vectors <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>v</mi><mi>m</mi></msub><msubsup><mo stretchy="false">}</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup></mrow><annotation encoding="application/x-tex">\{v_m\}_{m=1}^{M}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">}</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em"><span></span></span></span></span></span></span></span></span></span></span> 만 tuning 한다</li></ul></li><li>adapter-stype tuning 은 tunable parameters <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi></mrow><annotation encoding="application/x-tex">ω</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">ω</span></span></span></span></span> 를 가진 additional modules <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mi>ω</mi></msub><mo stretchy="false">(</mo><mo separator="true">⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">ϕ_ω(·)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">ω</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mclose">)</span></span></span></span></span> 을 pre-trained model 에 도입하여 pre-trained features <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span></span></span></span></span> 를 new features <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">f&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9463em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> 로 변환한다.<ul><li>일반적으로 adapter-stype tuning 은 다음과 같이 공식화될 수 있다:</li></ul></li></ul><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>f</mi><mo>+</mo><mi>α</mi><msub><mi>ϕ</mi><mi>ω</mi></msub><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} f&#x27; = f + αϕ_ω(f), \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">ω</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-2.85em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span></span></div><ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> : scaling factor</li><li>CLIP-Adapter 에선 adapter module <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mi>ω</mi></msub></mrow><annotation encoding="application/x-tex">\phi_\omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">ω</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> 이 two linear transformation layers 및 그 사이에 ReLU activation 으로 구성된다. </li><li>CLIP-Adapter 는 visual adapter 와 text adapter 모두를 조사하며, 즉, adapter module 을 CLIP 의 image branch 및 text branch 에 각각 적용한다. </li><li>이 연구는 two adapters 모두 유사한 성능을 보인다고 보여준다. </li><li>downstream tasks 에서의 training 동안, adapter-stype methods 는 오직 adapter module 만을 조정한다.</li></ul><h1>4. Approach</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-pitfalls-of-existing-etl-paradigms-on-vlms">4.1. Pitfalls of Existing ETL Paradigms on VLMs<a href="#41-pitfalls-of-existing-etl-paradigms-on-vlms" class="hash-link" aria-label="Direct link to 4.1. Pitfalls of Existing ETL Paradigms on VLMs" title="Direct link to 4.1. Pitfalls of Existing ETL Paradigms on VLMs">​</a></h2><p>저자는 pre-trained VLMs 의 prior knowledge 사용과 downstream tasks 에 대한 new knowledge 습득을 재고한다. </p><ul><li>한편으로는, large-scale VLMs 는 amount data 를 통해 general visual concepts 를 학습하였으며, 이는 다양한 downstream vision tasks 에 대해 동질화를 가능하게 한다. <ul><li>이러한 prior knowledge 은 transfer process 에서 잘 보존되어야 한다. </li></ul></li><li>다른 한편으로, pre-training 에 사용된 data 가 방대하더라도 downstream tasks 에서는 domain shifts 나 uncertain concepts 이 존재할 수 있다. <ul><li>downstream tasks 에 특화된 new knowledge 은 prior knowledge 에 적절히 보강되어야 한다. </li></ul></li></ul><p>그러나 기존 ETL 패러다임은 이러한 원칙을 충분히 고려하지 않으며, 다음과 같은 두 가지 문제가 있다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="pitfall-1-lack-of-guarantees-of-prior-knowledge-preservation-in-prompt-tuning">Pitfall 1: Lack of guarantees of prior knowledge preservation in prompt tuning.<a href="#pitfall-1-lack-of-guarantees-of-prior-knowledge-preservation-in-prompt-tuning" class="hash-link" aria-label="Direct link to Pitfall 1: Lack of guarantees of prior knowledge preservation in prompt tuning." title="Direct link to Pitfall 1: Lack of guarantees of prior knowledge preservation in prompt tuning.">​</a></h4><p>Prompt tuning paradigm 에서 pre-trained text branch modules (e.g., text encoder 및 projection)의 weights 를 동결하지만, original well-trained classification boundary 가 어느 정도 손상될 수 있다. </p><p>이는 input prompts tuning 이 new boundary 를 만들어내면서 old knowledge 를 잊어버리게 하기 때문이다. </p><p>결과적으로, prompt tuning 의 성능은 제한적이다. </p><p>예로, CoOp 의 성능은 Fig. 1 에서 보여주는 바와 같이 1-/2-shot learning 에서 Zero-shot CLIP 보다 좋지 않다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="pitfall-2-limited-flexibility-of-new-knowledge-exploration-in-adapter-style-tuning">Pitfall 2: Limited flexibility of new knowledge exploration in adapter-style tuning<a href="#pitfall-2-limited-flexibility-of-new-knowledge-exploration-in-adapter-style-tuning" class="hash-link" aria-label="Direct link to Pitfall 2: Limited flexibility of new knowledge exploration in adapter-style tuning" title="Direct link to Pitfall 2: Limited flexibility of new knowledge exploration in adapter-style tuning">​</a></h4><p>downstream tasks 의 data distribution 는 pre-training distribution 과 다를 수 있으며, 일부 task-specific 또는 fine-grained visual concepts/representations 는 pre-trained VLMs 에서 충분히 학습되지 않을 수 있다. </p><p>예로, CLIP 에서 satellite dataset EuroSAT 으로의 경우가 그렇다. </p><p>따라서 downstream tasks 에 대한 new knowledge 은 적절히 탐색되어야 한다. </p><p>저자는 adapter-style tuning 이 prior/pre-trained features 에 엄격히 limited input 을 사용하므로 task-specific knowledge 를 충분히 탐색하지 못한다고 관찰했다. </p><p>결과적으로 adapter-style tuning 은 new knowledge 를 학습하는 데 유연성이 제한된다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-task-residual-tuning">4.2. Task Residual Tuning<a href="#42-task-residual-tuning" class="hash-link" aria-label="Direct link to 4.2. Task Residual Tuning" title="Direct link to 4.2. Task Residual Tuning">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="fixed-base-classifier">Fixed base classifier.<a href="#fixed-base-classifier" class="hash-link" aria-label="Direct link to Fixed base classifier." title="Direct link to Fixed base classifier.">​</a></h4><p>Fig. 2(d) 와 같이, TaskRes 는 text-based classifier(i.e., text embedding) 에 directly tuning 을 수행한다. </p><p><em>base classifier 는</em> pre-trained vision-language model (e.g., CLIP) 의 text embedding 이다. </p><ul><li>저자는 base classifier 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>K</mi><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">t \in \mathbb{R}^{K \times D}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6542em;vertical-align:-0.0391em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">D</span></span></span></span></span></span></span></span></span></span></span></span></span> 로 나타낸다<ul><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> : category 수</li><li><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span></span> : feature dimension</li></ul></li><li>저자는 base classifier 의 weights 를 동결하여 base classifier 가 손상되는 것을 명시적으로 방지한다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="prior-independent-task-residual">Prior-independent task residual<a href="#prior-independent-task-residual" class="hash-link" aria-label="Direct link to Prior-independent task residual" title="Direct link to Prior-independent task residual">​</a></h4><p>task-specific knowledge 를 prior knowledge 에 제한되지 않고 학습하기 위해, 저자는 task residual 를 제안한다. </p><ul><li>이는 base classifier 에 의존하지 않는 tunable parameters set 이다.</li><li>task residual 은 scaling factor <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">α</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 로 tuning 되며, base classifier 에 element-wise 로 추가되어 new classifier <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">t&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> 를 형성한다. </li></ul><p>수식으로는 다음과 같이 표현된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>t</mi><mo>+</mo><mi>α</mi><mi>x</mi></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} t&#x27; = t + \alpha x \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2em;vertical-align:-0.35em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8019em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal">αx</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.85em"><span style="top:-2.85em"><span class="pstrut" style="height:2.84em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35em"><span></span></span></span></span></span></span></span></span></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="tuning-for-downstream-tasks">Tuning for downstream tasks<a href="#tuning-for-downstream-tasks" class="hash-link" aria-label="Direct link to Tuning for downstream tasks" title="Direct link to Tuning for downstream tasks">​</a></h4><ul><li>tuning 중 base classifier (image branch 포함)를 고정하고 prior-independent task residual 만 tuning 한다. <ul><li>이를 통해 old knowledge 의 신뢰할 수 있는 보존과 new knowledge 의 유연한 탐색이 가능하다. </li></ul></li><li>image 가 주어지면, CLIP 의 frozen image branch 는 embedding <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span>를 추출한다. </li></ul><p>class <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>에 대한 predicted probability 는 다음과 같이 계산된다:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mi>i</mi><mi mathvariant="normal">∣</mi><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mtext>sim</mtext><mo stretchy="false">(</mo><mi>z</mi><mo separator="true">,</mo><msubsup><mi>t</mi><mi>i</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>τ</mi><mo stretchy="false">)</mo></mrow><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mtext>sim</mtext><mo stretchy="false">(</mo><mi>z</mi><mo separator="true">,</mo><msubsup><mi>t</mi><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>τ</mi><mo stretchy="false">)</mo></mrow></mfrac><mi mathvariant="normal">.</mi></mrow></mstyle></mtd><mtd class="mtr-glue"></mtd><mtd class="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{equation} p(y = i | z) = \frac{\exp(\text{sim}(z, t&#x27;_i) / \tau)}{\sum_{j=1}^K \exp(\text{sim}(z, t&#x27;_j) / \tau)}. \end{equation}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.7359em;vertical-align:-1.118em"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.618em"><span style="top:-3.618em"><span class="pstrut" style="height:3.4289em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord mathnormal">i</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4289em"><span style="top:-2.1288em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">exp</span><span class="mopen">(</span><span class="mord text"><span class="mord">sim</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7337em"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span><span style="top:-3.0448em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mop">exp</span><span class="mopen">(</span><span class="mord text"><span class="mord">sim</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-2.4413em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.1132em">τ</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.307em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">.</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.118em"><span></span></span></span></span></span></span></span><span class="tag"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.618em"><span style="top:-3.618em"><span class="pstrut" style="height:3.4289em"></span><span class="eqn-num"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.118em"><span></span></span></span></span></span></span></span></span></div><ul><li>predicted probability 를 기반으로, downstream task loss(e.g., cross-entropy loss)은 standard backpropagation 을 통해 task residual 만 업데이트한다.</li></ul><h1>5. Experiment</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-setup">5.1. Setup<a href="#51-setup" class="hash-link" aria-label="Direct link to 5.1. Setup" title="Direct link to 5.1. Setup">​</a></h2><p>기존 efficient transfer learning(ETL) 연구를 따르며, 11 benchmark dataset (i.e., ImageNet, Caltech101, OxfordPets, StanfordCars, Flowers102, Food101, FGVCAircraft, SUN397, DTD, EuroSAT, UCF101)에서 few-shot evaluation 을 수행한다. </p><p>이 dataset 들은 generic object classification, fine-grained object classification, action, scenes 등 다양한 visual recognition tasks 를 포함한다. </p><p>특히, ETL 모델은 각각 1/2/4/8/16 shots 으로 훈련되며, full test sets 에서 평가된다. 또한, CoOp 을 따라, ImageNet 에서의 model generalization 성능을 ImageNet 의 variants (i.e., ImageNetV2, ImageNet-Sketch, ImageNet-A, ImageNet-R)에서 테스트한다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-implementation">5.2. Implementation<a href="#52-implementation" class="hash-link" aria-label="Direct link to 5.2. Implementation" title="Direct link to 5.2. Implementation">​</a></h2><p>저자의 접근 방법에는 base classifier 와 task residual 두 가지 주요 구성 요소가 있다. </p><p>base classifier 에는 두 가지 버전이 있다. </p><ul><li>하나는 pre-trained CLIP 의 text embedding weights 를 직접 사용하는 <em>regular base classifier</em> 이며, </li><li>다른 하나는 저자의 task residual tuning 전에 target task 에 대해 CLIP 의 text projection layer 를 tuning 하여 얻은 enhanced base classifier</li></ul><hr><ul><li>task residual 는 learnable parameters 로 채워진 matrix 이며, 초기값은 0 으로 설정된다. </li><li>task residual 는 base classifier 와 same shape 를 가지며, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">α</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> factor 로 scaling 되고 base classifier 에 element-wise 로 추가된다. </li><li>기본적으로, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">α</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 의 값은 all dataset 에서 0.5 로 설정되며, Flowers102 의 경우 1 을 사용한다. </li><li>또한, 후속 연구에서 learnable <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">α</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 의 사용도 탐색한다. </li><li>모든 모델은 pre-trained CLIP 을 기반으로 하며, 특별히 명시되지 않는 한, image branch 에 ResNet-50 backbone 을 사용하는 ResNet-50 버전의 CLIP 을 사용한다. </li><li>저자의 모델은 1-/2-/4-shot 실험에는 100 epochs, 8-/16-shot 실험에는 200 epochs 동안 training 하며, batch size 는 256이다. </li><li>또한, enhanced base classifier 는 50 epochs 동안 조정된다. </li><li>저자의 모델은 Adam 으로 최적화되며, initial learning rate 는 2e-3 이고 ImageNet 의 경우 2e-4 로 설정된다. </li><li>CoOp 를 따라, optimization process 는 cosine learning rate decay scheduler 와 warmup scheme 을 채택 (i.e., first epoch 의 learning rate 를 1e-5 로 고정). </li><li>training 후 모델 성능을 테스트하며, 모든 실험 결과는 3 random seeds 에서 평균화.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="53-performance-comparison">5.3. Performance Comparison<a href="#53-performance-comparison" class="hash-link" aria-label="Direct link to 5.3. Performance Comparison" title="Direct link to 5.3. Performance Comparison">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="531-few-shot-learning">5.3.1. Few-Shot Learning<a href="#531-few-shot-learning" class="hash-link" aria-label="Direct link to 5.3.1. Few-Shot Learning" title="Direct link to 5.3.1. Few-Shot Learning">​</a></h3><p>저자는 두 가지 버전의 TaskRes (i.e., regular base classifier 및 enhanced base classifier 를 <em>Ours/Ours*</em>) 를 개발하며, 이를 Zero-shot CLIP 및 SOTA ETL (i.e., CoOp, CLIP-Adapter, Tip-Adapter-F)과 비교한다. </p><p><img loading="lazy" alt="Figure 3" src="/assets/images/image-36-d5469d79abca5303a1b2d5be665b25c0.png" width="1859" height="1867" class="img_ev3q"></p><ul><li>전반적으로, Ours 와 Ours*는 11 dataset 에서 모든 few-shot setting 에서 최신 성능(SOTA) 또는 SOTA 에 준하는 성능을 달성하며, Zero-shot CLIP 보다 상당히 우수하다. </li><li>몇 가지 관찰 사항은 다음과 같다. <ol><li>shots 수가 증가함에 따라 Tip-Adapter-F 는 저자의 성능에 가까워지지만, Fig. 1 에서 볼 수 있듯이 tunable parameters 수가 선형적으로 증가하여 확장성이 제한된다. </li><li>TaskRes 는 OxfordPets 와 Food101에서 성능이 떨어진다. 이 두 가지 fine-grained dataset 에서 few-shot 으로 학습할 때 over-fitting 되는 경향이 있으며, 이는 CoOp 에서도 발견된 바 있다. </li><li>TaskRes 는 많은 클래스를 인식해야 할 때 (e.g., 1000 ImageNet classes 와 397 SUN397 classes 에서 1-shot 의 경우) 우수한 성능을 보인다. <ul><li>이는 TaskRes 가 pre-trained 특징에 지나치게 편향되지 않으므로 극단적인 상황에서도 task-specific knowledge 을 더 잘 탐색할 수 있기 때문이다.</li></ul></li></ol></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="532-domain-generalization">5.3.2. Domain Generalization<a href="#532-domain-generalization" class="hash-link" aria-label="Direct link to 5.3.2. Domain Generalization" title="Direct link to 5.3.2. Domain Generalization">​</a></h3><p><img loading="lazy" alt="Table 1" src="/assets/images/image-37-e0ae96cb97cbc395e171475ff589fe74.png" width="2917" height="1413" class="img_ev3q"></p><p>CoOp 의 지적처럼, specific domain 에서 훈련된 ETL 모델은 unseen domain 으로 일반화할 때 잘못된 상관관계를 학습할 위험이 있다. </p><p>저자는 domain generalization 과 관련하여 모델을 평가한다. </p><p>저자는 16-shot ImageNet 에서 모델을 훈련하고, trained models 의 generalization 성능을 4 unseen ImageNet variants dataset (ImageNet-V2, -Sketch, -A, -R)에서 테스트한다. </p><p><img loading="lazy" alt="Table 1" src="/assets/images/image-38-e0ae96cb97cbc395e171475ff589fe74.png" width="2917" height="1413" class="img_ev3q"></p><ul><li>TaskRes 는 다양한 CLIP visual backbone(ResNet-50, ResNet-101, ViT-B/32, ViT-B/16)에서 비교된 모델(Zero-shot CLIP, Linear Probe CLIP, CoOp)을 일관되게 능가한다. </li><li>또한, 저자의 TaskRes 는 regular base classifier (Ours)보다 enhanced base classifier(Ours*) 를 사용할 때 더 높은 정확도를 달성하지만, source dataset 에서 약간의 over-fitting 으로 인해 generalization 성능이 다소 감소하는 경향이 있다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="54-ablation-study">5.4. Ablation Study<a href="#54-ablation-study" class="hash-link" aria-label="Direct link to 5.4. Ablation Study" title="Direct link to 5.4. Ablation Study">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="importance-of-prior-knowledge-preservation-and-priorindependent-tuning">Importance of prior knowledge preservation and priorindependent tuning.<a href="#importance-of-prior-knowledge-preservation-and-priorindependent-tuning" class="hash-link" aria-label="Direct link to Importance of prior knowledge preservation and priorindependent tuning." title="Direct link to Importance of prior knowledge preservation and priorindependent tuning.">​</a></h4><p>저자는 4 target classifier <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">t&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> 를 구축하여 prior knowledge preservation 과 prior-independent tuning 의 중요성을 조사한다. </p><p>구체적으로, original pre-trained classifier 인 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">t&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> (i.e., regular base classifier), directly-adapted classifier <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><msub><mi>ϕ</mi><mi>ω</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t&#x27; = \phi_{\omega}(t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">ω</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span>, adapter-style classifier <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>t</mi><mo>+</mo><mi>α</mi><msub><mi>ϕ</mi><mi>ω</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t&#x27; = t + \alpha \phi_{\omega}(t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6984em;vertical-align:-0.0833em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">ω</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span>, 그리고 TaskRes classifier <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>t</mi><mo>+</mo><mi>α</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">t&#x27; = t + \alpha x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6984em;vertical-align:-0.0833em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">αx</span></span></span></span></span> 를 비교한다. </p><p>저자는 ImageNet 에서 all few-shot setting 에 대해 실험을 진행하며, scaling factor <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.5</span></span></span></span></span> 를 사용한다. </p><p><img loading="lazy" alt="Table 2" src="/assets/images/image-39-7799cacf5b44123f98f4c9c3742253c8.png" width="1780" height="805" class="img_ev3q"></p><p>Table 2의 결과는 다음을 지지한다: (i) prior knowledge 의 explicit preservation 이 중요하며, directly-adapted classifier 는 original pre-trained classifier 와 adapter-style 의 classifier 보다 성능이 훨씬 낮다. (ii) prior-independent parameter tuning 이 더 효과적이며, TaskRes classifier 가 adapter-style classifier 보다 확연히 우수하다.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="effectiveness-of-task-residual-learning">Effectiveness of task residual learning.<a href="#effectiveness-of-task-residual-learning" class="hash-link" aria-label="Direct link to Effectiveness of task residual learning." title="Direct link to Effectiveness of task residual learning.">​</a></h4><p>저자는 TaskRes 의 도입 효과에 대한 ablation study 를 수행한다. </p><p><img loading="lazy" alt="Table 3" src="/assets/images/image-40-5813187df9e7b5865e87c847b4f24d4f.png" width="1961" height="919" class="img_ev3q"></p><ul><li>11 dataset 에서 regular base classifier 와 TaskRes 를 장착한 모델 (i.e., Ours) 의 few-shot 결과를 평균화하여 보여준다.</li><li>TaskRes 는 regular base classifier 를 크게 개선하며, 예로 16-shot setting 에서 16.14% 의 정확도 향상을 보여준다. </li><li>또한, enhanced base classifier 에서도 성능 향상이 인상적이다. </li><li>이는 TaskRes 가 downstream tasks 에 적응할 때 항상 pre-trained base classifier 에 이점을 제공할 수 있음을 나타낸다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="variants-of-taskres">Variants of TaskRes<a href="#variants-of-taskres" class="hash-link" aria-label="Direct link to Variants of TaskRes" title="Direct link to Variants of TaskRes">​</a></h4><p>제안된 &quot;task residual&quot;는 CLIP 의 image branch 에도 적용할 수 있으며, 이를 <em>TaskRes-I</em> 로 나타낸다. </p><p>default TaskRes (i.e., Ours) 는 text branch 에 적용된 것을 나타내며, two branches 모두에 적용된 것을 <em>TaskRes-I&amp;T</em> 로 나타낸다. </p><p><img loading="lazy" alt="Table 4" src="/assets/images/image-41-20e0af405a84b78d5e31c107856b3268.png" width="2146" height="887" class="img_ev3q"></p><ul><li>ImageNet 성능 비교 결과는 TaskRes-T 가 TaskRes-I 보다 일관되게 우수함을 보여준다. </li><li>TaskRes-I 는 shots 수가 증가함에 따라 성능이 훨씬 더 느리게 증가하며, 이는 TaskRes-I 가 training 및 test image embedding 의 차이로 인해 over-fitting 문제를 겪기 때문이다. <ul><li>text embedding 에 대한 조정은 이러한 문제를 피할 수 있다. </li></ul></li><li>또한, image embedding 이미지 임베딩의 다양성이 text embedding 보다 커서 task-level information 을 학습하기 어렵다. </li><li>TaskRes-I&amp;T 는 TaskRes-T 보다 약간 낮은 성능을 보인다. <ul><li>이는 updated image embedding 과 base classifier 사이의 misalignment 로 인한 것일 수 있다.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-factor">Scaling factor.<a href="#scaling-factor" class="hash-link" aria-label="Direct link to Scaling factor." title="Direct link to Scaling factor.">​</a></h4><p>저자는 scaling factor <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 의 효과를 조사하며, manual value 및 learnable value 를 모두 Table 5에 보여준다. </p><p><img loading="lazy" alt="Table 5" src="/assets/images/image-42-00096aa044cbc339dee120e1923c22cd.png" width="1881" height="516" class="img_ev3q"></p><ul><li>manually set <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 의 경우, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.1</span></span></span></span></span> 로 scaling 해도 TaskRes 에서 주목할 만한 향상이 관찰되며, highest accuracy 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\alpha = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0.5</span></span></span></span></span> 에서 얻어진다. </li><li>반면, learnable <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span> 는 성능을 더 향상시킬 수 있다.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="visual-backbone">Visual backbone<a href="#visual-backbone" class="hash-link" aria-label="Direct link to Visual backbone" title="Direct link to Visual backbone">​</a></h4><p>저자는 다양한 CLIP visual backbone (i.e., ResNet-50 (RN50), ResNet-101 (RN101), ViT-B/32, ViT-B/16)에서 모델을 추가로 평가한다. </p><p><img loading="lazy" alt="Table 6" src="/assets/images/image-43-abb662e6a8c4f5d08ab3d4d0aeb34b53.png" width="1845" height="773" class="img_ev3q"></p><p>Table 6 에 따르면, 저자의 방법은 사용되는 backbone 에 관계없이 다른 대안들보다 일관되게 우수하다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="55--investigation-of-learned-task-residual">5.5.  Investigation of Learned Task Residual<a href="#55--investigation-of-learned-task-residual" class="hash-link" aria-label="Direct link to 5.5.  Investigation of Learned Task Residual" title="Direct link to 5.5.  Investigation of Learned Task Residual">​</a></h2><p>저자는 learned task residual 가 실제로 CLIP 을 downstream tasks 로 transferring difficulty 와 관련이 있는지 조사한다. </p><p>먼저, <em>relative transfer difficulty</em> 를 정의하여 pre-trained model 을 target task 로의 transferring difficulty 를 평가한다. </p><ul><li>구체적으로, relative transfer difficulty 는 baseline classifier precision 과 pre-trained model 의 zero-shot precision 의 비율로 정의된다. </li><li>baseline classifier 로는 random classifier 를 사용하며, 이 classifier 의 precision 은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>K</mi></mrow><annotation encoding="application/x-tex">1/K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">1/</span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> : downstream tasks 의 classes 수)이다. </li><li>그런 다음, CLIP 의 relative transfer difficulty 를 11 benchmark dataset 에 대해 계산하며, 16-shot setting 에서 각 dataset 의 learned task residual 의 평균 크기를 측정한다. </li></ul><p><img loading="lazy" alt="Figure 4" src="/assets/images/image-44-8398c178be59e392dd7a636810d03253.png" width="1928" height="1377" class="img_ev3q"></p><ul><li>몇 가지 흥미로운 발견이 있으며, 예로, CLIP 을 ImageNet(1000 classes)으로 transfer 이 가장 쉬운 반면, EuroSAT(10 classes)으로의 transfer 는 가장 어려운 것으로 나타났다. <ul><li>이는 ImageNet 의 inherent difficulty 가 크지만 CLIP 에게 &quot;familiar&quot; data distribution 와 object 를 포함하고 있는 반면, EuroSAT 은 반대이기 때문. </li></ul></li><li>또한, TaskRes 가 relative transfer difficulty 가 큰 task 에서 더 많은 역할을 할 수 있음을 보여준다. <ul><li>예로, EuroSAT (1-shot 정확도 향상: 23.71%) 와 DTD (1-shot 정확도 향상: 7.85%)에서의 향상은 그러한 예이다.</li></ul></li></ul><h1>6. Conclusion, Limitation and Future Work</h1><p>본 연구에서는 VLMs 를 조정하기 위한 새로운 접근 방식인 TaskRes 를 제안한다. </p><p>TaskRes 는 VLMs 에서 efficient transfer learning(ETL)을 수행하며, classifier 를 두 가지 핵심 부분으로 명시적으로 분리한다: rich prior knowledge 을 갖춘 손상되지 않은 base classifier 와 base classifier 에 독립적인 task residual 를 통해 task-specific knowledge 을 더 잘 탐색할 수 있게 한다. </p><p>흥미롭게도, learned task residual 의 크기는 pre-trained VLMs 를 target downstream tasks 로 transfer difficulty 와 밀접하게 관련이 있다.</p><p>이는 ETL 을 새로운 시각으로 바라보는 데 영감을 줄 수 있으며, 예로 &quot;task-to-task transfer difficulty&quot; modeling 을 고려할 수 있다. </p><p>광범위한 실험을 통해 TaskRes 의 효과성이 입증되었다.</p><hr><p>그럼에도 불구하고 여기에는 몇 가지 한계가 있다. 예로, OxfordPets (1-shot)와 Food101 (1-/2-/4-/8-shot) dataset 에서 negative transfer 문제가 발생한다. </p><p>이러한 문제는 두 가지 조건에서 발생한다고 추측한다: (i) downstream tasks 이 high relative transfer difficulty 를 갖고 있으며 (Fig. 4) (ii) Zero-shot CLIP 이 이미 공정한 정밀도를 달성한 경우이다. </p><p>또한, 본 연구에서 transfer difficulty 의 평가는 휴리스틱적이다. </p><p>foundation model 의 급속한 발전에 따라, pre-trained foundation model 을 downstream tasks 로의 transfer difficulty 를 정확하고 신뢰할 수 있는 metric 으로 평가하는 것이 점점 더 중요해지고 있다. </p><p>distribution analysis 를 포함한 포괄적인 transfer difficulty 연구가 요구된다. </p><p>또한, transfer difficulty 를 concept-wise level 로 확장하고, visual concepts 의 currence frequency 와 성능 간의 상관관계를 조사하기 위해 특정 dataset 에서 훈련된 CLIP (e.g., SLIP 을 YFCC15M 에서 훈련한 모델)을 탐색할 수 있다.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/task-res">TaskRes</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/adapter">Adapter</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/transfer-learning">Transfer Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/peft">PEFT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/etl">ETL</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/at">AT</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/residual-tuning">Residual Tuning</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/PEFT/Module/2022-11-TaskRes.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/PEFT/Module/Tip-Adapter"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/PEFT/Module/CLAP"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-vision-language-models" class="table-of-contents__link toc-highlight">2.1. Vision-Language Models</a></li><li><a href="#22-efficient-transfer-learning" class="table-of-contents__link toc-highlight">2.2. Efficient Transfer Learning</a></li><li><a href="#23-few-shot-learning" class="table-of-contents__link toc-highlight">2.3. Few-Shot Learning</a></li><li><a href="#31-contrastive-language-image-pre-training" class="table-of-contents__link toc-highlight">3.1. Contrastive Language-Image Pre-training</a></li><li><a href="#32-revisiting-previous-tuning-paradigms" class="table-of-contents__link toc-highlight">3.2. Revisiting Previous Tuning Paradigms</a></li><li><a href="#41-pitfalls-of-existing-etl-paradigms-on-vlms" class="table-of-contents__link toc-highlight">4.1. Pitfalls of Existing ETL Paradigms on VLMs</a></li><li><a href="#42-task-residual-tuning" class="table-of-contents__link toc-highlight">4.2. Task Residual Tuning</a></li><li><a href="#51-setup" class="table-of-contents__link toc-highlight">5.1. Setup</a></li><li><a href="#52-implementation" class="table-of-contents__link toc-highlight">5.2. Implementation</a></li><li><a href="#53-performance-comparison" class="table-of-contents__link toc-highlight">5.3. Performance Comparison</a><ul><li><a href="#531-few-shot-learning" class="table-of-contents__link toc-highlight">5.3.1. Few-Shot Learning</a></li><li><a href="#532-domain-generalization" class="table-of-contents__link toc-highlight">5.3.2. Domain Generalization</a></li></ul></li><li><a href="#54-ablation-study" class="table-of-contents__link toc-highlight">5.4. Ablation Study</a></li><li><a href="#55--investigation-of-learned-task-residual" class="table-of-contents__link toc-highlight">5.5.  Investigation of Learned Task Residual</a></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.b3655ab3.js"></script>
<script src="/assets/js/main.30f56a58.js"></script>
</body>
</html>