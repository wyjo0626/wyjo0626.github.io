<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Paper/Vision-Language/2023-03-Prismer">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Prismer: A Vision-Language Model with An Esemble of Experts | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://wyjo0626.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://wyjo0626.github.io/docs/Paper/Vision-Language/Prismer"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Prismer: A Vision-Language Model with An Esemble of Experts | My Site"><meta data-rh="true" name="description" content="논문 및 이미지 출처 :"><meta data-rh="true" property="og:description" content="논문 및 이미지 출처 :"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/Prismer"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/Prismer" hreflang="en"><link data-rh="true" rel="alternate" href="https://wyjo0626.github.io/docs/Paper/Vision-Language/Prismer" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="My Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="My Site Atom Feed">




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.d2ad26d0.css">
<link rel="preload" href="/assets/js/runtime~main.352e1c6f.js" as="script">
<link rel="preload" href="/assets/js/main.7d442e7c.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function e(e){document.documentElement.setAttribute("data-theme",e)}var t=function(){var e=null;try{e=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(e){}return e}()||function(){var e=null;try{e=localStorage.getItem("theme")}catch(e){}return e}();null!==t?e(t):window.matchMedia("(prefers-color-scheme: dark)").matches?e("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,e("light"))}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="WYJLab Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">WYJLab</b></a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/projects">Projects</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---basics">Tutorial - Basics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Basics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/tutorial---extras">Tutorial - Extras</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorial - Extras&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Paper</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Computer Vision/Adversarial Attack/AR">Computer Vision</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Machine Learning/Regularization/Dropout/Dropout">Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/NLP/Analysis/Contextualized Representation">NLP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Survey/Prompting">Survey</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Vision-Language</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/UPL">Unsupervised Prompt Learning for Vision-Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Paper/Vision-Language/Img2LLM">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Paper/Vision-Language/Prismer">Prismer: A Vision-Language Model with An Esemble of Experts</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Contrastive Learning/ALIGN">Contrastive Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/PEFT/Composition/CLIP-LoRA">PEFT</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Single-Stream/VLBERT">Single-Stream</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Paper/Vision-Language/Two-Stream/LXMERT">Two-Stream</a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Programming/Algorithm/">Programming</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Paper</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Vision-Language</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Prismer: A Vision-Language Model with An Esemble of Experts</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Prismer: A Vision-Language Model with An Esemble of Experts</h1></header><p>논문 및 이미지 출처 : <a href="https://arxiv.org/pdf/2303.02506.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2303.02506.pdf</a></p><h1>Abstract</h1><p>최근 vision-language 모델이 multi-modal 능력을 보이기 위해 거대한 학습이 필요하다.</p><p>이에 Prismer 로 데이터 및 파라미터에 효율적인 vision-language 를 소개</p><p>Prismer 는 적은 수의 구성요소만 학습하며, 대부분의 가중치는 미리 학습된 domain experts 로부터 상속받아 학습 중 동결 상태를 유지한다.</p><p>넓은 범위의 domain experts 를 모아, Prismer 는 효율적으로 expert knowledge 를 수집하고, 다양한 vision-language 추론 작업에 적용할 수 있음을 보여준다.</p><p>최대 2배 적은 데이터로도 Prismer 는 SOTA 를 달성한 모델들과 경쟁력 있는 fine-tuning 과 few-shot learning 성능에 도달 했다.</p><h1>1. Introduction</h1><p>pretraining 한 대규모 모델은 다양한 작업에 좋은 일반화 능력을 가졌지만 대량의 훈련 데이터 및 계산 비용이 든다.</p><p>특히, vision-language 은 image captioning, visual question answering 등과 같은 multi-modal 추론이 필요하므로 recognition, detection, counting, 3D perception 등의 많은 기술이 요구된다.</p><p>일반적으로 이들은 대규모 데이터를 학습한 모델로 해결한다.</p><p>대신에, 저자들의 접근법으로 <strong>experts</strong> 라고 하는 <strong>서로 분리된 sub-network</strong> 로 이러한 스킬들과 domain knowledge 을 학습시키는 것이다.</p><p>이 모델은 모든 것을 한번 학습하는 것보다, 여러 스킬과 domain knowledge 를 <strong>통합</strong>하는데 초점을 둔다. 이 방법은 multi-modal 학습을 <strong>축소</strong>하는데 효과적인 방법이다.</p><p>Prismer 의 핵심 설계는 다음 요소를 포함한다.</p><ul><li><strong>web-scale knowledge</strong> 에 대한 강력한 vision, language 백본 모델</li><li><strong>auxiliary knowledge</strong> 형태의 <strong>low-level vision signals</strong> (e.g. depth) 과 <strong>high-level vision signals</strong> (e.g. instance, segmentic label) 로 다양한 형태의 vision 정보를 인코딩한 vision experts</li></ul><p>모든 expert model 은 따로 따로 pre-trained 하여 동결하여 전체 network parameter 에 약 20% 에 해당하는 학습가능한 components 로 연결된다.</p><p>Prismer 는 13M 에도 불구하고 image captioning, image classification, visual question answering 등의 multi-modal 에 좋은 추론 성능을 보인다.</p><p>마지막으로, Prismer 의 다음 학습 방식을 분석한다.</p><ul><li>noisy experts 에 대해 강한 견고함을 보임</li><li>학습 성능 또한 experts 양이 증가함에 따라 호의적으로 확장</li></ul><h1>2. Related Work</h1><h1>3. Prismer: Open-ended Reasoning with Multi-modal Knowledge</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-model-overview">3.1 Model Overview<a href="#31-model-overview" class="hash-link" aria-label="Direct link to 3.1 Model Overview" title="Direct link to 3.1 Model Overview">​</a></h2><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/76352ffc-fabd-4123-977c-b254366c52c7/image.png" class="img_ev3q"></p><p>Prismer 는 <strong>encoder-decoder</strong> 인 transformer model 이다.</p><ul><li><p><strong>endoer</strong> : vision</p><ul><li>input : RGB image 와 multi-label labels (e.g. depth, surface normal, segmentation ..)</li><li>output : RGB 와 multi-modal features 의 sequence</li></ul></li><li><p><strong>decoder</strong> : auto-regressive language</p><ul><li>cross attention 을 통해 multi-modal features 를 조절</li><li>output : 텍스트 token 의 sequence</li></ul></li></ul><p>위 Prismer 는 다른 SOTA 만큼의 성능에 도달하는데 필요한 GPU 시간을 줄였다.</p><p>web-sacle 을 학습한 pretrained vision 과 language 인 top backbone 모델로 만들어 졌다.</p><p>또한 multi-modal signals 를 받아들이기 위해 vision encoder 를 확장하였으며, 이는 generated multi-modal auxiliary knowledge 와 capture semantic 을 가능케 했다.</p><p>예를 들어,</p><ul><li>&quot;text-reading&quot; 은 OCR detection expert 로 해결</li><li>&quot;object-recognition&quot; 은 object detection 으로 해결</li></ul><p>모든 visual expert labels 은 Prismer 에 포함한다.</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/f1c75f8d-be68-49f7-bc04-a581b7ce1e34/image.png" class="img_ev3q"></p><p>Prismer 는 생성 모델로, language modeling 이나 prefix language modeling 같은 vision-language 추론 작업을 새로 만들었다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-pre-trained-experts">3.2 Pre-trained Experts<a href="#32-pre-trained-experts" class="hash-link" aria-label="Direct link to 3.2 Pre-trained Experts" title="Direct link to 3.2 Pre-trained Experts">​</a></h2><p>Prismer 는 두 가지의 pre-trained experts 를 포함한다.</p><ul><li><strong>Backbone Experts</strong><ul><li>vision 과 language 모델 모두 transformer 아키텍처를 기반으로 한다</li><li>학습가능한 components 로 쉽게 연결</li><li>모델 parameter 에 encoding 된 domain knowledge 를 보존하기 위해 pretraining 면서 대부분의 weight 를 동결</li></ul></li><li><strong>Modality Experts</strong><ul><li>low-level vision signals: depth, surface, edge; high-level vision signals: object labels, segmentation labels, text labels; 를 인코딩한 6 modality expert 포함</li><li>위 mdality experts 는 <strong>black-box 예측기</strong></li><li>modality experts 의 weight 를 동결하여 <strong>어떤 설계</strong>도 가능하도록 함</li></ul></li></ul><p>위 predicted labels 에 modality 별로 후 처리 후 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="double-struck">R</mi><mrow><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^{H \times W \times C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em">W</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em">C</span></span></span></span></span></span></span></span></span></span></span></span></span> tensor 로 transforming 한다 (H, W, C 는 height, width, channel 임. e.g. depth 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">C = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span></span>, surface 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">C = 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">3</span></span></span></span></span>).</p><p>high-level semantic signal 를 인코딩한 모든 experts 에 대해, 각 픽셀을 이에 대응하는 pretrained CLIP model 로 text embedding 과 함께 tiling 한다.</p><p>이후 효과적인 훈련을 위해서 PCA 를 적용하여 차원수를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">C = 64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">64</span></span></span></span></span> 으로 down-sampling 한다.</p><p>모든 modality experts 에 대한 자세한 사항은 아래 테이블과 같다.</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/d3d65a86-1535-4819-bb0e-bc935d14eeb4/image.png" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-key-architectural-components">3.3 Key Architectural Components<a href="#33-key-architectural-components" class="hash-link" aria-label="Direct link to 3.3 Key Architectural Components" title="Direct link to 3.3 Key Architectural Components">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="modality-specific-convolutional-stem">Modality-Specific Convolutional Stem<a href="#modality-specific-convolutional-stem" class="hash-link" aria-label="Direct link to Modality-Specific Convolutional Stem" title="Direct link to Modality-Specific Convolutional Stem">​</a></h3><p>모든 experts labels 는 처음에 무작위로 초기화된 convolution layer 를 지난다.</p><ul><li>같은 차원수로 매핑하기 위함</li><li>5 convolution layers 를 적용</li><li>각각에 <code>[3 x 3]</code> 의 작은 커널을 구성</li><li>기존 ViT 의 큰 커널로 된 single convolutional layer 보다 성능이 좋음</li></ul><p>For high-level semantic labels</p><ul><li>실행중인 메모리를 보존하기 위해 해상도를 4배로 다운 샘플링</li><li>object instance 간의 차이를 식별하기 위해 학습 가능한 무작위로 샘플링된 embedding 을 추가 <strong>→ instance embedding</strong>, 128 로 설정</li></ul><p>For RGB images</p><ul><li>간단하게 pretrained vision 백본으로 convolutional stem 정의</li><li>모든 modality expert embedding 은 RGB 를 포함하며, transformer layer 를 지나기 전에, pretrained <strong>positional embedding</strong> 을 추가한다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="experts-resampler">Experts Resampler<a href="#experts-resampler" class="hash-link" aria-label="Direct link to Experts Resampler" title="Direct link to Experts Resampler">​</a></h3><p>self-attention 의 계산 복잡도는 patch 수에 비례하므로, modality experts 의 수가 크면 쉽게 많은 메모리를 요구할 수도 있다. </p><p>이 이슈를 해결하기 위해 <strong>Experts Resampler</strong> 를 제안한다.</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/76f4b7fe-5966-4edb-85ac-46c2dc2d3165/image.png" class="img_ev3q"></p><p>For Experts Resampler</p><ul><li>다양한 experts 를 input 으로 받음</li><li>고정된 수의 embedding 을 출력</li><li>language decoder 와 vision encoder 와 무관하게 self-attention 계산에 대해 일정한 메모리를 소모</li><li>모든 multi-modal features 에서 연결된 flattened embedding 을 cross-attend 하기 위해 pretrained latent query 를 학습</li><li>이후 multi-modal features 를 <strong>auxiliary knowledge distillation</strong> 형태처럼 latent query 의 수와 동일한 작은 수의 토큰으로 압축한다.</li><li>결국, 더 좋은 효과를 위해 multi-modal features 와 learned latent queries 를 연결하기 위해 key 와 value 로 설계</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="lightweight-adaptor">Lightweight Adaptor<a href="#lightweight-adaptor" class="hash-link" aria-label="Direct link to Lightweight Adaptor" title="Direct link to Lightweight Adaptor">​</a></h3><p>multi-modal features 의 표현력과 훈련성을 향상 시키기 위해 vision 과 language 백본 모델의 각 transformer layer 에 <strong>lightweight adaptor</strong> 를 삽입</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/f7bedfb4-dc09-41cd-81a1-4211c3e55cb4/image.png" class="img_ev3q"></p><p>For Lightweight Adaptor</p><ul><li>먼저, input feature 를 non-linearity 적용으로 작은 차원으로 down-projection 한다.</li><li>훈련 안정성을 위해 Squared ReLU 를 사용한다.</li><li>이후, input 의 원래 차원으로 되돌리기 위해 up-projection 한다.</li><li>residual connection 을 이용하여, identity function 을 일치시키기 위해 near-zero weights 로 모든 adaptor 를 초기화 한다.</li></ul><p>위 adaptor 를 통해서, language decoder 에서 cross attention block 과 연결하여 domain 별 vision 및 language 백본을 vision-language 로 자연스럽게 변환시킨다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-training-objective">3.4 Training Objective<a href="#34-training-objective" class="hash-link" aria-label="Direct link to 3.4 Training Objective" title="Direct link to 3.4 Training Objective">​</a></h2><p>저자는 Prismer 를 <strong>next token 을 autoregressive 하게 예측</strong>하기 위한 한 가지 목표로 훈련 시켰다.</p><p>표준 encoder-decoder 아키텍처에 따라 다음의 forward autoregressive factorisation 진행</p><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mtext> </mtext><mi>l</mi><mi>o</mi><mi>g</mi><mtext> </mtext><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>y</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo separator="true">,</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L = - \sum^T_{t=1}\ log\ p(y_t |y_{&lt;t},z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2809em;vertical-align:-0.2997em"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em"><span></span></span></span></span></span></span><span class="mspace"> </span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mspace"> </span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mclose">)</span></span></span></span></span></p><ul><li>vision encoder 의 multi-modal feature 예측값 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span></li><li>language decoder 는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span></span></span></span></span> 길이만큼 text caption <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span> 의 조건부 우도 (conditional likelihood)를 최대화하도록 학습</li></ul><p>위 목표는 gradient 계산을 위해 한 번의 forward pass 만 요구되며, 다른 VLMs 보다 효과적이고 능률적이다.</p><p>하지만 모델은 multi-modal language generation 에 초점을 두기 때문에, image-text retrieval 이나 visual entailment 와 같은 mutli-modal discriminative task 에는 적합하지 못하다.</p><h1>4. Experiments</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-prismer-model-variants">4.1 Prismer Model Variants<a href="#41-prismer-model-variants" class="hash-link" aria-label="Direct link to 4.1 Prismer Model Variants" title="Direct link to 4.1 Prismer Model Variants">​</a></h2><p>Prismer 말고도, Experts Resampler 없이 RGB 이미지 만으로 학습을 진행한 PrismerZ 도 있다.</p><p>두 모델은 vision encoder 에 pretrained CLIP, language decoder 에 RoBERTa 를 활용했다.</p><p>실험 초기엔 다른 언어 모델인 OPT 나 BLOOM 을 사용했지만 좋은 성능은 내지 못했다.</p><p>모델 사이즈는 LARGE, BASE 두 가지로 진행을 한다.</p><ul><li>BASE : ViT-B/16 and RoBERTa<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>B</mi><mi>A</mi><mi>S</mi><mi>E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{BASE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em">B</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.05764em">SE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span></li><li>LARGE : ViT-L/14 and RoBERTa<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow></mrow><mrow><mi>L</mi><mi>A</mi><mi>R</mi><mi>G</mi><mi>E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">_{LARGE}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4783em;vertical-align:-0.15em"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em"><span style="top:-2.55em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.05764em">RGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span></li></ul><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/319e0c6d-51eb-48ec-8325-0e207582a43c/image.png" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-training-and-evaluation-details">4.2 Training and Evaluation Details<a href="#42-training-and-evaluation-details" class="hash-link" aria-label="Direct link to 4.2 Training and Evaluation Details" title="Direct link to 4.2 Training and Evaluation Details">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="pre-training-datasets">Pre-training Datasets<a href="#pre-training-datasets" class="hash-link" aria-label="Direct link to Pre-training Datasets" title="Direct link to Pre-training Datasets">​</a></h3><p>in-domain 데이터셋</p><ul><li>COCO</li><li>Visual Genome</li></ul><p>web 데이터셋</p><ul><li>Conceptual Captions</li><li>SBU captions</li><li>Conceptual 12M</li></ul><p>web 데이터셋은 image captioner 로 pre-filter 와 re-caption 을 거쳤다.</p><p>데이터셋은 11M image 또는 12.7M 의 image/text 쌍을 포함하고 있다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimisation-and-implementation">Optimisation and Implementation<a href="#optimisation-and-implementation" class="hash-link" aria-label="Direct link to Optimisation and Implementation" title="Direct link to Optimisation and Implementation">​</a></h3><p><strong>Optimizer</strong></p><ul><li>AdamW</li><li>weight decay, 0.05</li></ul><p><strong>Model Sharding</strong></p><ul><li>model parameters 의 일부만 훈련이 가능하여 고해상도 fine-tuning 할 때만 적용</li><li>모든 GPU 에 교차하여 optimiser states 와 parameter gradient 가 가능한 ZeRO Stage 2 기술을 채용</li></ul><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary><b><u>ZeRO Stage</u></b></summary><div><div class="collapsibleContent_i85q">분산 학습 및 추론을 효율적이고, 효과적으로 만드는 딥러닝 최적화 라이브러리</div></div></details><p><strong>Mixed Precision</strong></p><ul><li>훈련 시간 감소를 위해 Automatic Mixed Precision (AMP) 사용</li><li>fp16 precision 적용</li></ul><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary><b><u>AMP</u></b></summary><div><div class="collapsibleContent_i85q">일반적으로 모델 학습 시 FP32 (부동소수점 표기법, 32bit) 를 사용하는데, 연산량 감소를 위해 FP16 을 적용하면 loss 가 올라가는 현상이 있음<image src="https://velog.velcdn.com/images/whdnjsdyd111/post/3a4e0a70-32cc-4d01-aa97-a13cf610db6d/image.png"></image>비트 수가 줄어든 만큼 backpropagation 을 진행하면서 정확한 수를 표현 못하기 때문이다. <br>이에 NVIDIA 측에서 이를 해결하고자 다음과 같은 automatic 방법을 제안한 것<image src="https://velog.velcdn.com/images/whdnjsdyd111/post/25f7c3b3-e0cf-4529-8c7f-92b4fe640762/image.png"></image>모델 최적화 및 훈련 속도 감소를 가능케 한다.</div></div></details><h3 class="anchor anchorWithStickyNavbar_LWe7" id="evalution-setting">Evalution Setting<a href="#evalution-setting" class="hash-link" aria-label="Direct link to Evalution Setting" title="Direct link to Evalution Setting">​</a></h3><p>image captioning 평가를 위해 텍스트 생성을 beam size 3 으로 beam search 를 사용한다.</p><p>fine-tining 된 image captioning 에 &quot;A picture of&quot; 라는 접두사 prompt 를 input text 에 추가하니 품질 개선에 도움이 되는 것을 발견했다.</p><p>VQA 및 image classification 평가에 대해서는 미리 정의된 답변 목록에서 token 단위로 log-likelihood 를 순위로 매겨서 closed ended 방식으로 평가한다.</p><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary><b><u>beam search</u></b></summary><div><div class="collapsibleContent_i85q">NLP 분야의 Decoder 알고리즘<br><br>모든 단어 조합을 생성하고 그 중 가장 가능성이 높은 조합을 선택하는데, 이때 단어 조합 수를 파라미터인 &quot;beam size&quot; 를 설정한다. 즉, beam size 는 후보군 개수이다.</div></div></details><details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary><b><u>Open-ended &amp; Close-ended</u></b></summary><div><div class="collapsibleContent_i85q">Open-ende 는 문제에 대한 미리 정의된 대답에 제한되지 않고, 개념적 이해나 문맥적 이해가 필요한 것. 예로 이미지 캡셔닝 작업에서 이미지에 대한 설명이나 이야기를 생성<hr>Close-ended 는 미리 정의된 목록에서 선택할 수 있는 한정된 대답만 가지고 문제를 해결 하는 것. 예로 이미지가 주어졌을 때 미리 정의된 카테고리 목록에 해당하여 분류 되는지 봄</div></div></details><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-results-on-vision-language-benchmarks">4.3 Results on Vision-Language Benchmarks<a href="#43-results-on-vision-language-benchmarks" class="hash-link" aria-label="Direct link to 4.3 Results on Vision-Language Benchmarks" title="Direct link to 4.3 Results on Vision-Language Benchmarks">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="fine-tuned-performance-coco-caption-nocaps-and-vqav2">Fine-tuned Performance COCO Caption, NoCaps and VQAv2<a href="#fine-tuned-performance-coco-caption-nocaps-and-vqav2" class="hash-link" aria-label="Direct link to Fine-tuned Performance COCO Caption, NoCaps and VQAv2" title="Direct link to Fine-tuned Performance COCO Caption, NoCaps and VQAv2">​</a></h3><p>표준 cross-entropy loss 로 COCO Caption 을 fine-tuning 진행</p><p>이후, COCO Caption test 와 NoCaps validation, VQAv2 dataset 와 Visual Genome training samples 로 평가</p><p>다른 VLMs 모델들과 다음과 같이 비교를 하였다.</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/072772c8-dfae-43a5-b12f-809dbc2cd614/image.png" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="zero-shot-performance-on-image-captioning">Zero-shot Performance on Image Captioning<a href="#zero-shot-performance-on-image-captioning" class="hash-link" aria-label="Direct link to Zero-shot Performance on Image Captioning" title="Direct link to Zero-shot Performance on Image Captioning">​</a></h3><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/aa0fdebf-ba6e-4ccb-a8c5-a4e32d9806f9/image.png" class="img_ev3q"></p><pre tabindex="0" class="codeBlockStandalone_MEMb thin-scrollbar codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><b>Fig 4</b></code></pre><p>Prismer approach 는 zero-shot 생성도 가능하며, 추가적인 fine-tuning 없이 image captioning 에 직접적으로 적용이 가능하다.</p><p>위 중앙 테이블을 살펴보자.</p><ul><li>NoCaps 데이터셋에서, 140회 정도의 훈련만으로 SimVLM 과 경쟁력이 있다.</li></ul><p>다음과 같은 Prismer 로 생성한 caption 목록 예제를 보여준다.</p><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/e9ce1f68-c3d3-473c-a4fa-ee16c4c2cf2a/image.png" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="few-shot-performance-on-imagenet-classification">Few-shot Performance on ImageNet Classification<a href="#few-shot-performance-on-imagenet-classification" class="hash-link" aria-label="Direct link to Few-shot Performance on ImageNet Classification" title="Direct link to Few-shot Performance on ImageNet Classification">​</a></h3><p>few-shot 으로 ImageNet 데이터셋을 통해 평가를 진행</p><p>&quot;A photo of a <code>[CLASS NAME]</code>&quot; 과 같은 임시 caption 으로 각 카테고리를 매핑하여 classification task 로 변환하였으며, log-likelihood 를 사용하여 모든 caption 에 점수를 매긴다.</p><p>Flamingo 는 gradient 업데이트 없이 in-context (문맥에 포함되는지) 를 통해 few-shot 한것과 달리, Prismer 는 가벼운 fine-tuning 으로 few-shot 하였다.</p><p>Fig 4 의 오른쪽을 살펴보자.</p><ul><li>GIT 이나 Flamingo 보다는 좋은 실적을 내지 못함.</li><li>few-shot 에서 백본 모델인 ViT-B 와 ViT-L 과 큰 차이로 성능이 좋음</li><li>위 사실로, 더 좋은 experts label 이나 vision backbone 으로 성능을 더 끌어올릴 수 있다는 것을 시사</li></ul><h1>5. Additional Analysis</h1><p>저자는 Prismer 에 대한 추가적인 조사와 발견을 위해 추가 실험을 수행, 여러 component 아키텍처 를 제거</p><p>모든 실험은 BASE 모델로 진행하였으며, 총 3M data 인 Conceptual Captions 와 SBU 를 결합하여 훈련 하였으며, 평가는 <code>[224 x 224]</code> 해상도의 VQAv2 로 평가했다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="51-intriguing-properties-of-prismer">5.1 Intriguing Properties of Prismer<a href="#51-intriguing-properties-of-prismer" class="hash-link" aria-label="Direct link to 5.1 Intriguing Properties of Prismer" title="Direct link to 5.1 Intriguing Properties of Prismer">​</a></h2><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/1ae80861-7b75-476c-bec4-d82502e14b0e/image.png" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="more-experts-better-performance">More Experts, Better Performance<a href="#more-experts-better-performance" class="hash-link" aria-label="Direct link to More Experts, Better Performance" title="Direct link to More Experts, Better Performance">​</a></h3><ul><li>위 그림의 (a) 와 같이 더 많은 modality experts 를 추가하니 성능 개선이 나타남</li><li>이유는 모델에 더 많은 domain knowledge 를 제공할 수 있기 때문</li><li>하지만 끝내 성능이 정체되며, 이후 추가되는 modality experts 는 확실한 이득을 제공하진 않음</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="better-experts-better-performance">Better Experts, Better Performance<a href="#better-experts-better-performance" class="hash-link" aria-label="Direct link to Better Experts, Better Performance" title="Direct link to Better Experts, Better Performance">​</a></h3><ul><li>위 그림 (b) 와 같이 expert 의 퀄리티에 대한 영향을 평가</li><li>일정 수의 예측된 depth labels 를 손상된 depth 로 교체 (균일하게 분포된 무작위 noise 를 샘플링)</li><li>위 사항으로 좋은 quality experts 는 더 정확한 domain knowledge 를 제공한다는 사실을 관찰</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="robustness-to-noisy-experts">Robustness to Noisy Experts<a href="#robustness-to-noisy-experts" class="hash-link" aria-label="Direct link to Robustness to Noisy Experts" title="Direct link to Robustness to Noisy Experts">​</a></h3><ul><li>위 그림 (c) 와 같이Prismer 가 noise 를 예측하는 expert 를 포함해도 성능이 유지되는 것을 관찰</li><li>RGB 이미지만 학습한 것 보다 noise 를 추가한 것이 정확도가 좋다.</li><li>위 사항은 암묵적으로 정규화로 간주될 수 있으며, Prismer 가 유익하지않은 expert 에 대해서도 안전하게 학습하고 성능 저하를 일으키지 않는 다는 것이다</li><li>표준적인 multi-task 나 auxiliary learning 보다 더 효과적인 학습 전략이라는 것을 암시</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="52-architecture-design-and-training-details">5.2 Architecture Design and Training Details<a href="#52-architecture-design-and-training-details" class="hash-link" aria-label="Direct link to 5.2 Architecture Design and Training Details" title="Direct link to 5.2 Architecture Design and Training Details">​</a></h2><p><img loading="lazy" src="https://velog.velcdn.com/images/whdnjsdyd111/post/943b65ef-c8fa-4101-9e09-6e192422f7ba/image.png" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="adaptor-design-and-size">Adaptor Design and Size<a href="#adaptor-design-and-size" class="hash-link" aria-label="Direct link to Adaptor Design and Size" title="Direct link to Adaptor Design and Size">​</a></h3><p>adaptor 설계에 대한 ablation study 는 위 표에서 확인할 수 있다.</p><p>표준 residual connection 과 encoder-decoder 구조가 포함된, 간단한 adaptor design 이 가장 성능이 좋았다.</p><p>각 transformer layer 끝마다 adaptor 를 추가하거나 learnable gating 메커니즘을 구성한 복잡한 설계에 대해서는 성능이 좋지 않았다.</p><p>나아가, 단일 adaptor 에 큰 bottleneck hidden size 를 주니 개선된 성능을 보였다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="resampler-design-and-multi-modal-sampling-strategy">Resampler Design and Multi-modal Sampling Strategy<a href="#resampler-design-and-multi-modal-sampling-strategy" class="hash-link" aria-label="Direct link to Resampler Design and Multi-modal Sampling Strategy" title="Direct link to Resampler Design and Multi-modal Sampling Strategy">​</a></h3><p>resampler 설계에 대한 ablation study 는 위 표에서 확인할 수 있다.</p><p>간단한 설계가 학습에 가장 적합했다.</p><p>무작위로 non-learnable 샘플링한 접근법은 learnable resampler 보다 성능이 낮았고, resampler 를 RGB 를 포함하여 모든 input signal 을 받아 들이니 (Prismer design 은 RGB 에 대해선 받지 않음) 성능 감소가 일어났다.</p><p>마지막으로 resampler 크기를 키우니 이득을 얻지 못하였다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-effect-of-frozen-backbones">The Effect of Frozen Backbones<a href="#the-effect-of-frozen-backbones" class="hash-link" aria-label="Direct link to The Effect of Frozen Backbones" title="Direct link to The Effect of Frozen Backbones">​</a></h3><p>모델을 freezing 한 것과, pre-training 및 fine-tuning 을 비교한 실험을 진행</p><p>freezing pre-trained 파라미터가 좋은 성능이 나타났으며, 과적합 및 학습하며 배운 knowledge 을 잊는 것을 피하였다.</p><p>또한 이 파라미터들을 freezing 을 하니 GPU 메모리의 상당 수의 양을 save 하였다.</p><p>심지어 다른 downstream task 에 fine-tuning 할 때도 이득을 얻었다.</p><h1>6. Conclusions, Limitations and Discussion</h1><p>이 논문의 결론으로 Prismer  가 적은 수의 trainable components 를 활용하여, Image captioning, VQA, image classification 등에 좋은 성능을 보인다는 것을 말한다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="multi-modal-in-context-learning">Multi-modal In-context Learning<a href="#multi-modal-in-context-learning" class="hash-link" aria-label="Direct link to Multi-modal In-context Learning" title="Direct link to Multi-modal In-context Learning">​</a></h3><ul><li>zero-shot in-context generalisation 은 큰 언어 모델에만 존재하는 신생적인 특성</li><li>Prismer 는 효율적인 학습을 중점으로 두어, 작은 규모라서 few-shot in-context prompting 수행 능력이 없음</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="zero-shot-adaptation-on-new-experts">Zero-shot Adaptation on New Experts<a href="#zero-shot-adaptation-on-new-experts" class="hash-link" aria-label="Direct link to Zero-shot Adaptation on New Experts" title="Direct link to Zero-shot Adaptation on New Experts">​</a></h3><ul><li>다른 데이터셋으로 pre-train 한 segmentation expert 로 pre-train 된 Prismer 의 추론을 실험</li><li>동일한 언어 모델로 semantic label 을 인코딩하지만, 서로 다른 semantic 정보에 대한 experts 에 대해 제한된 적응성을 보여, 성능 저하를 일으킴</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="free-form-inference-on-partial-experts">Free-form Inference on Partial Experts<a href="#free-form-inference-on-partial-experts" class="hash-link" aria-label="Direct link to Free-form Inference on Partial Experts" title="Direct link to Free-form Inference on Partial Experts">​</a></h3><ul><li>위 내용과 비슷하게, Prismer 가 pretraining 할 때 포함되는 모든 experts 의 multi-modal features 에 얽매이는 것을 발견</li><li>따라서 추론 중 일부 experts 만 있으면 성능 저하를 일으킴</li><li>마스킹된 auto-encoding 같은 다른 훈련 목표로, Prismer 를 임의의 수로 experts 를 기반으로 추론하니 성능 저하가 일어남 </li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="representation-of-expert-knowledge">Representation of Expert Knowledge<a href="#representation-of-expert-knowledge" class="hash-link" aria-label="Direct link to Representation of Expert Knowledge" title="Direct link to Representation of Expert Knowledge">​</a></h3><p>Prismer 는 모든 experts 에 대해 후 처리 후 이미지와 동일한 3차원 텐서로 변환하는데, object detection labels 을 텍스트 토큰 시퀀스로 변환과 같은 domain knowladge 을 나타내는 효과적인 방법도 있으며, 이는 앞으로의 연구에서 더 강한 추론과 안정적인 훈련이 가능할 것으로 보임.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/prismer">Prismer</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/vision-language">vision-language</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/docs/tags/experts">experts</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/2023-03-Prismer.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Paper/Vision-Language/Img2LLM"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Paper/Vision-Language/Contrastive Learning/ALIGN"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#31-model-overview" class="table-of-contents__link toc-highlight">3.1 Model Overview</a></li><li><a href="#32-pre-trained-experts" class="table-of-contents__link toc-highlight">3.2 Pre-trained Experts</a></li><li><a href="#33-key-architectural-components" class="table-of-contents__link toc-highlight">3.3 Key Architectural Components</a><ul><li><a href="#modality-specific-convolutional-stem" class="table-of-contents__link toc-highlight">Modality-Specific Convolutional Stem</a></li><li><a href="#experts-resampler" class="table-of-contents__link toc-highlight">Experts Resampler</a></li><li><a href="#lightweight-adaptor" class="table-of-contents__link toc-highlight">Lightweight Adaptor</a></li></ul></li><li><a href="#34-training-objective" class="table-of-contents__link toc-highlight">3.4 Training Objective</a></li><li><a href="#41-prismer-model-variants" class="table-of-contents__link toc-highlight">4.1 Prismer Model Variants</a></li><li><a href="#42-training-and-evaluation-details" class="table-of-contents__link toc-highlight">4.2 Training and Evaluation Details</a><ul><li><a href="#pre-training-datasets" class="table-of-contents__link toc-highlight">Pre-training Datasets</a></li><li><a href="#optimisation-and-implementation" class="table-of-contents__link toc-highlight">Optimisation and Implementation</a></li><li><a href="#evalution-setting" class="table-of-contents__link toc-highlight">Evalution Setting</a></li></ul></li><li><a href="#43-results-on-vision-language-benchmarks" class="table-of-contents__link toc-highlight">4.3 Results on Vision-Language Benchmarks</a><ul><li><a href="#fine-tuned-performance-coco-caption-nocaps-and-vqav2" class="table-of-contents__link toc-highlight">Fine-tuned Performance COCO Caption, NoCaps and VQAv2</a></li><li><a href="#zero-shot-performance-on-image-captioning" class="table-of-contents__link toc-highlight">Zero-shot Performance on Image Captioning</a></li><li><a href="#few-shot-performance-on-imagenet-classification" class="table-of-contents__link toc-highlight">Few-shot Performance on ImageNet Classification</a></li></ul></li><li><a href="#51-intriguing-properties-of-prismer" class="table-of-contents__link toc-highlight">5.1 Intriguing Properties of Prismer</a><ul><li><a href="#more-experts-better-performance" class="table-of-contents__link toc-highlight">More Experts, Better Performance</a></li><li><a href="#better-experts-better-performance" class="table-of-contents__link toc-highlight">Better Experts, Better Performance</a></li><li><a href="#robustness-to-noisy-experts" class="table-of-contents__link toc-highlight">Robustness to Noisy Experts</a></li></ul></li><li><a href="#52-architecture-design-and-training-details" class="table-of-contents__link toc-highlight">5.2 Architecture Design and Training Details</a><ul><li><a href="#adaptor-design-and-size" class="table-of-contents__link toc-highlight">Adaptor Design and Size</a></li><li><a href="#resampler-design-and-multi-modal-sampling-strategy" class="table-of-contents__link toc-highlight">Resampler Design and Multi-modal Sampling Strategy</a></li><li><a href="#the-effect-of-frozen-backbones" class="table-of-contents__link toc-highlight">The Effect of Frozen Backbones</a></li><li><a href="#multi-modal-in-context-learning" class="table-of-contents__link toc-highlight">Multi-modal In-context Learning</a></li><li><a href="#zero-shot-adaptation-on-new-experts" class="table-of-contents__link toc-highlight">Zero-shot Adaptation on New Experts</a></li><li><a href="#free-form-inference-on-partial-experts" class="table-of-contents__link toc-highlight">Free-form Inference on Partial Experts</a></li><li><a href="#representation-of-expert-knowledge" class="table-of-contents__link toc-highlight">Representation of Expert Knowledge</a></li></ul></li></ul></div></div></div><div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/wyjo0626/wyjo0626.github.io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Won-Yong Jo, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.352e1c6f.js"></script>
<script src="/assets/js/main.7d442e7c.js"></script>
</body>
</html>