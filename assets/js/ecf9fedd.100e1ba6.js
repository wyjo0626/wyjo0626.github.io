"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[32502],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>d});var i=a(67294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,i)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,i,n=function(e,t){if(null==e)return{};var a,i,n={},l=Object.keys(e);for(i=0;i<l.length;i++)a=l[i],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(i=0;i<l.length;i++)a=l[i],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=i.createContext({}),p=function(e){var t=i.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},m=function(e){var t=p(e.components);return i.createElement(s.Provider,{value:t},e.children)},g="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},c=i.forwardRef((function(e,t){var a=e.components,n=e.mdxType,l=e.originalType,s=e.parentName,m=r(e,["components","mdxType","originalType","parentName"]),g=p(a),c=n,d=g["".concat(s,".").concat(c)]||g[c]||u[c]||l;return a?i.createElement(d,o(o({ref:t},m),{},{components:a})):i.createElement(d,o({ref:t},m))}));function d(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var l=a.length,o=new Array(l);o[0]=c;var r={};for(var s in t)hasOwnProperty.call(t,s)&&(r[s]=t[s]);r.originalType=e,r[g]="string"==typeof e?e:n,o[1]=r;for(var p=2;p<l;p++)o[p]=a[p];return i.createElement.apply(null,o)}return i.createElement.apply(null,a)}c.displayName="MDXCreateElement"},62364:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>r,toc:()=>p});var i=a(87462),n=(a(67294),a(3905));const l={slug:"CLOSE",title:"I Can't Believe There's No Images! Learning Visual Tasks Using only Language Supervision",tags:["Vision-Language","Cross-Modal Transfer","Text-Only Training","Modality Gap Mitigation","Noise Injection","Stylistic Captioning","CLOSE"]},o=void 0,r={unversionedId:"Paper/Vision-Language/VQA-IC/Zero-shot/2022-11-CLOSE",id:"Paper/Vision-Language/VQA-IC/Zero-shot/2022-11-CLOSE",title:"I Can't Believe There's No Images! Learning Visual Tasks Using only Language Supervision",description:"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :",source:"@site/docs/Paper/Vision-Language/VQA-IC/Zero-shot/2022-11-CLOSE.md",sourceDirName:"Paper/Vision-Language/VQA-IC/Zero-shot",slug:"/Paper/Vision-Language/VQA-IC/Zero-shot/CLOSE",permalink:"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/CLOSE",draft:!1,editUrl:"https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/VQA-IC/Zero-shot/2022-11-CLOSE.md",tags:[{label:"Vision-Language",permalink:"/docs/tags/vision-language"},{label:"Cross-Modal Transfer",permalink:"/docs/tags/cross-modal-transfer"},{label:"Text-Only Training",permalink:"/docs/tags/text-only-training"},{label:"Modality Gap Mitigation",permalink:"/docs/tags/modality-gap-mitigation"},{label:"Noise Injection",permalink:"/docs/tags/noise-injection"},{label:"Stylistic Captioning",permalink:"/docs/tags/stylistic-captioning"},{label:"CLOSE",permalink:"/docs/tags/close"}],version:"current",frontMatter:{slug:"CLOSE",title:"I Can't Believe There's No Images! Learning Visual Tasks Using only Language Supervision",tags:["Vision-Language","Cross-Modal Transfer","Text-Only Training","Modality Gap Mitigation","Noise Injection","Stylistic Captioning","CLOSE"]},sidebar:"tutorialSidebar",previous:{title:"Language Models Can See: Plugging Visual Controls in Text Generation",permalink:"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/MAGIC"},next:{title:"Text-Only Training for Image Captioning using Noise-Injected CLIP",permalink:"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/CapDec"}},s={},p=[{value:"Model",id:"model",level:4},{value:"Modality Gap",id:"modality-gap",level:4},{value:"3.1. Setup",id:"31-setup",level:2},{value:"3.2. Results",id:"32-results",level:2},{value:"Image Captioning",id:"image-captioning",level:4},{value:"Visual Entailment",id:"visual-entailment",level:4},{value:"VQA",id:"vqa",level:4},{value:"Visual News",id:"visual-news",level:4},{value:"Discussion",id:"discussion",level:4},{value:"3.3. Training with Data from a Language Model",id:"33-training-with-data-from-a-language-model",level:2},{value:"4.1. Sensitivity Analysis",id:"41-sensitivity-analysis",level:2},{value:"4.2. Learned Adapter Analysis",id:"42-learned-adapter-analysis",level:2},{value:"Linear Adapter",id:"linear-adapter",level:4},{value:"Structured Noise with Covariance Matrix",id:"structured-noise-with-covariance-matrix",level:4},{value:"Results",id:"results",level:4},{value:"4.3. Performance Analysis of Different CLIP and T5 Models",id:"43-performance-analysis-of-different-clip-and-t5-models",level:2},{value:"Ego-Centric",id:"ego-centric",level:4},{value:"Uplifting",id:"uplifting",level:4},{value:"Character-Based",id:"character-based",level:4},{value:"Reviews",id:"reviews",level:4},{value:"Using Contrastive Models",id:"using-contrastive-models",level:4},{value:"Zero-Shot Vision Using Language Models",id:"zero-shot-vision-using-language-models",level:4},{value:"Cross-Modal Transfer Learning",id:"cross-modal-transfer-learning",level:4},{value:"Domain Invariant Representations",id:"domain-invariant-representations",level:4},{value:"Stylistic Captioning",id:"stylistic-captioning",level:4}],m={toc:p},g="wrapper";function u(e){let{components:t,...l}=e;return(0,n.kt)(g,(0,i.Z)({},m,l,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 : ",(0,n.kt)("a",{parentName:"p",href:"https://openaccess.thecvf.com/content/ICCV2023/papers/Gu_I_Cant_Believe_Theres_No_Images_Learning_Visual_Tasks_Using_ICCV_2023_paper.pdf"},"https://openaccess.thecvf.com/content/ICCV2023/papers/Gu_I_Cant_Believe_Theres_No_Images_Learning_Visual_Tasks_Using_ICCV_2023_paper.pdf")),(0,n.kt)("h1",{id:"abstract"},"Abstract"),(0,n.kt)("p",null,"\uc9c8\ubb38\uc744 parsing \ud558\ub294 \uac83, semantics \ub97c \ube44\uad50\ud558\uace0 \ub300\uc870\ud558\ub294 \uac83, description \uc744 \uc791\uc131\ud558\ub294 \uac83\uacfc \uac19\uc740 computer vision task \uc5d0 \ud544\uc694\ud55c \ub9ce\uc740 high-level skill \uc740 natural language processing \uacfc \uac19\uc740 \ub2e4\ub978 domain \uc5d0\uc11c\ub3c4 \ud544\uc694\ud558\ub2e4. "),(0,n.kt)("p",null,"\ubcf8 \ub17c\ubb38\uc5d0\uc11c \uc800\uc790\ub294 \uc774\ub7ec\ud55c skill \uc744 text data \ub85c\ubd80\ud130 \ud559\uc2b5\ud55c \ub4a4, visual training data \ub85c\ub294 \uc804\ud600 training \ud558\uc9c0 \uc54a\uace0\ub3c4 \uadf8\uac83\uc744 vision task \ub85c transfer \ud560 \uc218 \uc788\ub294\uc9c0 \uc9c8\ubb38\ud55c\ub2e4. "),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"\uc800\uc790\uc758 approach \uc758 \ud575\uc2ec\uc740 contrastive \ub85c training \ub41c vision encoder \uc640 language encoder \uc758 joint embedding space \ub97c \ud65c\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\uc2e4\uc81c\uc5d0\uc11c\ub294 contrastive model \uc5d0\uc11c modality \uac00 \ub2e4\ub978 embedding space \uc0ac\uc774\uc5d0 systematic difference \uac00 \uc874\uc7ac\ud560 \uc218 \uc788\uc73c\uba70, \uc800\uc790\ub294 \uc774\ub7ec\ud55c difference \uac00 \uc800\uc790\uc758 approach \uc5d0 \uc5b4\ub5a4 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \ubd84\uc11d\ud558\uace0, \uc774 \uc6b0\ub824\ub97c \uc644\ud654\ud558\uae30 \uc704\ud55c strategy \ub97c \uc5f0\uad6c\ud55c\ub2e4.")),(0,n.kt)("p",null,"\uc800\uc790\ub294 image captioning, visual entailment, visual question answering, visual news captioning \uc758 \ub124 \uac00\uc9c0 representative task \uc5d0 \ub300\ud574 \uc624\uc9c1 text training data \ub9cc\uc744 \uc0ac\uc6a9\ud558\uc5ec model \uc744 \ub9cc\ub4e4\uace0, image \ub97c \uc0ac\uc6a9\ud558\ub294 standard benchmark \uc5d0\uc11c \uc774\ub97c evaluation \ud55c\ub2e4. "),(0,n.kt)("p",null,"\uc774\ub7ec\ud55c model \uc740 image \ub85c training \ub41c model \uc5d0 \uadfc\uc811\ud55c performance \ub97c \ubcf4\uc774\uba70, text-only setting \uc5d0\uc11c captioning \uacfc visual entailment \uc5d0 \ub300\ud574 \uae30\uc874 work \ubcf4\ub2e4 9 point \uc774\uc0c1 \ub192\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uace0, visual news \uc5d0\uc11c\ub294 \ubaa8\ub4e0 \uae30\uc874 work \ubcf4\ub2e4 30 point \uc774\uc0c1 \ub192\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud55c\ub2e4. \ub610\ud55c \uc800\uc790\ub294 image data \uc640 human-curated language data \ub97c \uc804\ud600 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0, \ub300\uc2e0 book, web, language model \ub85c\ubd80\ud130 \uc5bb\uc740 readily-available \ud55c text data \ub97c \uc0ac\uc6a9\ud574 training \ub41c \ub2e4\uc591\ud55c \uc2a4\ud0c0\uc77c\uc758 image captioning model \uc744 \ubcf4\uc5ec\uc900\ub2e4."),(0,n.kt)("h1",{id:"1-introduction"},"1. Introduction"),(0,n.kt)("p",null,"vision task \uc640 natural language processing (NLP) task \ub294 \ubcf4\ud1b5 \ub9e4\uc6b0 \ub2e4\ub978 \uac83\uc73c\ub85c \uac04\uc8fc\ub418\uc9c0\ub9cc, \uc774\ub97c \uc218\ud589\ud558\uae30 \uc704\ud574 \ud544\uc694\ud55c skill \uc740 \uc885\uc885 \ub192\uc740 \uc815\ub3c4\uc758 overlap \uc774 \uc874\uc7ac\ud55c\ub2e4. visual question answering \uacfc reading comprehension question answering \uc740 \ubaa8\ub450 question \uc744 parsing \ud558\uace0 \uc774\ud574\ud558\ub294 \ub2a5\ub825\uc744 \ud544\uc694\ub85c \ud558\uba70, visual entailment \uc640 textual entailment \uc740 \uc11c\ub85c \ub2e4\ub978 semantic meaning \uc744 \ube44\uad50\ud558\ub294 \ub2a5\ub825\uc744 \uc694\uad6c\ud558\uace0, captioning \uacfc summarization \uc740 input \uc758 semantics \ub97c \uc694\uc57d\ud558\ub294 text \ub97c \uc791\uc131\ud558\ub294 \ub2a5\ub825\uc744 \uc694\uad6c\ud55c\ub2e4. "),(0,n.kt)("p",null,"\uc774\ub294 \ud765\ubbf8\ub85c\uc6b4 \uac00\ub2a5\uc131\uc744 \uc81c\uae30\ud55c\ub2e4. \uc989, model \uc774 input text \uc758 high-level semantic representation \uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ub7ec\ud55c task \uc911 \ud558\ub098\ub97c \uc218\ud589\ud558\ub294 \ubc95\uc744 \ud559\uc2b5\ud588\ub2e4\uba74, input image \uac00 \ub3d9\uc77c\ud55c semantic representation \uc73c\ub85c encoding \ub418\uae30\ub9cc \ud55c\ub2e4\uba74 \uc774\ub860\uc801\uc73c\ub85c \ud574\ub2f9\ub418\ub294 visual task \ub3c4 \uc989\uc2dc \uc218\ud589\ud560 \uc218 \uc788\ub2e4\ub294 \uac83\uc774\ub2e4. "),(0,n.kt)("p",null,"\uc800\uc790\ub294 \uc774\ub97c ",(0,n.kt)("em",{parentName:"p"},"zero-shot cross-modal transfer")," \ub77c\uace0 \ubd80\ub974\ub294\ub370, \uc774\ub294 \ud55c modality \uc5d0\uc11c \ud559\uc2b5\ud55c skill \uc744 \ub2e4\ub978 modality \uc5d0 \uc801\uc6a9\ud574\uc57c \ud558\uae30 \ub54c\ubb38\uc774\ub2e4. \uc774\ub97c \ub2ec\uc131\ud558\ub294 \uac83\uc740 \uac01 modality \ub9c8\ub2e4 expensive \ud55c training data \ub97c \ud544\uc694\ub85c \ud558\uc9c0 \uc54a\uace0 modality \uac04\uc5d0 skill \uc744 generalization \ud560 \uc218 \uc788\ub294 multimodal model \uc744 \uad6c\ucd95\ud558\ub294 \ubc29\ud5a5\uc73c\ub85c \ub098\uc544\uac00\ub294 \ud55c \ub2e8\uacc4\uc774\uba70, visual training data \ub294 \ubd80\uc871\ud558\uc9c0\ub9cc text data \ub294 \ube44\uad50\uc801 \uc27d\uac8c \uc218\uc9d1\ud560 \uc218 \uc788\ub294 task \uc5d0 \ud65c\uc6a9 \uac00\ub2a5\uc131\uc744 \uac00\uc9c4\ub2e4."),(0,n.kt)("p",null,"\uc774\ub97c \uc704\ud574\uc11c\ub294 image \uc640 text \ub97c shared semantic space \ub85c encoding \ud574\uc57c \ud55c\ub2e4. \uc800\uc790\ub294 contrastive loss \ub85c training \ub41c vision and language (V&L) model \uc744 \uc774 \ubaa9\uc801\uc5d0 \uc0ac\uc6a9\ud55c\ub2e4. \uc774\ub7ec\ud55c model \uc740 matching \ub418\ub294 image \uc640 caption \uc758 vector \uac00 \uc11c\ub85c \uac00\uae5d\uace0, \uad00\ub828 \uc5c6\ub294 image \uc640 caption \uc758 vector \ub294 \uba40\uc5b4\uc9c0\ub3c4\ub85d text \uc640 image \ub97c vector \ub85c embedding \ud55c\ub2e4. \uc774 loss \ub294 \uc6d0\ub798 representation learning \uacfc zero-shot classification \uc6a9\ub3c4\ub85c \uace0\uc548\ub418\uc5c8\uc9c0\ub9cc, \uc5ec\uae30\uc11c\ub294 cross-modal transfer \ub97c \uac00\ub2a5\ud558\uac8c \ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc778\ub2e4."),(0,n.kt)("p",null,"\uc774\ub97c \uc704\ud574 \uc800\uc790\ub294 ",(0,n.kt)("strong",{parentName:"p"},"Cross modaL transfer On Semantic Embeddings (CLOSE)")," \ub77c\ub294 \ubc29\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4. CLOSE \uc758 \uac1c\uc694\ub294 Fig. 1 \uc5d0 \ub098\ud0c0\ub098 \uc788\ub2e4. "),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Figure 1",src:a(26943).Z,width:"1331",height:"542"})),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"training \ub3d9\uc548, text input \uc740 contrastive model \uc758 (frozen) text encoder \ub85c vector \ub85c encoding \ub418\uace0, \uc774\ub294 model \uc758 input \uc73c\ub85c \uc0ac\uc6a9\ub41c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"testing \ub3d9\uc548\uc5d0\ub294, visual input \uc774 (frozen) image encoder \ub85c embedding \ub418\uc5b4 text embedding \ub300\uc2e0 \uc0ac\uc6a9\ub41c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\uc774\ub7ec\ud55c encoder \ub4e4\uc740 semantics \ub97c \ube44\uc2b7\ud55c \ubc29\uc2dd\uc73c\ub85c encoding \ud558\ub3c4\ub85d \uba85\uc2dc\uc801\uc73c\ub85c training \ub418\uc5c8\uae30 \ub54c\ubb38\uc5d0, text vector \ub97c \uc77d\uace0 \ucc98\ub9ac\ud558\ub294 \ubc95\uc744 \ubc30\uc6b0\ub294 \uac83\uc740 \uc790\uc5f0\uc2a4\ub7fd\uac8c image vector \ub97c \uc77d\uace0 \ucc98\ub9ac\ud558\ub294 \ub2a5\ub825\uc73c\ub85c \uc774\uc5b4\uc9c4\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 text-to-image transfer \uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uc9c0\ub9cc, \uc800\uc790\uc758 approach \ub294 video, point cloud, audio \uc640 \uac19\uc740 \ub2e4\ub978 contrastive model \uc5d0\ub3c4 \uc801\uc6a9 \uac00\ub2a5\ud558\uc5ec \ub354 \ub9ce\uc740 modality \uac04\uc758 transfer \ub97c \uac00\ub2a5\ud558\uac8c \ud55c\ub2e4.")),(0,n.kt)("p",null,"\uc774 \uc811\uadfc\uc5d0\uc11c \ud55c \uac00\uc9c0 \uc7a0\uc7ac\uc801 \uc5b4\ub824\uc6c0\uc740 contrastive embedding \uc774 modality \uac04\uc5d0 \uc5b4\ub290 \uc815\ub3c4 \uad6c\uc870\ub97c \uacf5\uc720\ud558\uae34 \ud558\uc9c0\ub9cc, \uc2e4\uc81c\ub85c\ub294 image vector \uc640 text vector \uc0ac\uc774\uc5d0 \uc5ec\uc804\ud788 \uc0c1\ub2f9\ud55c difference \uac00 \uc874\uc7ac\ud560 \uc218 \uc788\ub2e4\ub294 \uc810\uc774\ub2e4. \uc774\ub97c \uc644\ud654\ud558\uae30 \uc704\ud574 \uc800\uc790\ub294 training \ub3d9\uc548 \uc0ac\uc6a9\ub418\ub294 text vector \ub97c \uc218\uc815\ud558\ub294 adapter \ub97c \ucd94\uac00\uc801\uc73c\ub85c \uc0ac\uc6a9\ud560 \uac83\uc744 \uc81c\uc548\ud55c\ub2e4. Gaussian noise \ub97c \ub354\ud558\ub294 \uac83\uc774 performance \ub97c boosting \ud558\ub294 \ub370 \ub9e4\uc6b0 \ud6a8\uacfc\uc801\uc784\uc744 \ubc1c\uacac\ud588\uc73c\uba70, \ubd84\uc11d\uc5d0\uc11c \ub2e4\ub978 \ubc29\ubc95\ub3c4 \uace0\ub824\ud55c\ub2e4."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"text-to-image transfer \ub294 \ube44\uad50\uc801 \ubbf8\uac1c\ucc99\ub41c setting \uc774\uae30 \ub54c\ubb38\uc5d0, \uc800\uc790\ub294 CLOSE \uac00 text-to-image domain shift \ub97c major performance drop \uc5c6\uc774 \ucc98\ub9ac\ud560 \uc218 \uc788\ub294\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud574 \uad11\ubc94\uc704\ud55c \uc2e4\ud5d8\uc744 \uc218\ud589\ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\uc800\uc790\ub294 CLOSE \ub85c text \ub9cc \uc0ac\uc6a9\ud574 training \ub41c model \uacfc image \uc640 text \ub97c \ubaa8\ub450 \uc0ac\uc6a9\ud574 training \ub41c model \uc744 \uc138 \uac00\uc9c0 standard V&L task (captioning, visual questioning answers (VQA), visual entailment) \uc640 \ub354 \ubcf5\uc7a1\ud55c visual news captioning task \uc640 \ube44\uad50\ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"text-only model \uc740 \uc77c\ubc18\uc801\uc73c\ub85c image \ub85c training \ub41c version \uc5d0 reasonably close \ud55c performance \ub97c \ubcf4\uc774\uba70, CLOSE \uac00 modality \uac04\uc5d0 \uc5ec\ub7ec skill \uc744 \ud6a8\uacfc\uc801\uc73c\ub85c transfer \ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"text-only setting \uc5d0\uc11c captioning \uc740 \uae30\uc874 best method \ubcf4\ub2e4 17 CIDEr (78.2 vs. 95.4), visual entailment \uc740 9 point (66.6 vs. 75.9) \ud5a5\uc0c1\ub418\uba70, \uc774 setting \uc5d0\uc11c \ud070 margin \uc73c\ub85c state-of-the-art \ub97c \ub2ec\uc131\ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"VQA \uc640 visual news \uc5d0 \ub300\ud574\uc11c\ub294 prior result \uac00 \uc5c6\uc9c0\ub9cc, visual news \uc5d0 \ub300\ud574\uc11c\ub294 image \ub97c \uc0ac\uc6a9\ud55c \uae30\uc874 best reported result \ubcf4\ub2e4\ub3c4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \uae30\ub85d\ud55c\ub2e4 (50.5 vs. 80.8 CIDEr).")),(0,n.kt)("p",null,"\uc774 \uc2e4\ud5d8\ub4e4\uc740 efficient \ud55c text-to-image transfer \uac00 \uac00\ub2a5\ud558\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uc774\ub294 text training data \ub294 annotator \uac00 \uc9c1\uc811 \uad6c\uc131\ud558\uac70\ub098 \uae30\uc874 text dataset \uc5d0\uc11c mining \ud558\uac70\ub098 GPT-3 \uc640 \uac19\uc740 large language model \uc774 \uc0dd\uc131\ud560 \uc218 \uc788\uc5b4 visual training data \ub97c \uad6c\uc131\ud558\ub294 \uac83\ubcf4\ub2e4 \ud6e8\uc52c \uc801\uc740 \ube44\uc6a9\uc73c\ub85c \uac00\ub2a5\ud558\uae30 \ub54c\ubb38\uc5d0 \uc911\uc694\ud55c practical implication \uc744 \uac16\ub294\ub2e4. "),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"\uc800\uc790\ub294 prompt construction \ub9cc human annotation \uc774 \ud544\uc694\ud55c setting \uc5d0\uc11c\ub3c4 large language model \uc774 \uc0dd\uc131\ud55c text \ub85c CLOSE captioning model \uc744 \ud6a8\uacfc\uc801\uc73c\ub85c training \ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\ub610\ud55c labeled image \uc5c6\uc774\ub3c4 stylistic captioning model \uc5ec\ub7ec \uac1c\ub97c training \ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\uc800\uc790\ub294 internet review, book, GPT-3 generation \ub4f1 \ub2e4\uc591\ud55c source \ub85c\ubd80\ud130 style \uc774 \ub2e4\ub978 text \ub97c \uc218\uc9d1\ud558\uace0, \uc774\ub7ec\ud55c text \ub85c training \ub41c CLOSE model \uc774 image \uc5d0 \ub300\ud574 \uc815\ud655\ud558\uba74\uc11c\ub3c4 style \uc774 \uc798 \ubc18\uc601\ub41c caption \uc744 \uc0dd\uc131\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4 (Fig. 2 \ucc38\uc870).")),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Figure 2",src:a(35070).Z,width:"850",height:"730"})),(0,n.kt)("p",null,"\ub9c8\uc9c0\ub9c9\uc73c\ub85c \ub450 \uac00\uc9c0 \ubd84\uc11d\uc744 \uc218\ud589\ud55c\ub2e4. "),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"text vector \uc640 image vector \uac00 constant offset \ub9cc\ud07c \ub2e4\ub978 \uacbd\uc6b0\uc5d0\ub3c4 CLOSE \uac00 robust \ud558\ub2e4\ub294 sensitivity analysis \ub85c, \uc774\ub294 image/text embedding \uc0ac\uc774\uc5d0 \uac89\ubcf4\uae30\uc5d0\ub294 \ud070 difference \uac00 \uc874\uc7ac\ud558\ub354\ub77c\ub3c4 CLOSE \uac00 \uc791\ub3d9\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4. "),(0,n.kt)("li",{parentName:"ol"},"auxiliary vision and language corpus \ub97c \uc0ac\uc6a9\ud574 \ub354 \ub098\uc740 adapter \ub97c \uad6c\ucd95\ud558\ub294 \uac83\uc758 \ud6a8\uacfc\ub97c \uc5f0\uad6c\ud55c\ub2e4. ",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"\uc774\ub7ec\ud55c data \uc758 source \uc5d0 \ub530\ub77c improvement \ub294 \ub2ec\ub77c\uc9c0\uc9c0\ub9cc, auxiliary data \ub85c structured covariance matrix \ub97c \uacc4\uc0b0\ud574 Gaussian noise \ucd94\uac00\uc5d0 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ud2b9\ud788 \ud6a8\uacfc\uc801\uc778 \ubc29\ubc95\uc784\uc744 \ud655\uc778\ud55c\ub2e4.")))),(0,n.kt)("p",null,"\uc694\uc57d\ud558\uba74, \uc800\uc790\uc758 contribution \uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"zero-shot cross-modal transfer \ub97c \uc704\ud55c CLOSE model \uc744 \uc18c\uac1c\ud55c\ub2e4."),(0,n.kt)("li",{parentName:"ol"},"\ub124 \uac00\uc9c0 V&L task \uc5d0 \ub300\ud574 text data \ub9cc\uc73c\ub85c CLOSE \ub97c training \ud588\uc744 \ub54c image \uc640 text \ubaa8\ub450\ub85c training \ub41c model \uc5d0 \uadfc\uc811\ud55c \uacb0\uacfc\ub97c \ubcf4\uc784\uc744 \ubcf4\uc778\ub2e4."),(0,n.kt)("li",{parentName:"ol"},"\uc138 \uac1c task \uc5d0\uc11c text-only setting \uc758 SoTA \uacb0\uacfc\ub97c \ub2ec\uc131\ud55c\ub2e4."),(0,n.kt)("li",{parentName:"ol"},"stylistic captioning \uc5d0 CLOSE \ub97c \uc801\uc6a9\ud558\ub294 \uc0ac\ub840\ub97c \ubcf4\uc778\ub2e4."),(0,n.kt)("li",{parentName:"ol"},"contrastive model \uc758 image/text vector \uac04 difference \uc640 adapter \uc758 \uc885\ub958\uac00 CLOSE \uc758 performance \uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubd84\uc11d\ud55c\ub2e4.")),(0,n.kt)("h1",{id:"2-method"},"2. Method"),(0,n.kt)("h4",{id:"model"},"Model"),(0,n.kt)("p",null,"\uc800\uc790\uc758 approach \ub294 contrastive model \uc758 image/text encoder \ub97c \uc0ac\uc6a9\ud574 input \uc744 encoding \ud558\uace0, \uadf8 \ud6c4 \ub9ce\uc740 prior work \uc744 \ub530\ub77c pre-trained language model \uc744 fine-tuning \ud558\uc5ec input vector \uc640 \ucd94\uac00 input text \ub97c \ucc98\ub9ac\ud574 output text \ub97c \uc0dd\uc131\ud55c\ub2e4. "),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"\uba3c\uc800, input image \ub610\ub294 text vector \ub294 contrastive loss \uc5d0\uc11c \uc0ac\uc6a9\ub418\ub294 \uac83\uacfc \uc77c\uce58\ud558\ub3c4\ub85d unit length \ub85c normalized \ub41c\ub2e4. \uadf8\ub2e4\uc74c, \uc774 vector \ub294 language model \uc758 embedding layer \uc640 \ub3d9\uc77c\ud55c dimensionality \ub97c \uac16\ub294 \uc5ec\ub7ec \uac1c\uc758 vector \ub85c \ubcc0\ud658\ub418\ub294\ub370, \uc800\uc790\ub294 \uc2e4\ud5d8\uc5d0\uc11c 4 \uac1c\ub97c \uc0ac\uc6a9\ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\uc774 \ubcc0\ud658\uc740 linear layer \ub97c \uc0ac\uc6a9\ud574 \uc218\ud589\ub41c\ub2e4. \uc774\ud6c4, \ub2e4\ub978 input text (e.g., visual entailment \uc5d0\uc11c\uc758 hypothesis, VQA \uc5d0\uc11c\uc758 question) \ub294 tokenization \ub418\uace0 language model \uc758 embedding layer \ub85c embedding \ub41c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\uc774\ub7ec\ud55c embedding \ub4e4\uc740 input vector \ub85c\ubd80\ud130 \uc0dd\uc131\ub41c embedding \ub4e4\uacfc concatenation \ub418\uc5b4 language model \uc758 input sequence \ub97c \uad6c\uc131\ud55c\ub2e4.")),(0,n.kt)("p",null,"\ub2e8\uc21c\ud654\ub97c \uc704\ud574 \uc800\uc790\ub294 \ubaa8\ub4e0 task \uc5d0 \ub300\ud574 generative \ud558\uac8c model \uc744 training \ud55c\ub2e4. "),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"model \uc740 captioning, VQA, visual entailment task \uc5d0 \ub300\ud574 \uac01\uac01 caption, free-form question answer, class name \uc744 \uc0dd\uc131\ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"training \ub3d9\uc548 language model \uacfc linear layer \ub294 fine-tuning \ub418\uc9c0\ub9cc, text encoder \ub294 pre-training \ub3d9\uc548 \ud559\uc2b5\ub41c text \uc640 image vector \uac04\uc758 correspondence \ub97c \ubcf4\uc874\ud558\uae30 \uc704\ud574 frozen \uc0c1\ud0dc\ub85c \uc720\uc9c0\ub41c\ub2e4.")),(0,n.kt)("h4",{id:"modality-gap"},"Modality Gap"),(0,n.kt)("p",null,"\uc2e4\uc81c contrastive model \uc5d0\uc11c text vector \uc640 image vector \ub294 \uc0c1\ub2f9\ud788 \uba40\ub9ac \ub5a8\uc5b4\uc9c8 \uc218 \uc788\uc73c\uba70, \uc774\ub294 modality gap \uc73c\ub85c \uc54c\ub824\uc838 \uc788\ub2e4. \uc608\ub97c \ub4e4\uc5b4, COCO caption \uc5d0\uc11c image \uc640 paired caption \uc758 \ud3c9\uade0 cosine similarity \ub294 0.26 \uc5d0 \ubd88\uacfc\ud55c \ubc18\uba74, \ubb34\uad00\ud55c caption \ub450 \uac1c\uc758 \ud3c9\uade0 similarity \ub294 0.35 \uc774\ub2e4. "),(0,n.kt)("p",null,"Fig. 3a \ub294 \uc774\ub7ec\ud55c gap \ub54c\ubb38\uc5d0 image vector \uc640 text vector \uac00 vector space \uc5d0\uc11c \uc11c\ub85c \ub2e4\ub978 cluster \ub85c \ubd84\ub9ac\ub418\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uadfc\ubcf8 \uc6d0\uc778\uc740 contrastive model \uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 cross-entropy loss \uac00 paired image-text pair \uac00 random pair \uc5d0 \ube44\ud574 \uc0c1\ub300\uc801\uc73c\ub85c \uac00\uae5d\uae30\ub9cc \ud558\uba74 \ub418\uc9c0, absolute distance \uac00 \uac00\uae5d\ub3c4\ub85d \uc694\uad6c\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc774\ub2e4."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Figure 3",src:a(81090).Z,width:"1658",height:"547"})),(0,n.kt)("p",null,"\uc800\uc790\ub294 \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 \uac04\ub2e8\ud558\uba74\uc11c\ub3c4 \ud6a8\uacfc\uc801\uc778 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c\ub2e4. "),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"\uc989, standard normal distribution \uc5d0\uc11c sampling \ud55c Gaussian noise \ub97c hyper-parameter ",(0,n.kt)("span",{parentName:"li",className:"math math-inline"},(0,n.kt)("span",{parentName:"span",className:"katex"},(0,n.kt)("span",{parentName:"span",className:"katex-mathml"},(0,n.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,n.kt)("semantics",{parentName:"math"},(0,n.kt)("mrow",{parentName:"semantics"},(0,n.kt)("mi",{parentName:"mrow"},"w")),(0,n.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"w")))),(0,n.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,n.kt)("span",{parentName:"span",className:"base"},(0,n.kt)("span",{parentName:"span",className:"strut",style:{height:"0.4306em"}}),(0,n.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.02691em"}},"w")))))," \ub85c scaling \ud558\uc5ec training \uc911 text vector \uc5d0 \ucd94\uac00\ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\uc9c1\uad00\uc801\uc73c\ub85c \uc774 noise \ub294 text vector \ub97c \ud37c\ub728\ub824 modality gap \uc744 \uc904\uc774\uace0 image vector \uc640 overlap \uc744 \ud615\uc131\ud558\ub3c4\ub85d \ub3d5\ub294\ub2e4. ")),(0,n.kt)("p",null,"Fig. 3b \ub294 \uc801\uc740 \uc591\uc758 noise \ub9cc\uc73c\ub85c\ub3c4 image vector space \uc640 text vector space \uc758 overlap \uc774 \ud06c\uac8c \uac1c\uc120\ub418\ub294 \uac83\uc744 \uc2dc\uac01\uc801\uc73c\ub85c \ubcf4\uc5ec\uc900\ub2e4. \ub610\ud55c noise \ub294 input vector \uc758 \uc791\uc740 \ubcc0\ud654\ub098 variation \uc5d0 \ub300\ud574 model \uc744 \ub354\uc6b1 robust \ud558\uac8c \ub9cc\ub4e4\uc5b4, text vector \uc5d0\uc11c image vector \ub85c switching \ud560 \ub54c\uc758 shift \uc5d0 \ub354 \uc798 \ub300\ube44\ud558\ub3c4\ub85d \ud55c\ub2e4."),(0,n.kt)("hr",null),(0,n.kt)("p",null,"random noise \ub97c \uc0ac\uc6a9\ud558\ub294 \ub450 \ubc88\uc9f8 motivation \uc740 image vector \uac00 lighting, background, camera position \uacfc \uac19\uc774 text vector \uc5d0\ub294 \ubc18\uc601\ub418\uc9c0 \uc54a\ub294 subtle \ud55c visual detail \uc744 \ud3ec\ud568\ud55c\ub2e4\ub294 \uad00\ucc30\uc5d0\uc11c \ube44\ub86f\ub41c\ub2e4. Appendix 5 \uc758 \uc791\uc740 case study \uc608\uc2dc\uc5d0\uc11c \ubcf4\uc774\ub4ef, semantic change (e.g., caption \uc774\ub098 image \uc758 subject \ub97c \u201cdog\u201d \uc5d0\uc11c \u201ccat\u201d \uc73c\ub85c \ubcc0\uacbd) \ub294 text vector \uc5d0\ub294 \ube44\uad50\uc801 \uc77c\uad00\ub41c directional shift \ub97c \uc720\ubc1c\ud558\uc9c0\ub9cc image vector \uc5d0\ub294 \ud6e8\uc52c \ub354 erratic \ud55c \uc601\ud5a5\uc744 \uc900\ub2e4. text embedding \uc5d0 noise \ub97c \ucd94\uac00\ud558\uba74 semantic \uc774 \uc720\uc0ac\ud558\ub354\ub77c\ub3c4 image \uc640 text vector \uac04\uc5d0 \uc5ec\uc804\ud788 \uc791\uc740 \ucc28\uc774\uac00 \uc874\uc7ac\ud560 \uc218 \uc788\ub2e4\ub294 \uc0ac\uc2e4\uc744 simulation \ud558\uc5ec \uc774 \ubb38\uc81c\ub97c \uc644\ud654\ud55c\ub2e4."),(0,n.kt)("p",null,"noise \ub97c \ucd94\uac00\ud55c \ub4a4\uc5d0\ub294 evaluation \uc5d0\uc11c \uc0ac\uc6a9\ub420 image vector \uc640 \uc77c\uce58\ud558\ub3c4\ub85d \ub2e4\uc2dc unit length \ub85c re-normalization \ud55c\ub2e4. modality gap \ubc0f \uc774\ub97c \ucc98\ub9ac\ud558\uae30 \uc704\ud55c \ub2e4\ub978 approach \uc5d0 \ub300\ud55c \ubcf4\ub2e4 \uc790\uc138\ud55c \ubd84\uc11d\uc740 Sec. 4 \uc5d0\uc11c \ub17c\uc758\ud55c\ub2e4."),(0,n.kt)("h1",{id:"3-experiments"},"3. Experiments"),(0,n.kt)("p",null,"\uc800\uc790\ub294 \ub124 \uac00\uc9c0 V&L task \u2014 captioning, visual entailment, VQA, visual news \u2014 \uadf8\ub9ac\uace0 language model \uc774 \uc0dd\uc131\ud55c text \ub9cc\uc744 \uc0ac\uc6a9\ud574 CLOSE \ub97c training \ud558\ub294 setting \uc5d0\uc11c\uc758 \uacb0\uacfc\ub97c \ubcf4\uace0\ud55c\ub2e4."),(0,n.kt)("h2",{id:"31-setup"},"3.1. Setup"),(0,n.kt)("p",null,"\uc800\uc790\ub294 \uc774\ub7ec\ud55c task \ub4e4\uc744 \uc704\ud574 \uad00\ub828 training dataset \uc758 text annotation \uacfc, \uc77c\ubd80 task \uc5d0 \ub300\ud574\uc11c\ub294 training image \uc758 text caption \uc744 \ud65c\uc6a9\ud558\uc5ec \uc21c\uc218 text training dataset \uc744 \uad6c\uc131\ud55c\ub2e4. \uc8fc\uc694 \ube44\uad50 \ub300\uc0c1\uc740 training \uc5d0 image \ub97c \uc0ac\uc6a9\ud558\ub294 CLOSE model \uc774\uba70, \uc774 \uacbd\uc6b0 testing \ub54c\uc640 \ub3d9\uc77c\ud558\uac8c image encoder \ub85c image \uac00 encoding \ub41c\ub2e4. "),(0,n.kt)("p",null,"\uc774 model \uc740 domain shift \ub97c \uacbd\ud5d8\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 upper bound \ub85c \uac04\uc8fc\ub41c\ub2e4. \uc2e4\uc81c\uc5d0\uc11c\ub294 text training data \uac00 \ub2e4\ub978 source \uc5d0\uc11c\ub3c4 \uc5bb\uc5b4\uc9c8 \uc218 \uc788\uc9c0\ub9cc, \uc800\uc790\ub294 image \ub97c \uc0ac\uc6a9\ud558\ub294 model \uc774 \ud559\uc2b5\ud55c data \uc640 \uac00\uc7a5 \uadfc\uc811\ud55c text source \ub97c \uc0ac\uc6a9\ud558\uc5ec image-text domain shift \uc5d0 \uc758\ud574 \uc190\uc2e4\ub418\ub294 \uc131\ub2a5\ub9cc\uc744 \uace0\ub9bd\ud574 \ubd84\uc11d\ud55c\ub2e4. Sect. 5 \uc640 Sect. 3.3 \uc5d0\uc11c \uc774\ub7ec\ud55c text source \uc5d0 \ub300\ud55c \ucd94\uac00 \uc2e4\ud5d8\uc744 \uc81c\uc2dc\ud55c\ub2e4."),(0,n.kt)("p",null,"\uc800\uc790\ub294 \ubaa8\ub4e0 task \uc5d0 \ub300\ud574 \ub3d9\uc77c\ud55c hyper-parameter \uc138\ud2b8\ub97c \uc0ac\uc6a9\ud558\uba70, T5base \uc640 CLIPViT-L/14, noise level 0.08 \uc744 \uc0ac\uc6a9\ud55c\ub2e4. image/text validation set \uc5c6\uc774\ub3c4 \ud6a8\uacfc\uc801\uc778 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c \ud55c\ub2e4. hyper-parameter \uc138\ubd80 \ub0b4\uc6a9\uc740 Appendix 1 \uc5d0 \uc81c\uc2dc\ud55c\ub2e4. \ub610\ud55c validation set \uc5d0\uc11c noise level \uc744 tuning \ud588\uc744 \ub54c\uc640 noise \ub97c \uc81c\uac70\ud588\uc744 \ub54c\uc758 \uacb0\uacfc\ub3c4 \ud568\uaed8 \uc81c\uc2dc\ud558\uc5ec noise \uc758 \ud6a8\uacfc\ub97c \ubd84\uc11d\ud55c\ub2e4."),(0,n.kt)("h2",{id:"32-results"},"3.2. Results"),(0,n.kt)("p",null,"\uacb0\uacfc\ub294 Tab. 1 \uc5d0 \uc81c\uc2dc\ub41c\ub2e4. \uc9c0\uba74 \ud55c\uacc4\ub85c \uac01 task \ub2f9 \ud558\ub098\uc758 metric \ub9cc \uae30\uc7ac\ud558\uace0, \ucd94\uac00 metric \uc740 Appendix 2 \uc5d0 \ud3ec\ud568\ud55c\ub2e4. \ub610\ud55c image \ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc740 prior work \uc911 best method \ub3c4 \ud568\uaed8 \ube44\uad50\ud55c\ub2e4."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Table 1",src:a(81265).Z,width:"1656",height:"494"})),(0,n.kt)("h4",{id:"image-captioning"},"Image Captioning"),(0,n.kt)("p",null,"captioning \uc5d0\uc11c\ub294 text caption \uc744 input text \uc640 target output text \ub85c \uc0ac\uc6a9\ud55c\ub2e4. \uadf8\ub7ec\ub098 \ud558\ub098\uc758 scene \uc5d0 \ub300\ud574 \uc5ec\ub7ec caption \uc774 \uc874\uc7ac\ud560 \uacbd\uc6b0, \ub3d9\uc77c\ud55c image \ub97c \uc124\uba85\ud558\ub294 \uc5ec\ub7ec caption \uc911 \ud558\ub098\ub97c input \uc73c\ub85c, \ub2e4\ub978 \ud558\ub098\ub97c target \uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc720\ub9ac\ud558\ub2e4\ub294 \uc810\uc744 \ubc1c\uacac\ud588\ub2e4. \uc774\ub97c captioning (single) \uacfc captioning (multiple) \ub85c \uad6c\ubd84\ud558\uc5ec \ud3c9\uac00\ud55c\ub2e4. COCO Captioning dataset \uc5d0 \ub300\ud574 Karpathy split \uc744 \uc0ac\uc6a9\ud55c\ub2e4."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"CLOSE (text-only) \ub294 multiple-caption setting \uc5d0\uc11c 95.3 CIDEr \ub97c \uae30\ub85d\ud558\uba70, image \uc5c6\uc774\ub3c4 \ub9e4\uc6b0 \ub192\uc740 captioning \ub2a5\ub825\uc744 \ubcf4\uc5ec\uc900\ub2e4. single-caption setting \uc5d0\uc11c\ub294 \uc131\ub2a5\uc774 \uac10\uc18c\ud558\uc9c0\ub9cc noise level \uc744 \ub192\uc774\uba74 95.4 \uae4c\uc9c0 \uc0c1\uc2b9\ud55c\ub2e4."),(0,n.kt)("li",{parentName:"ul"},"\uc800\uc790\uc758 approach \ub294 MAGIC (49.3), Socratic Models (44.5) \ub4f1 \ucd5c\uadfc zero-shot \ubc29\ubc95\ub4e4\ubfd0 \uc544\ub2c8\ub77c text caption \uc744 \uc0ac\uc6a9\ud558\ub294 ESPER Style (78.2) \ubcf4\ub2e4\ub3c4 17 point \uc774\uc0c1 \uc6b0\uc218\ud558\ub2e4.")),(0,n.kt)("h4",{id:"visual-entailment"},"Visual Entailment"),(0,n.kt)("p",null,"visual entailment \uc740 premise image \uac00 hypothesis sentence \ub97c entail / contradict / neutral \uc911 \uc5b4\ub290 \uac83\uc73c\ub85c \ub9cc\ub4dc\ub294\uc9c0\ub97c \ud310\ubcc4\ud55c\ub2e4. training \uc740 image \ub300\uc2e0 text premise \ub97c \uc0ac\uc6a9\ud55c\ub2e4. hypothesis sentence \ub294 \ud56d\uc0c1 text \ub85c language model \ub85c encoding \ub41c\ub2e4."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"\uc800\uc790\ub294 SNLI (language-only dataset) \ub85c training \ud558\uace0, SNLI-VE (vision-language dataset) \ub85c evaluation \ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"image \ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc74c\uc5d0\ub3c4 CLOSE \ub294 image model \uacfc \uc720\uc0ac\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud55c\ub2e4. Song et al. \uc758 \uacb0\uacfc\ubcf4\ub2e4 Gaussian noise \ucd94\uac00\ub97c \ud1b5\ud574 9 point \uc774\uc0c1 \ud5a5\uc0c1\ub41c\ub2e4.")),(0,n.kt)("h4",{id:"vqa"},"VQA"),(0,n.kt)("p",null,"VQA model \uc744 training \ud558\uae30 \uc704\ud574 scene \uc744 \ubb18\uc0ac\ud558\ub294 sentence (text encoder \ub85c encoding), question (language model \ub85c encoding), target answer \uac00 \ud3ec\ud568\ub41c data \ub97c \uc0ac\uc6a9\ud55c\ub2e4. \ub450 dataset \uc744 \uace0\ub824\ud55c\ub2e4."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"COCO caption \uacfc VQA 2.0 question \uc744 pairing"),(0,n.kt)("li",{parentName:"ul"},"VQA-E (answer \uac00 caption \uc5d0 \ud3ec\ud568\ub41c subset)")),(0,n.kt)("p",null,"VQA 2.0 \uc5d0\uc11c\ub294 caption \uc5d0 \uc9c8\ubb38\uc758 \ub2f5\uc774 \ud3ec\ud568\ub418\uc9c0 \uc54a\uc740 \uacbd\uc6b0\uac00 \uc788\uc5b4 text-only model \uc774 \ub2f5\ud560 \uc218 \uc5c6\uae30 \ub54c\ubb38\uc5d0 alignment \ubb38\uc81c\ub85c \ub354 \uc5b4\ub824\uc6c0\uc744 \uacaa\ub294\ub2e4. \ub530\ub77c\uc11c VQA-E \ub85c\ub3c4 \ubcc4\ub3c4 training/evaluation \ud55c\ub2e4."),(0,n.kt)("p",null,"training set \uc758 question distribution \uc774 \ud06c\uac8c \ub2e4\ub974\uae30 \ub54c\ubb38\uc5d0, VQA 2.0 \uc73c\ub85c training \ud55c \uacbd\uc6b0 VQA 2.0 test-dev \uc5d0\uc11c \ud3c9\uac00\ud558\uace0, VQA-E \ub85c training \ud558\uba74 VQA-E validation set \uc5d0\uc11c \ud3c9\uac00\ud55c\ub2e4."),(0,n.kt)("p",null,"text-only setting \uc758 prior work \uc740 \uc5c6\uc9c0\ub9cc, CLOSE \ub294 CLIP \uae30\ubc18 zero-shot \ubc29\ubc95\uc778 TAPCViT-B/16 \ubcf4\ub2e4 \uc6b0\uc218\ud558\ub2e4. VQA-E \uc5d0\uc11c\ub294 image training \ub300\ube44 accuracy \uac00 3.5 point \uac10\uc18c\ud558\ub294 \uc218\uc900\uc5d0 \uadf8\uce58\uba70 baseline \uc744 \ucd08\uacfc\ud55c\ub2e4. VQA 2.0 \uc5d0\uc11c\ub294 caption-question alignment \ubb38\uc81c\ub85c gap \uc774 \ub354 \ud06c\uc9c0\ub9cc image model \ub300\ube44 5 point \ub0b4\ub85c \uadfc\uc811\ud55c\ub2e4."),(0,n.kt)("h4",{id:"visual-news"},"Visual News"),(0,n.kt)("p",null,"visual news \ub294 image \ub97c captioning \ud558\ub418 article \uc758 context \ub97c \ubc18\uc601\ud574\uc57c \ud558\ubbc0\ub85c \uc0ac\ub78c, \uc7a5\uc18c, \uc0ac\uac74 \ub4f1\uc744 article \uacfc \uacb0\ud569\ud574 \uae30\uc220\ud574\uc57c \ud55c\ub2e4. CLOSE \ub294 caption \uc744 image text \ubc0f target output \uc73c\ub85c \uc0ac\uc6a9\ud558\uace0, article \uc744 language model \uc758 \ucd94\uac00 context \ub85c \ud65c\uc6a9\ud568\uc73c\ub85c\uc368 \uc790\uc5f0\uc2a4\ub7fd\uac8c \ud655\uc7a5\ub41c\ub2e4."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"dataset \uc774 \ub9e4\uc6b0 \ud06c\uae30 \ub54c\ubb38\uc5d0 training \uc2dc epoch \ub2f9 15% training data \ub97c random sampling \ud55c\ub2e4. \ub610\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc704\ud574 CLIP \ub300\uc2e0 OpenCLIP \uc744 \uc0ac\uc6a9\ud55c\ub2e4."),(0,n.kt)("li",{parentName:"ul"},"CLSE (with images) \ub294 105 CIDEr \uc774\uc0c1\uc744 \uae30\ub85d\ud558\uba70 \uae30\uc874 best benchmark (50.5 CIDEr) \ub97c \ud06c\uac8c \ub6f0\uc5b4\ub118\ub294\ub2e4. image \uc5c6\uc774 training \ud574\ub3c4 \uae30\uc874 SoTA \ubcf4\ub2e4 \ub192\uc740 80.8 CIDEr \ub97c \uae30\ub85d\ud55c\ub2e4.")),(0,n.kt)("h4",{id:"discussion"},"Discussion"),(0,n.kt)("p",null,"\uc804\uccb4\uc801\uc73c\ub85c CLOSE \ub294 image \ub85c training \ud55c model \uacfc \ub9e4\uc6b0 \uc720\uc0ac\ud55c \uc131\ub2a5\uc744 \ub2ec\uc131\ud558\uba70, modality \uac04 skill transfer \uac00 \ud6a8\uacfc\uc801\uc784\uc744 \ubcf4\uc5ec\uc900\ub2e4. noise level tuning \uc740 \uba87\uba87 task \uc5d0 \ucd94\uac00\uc801\uc778 \uc774\ub4dd\uc744 \uc904 \uc218 \uc788\uc73c\uba70, \ub354 \ub098\uc740 heuristic \uc774\ub098 \uc18c\uaddc\ubaa8 image/text validation set \uc744 \uc0ac\uc6a9\ud560 \uacbd\uc6b0 \uc131\ub2a5\uc774 \ub354 \uac1c\uc120\ub420 \uc218 \uc788\ub2e4. \ubc18\uba74 noise \ub97c \uc81c\uac70\ud558\uba74 \uac70\uc758 \ubaa8\ub4e0 task \uc5d0\uc11c \uc131\ub2a5\uc774 \ud06c\uac8c \ud558\ub77d\ud558\ub294\ub370, \uc774\ub294 noise \uac00 modality gap \uc744 \ud574\uacb0\ud558\ub294 \ub370 \ud575\uc2ec\uc801 \uc5ed\ud560\uc744 \ud558\uae30 \ub54c\ubb38\uc774\ub2e4."),(0,n.kt)("h2",{id:"33-training-with-data-from-a-language-model"},"3.3. Training with Data from a Language Model"),(0,n.kt)("p",null,"\ub2e4\uc74c\uc73c\ub85c \uc800\uc790\ub294 language model \uc774 \uc0dd\uc131\ud55c synthetic data \ub85c CLOSE \ub97c \uc0ac\uc6a9\ud574 captioning model \uc744 training \ud55c\ub2e4. "),(0,n.kt)("p",null,"\uba3c\uc800, natural language instruction \uacfc \uba87 \uac1c\uc758 \uc608\uc2dc caption \uc744 \ud3ec\ud568\ud558\ub294 prompt \ub97c in-context learning \ubc29\uc2dd\uc73c\ub85c \uad6c\uc131\ud55c\ub2e4. Fig. 4 \uc5d0 \uc608\uc2dc\uac00 \uc81c\uc2dc\ub41c\ub2e4. \ub2e4\uc591\ud55c caption \uc744 \uc0dd\uc131\ud558\uae30 \uc704\ud574, \uac01 caption \uc55e\uc5d0\ub294 \ud574\ub2f9 caption \uc5d0 \ub4f1\uc7a5\ud558\ub294 \ub450 \uac1c\uc758 keyword \ub97c prefix \ub85c \ubd99\uc774\uace0, prompt \uc758 \ub05d\uc5d0\ub294 \uc0dd\uc131\ub420 caption \uc5d0 \ud3ec\ud568\ub418\uc5b4\uc57c \ud558\ub294 \ub450 \uac1c\uc758 \uc0c8\ub85c\uc6b4 keyword (\u201cfire\u201d, \u201chydrant\u201d \ub4f1) \ub97c \ubc30\uce58\ud55c\ub2e4. ending keyword pair \ub97c \ubc14\uafb8\uc5b4\uc8fc\uba74 \ub2e4\uc591\ud55c caption \uc744 \uc0dd\uc131\ud560 \uc218 \uc788\ub2e4."),(0,n.kt)("p",null,"\uc815\ub7c9\uc801 evaluation \uc5d0\uc11c caption style \uc774 \uc601\ud5a5\uc744 \uc8fc\ub294 \uac83\uc744 \uc904\uc774\uae30 \uc704\ud574 COCO caption \uc758 style \uacfc \uc720\uc0ac\ud558\ub3c4\ub85d \uc77c\ubd80 \uc870\uce58\ub97c \ucde8\ud558\uc9c0\ub9cc, style \uc774 \uc911\uc694\ud558\uc9c0 \uc54a\uc740 setting \uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \uacfc\uc815\uc774 \ubc18\ub4dc\uc2dc \ud544\uc694\ud558\uc9c0\ub294 \uc54a\ub2e4. \uc800\uc790\ub294 \ub2e4\uc74c \uc138 \uac00\uc9c0 generation \ubc29\ubc95\uc73c\ub85c\ubd80\ud130 \ucd1d 100k example \uc744 \uc0dd\uc131\ud55c\ub2e4."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"GPT-J RNG."),"\\\n6 billion parameter open-source language model \uc778 GPT-J \ub97c \uc0ac\uc6a9\ud55c\ub2e4. 50 \uac1c\uc758 in-context example \uc744 \ud3ec\ud568\ud55c\ub2e4. keyword \ub294 COCO training data \uc758 keyword \ub85c\ubd80\ud130 uniform sampling \ud55c\ub2e4."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"GPT-J Unigram."),"\\\nkeyword \ub97c COCO caption \uc758 unigram distribution \uc5d0 \ub9de\ucdb0 sampling \ud55c\ub2e4\ub294 \uc810\uc744 \uc81c\uc678\ud558\uba74 \uc704\uc640 \ub3d9\uc77c\ud558\ub2e4."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Curie Unigram."),"\\\nOpenAI Curie model \uc5d0\uc11c generation \ud558\uba70, 20 \uac1c example \uacfc unigram-matching sampling \uc744 \uc0ac\uc6a9\ud55c\ub2e4.")),(0,n.kt)("p",null,"COCO dataset \uc5d0 \ub300\ud55c \uacb0\uacfc\ub294 Tab. 2 \uc5d0 \ub098\ud0c0\ub09c\ub2e4. "),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Table 2",src:a(51866).Z,width:"812",height:"404"})),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"best result \ub294 78.9 CIDEr \ub97c \uae30\ub85d\ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\uc0dd\uc131\ub41c caption \uc744 inspection \ud574\ubcf4\uba74 keyword sampling \uc804\ub7b5\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ub9ce\uc740 error \uac00 style issue \uc5d0\uc11c \ube44\ub86f\ub428\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc73c\uba70, \ud2b9\ud788 Curie model \uc758 \uc131\ub2a5 \uac10\uc18c\ub294 style \ucc28\uc774\ub85c \uc124\uba85\ub41c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\uc608\ub97c \ub4e4\uc5b4 Curie model \uc774 \uc0dd\uc131\ud55c synthetic caption \uc740 COCO \ubc0f GPT-J caption \ubcf4\ub2e4 \u201copens\u201d \ub77c\ub294 \ub2e8\uc5b4\ub97c 23 \ubc30 \ub354 \uc790\uc8fc \uc0ac\uc6a9\ud558\uba70 (\u201ca living room that opens onto the balcony\u201d \ub4f1), COCO \uc5d0\uc11c \ub354 \ud754\ud55c \ud45c\ud604\uc778 \u201ccell phone\u201d \ub300\uc2e0 \u201ccellphone\u201d \uc744 \uc0ac\uc6a9\ud558\ub294 \uacbd\ud5a5\uc774 \uc788\ub2e4. \ubcf4\ub2e4 \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 Appendix 3 \uc5d0 \uc788\ub2e4.")),(0,n.kt)("p",null,"\uc774\ub7ec\ud55c observation \uc740 language model \uc744 \uc0ac\uc6a9\ud560 \ub54c caption style \uc5d0 subtle \ud55c \uc601\ud5a5\uc744 \ubbf8\uce60 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uadf8\ub7fc\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ubcf8 \ubc29\ubc95\uc740 \uc5ec\uc804\ud788 \ub9e4\uc6b0 \uac15\ub825\ud55c \uacb0\uacfc\ub97c \ub098\ud0c0\ub0b4\uba70 zero-shot \ubc29\ubc95\uc778 MAGIC \uc744 \ub2a5\uac00\ud55c\ub2e4."),(0,n.kt)("h1",{id:"4-analysis"},"4. Analysis"),(0,n.kt)("p",null,"\uc800\uc790\uc758 approach \ub294 \ub450 \uac00\uc9c0 \ud765\ubbf8\ub85c\uc6b4 \uc9c8\ubb38\uc744 \uc81c\uae30\ud55c\ub2e4:\\\n(1) text vector \uc640 image vector \uac00 \uc77c\ubc18\uc801\uc73c\ub85c \ub9e4\uc6b0 \uba40\ub9ac \ub5a8\uc5b4\uc838 \uc788\uc74c\uc5d0\ub3c4 embedding substitution \uc740 \uc65c \uc798 \uc791\ub3d9\ud558\ub294\uac00?\\\n(2) modality gap \uc744 \ub354 \uc798 \uc904\uc774\uae30 \uc704\ud574 \ucd94\uac00 data \ub97c \ud65c\uc6a9\ud558\ub294 \ubc29\ubc95\uc740 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\ub294\uac00?"),(0,n.kt)("p",null,"\uc800\uc790\ub294 \uc774 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\uae30 \uc704\ud574 \ub450 \uac00\uc9c0 \ubd84\uc11d\uc744 \uc218\ud589\ud558\uace0, contrastive embedding model \ub610\ub294 language model \uc120\ud0dd\uc774 \uc131\ub2a5\uc5d0 \uc5b4\ub5a4 \uc601\ud5a5\uc744 \uc8fc\ub294\uc9c0\ub3c4 \uc5f0\uad6c\ud55c\ub2e4."),(0,n.kt)("h2",{id:"41-sensitivity-analysis"},"4.1. Sensitivity Analysis"),(0,n.kt)("p",null,"\uccab \ubc88\uc9f8 \uc9c8\ubb38\uc5d0 \ub2f5\ud558\uae30 \uc704\ud574 \uc800\uc790\ub294 input text vector \uc5d0 \ub300\ud55c sensitivity analysis \ub97c \uc218\ud589\ud55c\ub2e4. \uc2e4\ud5d8\uc5d0\uc11c\ub294 normalized text vector \uc5d0 \uc77c\uc815\ud55c constant vector \ub97c \ub354\ud55c \ub4a4 \ub2e4\uc2dc re-normalization \ud558\uace0, \uc774\ub7ec\ud55c \ubcc0\ud615\ub41c vector \ub85c training \ud558\ub418 testing \uc740 \uae30\uc874\uacfc \ub3d9\uc77c\ud558\uac8c unaltered image vector \ub97c \uc0ac\uc6a9\ud55c\ub2e4. \uc774 \ubcc0\ud654\ub294 text vector \uac00 image vector \uc5d0 \ub300\ud574 \uc5b4\ub5a4 \ubd84\ud3ec\ub97c \ud615\uc131\ud558\ub294\uc9c0\ub9cc \ubc14\uafb8\uba70, text vector \uac04 \uc0c1\ub300\uc801 \ubd84\ud3ec\ub294 \uadf8\ub300\ub85c \uc720\uc9c0\ud55c\ub2e4."),(0,n.kt)("p",null,"\uc800\uc790\ub294 \ub2e4\uc74c \uc138 \uac00\uc9c0 \ubcc0\ud615\uc744 \ud3c9\uac00\ud55c\ub2e4."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Random vector (RNG):")," \ubb34\uc791\uc704 vector \ub97c \uc0ac\uc6a9\ud558\ub418, training \ub3d9\uc548 \ubaa8\ub4e0 sample \uc5d0 \ub300\ud574 \ub3d9\uc77c\ud55c vector \ub97c \uc0ac\uc6a9\ud55c\ub2e4. magnitude \ub97c \uc5ec\ub7ec \uc218\uc900\uc73c\ub85c \ubcc0\ud654\uc2dc\ud0a8\ub2e4."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Mean difference (mean):")," text vector \uc640 image vector \uc758 \ud3c9\uade0 \ucc28\uc774\ub97c \uc0ac\uc6a9\ud558\uc5ec text vector \uac00 image vector \ucabd\uc73c\ub85c shift \ub418\ub3c4\ub85d \ud55c\ub2e4."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Negated mean (-mean):")," \ud3c9\uade0 \ucc28\uc774\uc5d0 \uc74c\uc218\ub97c \ucde8\ud574 image vector \ub85c\ubd80\ud130 \uba40\uc5b4\uc9c0\ub3c4\ub85d \ub9cc\ub4e0\ub2e4.")),(0,n.kt)("p",null,"\ubaa8\ub4e0 \uacbd\uc6b0\uc5d0\uc11c Gaussian noise \ub294 \uae30\uc874\uacfc \ub3d9\uc77c\ud558\uac8c \ucd94\uac00\ub41c\ub2e4."),(0,n.kt)("p",null,"\uacb0\uacfc\ub294 Tab. 3 \uc5d0 \uc81c\uc2dc\ub41c\ub2e4. "),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Table 3",src:a(87341).Z,width:"819",height:"669"})),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"random vector \uc0ac\uc6a9 \uc2dc \ub9e4\uc6b0 \ud070 shift \ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub294 \ud55c \uc131\ub2a5 \uac10\uc18c\uac00 \ubbf8\ubbf8\ud558\uc5ec CLOSE \uac00 training \uc911 text vector \uc758 shift \uc5d0 \uc0c1\ub2f9\ud788 insensitive \ud568\uc744 \ubcf4\uc5ec\uc900\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"vector \ub97c image \ubc29\ud5a5\uc73c\ub85c shift (mean) \ud558\uba74 \uc57d\uac04\uc758 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \ubc1c\uc0dd\ud558\uace0, \ubc18\ub300\ub85c shift (-mean) \ud558\uba74 \ube44\uad50\uc801 \ud070 \uac10\uc18c\uac00 \ubc1c\uc0dd\ud558\uc5ec \uc644\uc804\ud788 insensitive \ud55c \uac83\uc740 \uc544\ub2d8\uc744 \ubcf4\uc5ec\uc900\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\uadf8\ub7ec\ub098 text vector \uc758 absolute position \uc774 \ud06c\uac8c \ubcc0\ud574\ub3c4 \ubaa8\ub378\uc774 \uc5ec\uc804\ud788 \uc798 \uc791\ub3d9\ud55c\ub2e4\ub294 \uc810\uc740 \uc8fc\ubaa9\ud560 \ub9cc\ud558\ub2e4.")),(0,n.kt)("p",null,"\uc800\uc790\ub294 \uc774\ub7ec\ud55c insensitivity \uc758 \uc6d0\uc778\uc744 \ub450 \uac00\uc9c0\ub85c \uac00\uc124\ud55c\ub2e4."),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"text vector \uc758 relative position \uc740 \ubcc0\ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0, shifted feature space \uc758 \ub300\ubd80\ubd84\uc758 \ubc29\ud5a5\uc740 \uc5ec\uc804\ud788 output \uc608\uce21\uc5d0 \ub3d9\uc77c\ud55c \ubc29\uc2dd\uc73c\ub85c predictive \ud558\ub2e4."),(0,n.kt)("li",{parentName:"ol"},"Gaussian noise \uac00 feature space \uc5d0\uc11c \uc911\uc694\ud558\uc9c0 \uc54a\uc740 \ubc29\ud5a5\uc758 shift \uc5d0 model \uc744 insensitive \ud558\ub3c4\ub85d \ud559\uc2b5\uc2dc\ud0a4\uba70, shift \ubc29\ud5a5\uc774 \uc885\uc885 \uc774\ub7ec\ud55c unimportant direction \uc911 \ud558\ub098\ub85c \ud3ec\ud568\ub41c\ub2e4.")),(0,n.kt)("p",null,"\uc774\ub294 \uc9c8\ubb38 (1) \uc758 \uc77c\ubd80 \ub2f5\uc744 \uc81c\uacf5\ud55c\ub2e4. modality gap \uc758 \uc8fc\uc694 \uc6d0\uc778\uc740 image vector \uc640 text vector \uc0ac\uc774\uc758 constant shift \uc774\uc9c0\ub9cc, CLOSE \ub294 text vector \uc758 absolute positioning \uc5d0 \ub9e4\uc6b0 \ubbfc\uac10\ud558\uc9c0 \uc54a\uc73c\ubbc0\ub85c \uc774\ub7ec\ud55c shift \ub97c \uc815\uad50\ud558\uac8c \ub9de\ucd94\ub294 \uac83\uc774 \uc0dd\uac01\ubcf4\ub2e4 \ub35c \uc911\uc694\ud558\ub2e4."),(0,n.kt)("h2",{id:"42-learned-adapter-analysis"},"4.2. Learned Adapter Analysis"),(0,n.kt)("p",null,"Fig. 3c \uac00 \uc2dc\uc0ac\ud558\ub4ef mean shift \ub294 text vector \uc640 image vector \ub97c \uc644\ubcbd\ud558\uac8c align \ud558\uc9c0 \ubabb\ud55c\ub2e4. \ub530\ub77c\uc11c \ub354 \uc815\uad50\ud55c adaptation \ubc29\ubc95\uc774 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\ub2e4\ub294 \uac00\uc124\uc744 \uc138\uc6b4\ub2e4. \uc774\ub7ec\ud55c adapter \ub294 \uc77c\ubc18\uc801\uc73c\ub85c paired image/text corpus \ub97c \ud544\uc694\ub85c \ud558\ubbc0\ub85c \uae30\ubcf8 CLOSE \ubc29\ubc95\uc5d0\uc11c\ub294 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub294\ub2e4. \uadf8\ub7ec\ub098 \uc5ec\uae30\uc11c\ub294 adapter \uac00 \uc5b4\ub290 \uc815\ub3c4\uc758 \uc7a0\uc7ac\uc801 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc904 \uc218 \uc788\ub294\uc9c0 \uc774\ud574\ud558\uae30 \uc704\ud574 \uc5f0\uad6c\ud55c\ub2e4."),(0,n.kt)("p",null,"\uc800\uc790\ub294 annotation \ud488\uc9c8\uc774 \ub192\uc740 COCO caption \uacfc web-scale dataset \uc778 CC3M (3M) \ub450 \uac00\uc9c0 auxiliary corpus \ub97c \uc0ac\uc6a9\ud55c\ub2e4. COCO \uc5d0\uc11c\ub294 Karpathy split \uc758 \u201crestval\u201d set \uc5d0 \ud574\ub2f9\ud558\ub294 30k caption \uc744, CC3M \uc5d0\uc11c\ub294 100k image/text pair \ub97c random sample \ud558\uc5ec \uc0ac\uc6a9\ud55c\ub2e4. \ub2e4\uc74c \ub450 \uac00\uc9c0 adapter \ub97c \uace0\ub824\ud55c\ub2e4."),(0,n.kt)("h4",{id:"linear-adapter"},"Linear Adapter"),(0,n.kt)("p",null,"text vector \ub97c image vector \uc5d0 \ub354 \uc798 align \ud558\uae30 \uc704\ud574, text vector \ub97c image vector \uc5d0 \ub9e4\uce6d\uc2dc\ud0a4\ub294 Euclidean distance \ub97c \ucd5c\uc18c\ud654\ud558\ub294 linear model \uc744 \ud559\uc2b5\ud55c\ub2e4. \ud559\uc2b5 \ud6c4\uc5d0\ub3c4 Gaussian noise \ub294 \ub3d9\uc77c\ud558\uac8c \ucd94\uac00\ud55c\ub2e4."),(0,n.kt)("h4",{id:"structured-noise-with-covariance-matrix"},"Structured Noise with Covariance Matrix"),(0,n.kt)("p",null,"\ubcf8\uc9c8\uc801\uc73c\ub85c text vector \uc640 image vector \uac04\uc5d0 \uc644\uc804\ud55c 1:1 mapping \uc774 \uc874\uc7ac\ud55c\ub2e4\uace0 \uae30\ub300\ud558\uae30 \uc5b4\ub835\ub2e4. \ud558\ub098\uc758 image \ub294 \uc11c\ub85c \ub2e4\ub978 \ubd80\ubd84\uc774\ub098 detail \uc744 \ubb18\uc0ac\ud558\ub294 \ub2e4\uc591\ud55c text \uc640 \uc720\uc0ac\ud55c embedding \uc744 \uac16\uae30 \ub54c\ubb38\uc774\ub2e4. \uc774\ub294 \ub2e8\uc21c mapping function \ub300\uc2e0 text vector \uac00 image vector \uc8fc\ubcc0\uc5d0 \uc5b4\ub5bb\uac8c \ubd84\ud3ec\ud558\ub294\uc9c0\ub97c \ub354 \uc798 \uc774\ud574\ud574\uc57c \ud55c\ub2e4\ub294 \ub3d9\uae30\ub97c \uc81c\uacf5\ud55c\ub2e4."),(0,n.kt)("p",null,"Appendix 4 \uc5d0\uc11c COCO image-caption pair \uc758 vector difference \uac00 \ud2b9\uc815\ud55c \u201cshape\u201d \uc744 \ub530\ub974\ub294 \uacbd\ud5a5\uc774 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uc774\ub97c \uae30\ubc18\uc73c\ub85c \uc800\uc790\ub294 text-image vector difference \uc758 mean \uacfc covariance \ub97c \ud559\uc2b5\ud558\uace0, \uc774\ub97c \uc774\uc6a9\ud574 text vector \uc5d0 structured Gaussian noise \ub97c \ucd94\uac00\ud55c\ub2e4. \uc774\ub294 evaluation \uc2dc \ubc1c\uc0dd\ud560 text-image shift \ub97c \ub354 \uc798 simulation \ud55c\ub2e4."),(0,n.kt)("h4",{id:"results"},"Results"),(0,n.kt)("p",null,"\uacb0\uacfc\ub294 Tab. 4 \uc5d0 \uc81c\uc2dc\ub41c\ub2e4. "),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Table 4",src:a(49877).Z,width:"821",height:"530"})),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"COCO \uae30\ubc18 adapter \uc0ac\uc6a9 \uc2dc captioning \uc5d0\uc11c\ub294 \ud070 \uac1c\uc120, VQA \uc640 visual news \uc5d0\uc11c\ub294 \uc911\uac04 \uc218\uc900\uc758 \uac1c\uc120, visual entailment \uc5d0\uc11c\ub294 \uc720\uc0ac\ud55c \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\ud2b9\ud788 structured noise \ubc29\ubc95\uc740 captioning \uc5d0\uc11c linear adapter \ubcf4\ub2e4 \ud6e8\uc52c \ud6a8\uacfc\uc801\uc774\uba70, \ub2e4\ub978 task \uc5d0\uc11c\ub294 \uc57d\uac04 \ub354 \ub0ae\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4. CC3M base adapter \ub3c4 \uc18c\uaddc\ubaa8 \uac1c\uc120\uc744 \uc81c\uacf5\ud558\uc9c0\ub9cc COCO \uae30\ubc18\ubcf4\ub2e4 \ub35c \ud6a8\uacfc\uc801\uc774\ub2e4."),(0,n.kt)("li",{parentName:"ul"},"\uc774\ub294 adapter \ub97c \ud559\uc2b5\ud560 \ub54c \uc0ac\uc6a9\ud558\ub294 data source \uac00 \ub9e4\uc6b0 \uc911\uc694\ud568\uc744 \ubcf4\uc5ec\uc8fc\uba70, Fig. 3c \ubc0f Fig. 3e \uc5d0\uc11c \uc774\ub7ec\ud55c qualitative \ucc28\uc774\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub2e4.")),(0,n.kt)("h2",{id:"43-performance-analysis-of-different-clip-and-t5-models"},"4.3. Performance Analysis of Different CLIP and T5 Models"),(0,n.kt)("p",null,"\ub9c8\uc9c0\ub9c9\uc73c\ub85c contrastive embedding model \ub610\ub294 language model \uc758 \uc120\ud0dd\uc774 CLOSE \uc758 \uc131\ub2a5\uc5d0 \uc5b4\ub5a4 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \uc5f0\uad6c\ud55c\ub2e4. captioning, visual entailment, E-VQA \uc5d0 \ub300\ud55c \uacb0\uacfc\ub294 Tab. 5 \uc5d0 \uc81c\uc2dc\ub41c\ub2e4. "),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Figure 5",src:a(4355).Z,width:"774",height:"761"})),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"\uc774 \uc2e4\ud5d8\uc5d0\uc11c\ub294 best-case \uc131\ub2a5\uc744 \ube44\uad50\ud558\uae30 \uc704\ud574 tuned noise value \ub97c \uc0ac\uc6a9\ud55c\ub2e4. \uc804\uccb4\uc801\uc73c\ub85c noise level \uc758 \ucd5c\uc801\uac12\uc740 model \uad6c\uc131 \uc694\uc18c\uac00 \ub2ec\ub77c\uc838\ub3c4 \uac70\uc758 \ubcc0\ud558\uc9c0 \uc54a\uc558\uae30 \ub54c\ubb38\uc5d0, \ubcf8 \uc2e4\ud5d8\uc5d0\uc11c\ub294 main result \uc640 \ub3d9\uc77c\ud55c noise level \uc744 \uc0ac\uc6a9\ud55c\ub2e4."),(0,n.kt)("li",{parentName:"ul"},"CLIP \uc758 version \uc744 \ubcc0\uacbd\ud558\uba74 ViT-L/14 \ub97c \uc0ac\uc6a9\ud560 \ub54c\ubcf4\ub2e4 \uc77c\uad00\ub418\uac8c \uc131\ub2a5\uc774 \uac10\uc18c\ud558\uba70, RN50\xd764 \ub9cc\uc774 \ube44\uad50 \uac00\ub2a5\ud55c \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4. ",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"\uc774\ub294 contrastive model \uc774 \ub354 powerful \ud560\uc218\ub85d CLOSE \uc758 \ud6a8\uacfc\uac00 \uc99d\uac00\ud55c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4. "))),(0,n.kt)("li",{parentName:"ul"},"T5 model size \uc5d0 \ub300\ud55c \uc758\uc874\uc131\uc740 \uc0c1\ub300\uc801\uc73c\ub85c \uc801\uc73c\uba70, large model \uc740 VQA \uc5d0\uc11c\ub9cc \uc131\ub2a5 \ud5a5\uc0c1\uc744 \ubcf4\uc774\uace0 \ub2e4\ub978 task \uc5d0\uc11c\ub294 \uc99d\uac00 \ud6a8\uacfc\uac00 \ud06c\uc9c0 \uc54a\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"OpenCLIP \uc740 \uc804\ubc18\uc801\uc73c\ub85c \ub354 \ud6a8\uacfc\uc801\uc774\uba70 captioning \uacb0\uacfc\ub97c 100 CIDEr \uc5d0 \uadfc\uc811\ud558\uac8c \ub04c\uc5b4\uc62c\ub9b0\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"EVA-CLIP model \uc740 VQA \uc810\uc218\ub97c \ub354\uc6b1 \ud5a5\uc0c1\uc2dc\ud0a4\uba70, image \ub97c \uc0ac\uc6a9\ud55c main result (67.9) \uc5d0 \uac00\uae4c\uc6b4 \uc218\uc900\uae4c\uc9c0 \ub3c4\ub2ec\ud55c\ub2e4. ",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"\uc774\ub294 contrastive model \uc790\uccb4\uc758 \uc131\ub2a5\uc774 \ud5a5\uc0c1\ub418\uba74 CLOSE \uc758 \uc131\ub2a5\ub3c4 \ud568\uaed8 \uc99d\uac00\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud55c\ub2e4.")))),(0,n.kt)("h1",{id:"5-stylistic-captioning"},"5. Stylistic Captioning"),(0,n.kt)("p",null,"\uc800\uc790\ub294 \ud2b9\uc815 writing style \uc744 \uac16\ub294 caption \uc744 \uc0dd\uc131\ud558\ub294 task \uc5d0 CLOSE \ub97c \uc801\uc6a9\ud574 \uadf8 \ud65c\uc6a9 \uac00\ub2a5\uc131\uc744 \uc2dc\uc5f0\ud55c\ub2e4. \uc804\uccb4\uc801\uc778 approach \ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"\ud2b9\uc815 style \uc744 \uac00\uc9c4 text-only training data \ub97c \uc218\uc9d1\ud55c\ub2e4."),(0,n.kt)("li",{parentName:"ol"},"\uc774\ub97c Sec. 3.2 \uc640 \ub3d9\uc77c\ud55c \ubc29\uc2dd\uc73c\ub85c text caption \uc73c\ub85c \ucde8\uae09\ud574 model \uc744 training \ud55c\ub2e4."),(0,n.kt)("li",{parentName:"ol"},"\uc774\ud6c4 image \uc5d0 model \uc744 \uc801\uc6a9\ud558\uc5ec \ud574\ub2f9 style \uc758 caption \uc744 \uc0dd\uc131\ud55c\ub2e4.")),(0,n.kt)("p",null,"\uc800\uc790\ub294 \ub2e4\uc591\ud55c natural language data source \ub97c \ud65c\uc6a9\ud574 \uc11c\ub85c \ub2e4\ub978 style \uc744 \ud559\uc2b5\ud55c \ub124 \uac00\uc9c0 captioning style \uc744 \uc81c\uc2dc\ud55c\ub2e4."),(0,n.kt)("h4",{id:"ego-centric"},"Ego-Centric"),(0,n.kt)("p",null,"Sec. 3.3 \uc5d0\uc11c language model \ub85c \uc0dd\uc131\ub41c data \ub9cc\uc73c\ub85c training \uc774 \uac00\ub2a5\ud568\uc744 \ubcf4\uc600\ub2e4. \uc5ec\uae30\uc11c\ub294 \uc774\ub97c ego-centric style \uc5d0 \uc801\uc6a9\ud55c\ub2e4. Fig. 4 \uc640 \ub3d9\uc77c\ud55c prompt format \uc744 \uc0ac\uc6a9\ud558\ub418, manually authored \ub41c first-person \uad00\uc810\uc758 caption 20 \uac1c\ub97c \uc608\uc2dc\ub85c \ud3ec\ud568\ud55c\ub2e4. "),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Figure 4",src:a(77622).Z,width:"815",height:"511"})),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"COCO training caption \uc5d0\uc11c keyword \ub97c random sampling \ud558\uc5ec diverse prompt \ub97c \uc0dd\uc131\ud558\uace0, GPT-3 \ub85c\ubd80\ud130 20k caption \uc744 \uc0dd\uc131\ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"COCO validation image \uc5d0 \uc801\uc6a9\ud55c \uacb0\uacfc\ub294 Fig. 5 top row \uc5d0 \uc81c\uc2dc\ub418\uba70, model \uc740 \uc774\ubbf8\uc9c0 \ub0b4\uc6a9\uc744 \uc815\ud655\ud788 \ubb18\uc0ac\ud558\uba74\uc11c \ub2e4\uc591\ud55c first-person \ud45c\ud604\uc744 \uc0ac\uc6a9\ud55c\ub2e4.")),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Figure 5",src:a(13548).Z,width:"1263",height:"1174"})),(0,n.kt)("h4",{id:"uplifting"},"Uplifting"),(0,n.kt)("p",null,"uplifting style caption \uc744 \uc704\ud574 publicly available dataset \uc5d0\uc11c image \uc5c6\uc774 6k \uc608\uc2dc\ub97c \uc218\uc9d1\ud55c\ub2e4. Fig. 5 second row \uc5d0 \ub098\ud0c0\ub09c \uacb0\uacfc\uc5d0\uc11c model \uc774 \ub530\ub73b\ud558\uace0 \uae0d\uc815\uc801\uc778 detail \uc744 caption \uc5d0 \ucd94\uac00\ud558\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\ub2e4."),(0,n.kt)("h4",{id:"character-based"},"Character-Based"),(0,n.kt)("p",null,"\ub2e4\uc74c\uc73c\ub85c proper noun \uc744 \ud65c\uc6a9\ud574 story \uc758 \uc77c\ubd80\ucc98\ub7fc caption \uc744 \uc0dd\uc131\ud558\ub294 character-based style \uc744 \ubaa9\ud45c\ub85c \ud55c\ub2e4. \uae30\uc874 system \uc740 image/name paired data \uac00 \ubd80\uc871\ud574 proper noun handling \uc774 \uc5b4\ub824\uc6b0\ub098, CLOSE \ub294 CLIP \uc758 \uc720\uba85 \uc778\ubb3c \uc778\uc2dd \ub2a5\ub825 \ub355\ubd84\uc5d0 \uc774 \ubb38\uc81c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\ub2e4."),(0,n.kt)("p",null,"\uc808\ucc28\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Harry Potter \uce90\ub9ad\ud130 33 \uba85\uc744 \uc120\ud0dd\ud55c\ub2e4."),(0,n.kt)("li",{parentName:"ul"},"Harry Potter book \ub610\ub294 fan fiction \uc758 \uc77c\ubd80 excerpt \ub97c \uc218\ub3d9\uc73c\ub85c \uc218\uc9d1\ud574 GPT-3 prompt \ub85c \uc0ac\uc6a9\ud55c\ub2e4."),(0,n.kt)("li",{parentName:"ul"},"\uc774 prompt \uc640 \uce90\ub9ad\ud130 \uc774\ub984\uc744 \uae30\ubc18\uc73c\ub85c GPT-3 \ub85c 13k caption \uc744 \uc0dd\uc131\ud55c\ub2e4.")),(0,n.kt)("p",null,"relevant photo \uc5d0 \ub300\ud55c \uacb0\uacfc\ub294 Fig. 5 third row \uc5d0 \uc81c\uc2dc\ub418\uba70, model \uc740 \uc774\ub984\uacfc image content \ub97c \uc815\ud655\ud788 \uc0ac\uc6a9\ud558\uace0, \ub54c\ub54c\ub85c \ucc45\uc774\ub098 \uc601\ud654\uc758 \ud55c \uc7a5\uba74\ucc98\ub7fc plausible \ud55c event \ub97c \ub367\ubd99\uc778\ub2e4."),(0,n.kt)("h4",{id:"reviews"},"Reviews"),(0,n.kt)("p",null,"\ub9c8\uc9c0\ub9c9\uc73c\ub85c product review \ud615\uc2dd\uc758 caption \uc744 \uc0dd\uc131\ud558\ub294 model \uc744 training \ud55c\ub2e4. "),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"training data \ub85c\ub294 publicly available Amazon product review \ub97c \uc0ac\uc6a9\ud558\uba70, length \uac00 \ucd5c\ub300 40 token \uc774\uba74\uc11c \uae0d\uc815\uc801\uc778 review \ub97c \uc120\ud0dd\ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"Fig. 5 bottom row \uc5d0 \ub098\ud0c0\ub09c \uacb0\uacfc\uc5d0\uc11c model \uc774 \ub2e4\uc591\ud55c \ud45c\ud604\uc744 \uc0ac\uc6a9\ud574 \uc0ac\uc9c4 \uc18d \ubb3c\uac74\uc5d0 \ub300\ud55c positive review \ud615\ud0dc\uc758 caption \uc744 \uc0dd\uc131\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.")),(0,n.kt)("h1",{id:"6-related-work"},"6. Related Work"),(0,n.kt)("h4",{id:"using-contrastive-models"},"Using Contrastive Models"),(0,n.kt)("p",null,"vision\u2013language contrastive model \uc740 CLIP, ALIGN, UniCL, OpenCLIP \ub4f1 \ub2e4\uc591\ud55c \ud615\ud0dc\ub85c \uad6c\ucd95\ub418\uc5b4 \uc654\ub2e4. \ucd5c\uadfc\uc5d0\ub294 contrastive training component \ub97c \ud3ec\ud568\ud558\ub294 multi-modal model \ub3c4 \ub4f1\uc7a5\ud558\uace0 \uc788\ub2e4. \uc774\ub7ec\ud55c model \uc740 \uc77c\ubc18\uc801\uc73c\ub85c image classification \uacfc \uac19\uc740 task \uc5d0\uc11c\ub294 zero-shot \uc73c\ub85c \ud6a8\uacfc\uc801\uc774\uc9c0\ub9cc, captioning \uc774\ub098 visual entailment \uacfc \uac19\uc740 \ub354 \ubcf5\uc7a1\ud55c task \uc5d0\uc11c\ub294 \uc5b4\ub824\uc6c0\uc744 \uacaa\ub294\ub2e4. \ub610\ub294 down-stream task \ub97c \uc704\ud55c feature extractor \ub85c \uc0ac\uc6a9\ub418\uae30\ub3c4 \ud55c\ub2e4. \uc800\uc790\uc758 work \ub294 zero-shot \uacfc fine-tuning \uc0ac\uc774\uc758 \uc808\ucda9\uc810\uc744 \uc81c\uacf5\ud558\uba70, image annotation \uc5c6\uc774 text-only data \ub9cc\uc73c\ub85c training \ud560 \uc218 \uc788\uc5b4 zero-shot \ub300\ube44 \ud070 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc81c\uacf5\ud55c\ub2e4."),(0,n.kt)("h4",{id:"zero-shot-vision-using-language-models"},"Zero-Shot Vision Using Language Models"),(0,n.kt)("p",null,"\ucd5c\uadfc large language model \uacfc pre-trained vision model \uc744 \uacb0\ud569\ud558\uc5ec vision task \ub97c zero-shot \uc73c\ub85c \uc218\ud589\ud558\ub294 \ubc29\ubc95\ub4e4\uc774 \uc81c\uc548\ub418\uc5c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, reinforcement learning \uc73c\ub85c CLIP embedding \uc5d0 \uc798 \ub9de\ub294 text \ub97c \uc0dd\uc131\ud558\ub3c4\ub85d \ud559\uc2b5\ud558\uac70\ub098, CLIP \uc744 LLM \uc758 inference \uc5d0 \ud65c\uc6a9\ud558\uac70\ub098, image \ub97c \uc124\uba85\ud558\ub294 text \ub97c \ub2e4\ub978 model \ub85c \uc0dd\uc131\ud574 language model \uc5d0 \uc785\ub825\ud558\ub294 \ubc29\uc2dd \ub4f1\uc774 \uc788\ub2e4. \uadf8\ub7ec\ub098 \uc774\ub7ec\ud55c \uc644\uc804 zero-shot \ubc29\uc2dd\uc740 caption style \uacfc \uac19\uc740 task-specific detail \uc744 \ud559\uc2b5\ud558\uae30 \uc5b4\ub835\uace0, \ub9e4\uc6b0 \ud070 language model \uc774 \ud544\uc694\ud574 computation cost \uac00 \ud06c\ub2e4. CLOSE \ub294 text-only data \ub85c \uc138\ubd80\uc801\uc774\uace0 task-specific \ud55c \uc694\uc18c\ub97c \ud559\uc2b5\ud560 \uc218 \uc788\uc73c\uba70, 220M parameter \uaddc\ubaa8\uc758 \uc0c1\ub300\uc801\uc73c\ub85c \uc791\uc740 model \ub85c\ub3c4 \ud6a8\uacfc\uc801\uc774\ub2e4."),(0,n.kt)("h4",{id:"cross-modal-transfer-learning"},"Cross-Modal Transfer Learning"),(0,n.kt)("p",null,"\uc804\ud1b5\uc801 transfer learning \uc740 \uac70\uc758 \ud56d\uc0c1 \ub3d9\uc77c modality \ub0b4\uc5d0\uc11c skill \uc744 \uc804\uc774\ud55c\ub2e4. \uc608\uc678\uc801\uc73c\ub85c CROMA \ub294 modality-invariant feature space \ub97c \ud65c\uc6a9\ud574 cross-modal transfer \ub97c \uc2dc\ub3c4\ud558\uc600\uc73c\ub098, classification \uc5d0\ub9cc \uc801\uc6a9\ub418\uace0 few-shot setting \uc774\ub2e4. pre-trained language model \uc774 modality \ub97c \ub118\uc5b4 \uc77c\ubd80 skill \uc744 \uc804\uc774\ud560 \uc218 \uc788\ub2e4\ub294 \uc5f0\uad6c\ub3c4 \uc788\uc73c\ub098, caption style \uac19\uc740 task-specific skill \uc744 \ud559\uc2b5\ud558\uc9c0\ub294 \ubabb\ud55c\ub2e4. multi-modal/multi-task model \uc740 \uc5ec\ub7ec modality \uc5d0 \uac78\uccd0 task \ub97c \ud559\uc2b5\ud568\uc73c\ub85c\uc368 skill transfer \uac00\ub2a5\uc131\uc744 \ubcf4\uc774\uc9c0\ub9cc, \ubcf8 \ub17c\ubb38\uc774 \ub2e4\ub8e8\ub294 zero-shot cross-modal setting \u2014 \uc989 target modality \uc758 training data \uc790\uccb4\uac00 \uc5c6\ub294 \uacbd\uc6b0 \u2014 \uc740 \ud6e8\uc52c \ub354 \uc5b4\ub835\ub2e4."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Song et al. \uc740 CLIP \uc744 \ud65c\uc6a9\ud55c vector-substitution \uc744 visual entailment \uc5d0 \uc801\uc6a9\ud588\uc73c\ub098, modality gap \uc744 \uc644\ud654\ud558\uae30 \uc704\ud55c noise \ub4f1\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc558\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"Yu et al. \uc740 CLIP \uc774 image \uc640 \uac00\uae4c\uc6b4 text \ub97c \uc798 \uc120\ud0dd\ud558\ub3c4\ub85d RL \uae30\ubc18 \ud559\uc2b5\uc744 \uc218\ud589\ud588\uc73c\uba70, caption style \ud559\uc2b5\ub3c4 \uc2dc\ub3c4\ud588\uc73c\ub098 vision task \uc758 text-only training \uc740 \uc218\ud589\ud558\uc9c0 \uc54a\uc558\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"Nukrai et al. \uacfc Wei et al. \uc740 CLOSE \uc640 \uc720\uc0ac\ud55c Gaussian noise \ub610\ub294 text embedding projection \ubc29\uc2dd\uc758 text-only \uc811\uadfc\uc744 \uc81c\uc548\ud588\uc73c\ub098, \uc800\uc790\ub294 \ub354 \ub2e4\uc591\ud55c task, language-model-generated data, \uc0c1\uc138\ud55c \ubd84\uc11d\uc744 \ud3ec\ud568\ud558\uba70 captioning \uc131\ub2a5\ub3c4 \ub354 \uc6b0\uc218\ud558\ub2e4.")),(0,n.kt)("h4",{id:"domain-invariant-representations"},"Domain Invariant Representations"),(0,n.kt)("p",null,"domain-invariant feature \ub97c \uc0ac\uc6a9\ud574 domain shift \ub97c \uadf9\ubcf5\ud558\ub294 \uc5f0\uad6c\ub294 \uc624\ub798\ub418\uc5c8\ub2e4. "),(0,n.kt)("p",null,"multi-domain training, target domain \uc758 \uc18c\ub7c9 label, unsupervised data \ub4f1 \ub2e4\uc591\ud55c \ubc29\uc2dd\uc73c\ub85c invariant feature \ub97c \uad6c\ucd95\ud574\uc654\ub2e4. adversarial learning, maximum mean discrepancy, data augmentation \ub4f1 \ub2e4\uc591\ud55c \uae30\ubc95\uc774 \uc0ac\uc6a9\ub418\uc5c8\ub2e4. Gaussian noise \uc758 domain shift robustness \uc81c\uace0 \ud6a8\uacfc\ub3c4 \uc54c\ub824\uc838 \uc788\ub2e4. \uadf8\ub7ec\ub098 \ubcf8 \ub17c\ubb38\uc774 \ub2e4\ub8e8\ub294 domain shift \ub294 modality \uc790\uccb4\uac00 \ubc14\ub00c\ub294 \ud6e8\uc52c \ub354 \uadf9\ub2e8\uc801\uc778 \uacbd\uc6b0\uc774\uba70, \uc800\uc790\ub294 large-scale contrastive model \uc774 \uc801\uc808\ud788 \ud65c\uc6a9\ub420 \uacbd\uc6b0 \ud6a8\uacfc\uc801\uc778 invariant feature source \uac00 \ub420 \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4."),(0,n.kt)("h4",{id:"stylistic-captioning"},"Stylistic Captioning"),(0,n.kt)("p",null,"stylistic captioning model \uc740 \uc6d0\ud558\ub294 style \uc758 caption \uc744 \uc9c1\uc811 \uc791\uc131\ud574 training \ud558\uac70\ub098, \ub2e4\ub978 style \uc758 caption \uc73c\ub85c\ubd80\ud130 style \uc744 transfer \ud558\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c\ub2e4. adversarial learning, multi-tasking, style/context factorization \ub4f1 \ub2e4\uc591\ud55c \ubc29\uc2dd\uc774 \uc788\ub2e4. "),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Tan et al. \uc740 image \uc640 text \ubaa8\ub450\uc5d0\uc11c text \ub97c \uc0dd\uc131\ud558\ub294 model \uc744 shared encoding space \uc640 style embedding \uacfc \ud568\uaed8 \ud559\uc2b5\ud558\uc600\uc73c\ub098 paired image/caption data \ub97c \ud544\uc694\ub85c \ud55c\ub2e4. "),(0,n.kt)("li",{parentName:"ul"},"\ubc18\uba74 CLOSE \ub294 paired image/caption data \uc5c6\uc774\ub3c4 stylistic captioning \uc774 \uac00\ub2a5\ud558\ub2e4.")),(0,n.kt)("h1",{id:"7-conclusion"},"7. Conclusion"),(0,n.kt)("p",null,"contrastive model \uc774 \ud559\uc2b5\ud558\ub294 multi-modal semantic vector space \ub294 CLOSE \ub97c \ud1b5\ud574 cross-modal generalization \uc5d0 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc74c\uc744 \ubcf4\uc600\ub2e4. \ub610\ud55c sensitivity \ubd84\uc11d \ubc0f adapter \ud559\uc2b5\uc744 \ud1b5\ud574 modality gap \uc5d0 \ub300\ud55c \uc774\ud574\ub97c \ub192\uc600\uc73c\uba70, \ub2e4\uc218\uc758 vision\u2013language task \uc5d0 \ub300\ud55c \uc2e4\ud5d8\uacfc stylistic captioning \uc751\uc6a9\uc744 \uc81c\uc2dc\ud588\ub2e4."),(0,n.kt)("p",null,"CLOSE \ub294 \ud55c modality \uc5d0\uc11c\ub294 data \uac00 \ud48d\ubd80\ud558\uc9c0\ub9cc \ub2e4\ub978 modality \uc5d0\uc11c\ub294 data \uac00 \ubd80\uc871\ud55c \uc0c1\ud669\uc5d0\uc11c \ub9e4\uc6b0 \uc720\uc6a9\ud558\ub2e4. \uc608\ub97c \ub4e4\uc5b4:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"image captioning data \ub85c 3D scene captioning model \uc744 training"),(0,n.kt)("li",{parentName:"ul"},"text summarization data \ub85c video summarization model \uc744 training"),(0,n.kt)("li",{parentName:"ul"},"table, graph, sensor \ub4f1 \ub35c \uc5f0\uad6c\ub41c modality \uc5d0 \ub300\ud574 VQA \ub098 captioning model \uc744 annotation \uc5c6\uc774 training")),(0,n.kt)("p",null,"\ub354 \uac15\ub825\ud558\uace0 \ub2e4\uc591\ud55c modality \ub97c \ud3ec\ud568\ud55c contrastive model \uc774 \uac1c\ubc1c\ub420\uc218\ub85d CLOSE \uc758 \uc131\ub2a5\uacfc \ud65c\uc6a9 \uac00\ub2a5\uc131\uc740 \ub354\uc6b1 \ucee4\uc9c8 \uac83\uc774\ub2e4."))}u.isMDXComponent=!0},26943:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/image-76-093f721845469eb5e8033358ea4f6f51.png"},35070:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/image-77-7371cf53498a92e7f34eb0dd01cd0751.png"},81090:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/image-78-bde6d77d0b2094a3ff5cbab47a9d8234.png"},81265:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/image-79-817b4c7ca6e04a33471924f6d07450f5.png"},51866:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/image-80-8d07e41b09bec5f139fa6ab4df989ba2.png"},77622:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/image-81-803edfdf970f44b66482a5f97a395ac6.png"},87341:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/image-82-b0a1c421466958bf109772eee1b4b43c.png"},49877:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/image-83-d9f9e8a8820dea3d2d2a8f30dfdbd2ec.png"},13548:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/image-84-d0d0148f1e927d30076e528645c60c72.png"},4355:(e,t,a)=>{a.d(t,{Z:()=>i});const i=a.p+"assets/images/image-85-297375762003466ae19e4295aaa409eb.png"}}]);