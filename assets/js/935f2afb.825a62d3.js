"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[80053],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Tutorial Intro","href":"/docs/intro","docId":"intro"},{"type":"category","label":"Tutorial - Basics","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Create a Page","href":"/docs/tutorial-basics/create-a-page","docId":"tutorial-basics/create-a-page"},{"type":"link","label":"Create a Document","href":"/docs/tutorial-basics/create-a-document","docId":"tutorial-basics/create-a-document"},{"type":"link","label":"Create a Blog Post","href":"/docs/tutorial-basics/create-a-blog-post","docId":"tutorial-basics/create-a-blog-post"},{"type":"link","label":"Markdown Features","href":"/docs/tutorial-basics/markdown-features","docId":"tutorial-basics/markdown-features"},{"type":"link","label":"Deploy your site","href":"/docs/tutorial-basics/deploy-your-site","docId":"tutorial-basics/deploy-your-site"},{"type":"link","label":"Congratulations!","href":"/docs/tutorial-basics/congratulations","docId":"tutorial-basics/congratulations"}],"href":"/docs/category/tutorial---basics"},{"type":"category","label":"Tutorial - Extras","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Manage Docs Versions","href":"/docs/tutorial-extras/manage-docs-versions","docId":"tutorial-extras/manage-docs-versions"},{"type":"link","label":"Translate your site","href":"/docs/tutorial-extras/translate-your-site","docId":"tutorial-extras/translate-your-site"}],"href":"/docs/category/tutorial---extras"},{"type":"category","label":"Paper","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Computer Vision","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Adversarial Attack","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Adversarial Reprogramming of Neural Networks","href":"/docs/Paper/Computer Vision/Adversarial Attack/AR","docId":"Paper/Computer Vision/Adversarial Attack/2018-06-AR"}]},{"type":"category","label":"Generation","collapsible":true,"collapsed":true,"items":[{"type":"category","label":" Concept Editing","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models","href":"/docs/Paper/Computer Vision/Generation/ Concept Editing/SLD","docId":"Paper/Computer Vision/Generation/ Concept Editing/2022-11-SLD"},{"type":"link","label":"Ablating Concepts in Text-to-Image Diffusion Models","href":"/docs/Paper/Computer Vision/Generation/ Concept Editing/CA","docId":"Paper/Computer Vision/Generation/ Concept Editing/2023-03-CA"},{"type":"link","label":"Editing Implicit Assumptions in Text-to-Image Diffusion Models","href":"/docs/Paper/Computer Vision/Generation/ Concept Editing/TIME","docId":"Paper/Computer Vision/Generation/ Concept Editing/2023-03-TIME"},{"type":"link","label":"Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models","href":"/docs/Paper/Computer Vision/Generation/ Concept Editing/SA","docId":"Paper/Computer Vision/Generation/ Concept Editing/2023-05-SA"},{"type":"link","label":"Erasing Concepts from Diffusion Models","href":"/docs/Paper/Computer Vision/Generation/ Concept Editing/ESD","docId":"Paper/Computer Vision/Generation/ Concept Editing/2023-07-ESD"},{"type":"link","label":"Unified Concept Editing in Diffusion Models","href":"/docs/Paper/Computer Vision/Generation/ Concept Editing/UCE","docId":"Paper/Computer Vision/Generation/ Concept Editing/2023-08-UCE"},{"type":"link","label":"Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation","href":"/docs/Paper/Computer Vision/Generation/ Concept Editing/SDID","docId":"Paper/Computer Vision/Generation/ Concept Editing/2023-11-SDID"},{"type":"link","label":"Mace: Mass concept erasure in diffusion models","href":"/docs/Paper/Computer Vision/Generation/ Concept Editing/MACE","docId":"Paper/Computer Vision/Generation/ Concept Editing/2024-03-MACE"},{"type":"link","label":"Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models","href":"/docs/Paper/Computer Vision/Generation/ Concept Editing/RECE","docId":"Paper/Computer Vision/Generation/ Concept Editing/2024-07-RECE"},{"type":"link","label":"Safree: Training-Free and Adaptive Guard for Safe Text-to-Image and Video Generation","href":"/docs/Paper/Computer Vision/Generation/ Concept Editing/SAFREE","docId":"Paper/Computer Vision/Generation/ Concept Editing/2024-10-SAFREE"}]},{"type":"category","label":"Interpretability","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Interpretable Diffusion via Information Decomposition","href":"/docs/Paper/Computer Vision/Generation/Interpretability/Info-Decomp","docId":"Paper/Computer Vision/Generation/Interpretability/2023-10-Information_Decomposition"}]}]},{"type":"category","label":"Image Classification","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"An Image Is Worth 16X16 Words: Transformers for image recognition at sacle","href":"/docs/Paper/Computer Vision/Image Classification/ViT","docId":"Paper/Computer Vision/Image Classification/2020-10-ViT"},{"type":"link","label":"EfficientNetV2: Smaller Models and Faster Training","href":"/docs/Paper/Computer Vision/Image Classification/EfficientNetV2","docId":"Paper/Computer Vision/Image Classification/2021-04-EfficientNetV2"}]},{"type":"category","label":"Multi-task","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"A Unified Sequence Interface for Vision Tasks","href":"/docs/Paper/Computer Vision/Multi-task/Unified Interface","docId":"Paper/Computer Vision/Multi-task/2022-06-Unified Interface"},{"type":"link","label":"UNINEXT: Universal Instance Perception as Object Discovery and Retrieval","href":"/docs/Paper/Computer Vision/Multi-task/UNINEXT","docId":"Paper/Computer Vision/Multi-task/2023-03-UNINEXT"}]},{"type":"category","label":"PEFT","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Composition","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach","href":"/docs/Paper/Computer Vision/PEFT/Composition/RLRR","docId":"Paper/Computer Vision/PEFT/Composition/2024-03-RLRR"},{"type":"link","label":"InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning","href":"/docs/Paper/Computer Vision/PEFT/Composition/InfLoRA","docId":"Paper/Computer Vision/PEFT/Composition/2024-04-InfLoRA"}]},{"type":"category","label":"Feature-Shift","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Scaling & Shifting Your Features: A New Baseline for Efficient Model Tuning","href":"/docs/Paper/Computer Vision/PEFT/Feature-Shift/SSF","docId":"Paper/Computer Vision/PEFT/Feature-Shift/2022-10-SSF"}]}]}]},{"type":"category","label":"Machine Learning","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Regularization","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Dropout","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Dropout: A Simple Way to Prevent Neural Networks from Overfitting","href":"/docs/Paper/Machine Learning/Regularization/Dropout/Dropout","docId":"Paper/Machine Learning/Regularization/Dropout/2014-01-Dropout"}]},{"type":"category","label":"GNI","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Training with Noise is Equivalent to Tikhonov Regularization","href":"/docs/Paper/Machine Learning/Regularization/GNI/Training Noise","docId":"Paper/Machine Learning/Regularization/GNI/1995-02-Training_Noise"},{"type":"link","label":"Whiteout: Gaussian Adaptive Noise Injection Regularization in Deep Neural Networks","href":"/docs/Paper/Machine Learning/Regularization/GNI/Whiteout","docId":"Paper/Machine Learning/Regularization/GNI/2016-12-Whiteout"},{"type":"link","label":"Explicit Regularisation in Gaussian Noise Injections","href":"/docs/Paper/Machine Learning/Regularization/GNI/GNIs","docId":"Paper/Machine Learning/Regularization/GNI/2021-07-GNIs"},{"type":"link","label":"Anticorrelated Noise Injection for Improved Generalization","href":"/docs/Paper/Machine Learning/Regularization/GNI/Anti_PGD","docId":"Paper/Machine Learning/Regularization/GNI/2022-02-Anti-PGD"},{"type":"link","label":"Explicit Regularization in Overparametrized Models via Noise Injection","href":"/docs/Paper/Machine Learning/Regularization/GNI/Before_GNIs","docId":"Paper/Machine Learning/Regularization/GNI/2022-06-BGNIs"},{"type":"link","label":"PAC-tuning: Fine-tuning Pretrained Language Models with PAC-driven Perturbed Gradient Descent","href":"/docs/Paper/Machine Learning/Regularization/GNI/PAC-tuning","docId":"Paper/Machine Learning/Regularization/GNI/2023-10-PAC-tuning"}]}]}]},{"type":"category","label":"NLP","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Analysis","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings","href":"/docs/Paper/NLP/Analysis/Contextualized Representation","docId":"Paper/NLP/Analysis/2019-11-Context_Representation"}]},{"type":"category","label":"Attacking","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Universal and Transferable Adversarial Attacks on Aligned Language Models","href":"/docs/Paper/NLP/Attacking/universal-adversarial-prompt","docId":"Paper/NLP/Attacking/2023-07-Adversarial-AutoPrompt"},{"type":"link","label":"Can a Large Language Model Be a Gaslighter?","href":"/docs/Paper/NLP/Attacking/Gaslighter","docId":"Paper/NLP/Attacking/2024-10-Gaslighting"}]},{"type":"category","label":"Augmentation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"PromptDA : Label-guided Data Augmentation for Prompt-based Few Shot Learners","href":"/docs/Paper/NLP/Augmentation/PromptDA","docId":"Paper/NLP/Augmentation/2023-05-PromptDA"}]},{"type":"category","label":"Model","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Attention Is All You Need","href":"/docs/Paper/NLP/Model/Transformer","docId":"Paper/NLP/Model/2017-06-Transformer"},{"type":"link","label":"Improving Language Understanding by Generative Pre-Training","href":"/docs/Paper/NLP/Model/GPT-1","docId":"Paper/NLP/Model/2018-06-GPT"},{"type":"link","label":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","href":"/docs/Paper/NLP/Model/BERT","docId":"Paper/NLP/Model/2018-10-BERT"}]},{"type":"category","label":"Multi-Task","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Scaling Instruction-Finetuned Language Models","href":"/docs/Paper/NLP/Multi-Task/Flan-T5","docId":"Paper/NLP/Multi-Task/2022-10-Flan-T5"},{"type":"link","label":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation","href":"/docs/Paper/NLP/Multi-Task/CodeT5+","docId":"Paper/NLP/Multi-Task/2023-05-CodeT5p"}]},{"type":"category","label":"PEFT","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Composition","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"LoRA: Low-Rank Adaptation of Large Language Models","href":"/docs/Paper/NLP/PEFT/Composition/LoRA","docId":"Paper/NLP/PEFT/Composition/2021-06-LoRA"},{"type":"link","label":"FedPara: Low-Rank Hadamard Product For Communication-Efficient Federated Learning","href":"/docs/Paper/NLP/PEFT/Composition/LoHa","docId":"Paper/NLP/PEFT/Composition/2021-08-LoHa"},{"type":"link","label":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models","href":"/docs/Paper/NLP/PEFT/Composition/BitFit","docId":"Paper/NLP/PEFT/Composition/2022-05-BitFit"},{"type":"link","label":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","href":"/docs/Paper/NLP/PEFT/Composition/IA\xb3","docId":"Paper/NLP/PEFT/Composition/2022-05-IA\xb3"},{"type":"link","label":"DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation","href":"/docs/Paper/NLP/PEFT/Composition/DyLoRA","docId":"Paper/NLP/PEFT/Composition/2022-10-DyLoRA"},{"type":"link","label":"KronA: Parameter Efficient Tuning with Kronecker Adapter","href":"/docs/Paper/NLP/PEFT/Composition/LoKr","docId":"Paper/NLP/PEFT/Composition/2022-12-LoKr"},{"type":"link","label":"Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning","href":"/docs/Paper/NLP/PEFT/Composition/AdaLoRA","docId":"Paper/NLP/PEFT/Composition/2023-03-AdaLoRA"},{"type":"link","label":"ReLoRA: High-Rank Training Through Low-Rank Updates","href":"/docs/Paper/NLP/PEFT/Composition/ReLoRA","docId":"Paper/NLP/PEFT/Composition/2023-07-ReLoRA"},{"type":"link","label":"IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning","href":"/docs/Paper/NLP/PEFT/Composition/IncreLoRA","docId":"Paper/NLP/PEFT/Composition/2023-08-IncreLoRA"},{"type":"link","label":"Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices","href":"/docs/Paper/NLP/PEFT/Composition/Delta-LoRA","docId":"Paper/NLP/PEFT/Composition/2023-09-Delta-LoRA"},{"type":"link","label":"LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models","href":"/docs/Paper/NLP/PEFT/Composition/LongLoRA","docId":"Paper/NLP/PEFT/Composition/2023-09-LongLoRA"},{"type":"link","label":"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning","href":"/docs/Paper/NLP/PEFT/Composition/SaLoRA","docId":"Paper/NLP/PEFT/Composition/2023-10-SaLoRA"},{"type":"link","label":"Sparse Low-rank Adaptation of Pre-trained Language Models","href":"/docs/Paper/NLP/PEFT/Composition/SoRA","docId":"Paper/NLP/PEFT/Composition/2023-11-SoRA"},{"type":"link","label":"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning","href":"/docs/Paper/NLP/PEFT/Composition/COLA","docId":"Paper/NLP/PEFT/Composition/2024-01-COLA"},{"type":"link","label":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","href":"/docs/Paper/NLP/PEFT/Composition/ApiQ","docId":"Paper/NLP/PEFT/Composition/2024-02-ApiQ"},{"type":"link","label":"DoRA: Weight-Decomposed Low-Rank Adaptation","href":"/docs/Paper/NLP/PEFT/Composition/DoRA","docId":"Paper/NLP/PEFT/Composition/2024-02-DoRA"},{"type":"link","label":"FLoRA: Low-Rank Adapters Are Secretly Gradient Compressors","href":"/docs/Paper/NLP/PEFT/Composition/FLoRA","docId":"Paper/NLP/PEFT/Composition/2024-02-FLoRA"},{"type":"link","label":"LoRA+: Efficient Low Rank Adaptation of Large Models","href":"/docs/Paper/NLP/PEFT/Composition/LoRA+","docId":"Paper/NLP/PEFT/Composition/2024-02-LoRA+"},{"type":"link","label":"MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning","href":"/docs/Paper/NLP/PEFT/Composition/MeLoRA","docId":"Paper/NLP/PEFT/Composition/2024-02-MeLoRA"},{"type":"link","label":"ResLoRA: Identity Residual Mapping in Low-Rank Adaption","href":"/docs/Paper/NLP/PEFT/Composition/ResLoRA","docId":"Paper/NLP/PEFT/Composition/2024-02-ResLoRA"},{"type":"link","label":"SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning","href":"/docs/Paper/NLP/PEFT/Composition/SIBO","docId":"Paper/NLP/PEFT/Composition/2024-02-SIBO"},{"type":"link","label":"ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models","href":"/docs/Paper/NLP/PEFT/Composition/ALoRA","docId":"Paper/NLP/PEFT/Composition/2024-03-ALoRA"},{"type":"link","label":"AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning","href":"/docs/Paper/NLP/PEFT/Composition/AutoLoRA","docId":"Paper/NLP/PEFT/Composition/2024-03-AutoLoRA"},{"type":"link","label":"BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models","href":"/docs/Paper/NLP/PEFT/Composition/BiLoRA","docId":"Paper/NLP/PEFT/Composition/2024-03-BiLoRA"},{"type":"link","label":"LoRA Meets Dropout under a Unified Framework","href":"/docs/Paper/NLP/PEFT/Composition/HiddenKey","docId":"Paper/NLP/PEFT/Composition/2024-03-HiddenKey"},{"type":"link","label":"LoRA Dropout as a Sparsity Regularizer for Overfitting Control","href":"/docs/Paper/NLP/PEFT/Composition/LoRA-Dropout","docId":"Paper/NLP/PEFT/Composition/2024-04-LoRA-Drop"},{"type":"link","label":"PISSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models","href":"/docs/Paper/NLP/PEFT/Composition/PISSA","docId":"Paper/NLP/PEFT/Composition/2024-04-PISSA"},{"type":"link","label":"DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution","href":"/docs/Paper/NLP/PEFT/Composition/DoRA2","docId":"Paper/NLP/PEFT/Composition/2024-05-DoRA"},{"type":"link","label":"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning","href":"/docs/Paper/NLP/PEFT/Composition/MoRA","docId":"Paper/NLP/PEFT/Composition/2024-05-MoRA"},{"type":"link","label":"MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning","href":"/docs/Paper/NLP/PEFT/Composition/MiLoRA","docId":"Paper/NLP/PEFT/Composition/2024-06-MiLoRA"},{"type":"link","label":"Mixture-of-Subspaces in Low-Rank Adaptation","href":"/docs/Paper/NLP/PEFT/Composition/MoSLoRA","docId":"Paper/NLP/PEFT/Composition/2024-06-MoSLoRA"},{"type":"link","label":"OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models","href":"/docs/Paper/NLP/PEFT/Composition/OLoRA","docId":"Paper/NLP/PEFT/Composition/2024-06-OLoRA"},{"type":"link","label":"RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning","href":"/docs/Paper/NLP/PEFT/Composition/RoseLoRA","docId":"Paper/NLP/PEFT/Composition/2024-06-RoseLoRA"},{"type":"link","label":"SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models","href":"/docs/Paper/NLP/PEFT/Composition/SinkLoRA","docId":"Paper/NLP/PEFT/Composition/2024-06-SinkLoRA"}]},{"type":"category","label":"Generalization","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape","href":"/docs/Paper/NLP/PEFT/Generalization/Flat-LoRA","docId":"Paper/NLP/PEFT/Generalization/2024-09-Flat-LoRA"}]},{"type":"category","label":"Mixture","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning","href":"/docs/Paper/NLP/PEFT/Mixture/UniPELT","docId":"Paper/NLP/PEFT/Mixture/2022-05-UniPELT"},{"type":"link","label":"Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients","href":"/docs/Paper/NLP/PEFT/Mixture/PEFT without Its Gradients","docId":"Paper/NLP/PEFT/Mixture/2023-12-No-Gradients"}]},{"type":"category","label":"Module","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Parameter-Efficient Transfer Learning for NLP","href":"/docs/Paper/NLP/PEFT/Module/Adapter","docId":"Paper/NLP/PEFT/Module/2019-02-Adapter"},{"type":"link","label":"AdapterFusion: Non-Destructive Task Composition for Transfer Learning","href":"/docs/Paper/NLP/PEFT/Module/AdapterFusion","docId":"Paper/NLP/PEFT/Module/2020-05.AdapterFusion"}]},{"type":"category","label":"Pruning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Pruning Pre-trained Language Models Without Fine-Tuning","href":"/docs/Paper/NLP/PEFT/Pruning/SMP","docId":"Paper/NLP/PEFT/Pruning/2023-07-SMP"}]},{"type":"category","label":"Quantization","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Fine-Tuning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models","href":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning","docId":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-10-AlphaTuning"},{"type":"link","label":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models","href":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant","docId":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-11-SmoothQuant"},{"type":"link","label":"QuIP: 2-Bit Quantization of Large Language Models With Guarantees","href":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/QuIP","docId":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2023-07-QuIP"},{"type":"link","label":"BitNet: Scaling 1-bit Transformers for Large Language Models","href":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet","docId":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2023-10-BitNet"},{"type":"link","label":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","href":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B","docId":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-02-BitNet-1.58b"},{"type":"link","label":"GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs","href":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/GIFT-SW","docId":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-08-GIFT-SW"},{"type":"link","label":"LLM-FP4: 4-Bit Floating-Point Quantized Transformers","href":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/LLM-FP4","docId":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-10-LLM-FP4"},{"type":"link","label":"BitNet a4.8: 4-bit Activations for 1-bit LLMs","href":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8","docId":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-11-BitNet-a4.8"},{"type":"link","label":"Optimizing Large Language Model Training Using FP4 Quantization","href":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/fp4","docId":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2025-01-fp4"}]},{"type":"category","label":"LoRA","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"QLORA: Efficient Finetuning of Quantized LLMs","href":"/docs/Paper/NLP/PEFT/Quantization/LoRA/QLoRA","docId":"Paper/NLP/PEFT/Quantization/LoRA/2023-05-QLoRA"},{"type":"link","label":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models","href":"/docs/Paper/NLP/PEFT/Quantization/LoRA/QA-LoRA","docId":"Paper/NLP/PEFT/Quantization/LoRA/2023-09-QA-LoRA"},{"type":"link","label":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models","href":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LoftQ","docId":"Paper/NLP/PEFT/Quantization/LoRA/2023-10-LoftQ"},{"type":"link","label":"LQ-LoRA: Low-Rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning","href":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LQ-LoRA","docId":"Paper/NLP/PEFT/Quantization/LoRA/2023-11-LQ-LoRA"},{"type":"link","label":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","href":"/docs/Paper/NLP/PEFT/Quantization/LoRA/ApiQ","docId":"Paper/NLP/PEFT/Quantization/LoRA/2024-02-ApiQ"},{"type":"link","label":"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention","href":"/docs/Paper/NLP/PEFT/Quantization/LoRA/IR-QLoRA","docId":"Paper/NLP/PEFT/Quantization/LoRA/2024-02-IR-QLoRA"},{"type":"link","label":"Low-Rank Quantization-Aware Training for LLMs","href":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LR-QAT","docId":"Paper/NLP/PEFT/Quantization/LoRA/2024-06-LR-QAT"},{"type":"link","label":"Accurate and Efficient Fine-Tuning Quantized Large Language Models Through Optimal Balance","href":"/docs/Paper/NLP/PEFT/Quantization/LoRA/Q-BaRA","docId":"Paper/NLP/PEFT/Quantization/LoRA/2024-07-Q-BaRA"},{"type":"link","label":"RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization","href":"/docs/Paper/NLP/PEFT/Quantization/LoRA/RoLoRA","docId":"Paper/NLP/PEFT/Quantization/LoRA/2024-07-RoLoRA"},{"type":"link","label":"QEFT: Quantization for Efficient Fine-Tuning of LLMs","href":"/docs/Paper/NLP/PEFT/Quantization/LoRA/QEFT","docId":"Paper/NLP/PEFT/Quantization/LoRA/2024-10-QEFT"}]}]},{"type":"category","label":"Soft Prompt","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","href":"/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning","docId":"Paper/NLP/PEFT/Soft Prompt/2021-01-Prefix-Tuning"},{"type":"link","label":"GPT Understands, Too","href":"/docs/Paper/NLP/PEFT/Soft Prompt/P-tuning","docId":"Paper/NLP/PEFT/Soft Prompt/2021-03-P-tuning"},{"type":"link","label":"The Power of Scale for Parameter-Efficient Prompt Tuning","href":"/docs/Paper/NLP/PEFT/Soft Prompt/Prompt Tuning","docId":"Paper/NLP/PEFT/Soft Prompt/2021-04-Prompt-Tuning"},{"type":"link","label":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks","href":"/docs/Paper/NLP/PEFT/Soft Prompt/P-tuning v2","docId":"Paper/NLP/PEFT/Soft Prompt/2021-10-P-tuning v2"},{"type":"link","label":"ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts","href":"/docs/Paper/NLP/PEFT/Soft Prompt/ATTEMPT","docId":"Paper/NLP/PEFT/Soft Prompt/2022-05-ATTEMPT"},{"type":"link","label":"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer","href":"/docs/Paper/NLP/PEFT/Soft Prompt/SPoT","docId":"Paper/NLP/PEFT/Soft Prompt/2022-05-SPoT"},{"type":"link","label":"XPrompt: Exploring the Extreme of Prompt Tuning","href":"/docs/Paper/NLP/PEFT/Soft Prompt/XPrompt","docId":"Paper/NLP/PEFT/Soft Prompt/2022-12-XPrompt"},{"type":"link","label":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","href":"/docs/Paper/NLP/PEFT/Soft Prompt/LLaMA-Adapter","docId":"Paper/NLP/PEFT/Soft Prompt/2023-03-LLaMA-Adapter"},{"type":"link","label":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","href":"/docs/Paper/NLP/PEFT/Soft Prompt/Multitask Prompt Tuning","docId":"Paper/NLP/PEFT/Soft Prompt/2023-03-MPT"},{"type":"link","label":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","href":"/docs/Paper/NLP/PEFT/Soft Prompt/LLaMA-Adapter V2","docId":"Paper/NLP/PEFT/Soft Prompt/2023-04-LLaMA-Adapter V2"},{"type":"link","label":"Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization","href":"/docs/Paper/NLP/PEFT/Soft Prompt/Residual Prompt Tuning","docId":"Paper/NLP/PEFT/Soft Prompt/2023-05-Residual-Prompt-Tuning"},{"type":"link","label":"DEPT: Decomposed Prompt Tuning For Parameter-Efficient Fine Tuning","href":"/docs/Paper/NLP/PEFT/Soft Prompt/DEPT","docId":"Paper/NLP/PEFT/Soft Prompt/2023-09-DEPT"},{"type":"link","label":"APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models","href":"/docs/Paper/NLP/PEFT/Soft Prompt/APrompt","docId":"Paper/NLP/PEFT/Soft Prompt/2023-12-APrompt"},{"type":"link","label":"SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts","href":"/docs/Paper/NLP/PEFT/Soft Prompt/SMoP","docId":"Paper/NLP/PEFT/Soft Prompt/2023-12-SMoP"}]}]},{"type":"category","label":"Prompt Engineering","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"CoT","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","href":"/docs/Paper/NLP/Prompt Engineering/CoT/Chain-of-Thought","docId":"Paper/NLP/Prompt Engineering/CoT/2022-01-CoT"}]}]},{"type":"category","label":"Prompt Tuning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"PTR: Prompt Tuning with Rules for Text Classification","href":"/docs/Paper/NLP/Prompt Tuning/PTR","docId":"Paper/NLP/Prompt Tuning/2022-11-PTR"}]},{"type":"category","label":"Reinforcement Learning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Reflexion: Language Agents with Verbal Reinforcement Learning","href":"/docs/Paper/NLP/Reinforcement Learning/Reflexion","docId":"Paper/NLP/Reinforcement Learning/2023-03-Reflexion"}]},{"type":"category","label":"Text Generation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Training language models to follow instructions with human feedback (+ ChatGPT)","href":"/docs/Paper/NLP/Text Generation/InstructGPT","docId":"Paper/NLP/Text Generation/2022-03-InstructGPT"}]}]},{"type":"category","label":"Survey","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","href":"/docs/Paper/Survey/Prompting","docId":"Paper/Survey/2021-07-Prompting"},{"type":"link","label":"Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey","href":"/docs/Paper/Survey/PEFT for PVMs","docId":"Paper/Survey/2024-02-PEFT Vision"},{"type":"link","label":"Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models","href":"/docs/Paper/Survey/LLM-Attacks","docId":"Paper/Survey/2024-03-Attacking"}]},{"type":"category","label":"Vision-Language","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"CLIP","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Few-shot","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Composition","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Low-Rank Few-Shot Adaptation of Vision-Language Models","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA","docId":"Paper/Vision-Language/CLIP/Few-shot/Composition/2024-05-CLIP-LoRA"}]},{"type":"category","label":"Module","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"CLIP-Adapter: Better Vision-Language Models with Feature Adapters","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/CLIP-Adapter","docId":"Paper/Vision-Language/CLIP/Few-shot/Module/2021-10-CLIP-Adapter"},{"type":"link","label":"Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/Tip-Adapter","docId":"Paper/Vision-Language/CLIP/Few-shot/Module/2021-11-Tip-Adapter"},{"type":"link","label":"Task Residual for Tuning Vision-Language Models","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/TaskRes","docId":"Paper/Vision-Language/CLIP/Few-shot/Module/2022-11-TaskRes"},{"type":"link","label":"Meta-Adapter: An Online Few-shot Learner for Vision-Language Model","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/Meta-Adapter","docId":"Paper/Vision-Language/CLIP/Few-shot/Module/2023-11-Meta-Adapter"},{"type":"link","label":"A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/CLAP","docId":"Paper/Vision-Language/CLIP/Few-shot/Module/2023-12-CLAP"}]},{"type":"category","label":"Multi-Modality","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"MaPLe: Multi-modal Prompt Learning","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/MaPLe","docId":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2022-10-MaPLe"},{"type":"link","label":"Adaptive Multi-Modality Prompt Learning","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/AMMPL","docId":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2023-12-AMMPL"},{"type":"link","label":"APoLLo: Unified Adapter and Prompt Learning for Vision Language Models","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/APoLLo","docId":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2023-12-APoLLo"},{"type":"link","label":"APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/APLe","docId":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2024-01-APLe"},{"type":"link","label":"MMA: Multi-Modal Adapter for Vision-Language Models","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/MMA","docId":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2024-01-MMA"}]},{"type":"category","label":"Prompting","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Pixel-Level","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Cross-modal Adversarial Reprogramming","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/CMAR","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2021-02-CMAR"},{"type":"link","label":"Exploring Visual Prompts for Adapting Large-Scale Models","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/VP","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-03-VP"},{"type":"link","label":"Watermarking for Out-of-distribution Detection","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/Watermarking","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-10-Watermarking"},{"type":"link","label":"Understanding and Improving Visual Prompting: A Label-Mapping Perspective","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/ILM-VP","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-11-ILM-VP"},{"type":"link","label":"Unleashing the Power of Visual Prompting At the Pixel Level","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/EVP","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-12-EVP"},{"type":"link","label":"Diversity-Aware Meta Visual Prompting","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/DAM-VP","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-03-DAM-VP"},{"type":"link","label":"Explicit Visual Prompting for Low-Level Structure Segmentations","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/EVP-L","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-03-EVP-L"},{"type":"link","label":"Hint-Aug: Drawing Hints from Foundation Vision Transformers towards Boosted Few-shot Parameter-Efficient Tuning","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/HinTAug","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-04-HintAug"},{"type":"link","label":"AutoVP: An Automated Visual Prompting Framework and Benchmark","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/AutoVP","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-10-AutoVP"},{"type":"link","label":"A2XP: Towards Private Domain Generalization","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/A2XP","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-11-A2XP"},{"type":"link","label":"Instruct Me More! Random Prompting for Visual In-Context Learning","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/InMeMo","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-11-InMeMo"},{"type":"link","label":"LaViP: Language-Grounded Visual Prompts","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/LaViP","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-12-LaViP"},{"type":"link","label":"AdaViPro: Region-Based Adaptive Visual Prompt For Large-Scale Models Adapting","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/AdaViPro","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2024-03-ADAVIPRO"},{"type":"link","label":"Exploring the Transferability of Visual Prompting for Multimodal Large Language Models","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/TVP","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2024-04-TVP"},{"type":"link","label":"Sample-specific Masks for Visual Reprogramming-based Prompting","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/SMM","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2024-06-SMM"}]},{"type":"category","label":"Textual-Token","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Learning to Prompt for Vision-Language Models","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/CoOp","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2021-09-CoOp"},{"type":"link","label":"Conditional Prompt Learning for Vision-Language Models","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/CoCoOp","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2022-03-CoCoOp"},{"type":"link","label":"Prompt-aligned Gradient for Prompt Tuning","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/ProGrad","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2022-05-ProGrad"},{"type":"link","label":"PLOT: Prompt Learning with Optimal Transport for Vision-Language Models","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/PLOT","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2022-10-PLOT"},{"type":"link","label":"Visual-Language Prompt Tuning with Knowledge-guided Context Optimization","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/KgCoOp","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2023-03-KgCoOp"}]},{"type":"category","label":"Visual-Token","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Visual Prompt Tuning","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/VPT","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2022-03-VPT"},{"type":"link","label":"Visual Prompt Tuning For Test-time Domain Adaptation","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/DePT","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2022-10-DePT"},{"type":"link","label":"LPT: Long-Tailed Prompt Tuning For Image Classification","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/LPT","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2022-10-LPT"},{"type":"link","label":"Convolutional Visual Prompt for Robust Visual Perception","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/CVP","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2023-03-CVP"},{"type":"link","label":"E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/E2VPT","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2023-07-E2VPT"},{"type":"link","label":"SA^2VPT: Spatially Aligned-and-Adapted Visual Prompt","href":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/SA2VP","docId":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2023-12-SA2VP"}]}]}]}]},{"type":"category","label":"Foundation","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Contrastive Learning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision","href":"/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN","docId":"Paper/Vision-Language/Foundation/Contrastive Learning/2021-02-ALIGN"},{"type":"link","label":"Learning Transferable Visual Models From Natural Language Supervision","href":"/docs/Paper/Vision-Language/Foundation/Contrastive Learning/CLIP","docId":"Paper/Vision-Language/Foundation/Contrastive Learning/2021-03-CLIP"}]},{"type":"category","label":"Single-Stream","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"VL-BERT: Pre-Training Of Generic Visual-Linguistic Representations","href":"/docs/Paper/Vision-Language/Foundation/Single-Stream/VLBERT","docId":"Paper/Vision-Language/Foundation/Single-Stream/2019-08-VLBERT"},{"type":"link","label":"VisualBERT: A Simple And Performance Baseline For Visual And Language","href":"/docs/Paper/Vision-Language/Foundation/Single-Stream/VisualBERT","docId":"Paper/Vision-Language/Foundation/Single-Stream/2019-08-VisualBERT"},{"type":"link","label":"UNITER: UNiversal Image-TExt Representation Learning","href":"/docs/Paper/Vision-Language/Foundation/Single-Stream/UNITER","docId":"Paper/Vision-Language/Foundation/Single-Stream/2019-09-UNITER"},{"type":"link","label":"ImageBERT: Cross-Modal Pre-Training with Large-Scale Weak-Supervised Image-Text Data","href":"/docs/Paper/Vision-Language/Foundation/Single-Stream/ImageBERT","docId":"Paper/Vision-Language/Foundation/Single-Stream/2020-01-ImageBERT"},{"type":"link","label":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers","href":"/docs/Paper/Vision-Language/Foundation/Single-Stream/Pixel-BERT","docId":"Paper/Vision-Language/Foundation/Single-Stream/2020-04-Pixel-BERT"},{"type":"link","label":"VD-BERT: A Unified Vision and Dialog Transformer with BERT","href":"/docs/Paper/Vision-Language/Foundation/Single-Stream/VD-BERT","docId":"Paper/Vision-Language/Foundation/Single-Stream/2020-04-VD-BERT"}]},{"type":"category","label":"Two-Stream","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers","href":"/docs/Paper/Vision-Language/Foundation/Two-Stream/LXMERT","docId":"Paper/Vision-Language/Foundation/Two-Stream/2019-08-LXMERT"},{"type":"link","label":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks","href":"/docs/Paper/Vision-Language/Foundation/Two-Stream/ViLBERT","docId":"Paper/Vision-Language/Foundation/Two-Stream/2019-08-ViLBERT"}]}]},{"type":"category","label":"VQA-IC","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Few-shot","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Multimodal Few-Shot Learning with Frozen Language Models","href":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot","docId":"Paper/Vision-Language/VQA-IC/Few-shot/2021-06-Few-shot"}]}]}]}]},{"type":"category","label":"Programming","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"What is the Algorithm?","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"BackJoon","href":"/docs/Programming/Algorithm/Problems","docId":"Programming/Algorithm/Problems"}],"href":"/docs/Programming/Algorithm/"}]}]},"docs":{"intro":{"id":"intro","title":"Tutorial Intro","description":"Let\'s discover Docusaurus in less than 5 minutes.","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Adversarial Attack/2018-06-AR":{"id":"Paper/Computer Vision/Adversarial Attack/2018-06-AR","title":"Adversarial Reprogramming of Neural Networks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Generation/ Concept Editing/2022-11-SLD":{"id":"Paper/Computer Vision/Generation/ Concept Editing/2022-11-SLD","title":"Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Generation/ Concept Editing/2023-03-CA":{"id":"Paper/Computer Vision/Generation/ Concept Editing/2023-03-CA","title":"Ablating Concepts in Text-to-Image Diffusion Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Generation/ Concept Editing/2023-03-TIME":{"id":"Paper/Computer Vision/Generation/ Concept Editing/2023-03-TIME","title":"Editing Implicit Assumptions in Text-to-Image Diffusion Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Generation/ Concept Editing/2023-05-SA":{"id":"Paper/Computer Vision/Generation/ Concept Editing/2023-05-SA","title":"Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Generation/ Concept Editing/2023-07-ESD":{"id":"Paper/Computer Vision/Generation/ Concept Editing/2023-07-ESD","title":"Erasing Concepts from Diffusion Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Generation/ Concept Editing/2023-08-UCE":{"id":"Paper/Computer Vision/Generation/ Concept Editing/2023-08-UCE","title":"Unified Concept Editing in Diffusion Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Generation/ Concept Editing/2023-11-SDID":{"id":"Paper/Computer Vision/Generation/ Concept Editing/2023-11-SDID","title":"Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Generation/ Concept Editing/2024-03-MACE":{"id":"Paper/Computer Vision/Generation/ Concept Editing/2024-03-MACE","title":"Mace: Mass concept erasure in diffusion models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Generation/ Concept Editing/2024-07-RECE":{"id":"Paper/Computer Vision/Generation/ Concept Editing/2024-07-RECE","title":"Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Generation/ Concept Editing/2024-10-SAFREE":{"id":"Paper/Computer Vision/Generation/ Concept Editing/2024-10-SAFREE","title":"Safree: Training-Free and Adaptive Guard for Safe Text-to-Image and Video Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Generation/Interpretability/2023-10-Information_Decomposition":{"id":"Paper/Computer Vision/Generation/Interpretability/2023-10-Information_Decomposition","title":"Interpretable Diffusion via Information Decomposition","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Image Classification/2020-10-ViT":{"id":"Paper/Computer Vision/Image Classification/2020-10-ViT","title":"An Image Is Worth 16X16 Words: Transformers for image recognition at sacle","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Image Classification/2021-04-EfficientNetV2":{"id":"Paper/Computer Vision/Image Classification/2021-04-EfficientNetV2","title":"EfficientNetV2: Smaller Models and Faster Training","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Multi-task/2022-06-Unified Interface":{"id":"Paper/Computer Vision/Multi-task/2022-06-Unified Interface","title":"A Unified Sequence Interface for Vision Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/Multi-task/2023-03-UNINEXT":{"id":"Paper/Computer Vision/Multi-task/2023-03-UNINEXT","title":"UNINEXT: Universal Instance Perception as Object Discovery and Retrieval","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/PEFT/Composition/2024-03-RLRR":{"id":"Paper/Computer Vision/PEFT/Composition/2024-03-RLRR","title":"Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/PEFT/Composition/2024-04-InfLoRA":{"id":"Paper/Computer Vision/PEFT/Composition/2024-04-InfLoRA","title":"InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Computer Vision/PEFT/Feature-Shift/2022-10-SSF":{"id":"Paper/Computer Vision/PEFT/Feature-Shift/2022-10-SSF","title":"Scaling & Shifting Your Features: A New Baseline for Efficient Model Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Machine Learning/Regularization/Dropout/2014-01-Dropout":{"id":"Paper/Machine Learning/Regularization/Dropout/2014-01-Dropout","title":"Dropout: A Simple Way to Prevent Neural Networks from Overfitting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Machine Learning/Regularization/GNI/1995-02-Training_Noise":{"id":"Paper/Machine Learning/Regularization/GNI/1995-02-Training_Noise","title":"Training with Noise is Equivalent to Tikhonov Regularization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Machine Learning/Regularization/GNI/2016-12-Whiteout":{"id":"Paper/Machine Learning/Regularization/GNI/2016-12-Whiteout","title":"Whiteout: Gaussian Adaptive Noise Injection Regularization in Deep Neural Networks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd94\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Machine Learning/Regularization/GNI/2021-07-GNIs":{"id":"Paper/Machine Learning/Regularization/GNI/2021-07-GNIs","title":"Explicit Regularisation in Gaussian Noise Injections","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Machine Learning/Regularization/GNI/2022-02-Anti-PGD":{"id":"Paper/Machine Learning/Regularization/GNI/2022-02-Anti-PGD","title":"Anticorrelated Noise Injection for Improved Generalization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Machine Learning/Regularization/GNI/2022-06-BGNIs":{"id":"Paper/Machine Learning/Regularization/GNI/2022-06-BGNIs","title":"Explicit Regularization in Overparametrized Models via Noise Injection","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Machine Learning/Regularization/GNI/2023-10-PAC-tuning":{"id":"Paper/Machine Learning/Regularization/GNI/2023-10-PAC-tuning","title":"PAC-tuning: Fine-tuning Pretrained Language Models with PAC-driven Perturbed Gradient Descent","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Analysis/2019-11-Context_Representation":{"id":"Paper/NLP/Analysis/2019-11-Context_Representation","title":"How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Attacking/2023-07-Adversarial-AutoPrompt":{"id":"Paper/NLP/Attacking/2023-07-Adversarial-AutoPrompt","title":"Universal and Transferable Adversarial Attacks on Aligned Language Models","description":"\ub17c\ubb38 \ubc0f \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Attacking/2024-10-Gaslighting":{"id":"Paper/NLP/Attacking/2024-10-Gaslighting","title":"Can a Large Language Model Be a Gaslighter?","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Augmentation/2023-05-PromptDA":{"id":"Paper/NLP/Augmentation/2023-05-PromptDA","title":"PromptDA : Label-guided Data Augmentation for Prompt-based Few Shot Learners","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Model/2017-06-Transformer":{"id":"Paper/NLP/Model/2017-06-Transformer","title":"Attention Is All You Need","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Model/2018-06-GPT":{"id":"Paper/NLP/Model/2018-06-GPT","title":"Improving Language Understanding by Generative Pre-Training","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Model/2018-10-BERT":{"id":"Paper/NLP/Model/2018-10-BERT","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","description":"\ub17c\ubb38 \uc774\ubbf8\uc9c0 \ubc0f \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Multi-Task/2022-10-Flan-T5":{"id":"Paper/NLP/Multi-Task/2022-10-Flan-T5","title":"Scaling Instruction-Finetuned Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Multi-Task/2023-05-CodeT5p":{"id":"Paper/NLP/Multi-Task/2023-05-CodeT5p","title":"CodeT5+: Open Code Large Language Models for Code Understanding and Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2021-06-LoRA":{"id":"Paper/NLP/PEFT/Composition/2021-06-LoRA","title":"LoRA: Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2021-08-LoHa":{"id":"Paper/NLP/PEFT/Composition/2021-08-LoHa","title":"FedPara: Low-Rank Hadamard Product For Communication-Efficient Federated Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2022-05-BitFit":{"id":"Paper/NLP/PEFT/Composition/2022-05-BitFit","title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2022-05-IA\xb3":{"id":"Paper/NLP/PEFT/Composition/2022-05-IA\xb3","title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2022-10-DyLoRA":{"id":"Paper/NLP/PEFT/Composition/2022-10-DyLoRA","title":"DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2022-12-LoKr":{"id":"Paper/NLP/PEFT/Composition/2022-12-LoKr","title":"KronA: Parameter Efficient Tuning with Kronecker Adapter","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2023-03-AdaLoRA":{"id":"Paper/NLP/PEFT/Composition/2023-03-AdaLoRA","title":"Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2023-07-ReLoRA":{"id":"Paper/NLP/PEFT/Composition/2023-07-ReLoRA","title":"ReLoRA: High-Rank Training Through Low-Rank Updates","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2023-08-IncreLoRA":{"id":"Paper/NLP/PEFT/Composition/2023-08-IncreLoRA","title":"IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2023-09-Delta-LoRA":{"id":"Paper/NLP/PEFT/Composition/2023-09-Delta-LoRA","title":"Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2023-09-LongLoRA":{"id":"Paper/NLP/PEFT/Composition/2023-09-LongLoRA","title":"LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2023-10-SaLoRA":{"id":"Paper/NLP/PEFT/Composition/2023-10-SaLoRA","title":"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2023-11-SoRA":{"id":"Paper/NLP/PEFT/Composition/2023-11-SoRA","title":"Sparse Low-rank Adaptation of Pre-trained Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-01-COLA":{"id":"Paper/NLP/PEFT/Composition/2024-01-COLA","title":"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-02-ApiQ":{"id":"Paper/NLP/PEFT/Composition/2024-02-ApiQ","title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-02-DoRA":{"id":"Paper/NLP/PEFT/Composition/2024-02-DoRA","title":"DoRA: Weight-Decomposed Low-Rank Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-02-FLoRA":{"id":"Paper/NLP/PEFT/Composition/2024-02-FLoRA","title":"FLoRA: Low-Rank Adapters Are Secretly Gradient Compressors","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-02-LoRA+":{"id":"Paper/NLP/PEFT/Composition/2024-02-LoRA+","title":"LoRA+: Efficient Low Rank Adaptation of Large Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-02-MeLoRA":{"id":"Paper/NLP/PEFT/Composition/2024-02-MeLoRA","title":"MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-02-ResLoRA":{"id":"Paper/NLP/PEFT/Composition/2024-02-ResLoRA","title":"ResLoRA: Identity Residual Mapping in Low-Rank Adaption","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-02-SIBO":{"id":"Paper/NLP/PEFT/Composition/2024-02-SIBO","title":"SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-03-ALoRA":{"id":"Paper/NLP/PEFT/Composition/2024-03-ALoRA","title":"ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models","description":"\ub17c\ubb38 \ubc0f \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-03-AutoLoRA":{"id":"Paper/NLP/PEFT/Composition/2024-03-AutoLoRA","title":"AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-03-BiLoRA":{"id":"Paper/NLP/PEFT/Composition/2024-03-BiLoRA","title":"BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-03-HiddenKey":{"id":"Paper/NLP/PEFT/Composition/2024-03-HiddenKey","title":"LoRA Meets Dropout under a Unified Framework","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-04-LoRA-Drop":{"id":"Paper/NLP/PEFT/Composition/2024-04-LoRA-Drop","title":"LoRA Dropout as a Sparsity Regularizer for Overfitting Control","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-04-PISSA":{"id":"Paper/NLP/PEFT/Composition/2024-04-PISSA","title":"PISSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-05-DoRA":{"id":"Paper/NLP/PEFT/Composition/2024-05-DoRA","title":"DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution","description":"\ub17c\ubb38 \ubc0f \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-05-MoRA":{"id":"Paper/NLP/PEFT/Composition/2024-05-MoRA","title":"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-06-MiLoRA":{"id":"Paper/NLP/PEFT/Composition/2024-06-MiLoRA","title":"MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-06-MoSLoRA":{"id":"Paper/NLP/PEFT/Composition/2024-06-MoSLoRA","title":"Mixture-of-Subspaces in Low-Rank Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-06-OLoRA":{"id":"Paper/NLP/PEFT/Composition/2024-06-OLoRA","title":"OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-06-RoseLoRA":{"id":"Paper/NLP/PEFT/Composition/2024-06-RoseLoRA","title":"RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Composition/2024-06-SinkLoRA":{"id":"Paper/NLP/PEFT/Composition/2024-06-SinkLoRA","title":"SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Generalization/2024-09-Flat-LoRA":{"id":"Paper/NLP/PEFT/Generalization/2024-09-Flat-LoRA","title":"Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Mixture/2022-05-UniPELT":{"id":"Paper/NLP/PEFT/Mixture/2022-05-UniPELT","title":"UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Mixture/2023-12-No-Gradients":{"id":"Paper/NLP/PEFT/Mixture/2023-12-No-Gradients","title":"Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Module/2019-02-Adapter":{"id":"Paper/NLP/PEFT/Module/2019-02-Adapter","title":"Parameter-Efficient Transfer Learning for NLP","description":"\ub17c\ubb38 \uc774\ubbf8\uc9c0 \ubc0f \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Module/2020-05.AdapterFusion":{"id":"Paper/NLP/PEFT/Module/2020-05.AdapterFusion","title":"AdapterFusion: Non-Destructive Task Composition for Transfer Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Pruning/2023-07-SMP":{"id":"Paper/NLP/PEFT/Pruning/2023-07-SMP","title":"Pruning Pre-trained Language Models Without Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-10-AlphaTuning":{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-10-AlphaTuning","title":"AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-11-SmoothQuant":{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-11-SmoothQuant","title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models","description":"Large language models (LLMs) \uc740 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uc9c0\ub9cc compute \uc640 memory \uac00 \ub9ce\uc774 \ud544\uc694\ud558\ub2e4. Quantization \uc740 memory \ub97c \uc904\uc774\uace0 inference \ub97c \ube60\ub974\uac8c \ud560 \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uae30\uc874 \ubc29\ubc95\ub4e4\uc740 accuracy \ubc0f hardware efficiency \ub97c \ub3d9\uc2dc\uc5d0 \uc720\uc9c0\ud558\uc9c0 \ubabb\ud55c\ub2e4.","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/Fine-Tuning/2023-07-QuIP":{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2023-07-QuIP","title":"QuIP: 2-Bit Quantization of Large Language Models With Guarantees","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/Fine-Tuning/2023-10-BitNet":{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2023-10-BitNet","title":"BitNet: Scaling 1-bit Transformers for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-02-BitNet-1.58b":{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-02-BitNet-1.58b","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-08-GIFT-SW":{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-08-GIFT-SW","title":"GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-10-LLM-FP4":{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-10-LLM-FP4","title":"LLM-FP4: 4-Bit Floating-Point Quantized Transformers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-11-BitNet-a4.8":{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-11-BitNet-a4.8","title":"BitNet a4.8: 4-bit Activations for 1-bit LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/Fine-Tuning/2025-01-fp4":{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2025-01-fp4","title":"Optimizing Large Language Model Training Using FP4 Quantization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/LoRA/2023-05-QLoRA":{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-05-QLoRA","title":"QLORA: Efficient Finetuning of Quantized LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/LoRA/2023-09-QA-LoRA":{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-09-QA-LoRA","title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/LoRA/2023-10-LoftQ":{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-10-LoftQ","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/LoRA/2023-11-LQ-LoRA":{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-11-LQ-LoRA","title":"LQ-LoRA: Low-Rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/LoRA/2024-02-ApiQ":{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-02-ApiQ","title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/LoRA/2024-02-IR-QLoRA":{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-02-IR-QLoRA","title":"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/LoRA/2024-06-LR-QAT":{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-06-LR-QAT","title":"Low-Rank Quantization-Aware Training for LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/LoRA/2024-07-Q-BaRA":{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-07-Q-BaRA","title":"Accurate and Efficient Fine-Tuning Quantized Large Language Models Through Optimal Balance","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/LoRA/2024-07-RoLoRA":{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-07-RoLoRA","title":"RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Quantization/LoRA/2024-10-QEFT":{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-10-QEFT","title":"QEFT: Quantization for Efficient Fine-Tuning of LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2021-01-Prefix-Tuning":{"id":"Paper/NLP/PEFT/Soft Prompt/2021-01-Prefix-Tuning","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2021-03-P-tuning":{"id":"Paper/NLP/PEFT/Soft Prompt/2021-03-P-tuning","title":"GPT Understands, Too","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2021-04-Prompt-Tuning":{"id":"Paper/NLP/PEFT/Soft Prompt/2021-04-Prompt-Tuning","title":"The Power of Scale for Parameter-Efficient Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2021-10-P-tuning v2":{"id":"Paper/NLP/PEFT/Soft Prompt/2021-10-P-tuning v2","title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2022-05-ATTEMPT":{"id":"Paper/NLP/PEFT/Soft Prompt/2022-05-ATTEMPT","title":"ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2022-05-SPoT":{"id":"Paper/NLP/PEFT/Soft Prompt/2022-05-SPoT","title":"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2022-12-XPrompt":{"id":"Paper/NLP/PEFT/Soft Prompt/2022-12-XPrompt","title":"XPrompt: Exploring the Extreme of Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-03-LLaMA-Adapter":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-03-LLaMA-Adapter","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-03-MPT":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-03-MPT","title":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-04-LLaMA-Adapter V2":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-04-LLaMA-Adapter V2","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-05-Residual-Prompt-Tuning":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-05-Residual-Prompt-Tuning","title":"Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-09-DEPT":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-09-DEPT","title":"DEPT: Decomposed Prompt Tuning For Parameter-Efficient Fine Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-12-APrompt":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-12-APrompt","title":"APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/PEFT/Soft Prompt/2023-12-SMoP":{"id":"Paper/NLP/PEFT/Soft Prompt/2023-12-SMoP","title":"SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Prompt Engineering/CoT/2022-01-CoT":{"id":"Paper/NLP/Prompt Engineering/CoT/2022-01-CoT","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Prompt Tuning/2022-11-PTR":{"id":"Paper/NLP/Prompt Tuning/2022-11-PTR","title":"PTR: Prompt Tuning with Rules for Text Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","sidebar":"tutorialSidebar"},"Paper/NLP/Reinforcement Learning/2023-03-Reflexion":{"id":"Paper/NLP/Reinforcement Learning/2023-03-Reflexion","title":"Reflexion: Language Agents with Verbal Reinforcement Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/NLP/Text Generation/2022-03-InstructGPT":{"id":"Paper/NLP/Text Generation/2022-03-InstructGPT","title":"Training language models to follow instructions with human feedback (+ ChatGPT)","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Survey/2021-07-Prompting":{"id":"Paper/Survey/2021-07-Prompting","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Survey/2024-02-PEFT Vision":{"id":"Paper/Survey/2024-02-PEFT Vision","title":"Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Survey/2024-03-Attacking":{"id":"Paper/Survey/2024-03-Attacking","title":"Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Composition/2024-05-CLIP-LoRA":{"id":"Paper/Vision-Language/CLIP/Few-shot/Composition/2024-05-CLIP-LoRA","title":"Low-Rank Few-Shot Adaptation of Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Module/2021-10-CLIP-Adapter":{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2021-10-CLIP-Adapter","title":"CLIP-Adapter: Better Vision-Language Models with Feature Adapters","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Module/2021-11-Tip-Adapter":{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2021-11-Tip-Adapter","title":"Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Module/2022-11-TaskRes":{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2022-11-TaskRes","title":"Task Residual for Tuning Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Module/2023-11-Meta-Adapter":{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2023-11-Meta-Adapter","title":"Meta-Adapter: An Online Few-shot Learner for Vision-Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Module/2023-12-CLAP":{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2023-12-CLAP","title":"A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2022-10-MaPLe":{"id":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2022-10-MaPLe","title":"MaPLe: Multi-modal Prompt Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2023-12-AMMPL":{"id":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2023-12-AMMPL","title":"Adaptive Multi-Modality Prompt Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2023-12-APoLLo":{"id":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2023-12-APoLLo","title":"APoLLo: Unified Adapter and Prompt Learning for Vision Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2024-01-APLe":{"id":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2024-01-APLe","title":"APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2024-01-MMA":{"id":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2024-01-MMA","title":"MMA: Multi-Modal Adapter for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2021-02-CMAR":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2021-02-CMAR","title":"Cross-modal Adversarial Reprogramming","description":"\uc774\ubbf8\uc9c0 \ubc0f \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-03-VP":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-03-VP","title":"Exploring Visual Prompts for Adapting Large-Scale Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-10-Watermarking":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-10-Watermarking","title":"Watermarking for Out-of-distribution Detection","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-11-ILM-VP":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-11-ILM-VP","title":"Understanding and Improving Visual Prompting: A Label-Mapping Perspective","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-12-EVP":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2022-12-EVP","title":"Unleashing the Power of Visual Prompting At the Pixel Level","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-03-DAM-VP":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-03-DAM-VP","title":"Diversity-Aware Meta Visual Prompting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-03-EVP-L":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-03-EVP-L","title":"Explicit Visual Prompting for Low-Level Structure Segmentations","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-04-HintAug":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-04-HintAug","title":"Hint-Aug: Drawing Hints from Foundation Vision Transformers towards Boosted Few-shot Parameter-Efficient Tuning","description":"\ub17c\ubb38 \ubc0f image \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-10-AutoVP":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-10-AutoVP","title":"AutoVP: An Automated Visual Prompting Framework and Benchmark","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-11-A2XP":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-11-A2XP","title":"A2XP: Towards Private Domain Generalization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-11-InMeMo":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-11-InMeMo","title":"Instruct Me More! Random Prompting for Visual In-Context Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-12-LaViP":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-12-LaViP","title":"LaViP: Language-Grounded Visual Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2024-03-ADAVIPRO":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2024-03-ADAVIPRO","title":"AdaViPro: Region-Based Adaptive Visual Prompt For Large-Scale Models Adapting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2024-04-TVP":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2024-04-TVP","title":"Exploring the Transferability of Visual Prompting for Multimodal Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2024-06-SMM":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2024-06-SMM","title":"Sample-specific Masks for Visual Reprogramming-based Prompting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2021-09-CoOp":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2021-09-CoOp","title":"Learning to Prompt for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2022-03-CoCoOp":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2022-03-CoCoOp","title":"Conditional Prompt Learning for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2022-05-ProGrad":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2022-05-ProGrad","title":"Prompt-aligned Gradient for Prompt Tuning","description":"\ub17c\ubb38 \ubc0f image \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2022-10-PLOT":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2022-10-PLOT","title":"PLOT: Prompt Learning with Optimal Transport for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2023-03-KgCoOp":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Textual-Token/2023-03-KgCoOp","title":"Visual-Language Prompt Tuning with Knowledge-guided Context Optimization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2022-03-VPT":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2022-03-VPT","title":"Visual Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2022-10-DePT":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2022-10-DePT","title":"Visual Prompt Tuning For Test-time Domain Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2022-10-LPT":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2022-10-LPT","title":"LPT: Long-Tailed Prompt Tuning For Image Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2023-03-CVP":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2023-03-CVP","title":"Convolutional Visual Prompt for Robust Visual Perception","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2023-07-E2VPT":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2023-07-E2VPT","title":"E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2023-12-SA2VP":{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Visual-Token/2023-12-SA2VP","title":"SA^2VPT: Spatially Aligned-and-Adapted Visual Prompt","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Foundation/Contrastive Learning/2021-02-ALIGN":{"id":"Paper/Vision-Language/Foundation/Contrastive Learning/2021-02-ALIGN","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision","description":"\ub17c\ubb38 \ubc0f image  \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Foundation/Contrastive Learning/2021-03-CLIP":{"id":"Paper/Vision-Language/Foundation/Contrastive Learning/2021-03-CLIP","title":"Learning Transferable Visual Models From Natural Language Supervision","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Foundation/Single-Stream/2019-08-VisualBERT":{"id":"Paper/Vision-Language/Foundation/Single-Stream/2019-08-VisualBERT","title":"VisualBERT: A Simple And Performance Baseline For Visual And Language","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Foundation/Single-Stream/2019-08-VLBERT":{"id":"Paper/Vision-Language/Foundation/Single-Stream/2019-08-VLBERT","title":"VL-BERT: Pre-Training Of Generic Visual-Linguistic Representations","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Foundation/Single-Stream/2019-09-UNITER":{"id":"Paper/Vision-Language/Foundation/Single-Stream/2019-09-UNITER","title":"UNITER: UNiversal Image-TExt Representation Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Foundation/Single-Stream/2020-01-ImageBERT":{"id":"Paper/Vision-Language/Foundation/Single-Stream/2020-01-ImageBERT","title":"ImageBERT: Cross-Modal Pre-Training with Large-Scale Weak-Supervised Image-Text Data","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Foundation/Single-Stream/2020-04-Pixel-BERT":{"id":"Paper/Vision-Language/Foundation/Single-Stream/2020-04-Pixel-BERT","title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers","description":"\uc800\uc790\ub294 unified end-to-end framework \ub85c visual \ubc0f language embedding \uc744 jointly learning \ud558\ub294 deep multi-modal transformer \ub97c \ud1b5\ud574 image pixels \ub97c text \uc640 align \ud558\ub294 Pixel-BERT \uc81c\uc548","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Foundation/Single-Stream/2020-04-VD-BERT":{"id":"Paper/Vision-Language/Foundation/Single-Stream/2020-04-VD-BERT","title":"VD-BERT: A Unified Vision and Dialog Transformer with BERT","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Foundation/Two-Stream/2019-08-LXMERT":{"id":"Paper/Vision-Language/Foundation/Two-Stream/2019-08-LXMERT","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/Foundation/Two-Stream/2019-08-ViLBERT":{"id":"Paper/Vision-Language/Foundation/Two-Stream/2019-08-ViLBERT","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Paper/Vision-Language/VQA-IC/Few-shot/2021-06-Few-shot":{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-06-Few-shot","title":"Multimodal Few-Shot Learning with Frozen Language Models","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","sidebar":"tutorialSidebar"},"Programming/Algorithm/Algorithm":{"id":"Programming/Algorithm/Algorithm","title":"What is the Algorithm?","description":"\ubb38\uc81c \ud574\uacb0\uc744 \uc704\ud55c \uc808\ucc28\ub098 \ubc29\ubc95","sidebar":"tutorialSidebar"},"Programming/Algorithm/Problems":{"id":"Programming/Algorithm/Problems","title":"BackJoon","description":"","sidebar":"tutorialSidebar"},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template.","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:","sidebar":"tutorialSidebar"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:","sidebar":"tutorialSidebar"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack).","sidebar":"tutorialSidebar"},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features.","sidebar":"tutorialSidebar"},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs.","sidebar":"tutorialSidebar"},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let\'s translate docs/intro.md to French.","sidebar":"tutorialSidebar"}}}')}}]);