"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[90554],{18478:i=>{i.exports=JSON.parse('{"label":"LLM","permalink":"/docs/tags/llm","allTagsPath":"/docs/tags","count":8,"items":[{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-11-BitNet-a4.8","title":"BitNet a4.8: 4-bit Activations for 1-bit LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2023-10-BitNet","title":"BitNet: Scaling 1-bit Transformers for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet"},{"id":"Paper/NLP/Multi-Task/2022-01-CoT","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Multi-Task/Chain-of-Thought"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-10-LLM-FP4","title":"LLM-FP4: 4-Bit Floating-Point Quantized Transformers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/LLM-FP4"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-03-MPT","title":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/Multitask Prompt Tuning"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2025-01-fp4","title":"Optimizing Large Language Model Training Using FP4 Quantization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/fp4"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-11-SmoothQuant","title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models","description":"Large language models (LLMs) \uc740 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uc9c0\ub9cc compute \uc640 memory \uac00 \ub9ce\uc774 \ud544\uc694\ud558\ub2e4. Quantization \uc740 memory \ub97c \uc904\uc774\uace0 inference \ub97c \ube60\ub974\uac8c \ud560 \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uae30\uc874 \ubc29\ubc95\ub4e4\uc740 accuracy \ubc0f hardware efficiency \ub97c \ub3d9\uc2dc\uc5d0 \uc720\uc9c0\ud558\uc9c0 \ubabb\ud55c\ub2e4.","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-02-BitNet-1.58b","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B"}]}')}}]);