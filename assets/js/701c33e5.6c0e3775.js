"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[25211],{74160:e=>{e.exports=JSON.parse('{"label":"meta-tuning","permalink":"/docs/tags/meta-tuning","allTagsPath":"/docs/tags","count":3,"items":[{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2025-02-CoTMT","title":"A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/CoTMT"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2023-02-Meta-Mapper","title":"Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper"},{"id":"Paper/Computer Vision/Few-shot/2024-01-AMT","title":"Mixture of Adversarial LoRAs: Boosting Robust Generalization in Meta-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Computer Vision/Few-shot/AMT"}]}')}}]);