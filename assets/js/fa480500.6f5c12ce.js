"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[50974],{7991:i=>{i.exports=JSON.parse('{"label":"BitNet","permalink":"/docs/tags/bit-net","allTagsPath":"/docs/tags","count":2,"items":[{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-11-BitNet-a4.8","title":"BitNet a4.8: 4-bit Activations for 1-bit LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-02-BitNet-1.58b","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B"}]}')}}]);