"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[64214],{63361:e=>{e.exports=JSON.parse('{"label":"RL","permalink":"/docs/tags/rl","allTagsPath":"/docs/tags","count":6,"items":[{"id":"Paper/Reinforce Learning/PPO/RLAIF/2023-05-ALMoST","title":"Aligning Large Language Models through Synthetic Feedback","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Reinforce Learning/PPO/RLAIF/ALMoST"},{"id":"Paper/Reinforce Learning/PPO/RLAIF/2022-12-ConstitutionalAI","title":"Constitutional AI: Harmlessness from AI Feedback","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Reinforce Learning/PPO/RLAIF/Constitutional AI"},{"id":"Paper/Reinforce Learning/PPO/RLAIF/2023-09-d-RLAIF","title":"RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Reinforce Learning/PPO/RLAIF/d-RLAIF"},{"id":"Paper/Reinforce Learning/PPO/RLAIF/2024-11-SER","title":"Self-Evolved Reward Learning for LLMs","description":"Reinforcement Learning from Human Feedback (RLHF) \ub294 language model \uc744 \uc778\uac04\uc758 \uc120\ud638\ub3c4\uc5d0 \ub9de\uac8c \uc815\ub82c\uc2dc\ud0a4\ub294 \ub370 \uc911\uc694\ud55c \uae30\uc220\ub85c, GPT-4, ChatGPT, Llama 2 \uc640 \uac19\uc740 conversational model \uc758 \uc131\uacf5\uc5d0 \ud575\uc2ec\uc801\uc778 \uc5ed\ud560\uc744 \ud55c\ub2e4. RLHF \ub97c \uc801\uc6a9\ud558\ub294 \ub370 \uc788\uc5b4 \ud575\uc2ec\uc801\uc778 \uc5b4\ub824\uc6c0\uc740 \uc2e0\ub8b0\ud560 \uc218 \uc788\ub294 reward model (RM) \uc744 \ud559\uc2b5\ud558\ub294 \uac83\uc774\ub2e4. RM \uc740 \uc77c\ubc18\uc801\uc73c\ub85c \uc778\uac04 \uc804\ubb38\uac00\ub098 \uace0\uae09 AI system \uc774 \uc81c\uacf5\ud558\ub294 \uace0\ud488\uc9c8 label \uc5d0 \uc758\uc874\ud55c\ub2e4. \uc774\ub7ec\ud55c \ubc29\ubc95\uc740 \ube44\uc6a9\uc774 \ub9ce\uc774 \ub4e4\uba70, language model \uc758 \uc751\ub2f5\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce60 \uc218 \uc788\ub294 bias \ub97c \ub3c4\uc785\ud560 \uac00\ub2a5\uc131\uc774 \uc788\ub2e4. language model \uc774 \ubc1c\uc804\ud568\uc5d0 \ub530\ub77c, \uc778\uac04\uc758 \uc785\ub825\uc740 \uadf8 \uc131\ub2a5\uc744 \ub354\uc6b1 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub370 \ub35c \ud6a8\uacfc\uc801\uc77c \uc218 \uc788\ub2e4.","permalink":"/docs/Paper/Reinforce Learning/PPO/RLAIF/SER"},{"id":"Paper/Reinforce Learning/DPO/RLHF/2024-01-Self-RLM","title":"Self-Rewarding Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Reinforce Learning/DPO/RLHF/Self-RLM"},{"id":"Paper/Reinforce Learning/PPO/RLAIF/2023-10-UltraFeedback","title":"UltraFeedback: Boosting Language Models with Scaled AI Feedback","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Reinforce Learning/PPO/RLAIF/UltraFeedback"}]}')}}]);