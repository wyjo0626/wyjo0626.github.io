"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[43198],{1492:e=>{e.exports=JSON.parse('{"label":"GPT-2","permalink":"/docs/tags/gpt-2","allTagsPath":"/docs/tags","count":4,"items":[{"id":"Paper/NLP/Analysis/2019-11-Context_Representation","title":"How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Analysis/Contextualized Representation"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2022-05-MAGIC","title":"Language Models Can See: Plugging Visual Controls in Text Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/MAGIC"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2025-04-11-MERCap","title":"Zero-Shot Image Captioning with Multi-type Entity Representations","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/MERCap"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2021-11-ZeroCap","title":"ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/ZeroCap"}]}')}}]);