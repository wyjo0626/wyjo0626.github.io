"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[34865],{67821:e=>{e.exports=JSON.parse('{"label":"Modality Gap Mitigation","permalink":"/docs/tags/modality-gap-mitigation","allTagsPath":"/docs/tags","count":5,"items":[{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2023-03-DeCap","title":"DeCap: Decoding CLIP Latents for Zer-Shot Captioning via Text-Only Training","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/DeCap"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2023-04-Knight","title":"From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/Knight"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2022-11-CLOSE","title":"I Can\'t Believe There\'s No Images! Learning Visual Tasks Using only Language Supervision","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/CLOSE"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2025-10-Patch-ioner","title":"One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/Patch-ioner"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2022-11-CapDec","title":"Text-Only Training for Image Captioning using Noise-Injected CLIP","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/CapDec"}]}')}}]);