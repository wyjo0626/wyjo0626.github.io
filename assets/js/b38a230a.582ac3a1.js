"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[85938],{94099:e=>{e.exports=JSON.parse('{"label":"vision-language","permalink":"/docs/tags/vision-language","allTagsPath":"/docs/tags","count":18,"items":[{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-10-FEWVLM","title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-09-PICa","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc8fc \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2022-03-TAP","title":"CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2022-12-Img2LLM","title":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM"},{"id":"Paper/Vision-Language/Foundation/Single-Stream/2020-01-ImageBERT","title":"ImageBERT: Cross-Modal Pre-Training with Large-Scale Weak-Supervised Image-Text Data","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Foundation/Single-Stream/ImageBERT"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/2023-12-LaViP","title":"LaViP: Language-Grounded Visual Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Prompting/Pixel-Level/LaViP"},{"id":"Paper/Vision-Language/Foundation/Contrastive Learning/2021-03-CLIP","title":"Learning Transferable Visual Models From Natural Language Supervision","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Foundation/Contrastive Learning/CLIP"},{"id":"Paper/Vision-Language/Foundation/Two-Stream/2019-08-LXMERT","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Foundation/Two-Stream/LXMERT"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2023-02-Meta-Mapper","title":"Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-06-Frozen","title":"Multimodal Few-Shot Learning with Frozen Language Models","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot"},{"id":"Paper/Vision-Language/Foundation/Single-Stream/2020-04-Pixel-BERT","title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers","description":"\uc800\uc790\ub294 unified end-to-end framework \ub85c visual \ubc0f language embedding \uc744 jointly learning \ud558\ub294 deep multi-modal transformer \ub97c \ud1b5\ud574 image pixels \ub97c text \uc640 align \ud558\ub294 Pixel-BERT \uc81c\uc548","permalink":"/docs/Paper/Vision-Language/Foundation/Single-Stream/Pixel-BERT"},{"id":"Paper/Vision-Language/Foundation/Contrastive Learning/2021-02-ALIGN","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision","description":"\ub17c\ubb38 \ubc0f image  \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Foundation/Contrastive Learning/ALIGN"},{"id":"Paper/Computer Vision/Multi-task/2023-03-UNINEXT","title":"UNINEXT: Universal Instance Perception as Object Discovery and Retrieval","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Computer Vision/Multi-task/UNINEXT"},{"id":"Paper/Vision-Language/Foundation/Single-Stream/2019-09-UNITER","title":"UNITER: UNiversal Image-TExt Representation Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Foundation/Single-Stream/UNITER"},{"id":"Paper/Vision-Language/Foundation/Single-Stream/2020-04-VD-BERT","title":"VD-BERT: A Unified Vision and Dialog Transformer with BERT","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Foundation/Single-Stream/VD-BERT"},{"id":"Paper/Vision-Language/Foundation/Two-Stream/2019-08-ViLBERT","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Foundation/Two-Stream/ViLBERT"},{"id":"Paper/Vision-Language/Foundation/Single-Stream/2019-08-VisualBERT","title":"VisualBERT: A Simple And Performance Baseline For Visual And Language","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","permalink":"/docs/Paper/Vision-Language/Foundation/Single-Stream/VisualBERT"},{"id":"Paper/Vision-Language/Foundation/Single-Stream/2019-08-VLBERT","title":"VL-BERT: Pre-Training Of Generic Visual-Linguistic Representations","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Foundation/Single-Stream/VLBERT"}]}')}}]);