"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[97704],{64278:e=>{e.exports=JSON.parse('{"label":"training-free","permalink":"/docs/tags/training-free","allTagsPath":"/docs/tags","count":3,"items":[{"id":"Paper/Computer Vision/Generation/ Concept Editing/2022-11-SLD","title":"Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Computer Vision/Generation/ Concept Editing/SLD"},{"id":"Paper/Computer Vision/Generation/ Concept Editing/2024-10-SAFREE","title":"Safree: Training-Free and Adaptive Guard for Safe Text-to-Image and Video Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Computer Vision/Generation/ Concept Editing/SAFREE"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2021-11-ZeroCap","title":"ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/ZeroCap"}]}')}}]);