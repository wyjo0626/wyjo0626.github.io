"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[61992],{65325:i=>{i.exports=JSON.parse('{"label":"Low-Rank","permalink":"/docs/tags/low-rank","allTagsPath":"/docs/tags","count":47,"items":[{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-07-Q-BaRA","title":"Accurate and Efficient Fine-Tuning Quantized Large Language Models Through Optimal Balance","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/Q-BaRA"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-02-IR-QLoRA","title":"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/IR-QLoRA"},{"id":"Paper/NLP/PEFT/Composition/2023-03-AdaLoRA","title":"Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/AdaLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-03-ALoRA","title":"ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models","description":"\ub17c\ubb38 \ubc0f \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ALoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-02-ApiQ","title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ApiQ"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-02-ApiQ","title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/ApiQ"},{"id":"Paper/NLP/PEFT/Composition/2024-03-AutoLoRA","title":"AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/AutoLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-03-BiLoRA","title":"BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/BiLoRA"},{"id":"Paper/NLP/PEFT/Composition/2023-10-SaLoRA","title":"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/SaLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-01-COLA","title":"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/COLA"},{"id":"Paper/NLP/PEFT/Composition/2023-09-Delta-LoRA","title":"Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/Delta-LoRA"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-09-DEPT","title":"DEPT: Decomposed Prompt Tuning For Parameter-Efficient Fine Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/DEPT"},{"id":"Paper/NLP/PEFT/Composition/2024-05-DoRA","title":"DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution","description":"\ub17c\ubb38 \ubc0f \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/DoRA2"},{"id":"Paper/NLP/PEFT/Composition/2024-02-DoRA","title":"DoRA: Weight-Decomposed Low-Rank Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/DoRA"},{"id":"Paper/NLP/PEFT/Composition/2022-10-DyLoRA","title":"DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/DyLoRA"},{"id":"Paper/NLP/PEFT/Composition/2021-08-LoHa","title":"FedPara: Low-Rank Hadamard Product For Communication-Efficient Federated Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoHa"},{"id":"Paper/NLP/PEFT/Generalization/2024-09-Flat-LoRA","title":"Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Generalization/Flat-LoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-02-FLoRA","title":"FLoRA: Low-Rank Adapters Are Secretly Gradient Compressors","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/FLoRA"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-08-GIFT-SW","title":"GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/GIFT-SW"},{"id":"Paper/NLP/PEFT/Composition/2023-08-IncreLoRA","title":"IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/IncreLoRA"},{"id":"Paper/Computer Vision/PEFT/Composition/2024-04-InfLoRA","title":"InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Computer Vision/PEFT/Composition/InfLoRA"},{"id":"Paper/NLP/PEFT/Composition/2022-12-LoKr","title":"KronA: Parameter Efficient Tuning with Kronecker Adapter","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoKr"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-10-LoftQ","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LoftQ"},{"id":"Paper/NLP/PEFT/Composition/2023-09-LongLoRA","title":"LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LongLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-04-LoRA-Drop","title":"LoRA Dropout as a Sparsity Regularizer for Overfitting Control","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoRA-Dropout"},{"id":"Paper/NLP/PEFT/Composition/2024-03-HiddenKey","title":"LoRA Meets Dropout under a Unified Framework","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/HiddenKey"},{"id":"Paper/NLP/PEFT/Composition/2021-06-LoRA","title":"LoRA: Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-02-LoRA+","title":"LoRA+: Efficient Low Rank Adaptation of Large Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoRA+"},{"id":"Paper/Vision-Language/PEFT/Composition/2024-05-CLIP-LoRA","title":"Low-Rank Few-Shot Adaptation of Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Composition/CLIP-LoRA"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-06-LR-QAT","title":"Low-Rank Quantization-Aware Training for LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LR-QAT"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-11-LQ-LoRA","title":"LQ-LoRA: Low-Rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LQ-LoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-02-MeLoRA","title":"MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/MeLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-06-MiLoRA","title":"MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/MiLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-06-MoSLoRA","title":"Mixture-of-Subspaces in Low-Rank Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/MoSLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-05-MoRA","title":"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/MoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-06-OLoRA","title":"OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/OLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-04-PISSA","title":"PISSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/PISSA"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-09-QA-LoRA","title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/QA-LoRA"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-10-QEFT","title":"QEFT: Quantization for Efficient Fine-Tuning of LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/QEFT"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-05-QLoRA","title":"QLORA: Efficient Finetuning of Quantized LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/QLoRA"},{"id":"Paper/NLP/PEFT/Composition/2023-07-ReLoRA","title":"ReLoRA: High-Rank Training Through Low-Rank Updates","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ReLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-02-ResLoRA","title":"ResLoRA: Identity Residual Mapping in Low-Rank Adaption","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ResLoRA"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-07-RoLoRA","title":"RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/RoLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-06-RoseLoRA","title":"RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/RoseLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-02-SIBO","title":"SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/SIBO"},{"id":"Paper/NLP/PEFT/Composition/2024-06-SinkLoRA","title":"SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/SinkLoRA"},{"id":"Paper/NLP/PEFT/Composition/2023-11-SoRA","title":"Sparse Low-rank Adaptation of Pre-trained Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/SoRA"}]}')}}]);