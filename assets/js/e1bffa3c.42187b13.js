"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[82498],{41919:e=>{e.exports=JSON.parse('{"label":"few-shot learning","permalink":"/docs/tags/few-shot-learning","allTagsPath":"/docs/tags","count":11,"items":[{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2025-02-CoTMT","title":"A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/CoTMT"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-10-FEWVLM","title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-09-PICa","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc8fc \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2022-03-TAP","title":"CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2021-10-CLIP-Adapter","title":"CLIP-Adapter: Better Vision-Language Models with Feature Adapters","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/CLIP-Adapter"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2024-03-LLMVQA","title":"Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA"},{"id":"Paper/Computer Vision/Few-shot/2024-01-AMT","title":"Mixture of Adversarial LoRAs: Boosting Robust Generalization in Meta-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Computer Vision/Few-shot/AMT"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-06-Frozen","title":"Multimodal Few-Shot Learning with Frozen Language Models","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot"},{"id":"Paper/Machine Learning/Regularization/GNI/2023-10-PAC-tuning","title":"PAC-tuning: Fine-tuning Pretrained Language Models with PAC-driven Perturbed Gradient Descent","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Machine Learning/Regularization/GNI/PAC-tuning"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2021-11-Tip-Adapter","title":"Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/Tip-Adapter"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-09-xGQA","title":"xGQA: Cross-Lingual Visual Question Answering","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA"}]}')}}]);