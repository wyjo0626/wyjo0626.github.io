"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[73316],{56001:a=>{a.exports=JSON.parse('{"label":"Multi-modal","permalink":"/docs/tags/multi-modal","allTagsPath":"/docs/tags","count":3,"items":[{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2021-10-CLIP-Adapter","title":"CLIP-Adapter: Better Vision-Language Models with Feature Adapters","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/CLIP-Adapter"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2022-10-MaPLe","title":"MaPLe: Multi-modal Prompt Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/MaPLe"},{"id":"Paper/Multi-Modal/PEFT/Composition/2025-06-Moka","title":"MokA: Multimodal Low-Rank Adaptation for MLLMs","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","permalink":"/docs/Paper/Multi-Modal/PEFT/Composition/Moka"}]}')}}]);