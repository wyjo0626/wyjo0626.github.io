"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[4250],{83779:i=>{i.exports=JSON.parse('{"label":"Quantization","permalink":"/docs/tags/quantization","allTagsPath":"/docs/tags","count":11,"items":[{"id":"Paper/NLP/PEFT/Composition/2024-02-IR-QLoRA","title":"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/IR-QLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-02-ApiQ","title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ApiQ"},{"id":"Paper/NLP/PEFT/Quantization/2024-11-BitNet-a4.8","title":"BitNet a4.8: 4-bit Activations for 1-bit LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/BitNet-a4.8"},{"id":"Paper/NLP/PEFT/Quantization/2023-10-BitNet","title":"BitNet: Scaling 1-bit Transformers for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/BitNet"},{"id":"Paper/NLP/PEFT/Quantization/2024-10-LLM-FP4","title":"LLM-FP4: 4-Bit Floating-Point Quantized Transformers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LLM-FP4"},{"id":"Paper/NLP/PEFT/Composition/2023-10-LoftQ","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoftQ"},{"id":"Paper/NLP/PEFT/Composition/2024-06-LR-QAT","title":"Low-Rank Quantization-Aware Training for LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LR-QAT"},{"id":"Paper/NLP/PEFT/Quantization/2025-01-fp4","title":"Optimizing Large Language Model Training Using FP4 Quantization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/fp4"},{"id":"Paper/NLP/PEFT/Composition/2023-09-QA-LoRA","title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/QA-LoRA"},{"id":"Paper/NLP/PEFT/Composition/2023-05-QLoRA","title":"QLORA: Efficient Finetuning of Quantized LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/QLoRA"},{"id":"Paper/NLP/PEFT/Quantization/2024-02-BitNet-1.58b","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/BitNet-1.58B"}]}')}}]);