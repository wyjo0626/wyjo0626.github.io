"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[22591],{27869:e=>{e.exports=JSON.parse('{"label":"CLIP","permalink":"/docs/tags/clip","allTagsPath":"/docs/tags","count":9,"items":[{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2022-03-TAP","title":"CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2021-10-CLIP-Adapter","title":"CLIP-Adapter: Better Vision-Language Models with Feature Adapters","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/CLIP-Adapter"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-11-ClipCap","title":"ClipCap: CLIP Prefix for Image Captioning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/ClipCap"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2023-03-DeCap","title":"DeCap: Decoding CLIP Latents for Zer-Shot Captioning via Text-Only Training","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/DeCap"},{"id":"Paper/Vision-Language/Foundation/Contrastive Learning/2021-03-CLIP","title":"Learning Transferable Visual Models From Natural Language Supervision","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Foundation/Contrastive Learning/CLIP"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Composition/2024-05-CLIP-LoRA","title":"Low-Rank Few-Shot Adaptation of Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Composition/CLIP-LoRA"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2022-11-CapDec","title":"Text-Only Training for Image Captioning using Noise-Injected CLIP","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/CapDec"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2021-11-Tip-Adapter","title":"Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/Tip-Adapter"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2023-06-ZeroGen","title":"ZeroGen: Zero-shot Multimodal Controllable Text Generation with Multiple Oracles","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/ZeroGen"}]}')}}]);