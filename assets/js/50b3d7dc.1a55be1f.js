"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[10389],{85005:e=>{e.exports=JSON.parse('{"label":"Frozen","permalink":"/docs/tags/frozen","allTagsPath":"/docs/tags","count":2,"items":[{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2022-03-TAP","title":"CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-06-Frozen","title":"Multimodal Few-Shot Learning with Frozen Language Models","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot"}]}')}}]);