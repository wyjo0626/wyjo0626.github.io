"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[4250],{83779:i=>{i.exports=JSON.parse('{"label":"Quantization","permalink":"/docs/tags/quantization","allTagsPath":"/docs/tags","count":16,"items":[{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-02-IR-QLoRA","title":"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/IR-QLoRA"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-10-AlphaTuning","title":"AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning"},{"id":"Paper/NLP/PEFT/Composition/2024-02-ApiQ","title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ApiQ"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-02-ApiQ","title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/ApiQ"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-11-BitNet-a4.8","title":"BitNet a4.8: 4-bit Activations for 1-bit LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-a4.8"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2023-10-BitNet","title":"BitNet: Scaling 1-bit Transformers for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-10-LLM-FP4","title":"LLM-FP4: 4-Bit Floating-Point Quantized Transformers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/LLM-FP4"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-10-LoftQ","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LoftQ"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-06-LR-QAT","title":"Low-Rank Quantization-Aware Training for LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LR-QAT"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-11-LQ-LoRA","title":"LQ-LoRA: Low-Rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LQ-LoRA"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2025-01-fp4","title":"Optimizing Large Language Model Training Using FP4 Quantization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/fp4"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-09-QA-LoRA","title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/QA-LoRA"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-05-QLoRA","title":"QLORA: Efficient Finetuning of Quantized LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/QLoRA"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2023-07-QuIP","title":"QuIP: 2-Bit Quantization of Large Language Models With Guarantees","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/QuIP"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-11-SmoothQuant","title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models","description":"Large language models (LLMs) \uc740 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uc9c0\ub9cc compute \uc640 memory \uac00 \ub9ce\uc774 \ud544\uc694\ud558\ub2e4. Quantization \uc740 memory \ub97c \uc904\uc774\uace0 inference \ub97c \ube60\ub974\uac8c \ud560 \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uae30\uc874 \ubc29\ubc95\ub4e4\uc740 accuracy \ubc0f hardware efficiency \ub97c \ub3d9\uc2dc\uc5d0 \uc720\uc9c0\ud558\uc9c0 \ubabb\ud55c\ub2e4.","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2024-02-BitNet-1.58b","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/BitNet-1.58B"}]}')}}]);