"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[1407],{2074:i=>{i.exports=JSON.parse('{"label":"Quantized LLMs","permalink":"/docs/tags/quantized-ll-ms","allTagsPath":"/docs/tags","count":14,"items":[{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-07-Q-BaRA","title":"Accurate and Efficient Fine-Tuning Quantized Large Language Models Through Optimal Balance","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/Q-BaRA"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-02-IR-QLoRA","title":"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/IR-QLoRA"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-10-AlphaTuning","title":"AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/AlphaTuning"},{"id":"Paper/NLP/PEFT/Composition/2024-01-ReFT","title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ApiQ"},{"id":"Paper/NLP/PEFT/Composition/2024-02-ApiQ","title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ApiQ"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-02-ApiQ","title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/ApiQ"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-10-LoftQ","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LoftQ"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-06-LR-QAT","title":"Low-Rank Quantization-Aware Training for LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LR-QAT"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-11-LQ-LoRA","title":"LQ-LoRA: Low-Rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/LQ-LoRA"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-09-QA-LoRA","title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/QA-LoRA"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-10-QEFT","title":"QEFT: Quantization for Efficient Fine-Tuning of LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/QEFT"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2023-05-QLoRA","title":"QLORA: Efficient Finetuning of Quantized LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/QLoRA"},{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2023-07-QuIP","title":"QuIP: 2-Bit Quantization of Large Language Models With Guarantees","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/QuIP"},{"id":"Paper/NLP/PEFT/Quantization/LoRA/2024-07-RoLoRA","title":"RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LoRA/RoLoRA"}]}')}}]);