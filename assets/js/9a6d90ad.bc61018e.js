"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[37194],{2830:a=>{a.exports=JSON.parse('{"label":"8-bit","permalink":"/docs/tags/8-bit","allTagsPath":"/docs/tags","count":1,"items":[{"id":"Paper/NLP/PEFT/Quantization/Fine-Tuning/2022-11-SmoothQuant","title":"SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models","description":"Large language models (LLMs) \uc740 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uc9c0\ub9cc compute \uc640 memory \uac00 \ub9ce\uc774 \ud544\uc694\ud558\ub2e4. Quantization \uc740 memory \ub97c \uc904\uc774\uace0 inference \ub97c \ube60\ub974\uac8c \ud560 \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uae30\uc874 \ubc29\ubc95\ub4e4\uc740 accuracy \ubc0f hardware efficiency \ub97c \ub3d9\uc2dc\uc5d0 \uc720\uc9c0\ud558\uc9c0 \ubabb\ud55c\ub2e4.","permalink":"/docs/Paper/NLP/PEFT/Quantization/Fine-Tuning/SmoothQuant"}]}')}}]);