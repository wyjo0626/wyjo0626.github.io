"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[32813],{3905:(e,a,t)=>{t.d(a,{Zo:()=>g,kt:()=>c});var n=t(67294);function l(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function r(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){l(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function o(e,a){if(null==e)return{};var t,n,l=function(e,a){if(null==e)return{};var t,n,l={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(l[t]=e[t]);return l}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(l[t]=e[t])}return l}var s=n.createContext({}),u=function(e){var a=n.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):r(r({},a),e)),t},g=function(e){var a=u(e.components);return n.createElement(s.Provider,{value:a},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},p=n.forwardRef((function(e,a){var t=e.components,l=e.mdxType,i=e.originalType,s=e.parentName,g=o(e,["components","mdxType","originalType","parentName"]),m=u(t),p=l,c=m["".concat(s,".").concat(p)]||m[p]||d[p]||i;return t?n.createElement(c,r(r({ref:a},g),{},{components:t})):n.createElement(c,r({ref:a},g))}));function c(e,a){var t=arguments,l=a&&a.mdxType;if("string"==typeof e||l){var i=t.length,r=new Array(i);r[0]=p;var o={};for(var s in a)hasOwnProperty.call(a,s)&&(o[s]=a[s]);o.originalType=e,o[m]="string"==typeof e?e:l,r[1]=o;for(var u=2;u<i;u++)r[u]=t[u];return n.createElement.apply(null,r)}return n.createElement.apply(null,t)}p.displayName="MDXCreateElement"},65682:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>s,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>u});var n=t(87462),l=(t(67294),t(3905));const i={slug:"xGQA",title:"xGQA: Cross-Lingual Visual Question Answering",tags:["Vision-Language","Multimodal","few-shot learning","GPT-3","Prompting"]},r=void 0,o={unversionedId:"Paper/Vision-Language/VQA-IC/Few-shot/2021-09-xGQA",id:"Paper/Vision-Language/VQA-IC/Few-shot/2021-09-xGQA",title:"xGQA: Cross-Lingual Visual Question Answering",description:"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :",source:"@site/docs/Paper/Vision-Language/VQA-IC/Few-shot/2021-09-xGQA.md",sourceDirName:"Paper/Vision-Language/VQA-IC/Few-shot",slug:"/Paper/Vision-Language/VQA-IC/Few-shot/xGQA",permalink:"/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA",draft:!1,editUrl:"https://github.com/wyjo0626/wyjo0626.github.io/tree/master/docs/docs/Paper/Vision-Language/VQA-IC/Few-shot/2021-09-xGQA.md",tags:[{label:"Vision-Language",permalink:"/docs/tags/vision-language"},{label:"Multimodal",permalink:"/docs/tags/multimodal"},{label:"few-shot learning",permalink:"/docs/tags/few-shot-learning"},{label:"GPT-3",permalink:"/docs/tags/gpt-3"},{label:"Prompting",permalink:"/docs/tags/prompting"}],version:"current",frontMatter:{slug:"xGQA",title:"xGQA: Cross-Lingual Visual Question Answering",tags:["Vision-Language","Multimodal","few-shot learning","GPT-3","Prompting"]},sidebar:"tutorialSidebar",previous:{title:"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA",permalink:"/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa"},next:{title:"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",permalink:"/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM"}},s={},u=[{value:"4.1 Multimodal \u2192 Multilingual",id:"41-multimodal--multilingual",level:2},{value:"4.2 Multilingual \u2192 Multimodal",id:"42-multilingual--multimodal",level:2},{value:"5.1 Language-Extension Phase",id:"51-language-extension-phase",level:2},{value:"5.2 Fine-tuning on GQA",id:"52-fine-tuning-on-gqa",level:2},{value:"6.1 Zero-Shot Cross-Lingual Transfer",id:"61-zero-shot-cross-lingual-transfer",level:2}],g={toc:u},m="wrapper";function d(e){let{components:a,...t}=e;return(0,l.kt)(m,(0,n.Z)({},g,t,{components:a,mdxType:"MDXLayout"}),(0,l.kt)("p",null,"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 : ",(0,l.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2109.06082"},"https://arxiv.org/pdf/2109.06082")),(0,l.kt)("h1",{id:"abstract"},"Abstract"),(0,l.kt)("p",null,"\ucd5c\uadfc multimodal vision \uacfc language modeling \uc758 \ubc1c\uc804\uc740 \uc8fc\ub85c multilingual multimodal dataset \uc758 \ubd80\uc871 \ub54c\ubb38\uc5d0 \ub300\ubd80\ubd84 English \uc5b8\uc5b4\uc5d0 \uc9d1\uc911\ub418\uc5b4 \uc654\ub2e4. \uc774 \uc5f0\uad6c\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \uaca9\ucc28\ub97c \ud574\uc18c\ud558\uae30 \uc704\ud574 visual question answering task \ub97c \uc704\ud55c \uc0c8\ub85c\uc6b4 multilingual evaluation benchmark \uc778 xGQA \ub97c \uc81c\uc548\ud55c\ub2e4. \uc800\uc790\ub294 \uae30\uc874 English GQA dataset \uc744 7 \uac1c\uc758 \uc720\ud615\uc801\uc73c\ub85c \ub2e4\uc591\ud55c \uc5b8\uc5b4\ub85c \ud655\uc7a5\ud558\uc5ec, cross-lingual visual question answering \uc5d0\uc11c \uc911\uc694\ud55c \ub3c4\uc804 \uacfc\uc81c\ub97c \ud0d0\uc9c0\ud558\uace0 \ubd84\uc11d\ud560 \uc218 \uc788\uac8c \ud55c\ub2e4. \ub610\ud55c multimodal transformer \uae30\ubc18 model \uc744 multilingual \ub85c \ud655\uc7a5\ud558\uae30 \uc704\ud55c adapter \uae30\ubc18 \uc811\uadfc\ubc95, \uadf8\ub9ac\uace0 \uadf8 \ubc18\ub300\ub85c multilingual model \uc744 multimodal \ub85c \ud655\uc7a5\ud558\uae30 \uc704\ud55c \uc811\uadfc\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4. \uc81c\uc548\ub41c \ubc29\ubc95\uc740 zero-shot cross-lingual setting \uc5d0\uc11c \ud604\uc7ac state-of-the-art multilingual multimodal model (e.g., M3P) \ubcf4\ub2e4 \uc6b0\uc218\ud55c \uc131\ub2a5\uc744 \ubcf4\uc774\uc9c0\ub9cc, \uc804\uccb4\uc801\uc73c\ub85c accuracy \ub294 \uc5ec\uc804\ud788 \ub0ae\ub2e4. target language \uc5d0\uc11c \uc57d 38 accuracy point \uc758 \uc131\ub2a5 \uc800\ud558\ub294 \uc774 task \uc5d0\uc11c zero-shot cross-lingual transfer \uc758 \uc5b4\ub824\uc6c0\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uc774\ub7ec\ud55c \uacb0\uacfc\ub294 multimodal model \uc758 \ub2e8\uc21c\ud55c cross-lingual transfer \uac00 \uc7a0\uc7ac\uc801\uc778 multilingual multimodal misalignment \ub97c \ucd08\ub798\ud55c\ub2e4\ub294 \uc810\uc744 \uc2dc\uc0ac\ud558\uba70, vision \uacfc multilingual language modeling \uc744 \uc704\ud55c \ub354 \uc815\uad50\ud55c \ubc29\ubc95\uc774 \ud544\uc694\ud568\uc744 \ubcf4\uc5ec\uc900\ub2e4."),(0,l.kt)("h1",{id:"1-introduction"},"1 Introduction"),(0,l.kt)("p",null,"Transformer \uae30\ubc18 architecture \ub294 NLP \uc640 CV \ubd84\uc57c\uc5d0\uc11c \ub110\ub9ac \uc0ac\uc6a9\ub418\uba70 \ub6f0\uc5b4\ub09c task \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uc5ec\ub7ec modality \ub97c \uc704\ud55c \uacf5\ud1b5 architecture \ub294 \uc815\ubcf4\uc758 \ud6a8\uacfc\uc801\uc778 fusion \uac00\ub2a5\uc131\uc744 \uc5f4\uc5b4\uc8fc\uc5c8\uace0, image captioning, phrase grounding, visual question answering, referring expression comprehension, image-text retrieval \uacfc \uac19\uc740 \ub2e4\uc591\ud55c multimodal task \uc5d0\uc11c \uc778\uc0c1\uc801\uc778 \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc774\ub04c\uc5b4\ub0c8\ub2e4. \uadf8\ub7ec\ub098 \uc774\ub7ec\ud55c \ubc1c\uc804\uc740 \ub300\ubd80\ubd84 English \uc5b8\uc5b4\uc5d0 \uad6d\ud55c\ub418\uc5b4 \uc788\uc73c\uba70, \uc8fc\uc694 multimodal dataset \uc774 English text \ub85c\ub9cc \uad6c\uc131\ub418\uc5b4 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. Multilingual evaluation benchmark \uc758 \ubd80\uc871\uc73c\ub85c \uc778\ud574 \uc774 \ubb38\uc81c\ub97c \ub2e4\ub8e8\ub294 model \uac1c\ubc1c\uc740 \uc81c\ud55c\uc801\uc774\uc5c8\ub2e4."),(0,l.kt)("p",null,"\uc774 \uaca9\ucc28\ub97c \ud574\uc18c\ud558\uae30 \uc704\ud574, \uc800\uc790\ub294 visual question answering task \ub97c \uc704\ud55c multilingual evaluation benchmark \uc778 xGQA \ub97c \uc81c\uc548\ud558\uba70, monolingual English \uc804\uc6a9 GQA dataset \uc744 \ud655\uc7a5\ud55c\ub2e4. xGQA \ub97c \uc704\ud574 \uc800\uc790\ub294 balanced GQA test-dev set \uc744 7 \uac1c \uc5b8\uc5b4 \uacc4\uc5f4\uc5d0\uc11c \uc628 7 \uac1c \uc5b8\uc5b4\ub85c \uc218\uc791\uc5c5 \ubc88\uc5ed \ubc0f \uc801\uc751\ud558\uc600\uc73c\uba70, \uc774\ub294 5 \uac1c\uc758 \uc11c\ub85c \ub2e4\ub978 script \ub97c \ud3ec\ud568\ud55c\ub2e4 (Sec. 1 \uacfc Tab. 1 \ucc38\uc870). \ucd94\uac00\ub85c, target language \uc5d0\uc11c \uc18c\uc218\uc758 \uc608\uc81c\ub9cc \ud65c\uc6a9\ud558\ub294 cross-lingual few-shot learning experiment \ub97c \uc704\ud55c \uc0c8\ub85c\uc6b4 \uace0\uc815 data split \ub3c4 \uc81c\uacf5\ud55c\ub2e4."),(0,l.kt)("p",null,"Pretraining \uc740 (i) \uace0\uc790\uc6d0 \uc5b8\uc5b4\uc758 \uacbd\uc6b0 \uacc4\uc0b0 \ube44\uc6a9\uc774 \ub9e4\uc6b0 \ud06c\uace0 (ii) multilingual multimodal resource \uc758 \uc591\uc774 \uc81c\ud55c\uc801\uc774\uae30 \ub54c\ubb38\uc5d0, \uc800\uc790\ub294 multilingual multimodal model \uc744 \uad6c\ucd95\ud558\uae30 \uc704\ud55c \ucd94\uac00 baseline \uc73c\ub85c \uacc4\uc0b0 \ud6a8\uc728\uc801\uc778 adapter \uae30\ubc18 \uc811\uadfc\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4. \uc989, English text \ub85c\ub9cc pretraining \ub41c multimodal model \uc744 multilingual \ub85c \ud655\uc7a5\ud558\uace0, \ubc18\ub300\ub85c multilingual model \uc744 multimodal \ub85c \ud655\uc7a5\ud55c\ub2e4. \uc774\ub97c \uc704\ud574 \uc800\uc790\ub294 Artetxe et al. \uacfc Pfeiffer et al. \uc758 \uc811\uadfc\uc744 \ub530\ub974\uba70, monolingual \ubc0f multilingual model \uc744 \uc0c8\ub85c\uc6b4 \uc5b8\uc5b4\uc640 script \ub85c \ud655\uc7a5\ud558\uae30 \uc704\ud574 \uc0c8\ub85c\uc6b4 tokenizer \uc640 word-embedding matrix, target language \uc6a9 adapter \ub97c \ud559\uc2b5\ud55c\ub2e4. \uc774\uc5b4\uc11c \ud574\ub2f9 multilingual multimodal adapter \uae30\ubc18 model \uc744 target task \ub85c transfer \ud558\uae30 \uc704\ud574, \uc800\uc790\ub294 modality \ubcc4 adapter weight \ub97c \uc0ac\uc6a9\ud558\ub294 \uc0c8\ub85c\uc6b4 modality-specific split architecture \ub97c \uc81c\uc548\ud55c\ub2e4 (Sec. 2 \ucc38\uc870)."),(0,l.kt)("p",null,"\uacb0\uacfc\uc801\uc73c\ub85c \uc81c\uc548\ub41c adapter \uae30\ubc18 architecture \ub294 \ucd5c\uadfc state-of-the-art multilingual multimodal pretrained model \uc778 M3P \ub97c zero-shot cross-lingual setting \uc5d0\uc11c \ub2a5\uac00\ud55c\ub2e4. \uadf8\ub7ec\ub098 \uc804\ubc18\uc801\uc778 zero-shot transfer \uc131\ub2a5\uc740 \uc5ec\uc804\ud788 \ub0ae\uc73c\uba70, target language \uc804\ubc18\uc5d0\uc11c \ud3c9\uade0 \uc57d 38 accuracy point \uc758 \uc131\ub2a5 \uc800\ud558\uac00 \ubc1c\uc0dd\ud55c\ub2e4. Few-shot setup \uc5d0\uc11c target language \uc608\uc81c\ub97c \uc18c\uc218 \uc0ac\uc6a9\ud558\uba74 \ubaa8\ub4e0 \uc811\uadfc\ubc95\uc5d0\uc11c \uc131\ub2a5\uc774 \ud06c\uac8c \ud5a5\uc0c1\ub418\uc9c0\ub9cc, cross-lingual transfer \uc131\ub2a5\uc740 source language \uc131\ub2a5\uc5d0 \ube44\ud574 \uc5ec\uc804\ud788 \uc0c1\ub2f9\ud788 \ub0ae\ub2e4. \uc774\ub294 \uc9c8\ubb38\uc774 template \uae30\ubc18\uc774\uba70 \ud3c9\uade0 8.5 \uac1c \ub2e8\uc5b4\ub9cc \ud3ec\ud568\ud558\ub294 \ube44\uad50\uc801 \ub2e8\uc21c\ud55c \uc9c8\ubb38\uc784\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, \ud574\ub2f9 task \uc758 \ubcf8\uc9c8\uc801\uc778 \uc5b4\ub824\uc6c0\uc744 \ubcf4\uc5ec\uc900\ub2e4."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Contributions")),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"cross-lingual visual question answering \uc744 \uc704\ud55c \ucd5c\ucd08\uc758 evaluation benchmark \ub97c \uc81c\uc548\ud558\uba70, 7 \uac1c\uc758 \ub2e4\uc591\ud55c target language \ub97c \ud3ec\ud568\ud55c\ub2e4."),(0,l.kt)("li",{parentName:"ol"},"multilingual multimodal model \uc0dd\uc131\uc744 \uc704\ud55c \uc0c8\ub85c\uc6b4 adapter \uae30\ubc18 \uc811\uadfc\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4."),(0,l.kt)("li",{parentName:"ol"},"state-of-the-art model \uacfc \uc0c8\ub85c\uc6b4 multilingual multimodal model \uc744 zero-shot \uacfc few-shot learning setup \uc5d0\uc11c \uccb4\uacc4\uc801\uc73c\ub85c benchmark \ud558\uc5ec task \uc758 \uc5b4\ub824\uc6c0\uc744 \uc785\uc99d\ud558\uace0 \ud5a5\ud6c4 \uc5f0\uad6c\ub97c \uc704\ud55c \uac15\ub825\ud55c reference point \ub97c \uc81c\uacf5\ud55c\ub2e4."),(0,l.kt)("li",{parentName:"ol"},"\ub2e4\uc591\ud55c \uc811\uadfc\ubc95\uc744 \ucca0\uc800\ud788 \ubd84\uc11d\ud558\uc5ec model failure \ub97c \uc720\ubc1c\ud558\ub294 \uc694\uc18c\uc640 question type \uc744 \ubc1d\ud600\ub0b4\uace0, \uc774 \uc601\uc5ed\uc5d0\uc11c\uc758 \ud5a5\ud6c4 \uc5f0\uad6c \ud544\uc694\uc131\uc744 \ub2e4\uc2dc\uae08 \uac15\uc870\ud55c\ub2e4.")),(0,l.kt)("h1",{id:"2-background-and-related-work"},"2 Background and Related Work"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Multilingual Language Models.")," Pretrained multilingual transformer \uae30\ubc18 LM \uc778 mBERT \uc640 XLM-R \uc740 \uac01\uac01 monolingual counterpart \uc778 BERT \uc640 RoBERTa \uc640 \ub3d9\uc77c\ud55c pretraining regime \uc744 \ub530\ub978\ub2e4. \uc774\ub4e4\uc740 100 \uac1c \uc774\uc0c1\uc758 \uc5b8\uc5b4\uac00 \ud3ec\ud568\ub41c concatenated text corpus \uc5d0 \ub300\ud574 self-supervised masked language modelling (MLM) objective \ub85c pretraining \ub41c\ub2e4. Text \ub294 WordPiece, SentencePiece, \ud639\uc740 BytePair encoding \uc73c\ub85c tokenization \ub41c\ub2e4. \uc774\ub7ec\ud55c multilingual model \uc740 \uc9c1\uc811\uc801\uc778 cross-lingual supervision (\uc608: parallel data, translation dictionary) \uc5c6\uc774\ub3c4 cross-lingual task \uc5d0\uc11c \ub180\ub77c\uc6b4 \uc131\ub2a5\uc744 \ubcf4\uc774\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\ub2e4."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Vision and Language Models.")," \ub300\ubd80\ubd84\uc758 transformer \uae30\ubc18 multimodal model \uc740 text token \uacfc image region feature \ub97c \ud568\uaed8 \uc778\ucf54\ub529\ud55c\ub2e4. \uc774\ubbf8\uc9c0\ub294 Faster R-CNN \uacfc \uac19\uc740 object detection model \uc744 \uc774\uc6a9\ud558\uc5ec region of interest (RoI) feature \ub97c \ucd94\ucd9c\ud55c\ub2e4. \ucd94\ucd9c\ub41c image region feature \ub294 affine layer \ub97c \ud1b5\uacfc\ud558\uba70 multimodal transformer \uc758 joint embedding space \ub85c projection \ub41c\ub2e4. RoI \uc758 bounding box \uc88c\ud45c\ub294 visual feature \uc758 positional embedding \uc73c\ub85c \uc791\ub3d9\ud558\uba70, \uc774 \ub610\ud55c affine transformation \uc744 \uac70\uccd0 image region representation \uacfc \uacb0\ud569\ub41c\ub2e4. Position-aware image region embedding \uc740 transformer \ub85c \uc804\ub2ec\ub418\uace0, multi-head attention \uc740 \uac01 layer \uc5d0\uc11c \ubaa8\ub4e0 text \uc640 image \uc785\ub825\uc5d0 \ub300\ud574 attention \uc744 \uc218\ud589\ud558\uba70 joint representation \uc744 \ud559\uc2b5\ud55c\ub2e4."),(0,l.kt)("p",null,"Kamath et al. \uc740 object detector \ub97c black-box \ub85c \uc0ac\uc804 feature \ucd94\ucd9c\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ub300\uc2e0, \uc774\ub97c multimodal transformer architecture \uc758 \uc911\uc2ec \uc694\uc18c\ub85c \ud1b5\ud569\ud558\uc600\ub2e4. Object detector \ub97c multimodal transformer \uc640 \ud568\uaed8 end-to-end \ub85c \ud559\uc2b5\ud558\uba74 \ub354 \ud070 \uc720\uc5f0\uc131\uacfc \ud45c\ud604\ub825\uc744 \ud655\ubcf4\ud560 \uc218 \uc788\ub2e4. Multimodal transformer \uae30\ubc18 model \uc740 MLM \uacfc \uc720\uc0ac\ud558\uac8c self-supervised objective \ub85c \ud559\uc2b5\ub418\uba70, masked feature regression, masked object detection, masked attribute detection, cross-modality matching \uacfc \uac19\uc740 contrastive loss \uac00 \uc0ac\uc6a9\ub41c\ub2e4. \uc8fc\ub85c COCO, Flickr30k, Conceptual Captions (CC), SBU \uc640 \uac19\uc740 image captioning dataset \uc774 pretraining \uc5d0 \ud65c\uc6a9\ub41c\ub2e4. Unimodal language model \uacfc \uc720\uc0ac\ud558\uac8c ","[","CLS] token \uc774 classification task \uc5d0\uc11c contextual representation \uc73c\ub85c \uc0ac\uc6a9\ub41c\ub2e4."),(0,l.kt)("p",null,"\ucd5c\uadfc\uc5d0\ub294 multilingual multimodal model \ub3c4 \uc81c\uc548\ub418\uc5c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, M3P \ub294 50 \uac1c \uc5b8\uc5b4\uc758 Wikipedia \uc640 English multimodal CC dataset \uc73c\ub85c \ud559\uc2b5\ub41c\ub2e4. English \uc774\uc678\uc758 \uc5b8\uc5b4 token \uc744 image representation \uacfc \uc815\ub82c\ud558\uae30 \uc704\ud574, M3P \ub294 code-switching \uba54\ucee4\ub2c8\uc998\uc744 \uc0ac\uc6a9\ud558\uba70, English CC \uc608\uc81c\uc758 \ub2e8\uc5b4\ub97c \ub300\uc751\ub418\ub294 bilingual dictionary \uc758 \ub2e8\uc5b4\ub85c \ubb34\uc791\uc704 \ub300\uccb4\ud55c\ub2e4. UC2 \uc5d0\uc11c\ub294 English multimodal dataset \uc744 machine translation \uc73c\ub85c \ub2e4\ub978 \uc5b8\uc5b4\ub85c \ud655\uc7a5\ud558\uace0, masked region-to-token modeling \ubc0f visual translation language modeling \uc744 \uc81c\uc548\ud55c\ub2e4."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Adapters.")," Adapter \ub294 NLP \uc640 CV \uc5d0\uc11c transfer learning \uc744 \uc704\ud55c \ud6a8\uc728\uc801\uc778 fine-tuning \uc804\ub7b5\uc73c\ub85c \ub3c4\uc785\ub418\uc5c8\ub2e4. Pretrained model \uc758 \ubaa8\ub4e0 weight \ub97c fine-tuning \ud558\ub294 \ub300\uc2e0, \uac01 layer \uc5d0 \uc791\uc740 feed-forward layer \ub97c \uc0bd\uc785\ud55c\ub2e4. Task fine-tuning \uc2dc\uc5d0\ub294 adapter weight \ub9cc \uc5c5\ub370\uc774\ud2b8\ub418\uace0 pretrained parameter \ub294 \uace0\uc815(frozen)\ub41c\ub2e4. Adapter \ub294 \ud559\uc2b5 \ud6a8\uc728\uc131\uc774 \ub9e4\uc6b0 \ub6f0\uc5b4\ub09c \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc73c\uba70, domain \uacfc task \uac04 transfer, machine translation, cross-lingual transfer \uc640 \uac19\uc740 \ub2e4\uc591\ud55c \uc751\uc6a9 \ubd84\uc57c\uc5d0\uc11c \uc0ac\uc6a9\ub41c\ub2e4."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Datasets.")," Multilingual multimodal model \uc744 \uc704\ud55c pretraining \ubc0f fine-tuning data \ub294 \uc8fc\ub85c Wikipedia \uae30\ubc18 multimodal \uc815\ubcf4(WikiCaps, WIT) \ub610\ub294 downstream task data \uc5d0 \uc758\uc874\ud55c\ub2e4. Multi30k \ub294 English, German, French, Czech \uc744 \ud3ec\ud568\ud558\ub294 multilingual image captioning dataset \uc774\uba70 retrieval task \ub97c \ub2e4\ub8ec\ub2e4. GEM \uc740 image \uc640 video retrieval task \ub97c \uac01\uac01 20, 30 \uac1c \uc5b8\uc5b4\uc5d0\uc11c \ub2e4\ub8ec\ub2e4. HowTo100M \uc740 multilingual, multimodal pretraining dataset \uc73c\ub85c image \uc640 video retrieval \uc5d0 \uc0ac\uc6a9\ub41c\ub2e4. MultiSubs \ub294 \ube48\uce78 \ucc44\uc6b0\uae30 \ubc0f lexical translation task \uc5d0 \uc9d1\uc911\ud558\uba70 English, Spanish, German, Portuguese, French \ub97c \ud3ec\ud568\ud55c\ub2e4. Gao et al. \uacfc Shimizu et al. \uc740 bilingual visual question answering dataset \uc744 \uac01\uac01 English\u2013Chinese, English\u2013Japanese \ub85c \uc81c\uc548\ud558\uc600\ub2e4. Liu et al. \uc740 MaRVL dataset \uc744 \uc81c\uc548\ud558\uc600\ub294\ub370, \uc774\ub294 NLVR2 \uc640 \uc720\uc0ac\ud55c binary multilingual question answering dataset \uc73c\ub85c Chinese, Tamil, Swahili, Indonesian, Turkish \ub4f1 5 \uac1c\uc758 \ub2e4\uc591\ud55c \uc5b8\uc5b4\ub97c \ud3ec\ud568\ud55c\ub2e4."),(0,l.kt)("p",null,"\uae30\uc874 dataset \uc740 retrieval \uc911\uc2ec task (\uc0c1\ub300\uc801\uc73c\ub85c \ub2e8\uc21c), \uc18c\uc218\uc758 \uc720\uc0ac \uc5b8\uc5b4\ub9cc \ud3ec\ud568, \ud639\uc740 binary question \ub9cc \ub2e4\ub8e8\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc558\ub2e4. \uc774\uc5d0 \ubc18\ud574 \uc800\uc790\ub294 \ub354 \ub2e4\uc591\ud55c \uc5b8\uc5b4 \uc9d1\ud569\uc744 \ud3ec\uad04\ud558\ub294 \ucd5c\ucd08\uc758 multilingual visual question answering dataset \uc744 \uc81c\uc548\ud55c\ub2e4."),(0,l.kt)("p",null,"\uac00\uc7a5 \ucd5c\uadfc\uc5d0\ub294 IGLUE \uac00 \uc81c\uc548\ub418\uc5c8\uc73c\uba70, \uc774\ub294 multilingual multimodal benchmark \ub85c\uc11c xGQA \ub97c \ud1b5\ud569\ud55c\ub2e4. IGLUE \ub294 visual question answering, cross-modal retrieval, grounded reasoning, grounded entailment task \ub97c 20 \uac1c \uc774\uc0c1\uc758 \ub2e4\uc591\ud55c \uc5b8\uc5b4\uc5d0\uc11c \ub2e4\ub8ec\ub2e4."),(0,l.kt)("h1",{id:"3-xgqa"},"3 xGQA"),(0,l.kt)("p",null,"\uc6d0\ub798 English GQA dataset \uc740 Visual Genome scene graph \ub97c \ud65c\uc6a9\ud558\uc5ec \uad6c\ucd95\ub418\uc5c8\ub2e4. English question engine \uc740 content (\uc989, object, attribute, relation \uc5d0 \uad00\ud55c \uc815\ubcf4) \uc640 structure (\uc218\ubc31 \uac1c\uc758 \uad6c\uc870\uc801 \ud328\ud134\uacfc \uc815\uad50\ud55c lexical semantic resource \ub97c \uacb0\ud569\ud55c \uc5b8\uc5b4\uc801 grammar) \ub97c \ud65c\uc6a9\ud558\uc5ec image scene graph \uc5d0 \uc2dc\uac01\uc801\uc73c\ub85c \uae30\ubc18\ud55c 2,200 \ub9cc \uac1c \uc774\uc0c1\uc758 \ub2e4\uc591\ud55c question \uc744 \uc0dd\uc131\ud558\uc600\ub2e4. Question \uc740 template \uc744 \uc0ac\uc6a9\ud558\uc5ec \uc790\ub3d9 \uc0dd\uc131\ub418\uc5c8\uae30 \ub54c\ubb38\uc5d0 \uc790\uc5f0 \uc5b8\uc5b4\uc758 \uad11\ubc94\uc704\ud55c \uc2a4\ud399\ud2b8\ub7fc\uc744 \ubc18\ub4dc\uc2dc \ubc18\uc601\ud558\uc9c0\ub294 \uc54a\uc73c\uba70, \uc2e4\uc81c \ud658\uacbd\uc5d0\uc11c\uc758 \uc131\ub2a5\uc5d0 \ub300\ud574 \uac00\uc815\ud558\uae30\ub294 \uc5b4\ub835\ub2e4."),(0,l.kt)("p",null,"\uac01 question \uc740 \ucd94\uac00 metadata \uc640 \uc5f0\uad00\ub418\uba70, \uc774\ub294 \uad6c\uc870\uc801 \uc720\ud615\uc5d0 \ub530\ub77c \ub2e4\uc74c\uacfc \uac19\uc774 \uad6c\ubd84\ub41c\ub2e4."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("strong",{parentName:"li"},"verify"),': yes/no question (\uc608: "Do you see any cats?")'),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("strong",{parentName:"li"},"query"),': open question (\uc608: "Who is wearing jeans?")'),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("strong",{parentName:"li"},"choose"),': \ub450 \uac1c\uc758 \ub300\uc548\uc744 \uc81c\uc2dc\ud558\ub294 question (\uc608: "Is it red or blue?")'),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("strong",{parentName:"li"},"logical"),': \ub17c\ub9ac\uc801 \ucd94\ub860\uc744 \ud3ec\ud568\ud558\ub294 question (\uc608: "Is the field soft and snowy?")'),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("strong",{parentName:"li"},"compare"),': \ub450 \uac1c \uc774\uc0c1\uc758 object \ub97c \ube44\uad50\ud558\ub294 question (\uc608: "Are all the animals zebras?")')),(0,l.kt)("p",null,"\uc790\uc138\ud55c metadata \ub294 Hudson and Manning \uc758 \uc5f0\uad6c\ub97c \ucc38\uace0\ud55c\ub2e4."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Dataset Design.")," xGQA \ub97c \uc124\uacc4\ud560 \ub54c\uc758 \uc8fc\uc694 \ubaa9\ud45c\ub294 visual question answering \uc744 \uc704\ud55c \uc720\ud615\uc801\uc73c\ub85c \uc9c4\uc815\uc73c\ub85c \ub2e4\uc591\ud55c multilingual multimodal evaluation benchmark \ub97c \ub9cc\ub4dc\ub294 \uac83\uc774\uc5c8\ub2e4. \uc800\uc790\ub294 GQA \uc758 balanced test-dev set \uc744 \ud65c\uc6a9\ud558\uc600\uc73c\uba70, \uc774\ub294 398 \uac1c image \uc5d0 \ub300\ud55c 12,578 \uac1c question \uc73c\ub85c \uad6c\uc131\ub41c\ub2e4. \uc815\ud574\uc9c4 \uad6c\uc870\uc801 \ud328\ud134 \ub355\ubd84\uc5d0 question \uc758 \ud45c\ud604\uc740 \ub2e8\uc21c\ud558\uba70, \ud3c9\uade0 \uae38\uc774\ub294 8.5 \ub2e8\uc5b4\uc774\ub2e4. \ucd5c\uc885\uc801\uc73c\ub85c xGQA dataset \uc740 7 \uac1c \uc5b8\uc5b4\ub85c \ubc88\uc5ed\uc744 \ud3ec\ud568\ud558\uba70, \uac01 \uc5b8\uc5b4\ub294 \uc11c\ub85c \ub2e4\ub978 \uc5b8\uc5b4 \uacc4\uc5f4\uc744 \ub300\ud45c\ud558\uace0, 5 \uac1c\uc758 script \ub97c \ud3ec\ud568\ud55c\ub2e4 (Tab. 1 \ucc38\uc870)."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Few-Shot Data Splits.")," Cross-lingual few-shot learning experiment \ub97c \uc218\ud589\ud558\uae30 \uc704\ud574, \uc800\uc790\ub294 \ub2e4\uc591\ud55c \ud06c\uae30\uc758 \uc0c8\ub85c\uc6b4 data split \uc744 \uc81c\uacf5\ud55c\ub2e4. Split \uc740 image \ub2e8\uc704\ub85c \uc9c4\ud589\ub418\uba70, \ud574\ub2f9 image \uc640 \uc5f0\uad00\ub41c \ubaa8\ub4e0 question \uc744 \ud574\ub2f9 set \uc5d0 \ucd94\uac00\ud55c\ub2e4. Development set \uacfc test set \uc740 \uac01\uac01 50 \uac1c\uc640 300 \uac1c image \ub85c \uad6c\uc131\ub41c\ub2e4. Training split \uc740 1, 5, 10, 20, 25, 48 \uac1c image \ub85c \uad6c\uc131\ub41c\ub2e4 (Tab. 2 \ucc38\uc870). \uac01 set \uc5d0\uc11c structural type \uc758 \ubd84\ud3ec\uac00 \uc720\uc9c0\ub418\ub3c4\ub85d \ubcf4\uc7a5\ud558\uc600\ub2e4."),(0,l.kt)("p",null,"xGQA \ub294 \ucd5c\ucd08\uc758 \uc9c4\uc815\ud55c \uc720\ud615\uc801\uc73c\ub85c \ub2e4\uc591\ud55c multilingual multimodal benchmark \ub85c, cross-lingual zero-shot \ubc0f few-shot \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc0c8\ub85c\uc6b4 \uc2e4\ud5d8\uacfc \ubd84\uc11d \uae30\ud68c\ub97c \uc81c\uacf5\ud55c\ub2e4. xGQA \uc758 question \uc740 \uc9c1\uad00\uc801\uc774\uace0 \uc778\uac04\uc774 \ud480\uae30\uc5d0\ub294 \uc27d\uc9c0\ub9cc, \uc774\ud6c4 \uacb0\uacfc\uc5d0\uc11c \ubcf4\uc774\ub4ef \ud604\uc7ac state-of-the-art model \ub4e4\uc740 \uc5ec\uc804\ud788 transfer \uc5d0 \uc5b4\ub824\uc6c0\uc744 \uacaa\ub294\ub2e4."),(0,l.kt)("h1",{id:"4-baselines"},"4 Baselines"),(0,l.kt)("p",null,"xGQA \uc758 \uc131\ub2a5\uacfc \ud604\uc7ac\uc758 \ud55c\uacc4\ub97c \ubd84\uc11d\ud558\uae30 \uc704\ud574, \uba3c\uc800 multilingual \ubc0f multimodal data \ub85c pretraining \ub41c \ucd5c\uadfc\uc758 M3P model \uc744 \ud3c9\uac00\ud55c\ub2e4. \uadf8\ub7ec\ub098 pretraining \uc740 \uacc4\uc0b0 \ube44\uc6a9\uc774 \ud06c\uace0 multilingual multimodal resource \ub294 \uc81c\ud55c\uc801\uc774\ub2e4. \ub530\ub77c\uc11c \uc800\uc790\ub294 \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc778 \uc0c8\ub85c\uc6b4 \uc811\uadfc\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4. (1) state-of-the-art multilingual language model \uc744 multimodal domain \uc73c\ub85c \ud655\uc7a5\ud558\uace0, (2) state-of-the-art multimodal model \uc5d0 multilingual \uae30\ub2a5\uc744 \uc81c\uacf5\ud558\ub294 \ubc29\uc2dd\uc774\ub2e4."),(0,l.kt)("p",null,"\ubcc4\ub3c4\uc758 \uc5b8\uae09\uc774 \uc5c6\ub294 \ud55c, \uc800\uc790\ub294 GQA \uc5d0\uc11c \uc77c\ubc18\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 fine-tuning \uc804\ub7b5\uc744 \ub530\ub978\ub2e4. Pretrained transformer \uc758 output \uc704\uc5d0 prediction head \ub97c \uc5b9\uace0, GQA task \uc758 \uac00\ub2a5\ud55c 1853 \uac1c\uc758 \ub2f5\ubcc0\uc744 class label \ub85c \ub9e4\ud551\ud55c\ub2e4. Image \uc640 \uc5f0\uad00\ub41c question \uacfc position-aware region feature \uac00 transformer \uc758 \uc785\ub825\uc73c\ub85c \uc804\ub2ec\ub418\uba70, cross-entropy loss \ub85c \ud559\uc2b5\ub41c\ub2e4."),(0,l.kt)("h2",{id:"41-multimodal--multilingual"},"4.1 Multimodal \u2192 Multilingual"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"OSCAR+Emb."),"\nArtetxe et al. \uc740 monolingual transformer LM \uc744 multilingual domain \uc73c\ub85c \ud655\uc7a5\ud558\uae30 \uc704\ud574 target language \uc5d0\uc11c \uc0c8\ub85c\uc6b4 word-embedding layer \ub97c fine-tuning \ud558\uc600\ub2e4. \uc774 \uc544\uc774\ub514\uc5b4\ub97c \uae30\ubc18\uc73c\ub85c, \uc800\uc790\ub294 monolingual multimodal transformer model \uc778 OSCAR+ \ub97c target language \uc6a9 embedding \uc744 \ud559\uc2b5\ud558\ub3c4\ub85d \ud655\uc7a5\ud55c\ub2e4."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Language-extension phase:")," OSCAR+ \uc758 embedding matrix \ub97c \ubb34\uc791\uc704\ub85c \ucd08\uae30\ud654\ub41c embedding matrix \ub85c \uad50\uccb4\ud55c\ub2e4. Transformer weight \ub294 \uace0\uc815\ub418\uace0, \uc0c8\ub85c\uc6b4 embedding \ub9cc target language \uc758 unlabeled text data \ub85c MLM objective \ub97c \uc0ac\uc6a9\ud558\uc5ec fine-tuning \ub41c\ub2e4."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Target-task phase:")," \uc6d0\ub798 OSCAR+ model \uc740 English GQA training data \ub85c fine-tuning \ub418\uba70, \uc774\ub54c transformer layer \ub294 fine-tuning \ub418\uace0 embedding layer \ub294 \uace0\uc815\ub41c\ub2e4. Inference \uc2dc\uc5d0\ub294 embedding layer \uac00 target language \uc758 embedding \uc73c\ub85c \uad50\uccb4\ub41c\ub2e4.")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"OSCAR+Ada."),"\n\uc5ec\uae30\uc5d0 adapter \ub97c \ucd94\uac00\ud55c \ud655\uc7a5 \ubc29\uc2dd\uc774\ub2e4."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Language-extension phase:")," Pfeiffer et al. \uc758 \ubc29\ubc95\uc744 \ub530\ub974\uba70, OSCAR+Emb \uc640 \uc720\uc0ac\ud558\uac8c \uc0c8\ub85c\uc6b4 embedding layer \ub97c \ud559\uc2b5\ud55c\ub2e4. \ub610\ud55c \uac01 transformer layer \uc5d0 language adapter \ub97c \ucd94\uac00\ud55c\ub2e4. OSCAR+ \uac00 English text \ub85c \ud559\uc2b5\ub418\uc5c8\uae30 \ub54c\ubb38\uc5d0, English language adapter module \uc740 embedding matrix \ub97c \uad50\uccb4\ud558\uc9c0 \uc54a\uace0 \ud559\uc2b5\ub41c\ub2e4. Transformer weight \ub294 \uace0\uc815\ub418\uace0, \uc0c8\ub85c\uc6b4 embedding \uacfc language adapter weight \ub9cc target language \uc758 unlabeled text data \ub85c fine-tuning \ub41c\ub2e4.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Target-task phase:")," \uc800\uc790\ub294 cross-lingual transfer \ubc29\ubc95\uc5d0\uc11c \uc601\uac10\uc744 \ubc1b\uc544 \uc0c8\ub85c\uc6b4 modality-split architecture \ub97c \uc81c\uc548\ud55c\ub2e4 (Fig. 2). \uac01 transformer layer \uc5d0\uc11c text \uc640 image representation \uc740 pretrained multi-head attention (MHA) \uc640 feed-forward (FFN) layer \ub97c \ud1b5\uacfc\ud55c\ub2e4. \ub3d9\uc2dc\uc5d0 image \uc640 text representation \uc740 pretrained language adapter \ub97c \ud1b5\uacfc\ud55c\ub2e4. \uc774\ud6c4 \uac01 modality \ub294 modality-specific task adapter (text, image) \ub97c \uac70\uce58\uace0, \ub9c8\uc9c0\ub9c9\uc73c\ub85c shared multimodal alignment adapter \ub97c \ud1b5\uacfc\ud55c\ub2e4. Training \uc2dc transformer, embedding, language adapter weight \ub294 \uace0\uc815\ub418\uba70, task adapter \uc640 multimodal aligner adapter weight, prediction head \ub9cc fine-tuning \ub41c\ub2e4. Inference \uc2dc\uc5d0\ub294 embedding layer \uc640 language adapter \uac00 target language weight \ub85c \uad50\uccb4\ub41c\ub2e4."))),(0,l.kt)("h2",{id:"42-multilingual--multimodal"},"4.2 Multilingual \u2192 Multimodal"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"mBERTAda."),"\nMultilingual model \uc744 multimodal \ub85c \ud655\uc7a5\ud558\ub294 \uc2e4\ud5d8\uc5d0\uc11c\ub294 mBERT \ub97c \uc0ac\uc6a9\ud55c\ub2e4."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Language-extension phase:")," mBERT \ub294 \uc774\ubbf8 \ub2e4\uc591\ud55c \uc5b8\uc5b4\ub97c \ud45c\ud604\ud560 \uc218 \uc788\uc73c\ubbc0\ub85c target language \ub97c \uc704\ud55c \uc0c8\ub85c\uc6b4 embedding layer \ub97c \ud559\uc2b5\ud560 \ud544\uc694\uac00 \uc5c6\ub2e4. \ub300\uc2e0 AdapterHub.ml \uc5d0\uc11c \uc81c\uacf5\ub418\ub294 mBERT \ud638\ud658 language adapter \ub97c \ud65c\uc6a9\ud55c\ub2e4.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Target-task phase:")," Image representation layer \ub294 OSCAR+ \ubc29\uc2dd\uacfc \ub3d9\uc77c\ud558\uac8c, image feature \ub97c positional information \uacfc \uacb0\ud569\ud55c \ud6c4 affine transformation layer \ub97c \ud1b5\uacfc\uc2dc\ud0a8\ub2e4. Fig. 2 \uc758 adapter architecture \ub97c OSCAR+Ada \uc640 \ub3d9\uc77c\ud558\uac8c \uc0ac\uc6a9\ud55c\ub2e4. Training \uc2dc transformer, embedding, language adapter weight \ub294 \uace0\uc815\ub41c\ub2e4. \uadf8\ub7ec\ub098 OSCAR+ \uc640 \ub2ec\ub9ac affine image transformation layer \ub294 \ubb34\uc791\uc704 \ucd08\uae30\ud654 \ud6c4 fine-tuning \ub41c\ub2e4. Task adapter, multimodal aligner adapter weight, prediction head \ub3c4 GQA task \uc5d0 \ub300\ud574 fine-tuning \ub41c\ub2e4. Inference \uc2dc\uc5d0\ub294 embedding layer \uc640 language adapter \uac00 target language weight \ub85c \uad50\uccb4\ub41c\ub2e4."))),(0,l.kt)("h1",{id:"5-experimental-setup"},"5 Experimental Setup"),(0,l.kt)("h2",{id:"51-language-extension-phase"},"5.1 Language-Extension Phase"),(0,l.kt)("p",null,"OSCAR+Emb \uacfc OSCAR+Ada \uc758 \uacbd\uc6b0, \uc800\uc790\ub294 Pfeiffer et al. \uc774 \uc81c\uc548\ud55c \uc77c\ubc18\uc801\uc778 \uc124\uc815\uc744 \ub530\ub978\ub2e4. \uac01 target language \uc5d0 \ub300\ud574 vocabulary size \uac00 30k \uc778 \uc0c8\ub85c\uc6b4 word-piece tokenizer \ub97c \ud559\uc2b5\ud55c\ub2e4. \ubb34\uc791\uc704\ub85c \ucd08\uae30\ud654\ub41c embedding layer \uc640 (OSCAR+Ada \uc758 \uacbd\uc6b0) adapter layer \ub97c batch size 64, learning rate ","$","1e^{-4}","$"," \ub85c 100k update step \ub3d9\uc548 fine-tuning \ud55c\ub2e4. mBERTAda \uc5d0 \ub300\ud574\uc11c\ub294 AdapterHub.ml \uc758 language adapter \ub97c \ud65c\uc6a9\ud55c\ub2e4."),(0,l.kt)("h2",{id:"52-fine-tuning-on-gqa"},"5.2 Fine-tuning on GQA"),(0,l.kt)("p",null,"\uc800\uc790\ub294 Li et al. \uc774 \uc81c\uc548\ud55c standard setup \uc744 \ub530\ub974\uba70, ","[","CLS] token \uc758 representation \uc744 prediction head \ub85c \uc804\ub2ec\ud55c\ub2e4. \uac01 model \uc740 cross-entropy loss \ub85c fine-tuning \ub418\uba70, label \uc740 GQA dataset \uc758 \ubaa8\ub4e0 \uac00\ub2a5\ud55c answer \ub85c \uc124\uc815\ub41c\ub2e4. Li et al. \uc758 prior work \uc5d0 \ub530\ub77c batch size \ub294 192, \ud559\uc2b5 epoch \uc740 5 \ub85c \uc124\uc815\ud558\uace0, unbalanced GQA training set \uc744 \uc0ac\uc6a9\ud55c\ub2e4."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"M3P:")," pretrained model \uc758 \ubaa8\ub4e0 weight \ub97c learning rate ","$","3e^{-5}","$"," \ub85c fine-tuning \ud55c\ub2e4.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"OSCAR+Emb, OSCAR+Ada, mBERTAda:")," Zhang et al. \uc774 \uc81c\uacf5\ud55c pretrained weight \uc640 image region feature \ub97c \uc0ac\uc6a9\ud55c\ub2e4. \uadf8\ub7ec\ub098 object attribute label \uc740 \uc785\ub825\uc73c\ub85c \uc804\ub2ec\ud558\uc9c0 \uc54a\ub294\ub2e4. \uc774\ub294 English \ub85c\ub9cc \uc81c\uacf5\ub418\uba70, cross-lingual scenario \uc5d0 \ud65c\uc6a9\ud558\uae30\uac00 \ube44\uc9c1\uad00\uc801\uc774\uae30 \ub54c\ubb38\uc774\ub2e4. \ud574\ub2f9 \ubd80\ubd84\uc740 future work \uc73c\ub85c \ub0a8\uae34\ub2e4."),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"OSCAR+Emb:")," transformer weight \uc640 prediction head \ub9cc fine-tuning \ud558\uace0, embedding layer \ub294 freeze \ud55c\ub2e4. Learning rate \ub294 ","$","3e^{-5}","$"," \ub97c \uc0ac\uc6a9\ud55c\ub2e4."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"OSCAR+Ada, mBERTAda:")," Sec. 4.1 \uacfc Fig. 2 \uc5d0\uc11c \uc124\uba85\ub41c \ub300\ub85c adapter layer \ub97c \ucd94\uac00\ud55c\ub2e4. Embedding, transformer layer, language adapter \ub97c \ud3ec\ud568\ud55c \ubaa8\ub4e0 pretrained weight \ub294 freeze \ud558\uace0, \uc0c8\ub85c \ucd94\uac00\ub41c adapter \uc640 prediction head \ub9cc fine-tuning \ud55c\ub2e4."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"mBERTAda:")," affine image transformation layer \ub97c \ucd94\uac00\ud558\uace0 \ud559\uc2b5\ud55c\ub2e4. Adapter \uae30\ubc18 model \uc740 learning rate ","$","1e^{-4}","$"," \ub85c fine-tuning \ud55c\ub2e4.")))),(0,l.kt)("h1",{id:"53-zero-shot-cross-lingual-transfer"},"5.3 Zero-Shot Cross-Lingual Transfer"),(0,l.kt)("p",null,"Zero-shot cross-lingual evaluation \uc744 \uc704\ud574, \uc800\uc790\ub294 GQA training data \ub85c fine-tuning \ub41c model \uc744 multilingual xGQA test data \uc5d0\uc11c \ud3c9\uac00\ud55c\ub2e4. English GQA validation data \uc5d0\uc11c \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778 model checkpoint \uac00 transfer \uc5d0 \uc0ac\uc6a9\ub41c\ub2e4."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"M3P:")," \uc774 model \uc740 pretraining \uc2dc xGQA \uc5b8\uc5b4\ub97c \ud3ec\ud568\ud558\ubbc0\ub85c cross-lingual transfer \ub97c \uc704\ud574 \ucd94\uac00\uc801\uc778 \ub2e8\uacc4\uac00 \ud544\uc694 \uc5c6\ub2e4."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"OSCAR+Emb:")," English embedding layer \ub97c target language embedding layer \ub85c \uad50\uccb4\ud55c\ub2e4."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"OSCAR+Ada:")," English embedding \uacfc language adapter layer \ub97c target language \uc758 embedding \uacfc adapter layer \ub85c \uad50\uccb4\ud55c\ub2e4."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"mBERTAda:")," language adapter layer \ub97c target language \uc758 adapter layer \ub85c \uad50\uccb4\ud55c\ub2e4.")),(0,l.kt)("h1",{id:"54-few-shot-cross-lingual-transfer"},"5.4 Few-Shot Cross-Lingual Transfer"),(0,l.kt)("p",null,"Few-shot cross-lingual \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c\ub294 Lauscher et al. \uc758 \uc811\uadfc\uc744 \ub530\ub974\uba70, zero-shot transfer \uc640 \ub3d9\uc77c\ud55c fine-tuning model (\xa75.3) \uc5d0\uc11c \uc2dc\uc791\ud55c\ub2e4. \uc774\ud6c4 \xa75.2 \uc758 English training data \ud559\uc2b5\uacfc \ub3d9\uc77c\ud55c \ubd80\ubd84\uc744 target language \uc758 \uc18c\ub7c9 multimodal data \ub85c fine-tuning \ud55c\ub2e4."),(0,l.kt)("p",null,"Training \uc740 1, 5, 10, 15, 20, 25, 48 \uac1c image \ub85c \uad6c\uc131\ub41c \uc11c\ub85c \ub2e4\ub978 data split (Tab. 2 \ucc38\uc870) \uc5d0 \ub300\ud574 \uc218\ud589\ub41c\ub2e4. Epoch \uc218\ub294 5, 10, learning rate \ub294 M3P \uc640 OSCAR+Emb \uc758 \uacbd\uc6b0 ","$","1e^{-5}","$",", ","$","5e^{-5}","$"," \ub97c, OSCAR+Ada \uc640 mBERTAda \uc758 \uacbd\uc6b0 ","$","5e^{-5}","$",", ","$","1e^{-4}","$"," \ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud5d8\ud558\uc600\ub2e4."),(0,l.kt)("p",null,"\uadf8 \uacb0\uacfc, \ub354 \uae34 \ud559\uc2b5\uacfc \ub354 \ud070 learning rate \ub97c \uc0ac\uc6a9\ud560 \ub54c \ubaa8\ub4e0 setting \uc5d0\uc11c \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\ub2e4."),(0,l.kt)("h1",{id:"6-results-and-discussion"},"6 Results and Discussion"),(0,l.kt)("p",null,"\uc8fc\uc694 \uacb0\uacfc\ub294 Tab. 3 (zero-shot \uc2e4\ud5d8) \uacfc Tab. 4 (few-shot \uc2e4\ud5d8) \uc5d0 \uc81c\uc2dc\ub41c\ub2e4."),(0,l.kt)("h2",{id:"61-zero-shot-cross-lingual-transfer"},"6.1 Zero-Shot Cross-Lingual Transfer"),(0,l.kt)("p",null,"\ud575\uc2ec\uc801\uc778 \ubc1c\uacac \uc911 \ud558\ub098\ub294 multimodal zero-shot cross-lingual transfer \uac00 \ub9e4\uc6b0 \uc5b4\ub835\ub2e4\ub294 \uac83\uc774\ub2e4. xGQA dataset \uc758 target language \uc5d0\uc11c accuracy \uac00 English GQA \uc810\uc218\uc640 \ube44\uad50\ud588\uc744 \ub54c \ud3c9\uade0 38 point \uc774\uc0c1 \uac10\uc18c\ud558\ub294 \ud604\uc0c1\uc774 \uad00\ucc30\ub41c\ub2e4 (\uc608: M3P \uacb0\uacfc \ube44\uad50)."),(0,l.kt)("p",null,"\uc608\uc0c1\ub300\ub85c OSCAR+ \uac00 English test set \uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 accuracy \ub97c \uae30\ub85d\ud588\uc9c0\ub9cc, massively multilingual model \uc778 M3P \uc640 mBERT \uac00 cross-lingual transfer \uc5d0\uc11c\ub294 \ud6e8\uc52c \ub354 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc778\ub2e4. \uc774\ub294 joint multilingual pretraining \uc774 \uc911\uc694\ud558\uba70, \ub2e8\uc21c\ud55c multilingual adapter \uae30\ubc18 \ub610\ub294 embedding \uae30\ubc18\uc758 monolingual model \ud655\uc7a5\uc740 cross-lingual \uc131\ub2a5\uc774 \ub5a8\uc5b4\uc9d0\uc744 \uc758\ubbf8\ud55c\ub2e4."),(0,l.kt)("p",null,"Pretraining \uae30\ubc18\uc758 M3P \uac00 English test set \uc5d0\uc11c \ub354 \ub098\uc740 accuracy \ub97c \uae30\ub85d\ud558\uc600\uc9c0\ub9cc, adapter \uae30\ubc18 multimodal \ud655\uc7a5\uc744 \uc801\uc6a9\ud55c mBERT \ub294 cross-lingual transfer \uc5d0\uc11c M3P \ub97c \ub2a5\uac00\ud558\uc600\ub2e4. \uc800\uc790\ub294 monolingual multimodal data \ub85c transformer weight \ub97c \ubaa8\ub450 fine-tuning \ud558\uba74, M3P \ub0b4\uc758 cross-lingual alignment \uac00 \uae68\uc9c8 \uc218 \uc788\ub2e4\uace0 \uac00\uc815\ud55c\ub2e4. \uadf8\ub7ec\ub098 adapter \uae30\ubc18 setting \uc5d0\uc11c\ub294 multilingual weight \uac00 freeze \ub418\uc5b4 alignment \uac00 \uc720\uc9c0\ub418\ubbc0\ub85c \uc774\ub7ec\ud55c \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud558\uc9c0 \uc54a\ub294\ub2e4."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Structural Question Type \ubd84\uc11d."),"\nFig. 3 \uc740 zero-shot \uc2e4\ud5d8\uc5d0\uc11c structural question type \uc5d0 \ub300\ud55c \ubd84\uc11d\uc744 \ubcf4\uc5ec\uc900\ub2e4. \ud2b9\ud788 query \uc640 choose \uc720\ud615 question \uc5d0\uc11c accuracy \uac00 \ud06c\uac8c \ub5a8\uc5b4\uc9c0\ub294 \uac83\uc774 \uad00\ucc30\ub41c\ub2e4."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Query type question")," \uc740 \uc790\uc720\ud615\uc774\uba70 \uc758\ubbf8\uc801\uc73c\ub85c \uac00\uc7a5 \uc5b4\ub835\ub2e4. Source language (English) \uc5d0\uc11c\uc870\ucc28 \ub2f5\ubcc0\ud558\uae30 \uc5b4\ub824\uc6b4 \uc720\ud615\uc774\uae30 \ub54c\ubb38\uc5d0, zero-shot setting \uc5d0\uc11c \uc804\ubc18\uc801\uc73c\ub85c \ub0ae\uc740 accuracy \uac00 \ub098\ud0c0\ub09c\ub2e4."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("strong",{parentName:"li"},"Choose type question"),' \uc740 source language \uc5d0\uc11c model \uc774 \uc798 \uc218\ud589\ud558\ub294 \uc720\ud615\uc774\uc9c0\ub9cc, zero-shot cross-lingual transfer \uc5d0\uc11c accuracy \uac00 \ud06c\uac8c \uac10\uc18c\ud55c\ub2e4. \uc774\ub294 question \uc758 \uad6c\uc870\uc640 model \uad6c\ud604 \ubc29\uc2dd\uc5d0 \uae30\uc778\ud55c\ub2e4. Choose \uc720\ud615 question \uc740 "Is it red or blue?" \uc640 \uac19\uc774 \uc815\ub2f5\uc774 question \uc5d0 \ud3ec\ud568\ub41c \ub2e8\uc5b4 \ub610\ub294 \uad6c\ub85c \uad6c\uc131\ub41c\ub2e4. Label class \uc640 prediction head \ub294 dataset \uc5d0 \ub4f1\uc7a5\ud558\ub294 \ubaa8\ub4e0 answer \ub85c \uad6c\ucd95\ub418\uba70, model \uc740 \ucd5c\uc885 layer \uc5d0\uc11c \uac01 answer \uc758 \ubd84\uc0b0 \ud45c\ud604\uc744 \ud559\uc2b5\ud55c\ub2e4. \ub530\ub77c\uc11c cross-lingual transfer \uc2dc\uc5d0\ub294 translated question \ub0b4\uc758 option ("red" \ub610\ub294 "blue") \uc744 model \uc758 prediction head \uc5d0\uc11c \ud559\uc2b5\ub41c English latent representation \uacfc \uc790\ub3d9\uc73c\ub85c \uc815\ub82c\ud574\uc57c \ud55c\ub2e4. \uc774 \uc720\ud615\uc5d0\uc11c \ub9e4\uc6b0 \ub0ae\uc740 \uc131\ub2a5\uc740 zero-shot \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uc774\ub7ec\ud55c cross-lingual word alignment \uac00 \ubb34\ub108\uc9d0\uc744 \ubcf4\uc5ec\uc900\ub2e4.')),(0,l.kt)("p",null,"\uc885\ud569\uc801\uc73c\ub85c, \uc800\uc790\uac00 \uc81c\uc548\ud55c multimodal adapter \uae30\ubc18 mBERT \ud655\uc7a5(mBERTAda) \uc740 best accuracy \ub97c \uae30\ub85d\ud558\uba70, M3P \ub300\ube44 \uc57d 3 point, OSCAR+ \ub300\ube44 \uc57d 5 point \ud5a5\uc0c1\ub418\uc5c8\ub2e4. \uadf8\ub7ec\ub098 \ubaa8\ub4e0 \uc811\uadfc\ubc95\uc758 overall accuracy \ub294 English \uacb0\uacfc\uc640 \ube44\uad50\ud558\uba74 \uc5ec\uc804\ud788 \ub0ae\ub2e4. \uc774\ub294 zero-shot multimodal cross-lingual transfer \uac00 \ub9e4\uc6b0 \uc5b4\ub835\uace0, visual representation \uacfc cross-lingual internal representation \uac04\uc758 misalignment \ubb38\uc81c \ub54c\ubb38\uc77c \uac00\ub2a5\uc131\uc774 \ud06c\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uc774\ub97c \ub354 \uc870\uc0ac\ud558\uae30 \uc704\ud574 \uc800\uc790\ub294 few-shot setup \uc5d0\uc11c \uc720\uc0ac\ud55c \uc2e4\ud5d8\uc744 \uc218\ud589\ud558\uc600\uc73c\uba70, \uc774\ub294 zero-shot setup \uc5d0\uc11c \uad00\ucc30\ub41c misalignment \ubb38\uc81c\ub97c \uc644\ud654\ud560 \uac00\ub2a5\uc131\uc774 \uc788\ub2e4."),(0,l.kt)("h1",{id:"62-few-shot-cross-lingual-transfer"},"6.2 Few-Shot Cross-Lingual Transfer"),(0,l.kt)("p",null,"Few-shot \uc2e4\ud5d8\uc758 \uc8fc\uc694 \uacb0\uacfc\ub294 Tab. 4 \uc5d0 \uc81c\uc2dc\ub418\uba70, training data \uc591\uc758 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc8fc\ub294 plot \uc740 Fig. 5 \uc5d0 \ub098\ud0c0\ub098 \uc788\ub2e4. \ud575\uc2ec\uc801\uc778 \ubc1c\uacac\uc740 \uc608\uc0c1\ub300\ub85c target language \uc758 data \uc591\uc774 \uc99d\uac00\ud560\uc218\ub85d \ubaa8\ub4e0 \ubc29\ubc95\uc5d0\uc11c accuracy \uac00 \uafb8\uc900\ud788 \ud5a5\uc0c1\ub41c\ub2e4\ub294 \uc810\uc774\ub2e4. Target language \uc5d0\uc11c \ub2e8 48 \uac1c image \ub85c model \uc744 \ud559\uc2b5\ud588\uc744 \ub54c accuracy \uac00 \ucd5c\ub300 20 point \uac1c\uc120\ub418\uc5c8\ub2e4. \uc774\ub294 \uc18c\uc218\uc758 target language \uc608\uc81c\ub9cc\uc73c\ub85c\ub3c4 model \uc774 \ub0b4\ubd80 cross-lingual multimodal alignment \ub97c \ubd80\ubd84\uc801\uc73c\ub85c \ubcf5\uad6c\ud560 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud55c\ub2e4. \ud765\ubbf8\ub86d\uac8c\ub3c4 \ub2e8 5 \uac1c image \uc640 \uadf8\uc5d0 \ud574\ub2f9\ud558\ub294 question \ub9cc\uc73c\ub85c\ub3c4 M3P \uac00 mBERTAda (zero-shot setting \uc5d0\uc11c best performing model) \ubcf4\ub2e4 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uae30 \uc2dc\uc791\ud55c\ub2e4."),(0,l.kt)("p",null,"\uc800\uc790\ub294 structural question type \ubcc4\ub85c few-shot learning \uc758 \uc601\ud5a5\uc744 \ub2e4\uc2dc \ubd84\uc11d\ud588\uc73c\uba70, \uacb0\uacfc\ub294 Fig. 4 \uc5d0 \uc81c\uc2dc\ub41c\ub2e4. Zero-shot \uc2dc\ub098\ub9ac\uc624(Fig. 3) \uc640 \ube44\uad50\ud588\uc744 \ub54c \uc804\ubc18\uc801\uc73c\ub85c accuracy \uac00 \ubaa8\ub4e0 \uc720\ud615\uc5d0\uc11c \ud5a5\uc0c1\ub418\uc5c8\ub2e4. \ud2b9\ud788 zero-shot setup \uc5d0\uc11c \uac00\uc7a5 \ub0ae\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\ub358 query \uc640 choose \uc720\ud615\uc5d0\uc11c \ub450\ub4dc\ub7ec\uc9c4 \ud5a5\uc0c1\uc774 \ub098\ud0c0\ub0ac\ub2e4. \uc774\ub294 target language \uc5d0\uc11c \uc18c\uc218\uc758 \uc608\uc81c\ub85c fine-tuning \ud558\ub294 \uacfc\uc815\uc774 latent multimodal representation \uacfc multilingual representation \uac04 alignment \ub97c \uac1c\uc120\ud588\uc74c\uc744 \uc758\ubbf8\ud55c\ub2e4."),(0,l.kt)("h1",{id:"63-language-transfer"},"6.3 Language Transfer"),(0,l.kt)("p",null,"\uc800\uc790\uc758 model \uc740 \uae30\uc874 \uc5f0\uad6c\uc5d0\uc11c \ubcf4\uace0\ub41c \uac83\uacfc \uc720\uc0ac\ud55c cross-lingual transfer \ud328\ud134\uc744 \ubcf4\uc600\ub2e4. Typologically \uac00\uae4c\uc6b4 \uc5b8\uc5b4\uc5d0\uc11c \uac00\uc7a5 \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\ub294\ub370, German (de) \uacfc Portuguese (pt) \uac00 \uadf8 \uc608\uc774\ub2e4. \ub450 \uc5b8\uc5b4\ub294 \ubaa8\ub450 Indo-European (IE) \uacc4\uc5f4\uc5d0 \uc18d\ud558\uba70 source language \uc778 English (en) \uacfc \uac19\uc740 Latin script \ub97c \uacf5\uc720\ud55c\ub2e4. Russian (ru), Indonesian (id), Chinese (zh) \uc5d0\uc11c\ub294 accuracy \uac00 \uc18c\ud3ed \uac10\uc18c\ud588\uace0, Bengali (bn), Korean (ko) \uc5d0\uc11c\ub294 \ub354 \ud070 \uac10\uc18c\uac00 \uad00\ucc30\ub418\uc5c8\ub2e4. \uc774\ub4e4 \uc5b8\uc5b4\ub294 source language \uc640 \uc720\ud615\uc801\uc73c\ub85c \ub2e4\ub974\uba70 \ub300\ubd80\ubd84 script \ub3c4 \uacf5\uc720\ud558\uc9c0 \uc54a\ub294\ub2e4. \uc774\ub7ec\ud55c \ucc28\uc774\ub294 cross-lingual transfer \uc5d0\uc11c \uc5b8\uc5b4 \ub2e4\uc591\uc131\uc774 \uc911\uc694\ud568\uc744 \uac15\uc870\ud55c\ub2e4. \uc800\uc790\uc758 benchmark \ub294 \uc774\ub7ec\ud55c \ub2e4\uc591\uc131\uc744 \ud3ec\uad04\ud568\uc73c\ub85c\uc368 multilingual multimodal model \uc744 \uc9c4\uc815\uc73c\ub85c \uc720\ud615\uc801\uc73c\ub85c \ub2e4\uc591\ud55c \uc5b8\uc5b4 \uc9d1\ud569\uc5d0\uc11c \uc2e4\ud5d8\ud558\uace0 \ud3c9\uac00\ud560 \uc218 \uc788\uac8c \ud55c\ub2e4."),(0,l.kt)("h1",{id:"7-contemporary-work"},"7 Contemporary Work"),(0,l.kt)("p",null,"Multilingual vision \uacfc language learning \uc5d0 \ub300\ud55c \uad00\uc2ec\uc774 \ucd5c\uadfc \uae09\uaca9\ud788 \uc99d\uac00\ud558\uba74\uc11c, contemporay work \uc5d0\uc11c\ub294 \uc774\ubbf8 \uc81c\uc548\ub41c xGQA dataset \uc744 \ub354 \ubd84\uc11d\ud558\uace0 \ud655\uc7a5\ud558\uc600\ub2e4."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Further Analysis.")," Liu et al. (2022) \ub294 cross-lingual visual question answering \uc73c\ub85c \ud559\uc2b5\ub41c multilingual, multimodal model \uc744 \uad11\ubc94\uc704\ud558\uac8c \ubd84\uc11d\ud558\uace0, Sec. 6.1 \uc5d0\uc11c \ub17c\uc758\ub41c multilingual misalignment \ubb38\uc81c\ub97c \uc644\ud654\ud558\uae30 \uc704\ud55c \uc5ec\ub7ec \uc811\uadfc\ubc95\uc744 \uc81c\uc548\ud558\uc600\ub2e4. \uadf8 \uacb0\uacfc, text-only cross-lingual transfer scenario \uc5d0\uc11c \uc0ac\uc6a9\ub418\ub358 \uc77c\ubc18\uc801\uc778 \uc811\uadfc\ubc95\uc740 pretrained model \uc758 multilingual capability \ub97c \ucda9\ubd84\ud788 \ud65c\uc6a9\ud558\uc9c0 \ubabb\ud55c\ub2e4\ub294 \uc0ac\uc2e4\uc774 \ub4dc\ub7ec\ub0ac\ub2e4. \ud765\ubbf8\ub86d\uac8c\ub3c4 prediction head \ub97c \ub354 \uae4a\uac8c \ub9cc\ub4dc\ub294 \uac83\uc740 source language \uc131\ub2a5\uc5d0 \uc601\ud5a5\uc744 \uc8fc\uc9c0 \uc54a\uc9c0\ub9cc, \ubaa8\ub4e0 target language \uc758 zero-shot transfer \uc131\ub2a5\uc740 \uc0c1\ub2f9\ud788 \uac1c\uc120\ub418\uc5c8\ub2e4.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Translated Test Data.")," Bugliarello et al. (2022) \ub294 modality, task, language \ub97c \uc544\uc6b0\ub974\ub294 transfer learning benchmark \ub97c \ucc98\uc74c\uc73c\ub85c \uc81c\uc548\ud588\uc73c\uba70, \uc5ec\uae30\uc5d0\ub294 visual question answering, cross-modal retrieval, grounded reasoning, grounded entailment task \uac00 \ud3ec\ud568\ub418\uba70 \ucd1d 20 \uac1c\uc758 \ub2e4\uc591\ud55c \uc5b8\uc5b4\ub97c \ub2e4\ub8ec\ub2e4. \uc774\ub4e4\uc740 machine translation \uc73c\ub85c \ubc88\uc5ed\ub41c test set question \uc744 \ucd94\uac00\ud558\uc5ec xGQA dataset \uc744 \ud655\uc7a5\ud558\uace0, translate-test setup \uc5d0\uc11c state-of-the-art monolingual multimodal model \uc744 \ud3c9\uac00\ud558\uc600\ub2e4. \uc774 setting \uc5d0\uc11c\ub294 \uc57d\uac04 \ub354 \ub098\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\uc73c\ub098, \uc5ec\uc804\ud788 source language \uc131\ub2a5\uc5d0\ub294 \ud06c\uac8c \ubbf8\uce58\uc9c0 \ubabb\ud588\ub2e4. Translate-test data \ub294 ",(0,l.kt)("a",{parentName:"p",href:"https://iglue-benchmark.github.io"},"iglue-benchmark.github.io")," \uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\ub2e4."))),(0,l.kt)("h1",{id:"8-conclusion"},"8 Conclusion"),(0,l.kt)("p",null,"\uc800\uc790\ub294 visual question answering task \ub97c \uc704\ud55c \ucd5c\ucd08\uc758 cross-lingual evaluation benchmark \uc778 xGQA \ub97c \uc81c\uc548\ud558\uc600\ub2e4. xGQA \ub294 English GQA dataset \uc744 7 \uac1c\uc758 \uc720\ud615\uc801\uc73c\ub85c \ub2e4\uc591\ud55c \uc5b8\uc5b4\ub85c \ud655\uc7a5\ud558\uc600\uc73c\uba70, 5 \uac1c script \ub97c \ud3ec\ud568\ud55c development \ubc0f test data \ub97c \uc81c\uacf5\ud55c\ub2e4. \ucd94\uac00 baseline \uc73c\ub85c, \uc800\uc790\ub294 unimodal multilingual model \uc744 multimodal \ub85c \ud655\uc7a5\ud558\uac70\ub098 \uadf8 \ubc18\ub300\ub85c monolingual multimodal model \uc744 multilingual \ub85c \ud655\uc7a5\ud558\ub294 adapter \uae30\ubc18 \ubc29\ubc95\uc744 \uc81c\uc548\ud558\uc600\ub2e4."),(0,l.kt)("p",null,"\uc2e4\ud5d8 \uacb0\uacfc, (1) \ud6a8\uc728\uc801\uc778 adapter \uae30\ubc18 \ubc29\ubc95\uc740 zero-shot scenario \uc5d0\uc11c pretrained multilingual multimodal model \uc778 M3P \ub97c \uc57d\uac04 \ub2a5\uac00\ud588\uc9c0\ub9cc, (2) \ubaa8\ub4e0 model \uc740 English \uc131\ub2a5\uacfc \ube44\uad50\ud560 \ub54c zero-shot cross-lingual transfer \uc5d0\uc11c \uc2ec\uac01\ud55c accuracy \uac10\uc18c\ub97c \ubcf4\uc600\ub2e4. \uadf8\ub7ec\ub098 target language \uc758 \uc18c\ub7c9 training data \ub97c \ud65c\uc6a9\ud558\ub294 few-shot learning \uc744 \ud1b5\ud574 accuracy \ub97c \ubd80\ubd84\uc801\uc73c\ub85c \ubcf5\uad6c\ud560 \uc218 \uc788\uc5c8\ub2e4. \uadf8\ub7fc\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ud070 \uc131\ub2a5 \uaca9\ucc28\ub294 \uc5ec\uc804\ud788 \ub0a8\uc544 \uc788\uc73c\uba70, \uc774\ub294 \ud574\ub2f9 task \uac00 \uc9c1\uad00\uc801\uc774\uace0 \uc778\uac04(\ud2b9\ud788 bilingual) \uc5d0\uac8c\ub294 \ub9e4\uc6b0 \uc26c\uc6c0\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ubcf8\uc9c8\uc801\uc73c\ub85c \ub192\uc740 \ubcf5\uc7a1\uc131\uc744 \uac00\uc9c4\ub2e4\ub294 \uc810\uc744 \uc2dc\uc0ac\ud55c\ub2e4."),(0,l.kt)("p",null,"\uc800\uc790\ub294 \uc81c\uc548\ub41c dataset \uacfc error analysis \uac00 \uc774 task \ub294 \ubb3c\ub860, \ub354 \ub113\uac8c\ub294 multilingual multimodal representation learning \uc774\ub77c\ub294 \uc0c8\ub86d\uac8c \ubd80\uc0c1\ud558\ub294 \ud765\ubbf8\ub85c\uc6b4 \uc5f0\uad6c \uc601\uc5ed\uc5d0\uc11c \ud5a5\ud6c4 \uc5f0\uad6c\ub97c \ucd09\uc9c4\ud558\uae30\ub97c \uae30\ub300\ud55c\ub2e4."))}d.isMDXComponent=!0}}]);