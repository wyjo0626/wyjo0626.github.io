"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[51316],{26977:e=>{e.exports=JSON.parse('{"label":"Multimodal","permalink":"/docs/tags/multimodal","allTagsPath":"/docs/tags","count":21,"items":[{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2025-02-CoTMT","title":"A Chain-of-Thought Subspace Meta-Learning for Few-shot Image Captioning with Large Vision and Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/CoTMT"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-10-FEWVLM","title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/FEWVLM"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-09-PICa","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc8fc \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/PICa"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2022-03-TAP","title":"CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/TAP"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-11-ClipCap","title":"ClipCap: CLIP Prefix for Image Captioning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/ClipCap"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2023-03-DeCap","title":"DeCap: Decoding CLIP Latents for Zer-Shot Captioning via Text-Only Training","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/DeCap"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2024-03-LLMVQA","title":"Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/LLMVQA"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2022-12-Img2LLM","title":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/Img2LLM"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2022-05-MAGIC","title":"Language Models Can See: Plugging Visual Controls in Text Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/MAGIC"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2022-09-MAGMA","title":"Linearly Mapping from Image to Text Space","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/MGMA"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2023-02-Meta-Mapper","title":"Meta Learning to Bridge Vision and Language Models for MultiModal Few-shot Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/Meta-Mapper"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2025-06-MAPD","title":"Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/MAPD"},{"id":"Paper/Multi-Modal/PEFT/Composition/2025-06-Moka","title":"MokA: Multimodal Low-Rank Adaptation for MLLMs","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","permalink":"/docs/Paper/Multi-Modal/PEFT/Composition/Moka"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-06-Frozen","title":"Multimodal Few-Shot Learning with Frozen Language Models","description":"\uc774\ubbf8\uc9c0 \ubc0f \ub17c\ubb38 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/VQA Few-shot"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2022-10-PNP-VQA","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/PNP-VQA"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2023-03-Prophet","title":"Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/Prophet"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2022-11-CapDec","title":"Text-Only Training for Image Captioning using Noise-Injected CLIP","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/CapDec"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2021-09-xGQA","title":"xGQA: Cross-Lingual Visual Question Answering","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/xGQA"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2025-04-11-MERCap","title":"Zero-Shot Image Captioning with Multi-type Entity Representations","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/MERCap"},{"id":"Paper/Vision-Language/VQA-IC/Few-shot/2023-05-LAMOC","title":"Zero-shot Visual Question Answering with Language Model Feedback","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Few-shot/LAMOC"},{"id":"Paper/Vision-Language/VQA-IC/Zero-shot/2021-11-ZeroCap","title":"ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/VQA-IC/Zero-shot/ZeroCap"}]}')}}]);