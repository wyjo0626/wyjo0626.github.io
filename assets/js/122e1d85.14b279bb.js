"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[4786],{86747:i=>{i.exports=JSON.parse('{"label":"Visual Prompt","permalink":"/docs/tags/visual-prompt","allTagsPath":"/docs/tags","count":24,"items":[{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-11-A2XP","title":"A2XP: Towards Private Domain Generalization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/A2XP"},{"id":"Paper/Vision-Language/PEFT/Multi-Modality/2023-12-AMMPL","title":"Adaptive Multi-Modality Prompt Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Multi-Modality/AMMPL"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-03-ADAVIPRO","title":"AdaViPro: Region-Based Adaptive Visual Prompt For Large-Scale Models Adapting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/AdaViPro"},{"id":"Paper/Vision-Language/PEFT/Multi-Modality/2023-12-APoLLo","title":"APoLLo: Unified Adapter and Prompt Learning for Vision Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Multi-Modality/APoLLo"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-10-AutoVP","title":"AutoVP: An Automated Visual Prompting Framework and Benchmark","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/AutoVP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-03-CVP","title":"Convolutional Visual Prompt for Robust Visual Perception","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/CVP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-03-DAM-VP","title":"Diversity-Aware Meta Visual Prompting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/DAM-VP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-07-E2VPT","title":"E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/E2VPT"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-03-EVP-L","title":"Explicit Visual Prompting for Low-Level Structure Segmentations","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/EVP-L"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-04-TVP","title":"Exploring the Transferability of Visual Prompting for Multimodal Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/TVP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-03-VP","title":"Exploring Visual Prompts for Adapting Large-Scale Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/VP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-04-HintAug","title":"Hint-Aug: Drawing Hints from Foundation Vision Transformers towards Boosted Few-shot Parameter-Efficient Tuning","description":"\ub17c\ubb38 \ubc0f image \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/HinTAug"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-11-InMeMo","title":"Instruct Me More! Random Prompting for Visual In-Context Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/InMeMo"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-12-LaViP","title":"LaViP: Language-Grounded Visual Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/LaViP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-10-LPT","title":"LPT: Long-Tailed Prompt Tuning For Image Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/LPT"},{"id":"Paper/Vision-Language/PEFT/Multi-Modality/2022-10-MaPLe","title":"MaPLe: Multi-modal Prompt Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Multi-Modality/MaPLe"},{"id":"Paper/Vision-Language/PEFT/Multi-Modality/2024-01-MMA","title":"MMA: Multi-Modal Adapter for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Multi-Modality/MMA"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-12-SA2VP","title":"SA^2VPT: Spatially Aligned-and-Adapted Visual Prompt","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/SA2VP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-06-SMM","title":"Sample-specific Masks for Visual Reprogramming-based Prompting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/SMM"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-11-ILM-VP","title":"Understanding and Improving Visual Prompting: A Label-Mapping Perspective","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/ILM-VP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-12-EVP","title":"Unleashing the Power of Visual Prompting At the Pixel Level","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/EVP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-03-VPT","title":"Visual Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/VPT"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-10-DePT","title":"Visual Prompt Tuning For Test-time Domain Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/DePT"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-10-Watermarking","title":"Watermarking for Out-of-distribution Detection","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/Watermarking"}]}')}}]);