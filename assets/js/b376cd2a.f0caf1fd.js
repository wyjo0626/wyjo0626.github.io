"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[98817],{19176:e=>{e.exports=JSON.parse('{"label":"Adapter","permalink":"/docs/tags/adapter","allTagsPath":"/docs/tags","count":18,"items":[{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2023-12-CLAP","title":"A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/CLAP"},{"id":"Paper/NLP/PEFT/Module/2022-05-AdaMix","title":"AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Module/AdaMix"},{"id":"Paper/NLP/PEFT/Module/2021-10-AdapterDrop","title":"AdapterDrop: On the Efficiency of Adapters in Transformers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Module/AdapterDrop"},{"id":"Paper/NLP/PEFT/Module/2020-05.AdapterFusion","title":"AdapterFusion: Non-Destructive Task Composition for Transfer Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Module/AdapterFusion"},{"id":"Paper/NLP/PEFT/Composition/2023-03-AdaLoRA","title":"Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/AdaLoRA"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2023-12-APoLLo","title":"APoLLo: Unified Adapter and Prompt Learning for Vision Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/APoLLo"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2021-10-CLIP-Adapter","title":"CLIP-Adapter: Better Vision-Language Models with Feature Adapters","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/CLIP-Adapter"},{"id":"Paper/NLP/PEFT/Module/2021-06-Compacter","title":"COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Module/Compacter"},{"id":"Paper/NLP/PEFT/Module/2020-02-K-Adapter","title":"K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Module/K-Adapter"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-03-LLaMA-Adapter","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/LLaMA-Adapter"},{"id":"Paper/NLP/PEFT/Composition/2021-06-LoRA","title":"LoRA: Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoRA"},{"id":"Paper/NLP/PEFT/Module/2020-05-MAD-X","title":"MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Module/MAD-X"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2023-11-Meta-Adapter","title":"Meta-Adapter: An Online Few-shot Learner for Vision-Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/Meta-Adapter"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/2024-01-MMA","title":"MMA: Multi-Modal Adapter for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Multi-Modality/MMA"},{"id":"Paper/NLP/PEFT/Module/2019-02-Adapter","title":"Parameter-Efficient Transfer Learning for NLP","description":"\ub17c\ubb38 \uc774\ubbf8\uc9c0 \ubc0f \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Module/Adapter"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2022-11-TaskRes","title":"Task Residual for Tuning Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/TaskRes"},{"id":"Paper/Vision-Language/CLIP/Few-shot/Module/2021-11-Tip-Adapter","title":"Tip-Adapter: Training-Free Adaption of CLIP for Few-Shot Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP/Few-shot/Module/Tip-Adapter"},{"id":"Paper/NLP/PEFT/Mixture/2022-05-UniPELT","title":"UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Mixture/UniPELT"}]}')}}]);