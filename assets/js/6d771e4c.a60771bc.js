"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[15922],{3607:a=>{a.exports=JSON.parse('{"label":"Two-Stream","permalink":"/docs/tags/two-stream","allTagsPath":"/docs/tags","count":2,"items":[{"id":"Paper/Vision-Language/Two-Stream/2019-08-LXMERT","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Two-Stream/LXMERT"},{"id":"Paper/Vision-Language/Two-Stream/2019-08-ViLBERT","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Two-Stream/ViLBERT"}]}')}}]);