"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[13412],{7603:e=>{e.exports=JSON.parse('{"label":"BERT","permalink":"/docs/tags/bert","allTagsPath":"/docs/tags","count":2,"items":[{"id":"Paper/NLP/Analysis/2019-11-Context_Representation","title":"How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Analysis/Contextualized Representation"},{"id":"Paper/Vision-Language/Foundation/Two-Stream/2019-08-ViLBERT","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Foundation/Two-Stream/ViLBERT"}]}')}}]);