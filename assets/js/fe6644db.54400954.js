"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[554],{18478:i=>{i.exports=JSON.parse('{"label":"LLM","permalink":"/docs/tags/llm","allTagsPath":"/docs/tags","count":5,"items":[{"id":"Paper/NLP/PEFT/Quantization/2023-10-BitNet","title":"BitNet: Scaling 1-bit Transformers for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/BitNet"},{"id":"Paper/NLP/Multi-Task/2022-01-CoT","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Multi-Task/Chain-of-Thought"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-03-MPT","title":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/Multitask Prompt Tuning"},{"id":"Paper/NLP/PEFT/Quantization/2025-01-fp4","title":"Optimizing Large Language Model Training Using FP4 Quantization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/fp4"},{"id":"Paper/NLP/PEFT/Quantization/2024-02-BitNet-1.58b","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/BitNet"}]}')}}]);