"use strict";(self.webpackChunkwyj_lab=self.webpackChunkwyj_lab||[]).push([[8913],{3905:(a,t,e)=>{e.d(t,{Zo:()=>o,kt:()=>c});var n=e(7294);function i(a,t,e){return t in a?Object.defineProperty(a,t,{value:e,enumerable:!0,configurable:!0,writable:!0}):a[t]=e,a}function l(a,t){var e=Object.keys(a);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(a);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(a,t).enumerable}))),e.push.apply(e,n)}return e}function r(a){for(var t=1;t<arguments.length;t++){var e=null!=arguments[t]?arguments[t]:{};t%2?l(Object(e),!0).forEach((function(t){i(a,t,e[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(a,Object.getOwnPropertyDescriptors(e)):l(Object(e)).forEach((function(t){Object.defineProperty(a,t,Object.getOwnPropertyDescriptor(e,t))}))}return a}function s(a,t){if(null==a)return{};var e,n,i=function(a,t){if(null==a)return{};var e,n,i={},l=Object.keys(a);for(n=0;n<l.length;n++)e=l[n],t.indexOf(e)>=0||(i[e]=a[e]);return i}(a,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(a);for(n=0;n<l.length;n++)e=l[n],t.indexOf(e)>=0||Object.prototype.propertyIsEnumerable.call(a,e)&&(i[e]=a[e])}return i}var p=n.createContext({}),m=function(a){var t=n.useContext(p),e=t;return a&&(e="function"==typeof a?a(t):r(r({},t),a)),e},o=function(a){var t=m(a.components);return n.createElement(p.Provider,{value:t},a.children)},u="mdxType",d={inlineCode:"code",wrapper:function(a){var t=a.children;return n.createElement(n.Fragment,{},t)}},g=n.forwardRef((function(a,t){var e=a.components,i=a.mdxType,l=a.originalType,p=a.parentName,o=s(a,["components","mdxType","originalType","parentName"]),u=m(e),g=i,c=u["".concat(p,".").concat(g)]||u[g]||d[g]||l;return e?n.createElement(c,r(r({ref:t},o),{},{components:e})):n.createElement(c,r({ref:t},o))}));function c(a,t){var e=arguments,i=t&&t.mdxType;if("string"==typeof a||i){var l=e.length,r=new Array(l);r[0]=g;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=a,s[u]="string"==typeof a?a:i,r[1]=s;for(var m=2;m<l;m++)r[m]=e[m];return n.createElement.apply(null,r)}return n.createElement.apply(null,e)}g.displayName="MDXCreateElement"},878:(a,t,e)=>{e.r(t),e.d(t,{assets:()=>p,contentTitle:()=>r,default:()=>d,frontMatter:()=>l,metadata:()=>s,toc:()=>m});var n=e(7462),i=(e(7294),e(3905));const l={slug:"LLaMA-Adapter V2",title:"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",tags:["PEFT","Adapter V2","LLaMA"]},r=void 0,s={unversionedId:"Paper/NLP/PEFT/2023-09-09-LLaMA-Adapter V2",id:"Paper/NLP/PEFT/2023-09-09-LLaMA-Adapter V2",title:"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",description:"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :",source:"@site/docs/Paper/NLP/PEFT/2023-09-09-LLaMA-Adapter V2.md",sourceDirName:"Paper/NLP/PEFT",slug:"/Paper/NLP/PEFT/LLaMA-Adapter V2",permalink:"/docs/Paper/NLP/PEFT/LLaMA-Adapter V2",draft:!1,editUrl:"https://github.com/whdnjsdyd111/whdnjsdyd111.github.io/tree/master/docs/docs/Paper/NLP/PEFT/2023-09-09-LLaMA-Adapter V2.md",tags:[{label:"PEFT",permalink:"/docs/tags/peft"},{label:"Adapter V2",permalink:"/docs/tags/adapter-v-2"},{label:"LLaMA",permalink:"/docs/tags/l-la-ma"}],version:"current",frontMatter:{slug:"LLaMA-Adapter V2",title:"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",tags:["PEFT","Adapter V2","LLaMA"]},sidebar:"tutorialSidebar",previous:{title:"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention",permalink:"/docs/Paper/NLP/PEFT/LLaMA-Adapter"},next:{title:"GPT Understands, Too",permalink:"/docs/Paper/NLP/PEFT/P-tuning"}},p={},m=[{value:"Zero-initialized Attention",id:"zero-initialized-attention",level:3},{value:"Simple Multi-modal Variant",id:"simple-multi-modal-variant",level:3},{value:"Open-ended Multi-modal Reasoning",id:"open-ended-multi-modal-reasoning",level:3},{value:"4.1 Bias Tuning of Linear Layers",id:"41-bias-tuning-of-linear-layers",level:2},{value:"Discussion",id:"discussion",level:3},{value:"4.2 Joint Training with Disjoint Parameters",id:"42-joint-training-with-disjoint-parameters",level:2},{value:"Discussion",id:"discussion-1",level:3},{value:"4.3 Early Fusion of Visual Knowledge",id:"43-early-fusion-of-visual-knowledge",level:2},{value:"4.4 Integration with Experts",id:"44-integration-with-experts",level:2},{value:"5.1 Experimental Setups",id:"51-experimental-setups",level:2},{value:"Training Data",id:"training-data",level:3},{value:"Implementation Details",id:"implementation-details",level:3},{value:"5.2 Stronger Language Instruction Model",id:"52-stronger-language-instruction-model",level:2},{value:"5.3 Visual Instruction Model",id:"53-visual-instruction-model",level:2},{value:"Image Captioning",id:"image-captioning",level:3},{value:"Visual Understanding",id:"visual-understanding",level:3},{value:"Integration with Experts",id:"integration-with-experts",level:3}],o={toc:m},u="wrapper";function d(a){let{components:t,...l}=a;return(0,i.kt)(u,(0,n.Z)({},o,l,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 : ",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/2304.15010.pdf"},"https://arxiv.org/pdf/2304.15010.pdf")),(0,i.kt)("h1",{id:"abstract"},"Abstract"),(0,i.kt)("p",null,"\ucd5c\uadfc LLaMA-Adapter \uac00 LLMs \uc640 visual input \uc744 \ub2e4\ub8e8\ub294 \uc7a0\uc7ac\ub825\uc744 \ubcf4\uc5ec\uc8fc\uc9c0\ub9cc \uc5ec\uc804\ud788 open-ended visual instruction \uc740 \uc798 \ucc98\ub9ac\ud558\uc9c0 \ubabb\ud558\uba70 GPT-4 \uc5d0 \ub4a4\uccd0\uc9c0\uace0 \uc788\ub2e4."),(0,i.kt)("p",null,"\ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 parameter-efficient visual instruction model \uc778 ",(0,i.kt)("strong",{parentName:"p"},"LLaMA-Adapter V2")," \uc81c\uc548"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"LLaMA-Adapter \ub97c \ub354 \ub9ce\uc740 learnable parameter (norm, bias \ubc0f scale \ub4f1) \uc744 \ud65c\uc6a9\ud558\uc5ec \ubcf4\uac15",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Adapter \uc678\uc758 \uc804\uccb4 LLaMA model \uc758 instruction-following \ub2a5\ub825\uc744 \ubd84\uc0b0"))),(0,i.kt)("li",{parentName:"ul"},"visual token \uc744 early LLM layer \uc5d0\ub9cc \uc8fc\uc785\ud558\ub294 early fusion \uc804\ub7b5\uc744 \uc81c\uc548",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"visual knowledge incorporation \uac1c\uc120"))),(0,i.kt)("li",{parentName:"ul"},"image-text pair \ubc0f instruction-following data \uc758 joint training paradigm \uc744 \ub3c4\uc785",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"learnable parameter \uc758 \ubd84\ub9ac\ub41c \uadf8\ub8f9\uc744 \ucd5c\uc801\ud654"),(0,i.kt)("li",{parentName:"ul"},"image-text alignment \ubc0f instruction-following \ub450 \uc791\uc5c5 \uac04\uc758 \uac04\uc12d\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \uc644\ud654"),(0,i.kt)("li",{parentName:"ul"},"\uc18c\uaddc\ubaa8 image-text \ubc0f instruction-following dataset \ub9cc\uc73c\ub85c \uac15\ub825\ud55c multi-modal reasoning \ub2ec\uc131")))),(0,i.kt)("p",null,"inference \ub2e8\uacc4"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"LLaMA-Adapter \uc5d0 \ucd94\uac00\uc801\uc778 expert models (e.g. captioning/OCR) \uc744 \ud1b5\ud569"),(0,i.kt)("li",{parentName:"ul"},"training cost \ubc1c\uc0dd\ud558\uc9c0 \uc54a\uace0 image understanding \ub2a5\ub825 \ub354\uc6b1 \ud5a5\uc0c1")),(0,i.kt)("p",null,"\uae30\uc874 LLaMA-Adapter \uc640 \ube44\uad50\ud558\uc5ec LLaMA-Adapter V2 \ub294 LLaMA \uc5d0 14M parameter \ucd94\uac00\ub9cc\uc73c\ub85c open-ended multi-modal instruction \uc218\ud589\u3145 \uac00\ub2a5"),(0,i.kt)("p",null,"\uadf8\ub9ac\uace0 \uc774 \uc124\uacc4\ub294 language-only instruction-following \uc744 \ub354\uc6b1 \uac15\ud654\uc2dc\ud0a4\uba70 \ucc44\ud305 \uc0c1\ud638\uc791\uc6a9\uc5d0\ub3c4 \ub6f0\uc5b4\ub09c \uc131\ub2a5\uc744 \ubcf4\uc784"),(0,i.kt)("h1",{id:"1-introduction"},"1. Introduction"),(0,i.kt)("p",null,"\ucd5c\uadfc LLM \uc744 instruction-following model \ub85c \ubcc0\ud658\ud558\ub294 \uc5f0\uad6c\uac00 \uc9c4\ud589"),(0,i.kt)("p",null,"Stanford Alpaca \ub294 InstructGPT model \ub85c \uc0dd\uc131\ub41c instruction examples \ub97c \uc0ac\uc6a9\ud558\uc5ec LLaMA \ub97c instruction-following model \ub85c fine-tuning \ud55c\ub2e4."),(0,i.kt)("p",null,"LLaMA-Adapter \ub294 \uac00\ubcbc\uc6b4 Adapter \uc640 zero-initialized attention \uc758 \ub3c4\uc785\uc73c\ub85c frozen LLaMA \uc5d0\uac8c parameter-efficient fine-tuning \uc73c\ub85c multi-modal knowledge \ub97c \uc8fc\uc785\ud55c\ub2e4."),(0,i.kt)("p",null,"\uac00\uc7a5 \ucd5c\uadfc\uc740 MiniGPT-4 \ubc0f LLaVA \uac19\uc740 \uc5f0\uad6c\ub85c, language-only instruction model \uc744 multi-modal \ub85c \ud655\uc7a5\ud558\uc5ec visual reasoning \ub2a5\ub825\uc744 \ubd80\uc5ec\ud558\ub294 \uc0c8\ub85c\uc6b4 \uc5f0\uad6c \ud30c\ub3d9\uc744 \uc77c\uc73c\ucf30\ub2e4."),(0,i.kt)("p",null,"\ubcf8 \ub17c\ubb38\uc740 parameter-efficient visual instruction model \uc124\uacc4\ub97c \ubaa9\ud45c\ub85c \ud55c\ub2e4."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"LLaMA-Adapter \uae30\ubc18\uc758 \uc0c8\ub85c\uc6b4 method \uc778 ",(0,i.kt)("strong",{parentName:"li"},(0,i.kt)("em",{parentName:"strong"},"LLaMA-Adapter V2"))," \uac1c\ubc1c",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"LLaMA-Adapter \ub294 instruction-following model"),(0,i.kt)("li",{parentName:"ul"},"visual feature \ub97c adaptation prompts \ub85c \uc8fc\uc785\ud558\uc5ec visual instruction model \ub85c \ubcc0\ud658"),(0,i.kt)("li",{parentName:"ul"},"multi-modal instruction tuning data \uc758 \ubd80\uc871\uc73c\ub85c \uc804\ud1b5\uc801\uc778 vision-language model \ub85c \uc81c\ud55c\ub428"),(0,i.kt)("li",{parentName:"ul"},'\uc608\ub85c, COCO Caption \uc5d0\uc11c \ud6c8\ub828\ub41c LLaMA-Adapter \ub294 "\u201cGenerate caption for this image" \uac19\uc740 prompt \uc5d0 \ub300\ud574 \uc9e7\uc740 caption \ub9cc \uc0dd\uc131 \uac00\ub2a5'),(0,i.kt)("li",{parentName:"ul"},"\ubcf5\uc7a1\ud55c visual reasoning \ubc0f QA task \uac19\uc740 open-ended multi-modal instruction \uc5d0\ub294 adaptation \uc774 \ubd88\uac00\ub2a5"))),(0,i.kt)("li",{parentName:"ul"},"frozen LLaMA-Adapter \ub97c \uc0ac\uc6a9\ud558\uc5ec image-text pairs \uc5d0\uc11c visual projection layer \ub97c \ucd5c\uc801\ud654\ud558\uc5ec vision-language alignment \ub97c \ubcf4\uc7a5\ud558\ub3c4\ub85d \uac1c\uc120",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"visual feature \uac00 adaptation prompts \uc5d0 \ub450\ub4dc\ub7ec\uc9c0\uba70 instruction-following \ub2a5\ub825\uc774 \ube60\ub974\uac8c \uc800\ud558\ub418\ub294 \uac83 \uad00\ucc30"))),(0,i.kt)("li",{parentName:"ul"},"\uc774\ub97c \ub300\uc751\ud558\uae30 \uc704\ud574 image-text alignment \uc640 language instruction funing \ub450 \uac00\uc9c0 task \uac04\uc758 \uac04\uc12d\uc744 \ud574\uacb0\ud558\ub294 \uac04\ub2e8\ud55c ",(0,i.kt)("strong",{parentName:"li"},(0,i.kt)("em",{parentName:"strong"},"early fusion of visual knowledge"))," \uc81c\uc548",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"LLaMA-Adapter \uc758 dynamic visual prompts \ub294 last ",(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"L")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"L")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6833em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"L")))))," layer \uc758 static adaptation prompts \uc5d0 \ud1b5\ud569"),(0,i.kt)("li",{parentName:"ul"},"LLaMA-Adapter V2 \uc5d0\uc11c\ub294 dynamic visual prompt \ub97c \ucc98\uc74c ",(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"K")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"K")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6833em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.07153em"}},"K")))))," layer \uc5d0\ub9cc \ubd84\ubc30",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"K"),(0,i.kt)("mo",{parentName:"mrow"},"<"),(0,i.kt)("mi",{parentName:"mrow"},"N"),(0,i.kt)("mo",{parentName:"mrow"},"\u2212"),(0,i.kt)("mi",{parentName:"mrow"},"L")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"K < N - L")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.7224em",verticalAlign:"-0.0391em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.07153em"}},"K"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.kt)("span",{parentName:"span",className:"mrel"},"<"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.10903em"}},"N"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.kt)("span",{parentName:"span",className:"mbin"},"\u2212"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6833em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"L")))))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"N")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"N")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6833em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.10903em"}},"N")))))," : total number of Transformer layers"))),(0,i.kt)("li",{parentName:"ul"},"\uc774\ub97c \ud1b5\ud574 image-text alignment \uac00 model \uc758 instruction-following \ub2a5\ub825 \ubc29\ud574\ud558\uc9c0 \uc54a\uc74c"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},(0,i.kt)("em",{parentName:"strong"},"joint training with disjoint parameter")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"\uace0\ud488\uc9c8\uc758 multi-modal instruction data \uc5c6\uc774, image caption \ubc0f instruction-following data \ub85c \ubd84\ub9ac\ub41c parameter \ub97c joint training \ud558\uc5ec \uc6b0\uc218\ud55c visual instruction learning \uac00\ub2a5"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},(0,i.kt)("em",{parentName:"strong"},"bias tuning of linear layers")),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"LLaMA-Adapter \ub97c normalization, layer bias \ubc0f scale \uac19\uc740 learnable parameter \ub97c unlocking \ud558\uc5ec \ubcf4\uc644"),(0,i.kt)("li",{parentName:"ul"},"tunable capacity \ub97c \uc99d\uac00\uc2dc\ucf1c instruction-following knowledge \ub97c LLM \uc804\uccb4\uc5d0 \ubd84\uc0b0"),(0,i.kt)("li",{parentName:"ul"},"\uc774\ub7ec\ud55c parameter \ub294 \ubaa8\ub378 \uc804\uccb4\uc758 \uc57d 0.04% \ub9cc \ucc28\uc9c0"),(0,i.kt)("li",{parentName:"ul"},"\uc774\ub97c \ud1b5\ud574 parameter-efficient approach \uc720\uc9c0"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},(0,i.kt)("em",{parentName:"strong"},"additional expert models"))," (captioning, detection \ubc0f OCR system)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"expert model \uacfc \ud611\ub825\ud558\uc5ec LLaMA-Adapter V2 \ub294 \ub300\uaddc\ubaa8 image-text pair \ubd88\ud544\uc694"),(0,i.kt)("li",{parentName:"ul"},"\ub2e4\uc591\ud55c expert \ub97c plugging \uac00\ub2a5\ud558\uc5ec \uc720\uc5f0\uc131 \uc5bb\uc74c")))),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Figure 1",src:e(602).Z,width:"542",height:"550"})),(0,i.kt)("p",null,"\ub2e4\uc74c\uc740 \uc8fc\uc694 \uae30\uc5ec \uc694\uc57d"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Stronger Language Instruction Model",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"parameter-efficient tuning \ubc0f high-quality language instruction data \uc0ac\uc6a9\ud558\uc5ec LLaMA-Adapter \ub2a5\uac00"))),(0,i.kt)("li",{parentName:"ul"},"Balanced Visual Instruction Tuning",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"image-text alignment \uc640 instruction-following object \uac04\uc758 \uac04\uc12d \ud574\uacb0\uc744 \uc704\ud574 early fusion \uc804\ub7b5 \uc0ac\uc6a9"),(0,i.kt)("li",{parentName:"ul"},"multi-modal instruction training data \uc5c6\uc774 captioning data \ubc0f instruction-following data \uc758 \ubd84\ub9ac\ub41c parameter \ub97c joint training"))),(0,i.kt)("li",{parentName:"ul"},"Integration of Expert Systems",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"\ub2e4\uc591\ud55c expert model \ud1b5\ud569")))),(0,i.kt)("h1",{id:"2-related-work"},"2. Related Work"),(0,i.kt)("h1",{id:"3-a-revisit-of-llama-adapter"},"3. A Revisit of LLaMA-Adapter"),(0,i.kt)("h3",{id:"zero-initialized-attention"},"Zero-initialized Attention"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"instruction-following \ub2a5\ub825 \uc2b5\ub4dd\uc744 \uc704\ud55c parameter-efficient fine-tuning"),(0,i.kt)("li",{parentName:"ul"},"LLaMA-Adapter \ub294 LLaMA \ub97c freezing \ud558\uace0 1.2M \ucd94\uac00 adapter module \ub9cc \ub3c4\uc785"),(0,i.kt)("li",{parentName:"ul"},"adapter layer \ub294 LLaMA \uc758 Transformer layer \uc0c1\ub2e8\uc5d0 \uc0ac\uc6a9"),(0,i.kt)("li",{parentName:"ul"},"leanable soft prompt set \uc744 word token \uc758 prefix \uc5d0 \uc5f0\uacb0"),(0,i.kt)("li",{parentName:"ul"},"new adapting knowledge \ub97c LLaMA \uc5d0 \ud1b5\ud569\ud558\uae30 \uc704\ud574 zero-initialized attention \uc0ac\uc6a9"),(0,i.kt)("li",{parentName:"ul"},"\uc774\ub97c \ud1b5\ud574 adaptation prompt word token \uc5d0 \ub300\ud55c \uae30\uc5ec\ub97c \ud559\uc2b5 \ucd08\uae30\uc5d0 0\uc73c\ub85c \ucd08\uae30\ud654\ub41c gating factor \ub97c \ud559\uc2b5\ud558\uc5ec \uc870\uc808"),(0,i.kt)("li",{parentName:"ul"},"\ud6c8\ub828 \uc911 gating \ud06c\uae30\ub294 \uc810\uc9c4\uc801\uc73c\ub85c \ucee4\uc9c0\uba70 LLaMA \uc5d0 \uc8fc\uc785")),(0,i.kt)("p",null,"\uc774 \uc804\ub7b5\uc740 \ud6c8\ub828 \ucd08\uae30, LLaMA \uc758 \uc5b8\uc5b4 \uc0dd\uc131 \ub2a5\ub825\uc744 \ubcf4\uc874\ud558\uba70 \uc0c8\ub85c\uc6b4 \uc9c0\uc2dd\uc744 \uc9c0\uc18d\uc801\uc73c\ub85c \ud1b5\ud569\ud558\uc5ec \uac15\ub825\ud55c instruction-following \ub2a5\ub825\uc744 \ub9cc\ub4e0\ub2e4."),(0,i.kt)("h3",{id:"simple-multi-modal-variant"},"Simple Multi-modal Variant"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"multi-modal reasoning \uc744 \uc704\ud574 image \ubc0f video \ud1b5\ud569 \uac00\ub2a5"),(0,i.kt)("li",{parentName:"ul"},"pre-trained visual encoder \ub97c \uc9c0\ub2cc CLIP \uc73c\ub85c multi-scale visual feature \ucd94\ucd9c"),(0,i.kt)("li",{parentName:"ul"},"learnable projection layer \ub97c \ud1b5\uacfc\ud558\uc5ec visual semantics \ub97c language embedding space \uc640 alignment"),(0,i.kt)("li",{parentName:"ul"},"\uc774\ud6c4 visual feature \ub294 Transformer layer \uc0c1\ub2e8\uc5d0\uc11c element-wisely added")),(0,i.kt)("p",null,"\uc704 \uacfc\uc815\uc73c\ub85c LLaMA-Adapter \ub294 text \ubc0f visual input \uc744 \uae30\ubc18\uc73c\ub85c response \uc0dd\uc131 \uac00\ub2a5\ud558\uc5ec ScienceQA \uc5d0\uc11c comparable"),(0,i.kt)("h3",{id:"open-ended-multi-modal-reasoning"},"Open-ended Multi-modal Reasoning"),(0,i.kt)("p",null,"LLaMA-Adapter \ub97c \uc0ac\uc6a9\ud558\uc5ec COCO Caption dataset \uc5d0\uc11c adapter module \ubc0f visual projection layer \ub97c fine-tuning \ud558\ub294 \uc2e4\ud5d8 \uc218\ud589"),(0,i.kt)("p",null,"\uc0c8\ub85c\uc6b4 visual \ub2e8\uc11c\uac00 adaptation prompt \ub97c \uac04\uc12d\ud558\ub294 \uacbd\ud5a5\uc774 \ub098\ud0c0\ub098 instruction-following feature \ub97c \ub36e\uc5b4\ubc84\ub9bc."),(0,i.kt)("p",null,"\ub530\ub77c\uc11c LLaMA-Adapter V2 \ub97c \uc81c\uc548\ud558\uc5ec, multi-modal \uc7a0\uc7ac\ub825\uc744 \uc644\uc804\ud788 \ubc1c\ud718\ud558\ub3c4\ub85d \ud568"),(0,i.kt)("h1",{id:"4-llama-adapter-v2"},"4. LLaMA-Adapter V2"),(0,i.kt)("h2",{id:"41-bias-tuning-of-linear-layers"},"4.1 Bias Tuning of Linear Layers"),(0,i.kt)("p",null,"LLaMA-Adapter \ub294 zero-initialized attention \uc73c\ub85c adaptation prompt \ub97c frozen LLaMA \uc5d0 \uc0ac\uc6a9"),(0,i.kt)("p",null,"\uc774\ub294 new knowledge \ub97c \ud1b5\ud569\ud558\uc9c0\ub9cc LLM \ub0b4\ubd80 parameter \ub97c \uc218\uc815\ud558\uc9c0 \uc54a\uace0\ub294 parameter update \uac00 adaptation prompt \ubc0f gating factor \ub85c \uc81c\ud55c\ub428"),(0,i.kt)("p",null,"\uc774 \ub54c\ubb38\uc5d0 deep fine-tuning \uc218\ud589 \ub2a5\ub825\uc774 \uc81c\ud55c\ub41c\ub2e4."),(0,i.kt)("p",null,"\uc774\ub97c \uace0\ub824\ud558\uc5ec \ub354\uc6b1 \ud6a8\uacfc\uc801\uc778 \ud1b5\ud569\uc744 \uc704\ud574 ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("em",{parentName:"strong"},"bias tuning"))," \uc804\ub7b5 \uc81c\uc548"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"instruction-following data \ub97c adaptively handle \ud558\uae30 \uc704\ud574 LLaMA \uc758 \ubaa8\ub4e0 normalization layers \ub97c unfreezing"),(0,i.kt)("li",{parentName:"ul"},"Transformer \uc758 \uac01 linear layer \uc5d0 \ub300\ud574 learnable parameter \ub85c bias \ubc0f scale factor \ucd94\uac00"),(0,i.kt)("li",{parentName:"ul"},"\ud2b9\uc815 linear layer \ubc0f pre-trained weights \ub97c ",(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mtext",{parentName:"mrow"},"x")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\text{x}")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.4306em"}}),(0,i.kt)("span",{parentName:"span",className:"mord text"},(0,i.kt)("span",{parentName:"span",className:"mord"},"x"))))))," \ubc0f ",(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"W")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"W")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6833em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.13889em"}},"W")))))," \ub85c \ud45c\uc2dc"),(0,i.kt)("li",{parentName:"ul"},"LLaMA-Adapter V2 \uc5d0\uc120, bias ",(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"b")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"b")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6944em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"b")))))," \ubc0f scale ",(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"s")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"s")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.4306em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"s")))))," \ub97c \uc0ac\uc6a9\ud558\uc5ec linear layer \uc218\uc815")),(0,i.kt)("div",{className:"math math-display"},(0,i.kt)("span",{parentName:"div",className:"katex-error",title:"ParseError: KaTeX parse error: Multiple \\tag",style:{color:"#cc0000"}},"\\begin{align} y = W \\cdot \\text{x} \\rightarrow y = s \\cdot (W \\cdot \\text{x} + b), \\tag{1} \\\\ \\text{where} \\ b = \\text{Init}(0),\\ s = \\text{Init}(1). \\tag{2} \\end{align}")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"bias \uc640 scale factor \ub294 \uac01\uac01 0 \uacfc 1 \ub85c \ucd08\uae30\ud654\ud558\uc5ec \ucd08\uae30 \ub2e8\uacc4\uc5d0 \uc548\uc815\ud654"),(0,i.kt)("li",{parentName:"ul"},"bias tuning \ubc0f high-quality instruction data \ub97c \ud1b5\ud569\ud558\uc5ec \uc6b0\uc218\ud55c instruction-following \ub2a5\ub825\uc744 \uc5bb\uc74c"),(0,i.kt)("li",{parentName:"ul"},"\ud2b9\ud788, newly added parameter \ub294 \uc804\uccb4 LLaMA \uc758 0.04% (\uc57d 5M) \ub9cc \ucc28\uc9c0"),(0,i.kt)("li",{parentName:"ul"},"LLaMA-Adapter V2 \ub294 \uc5ec\uc804\ud788 highly parameter-efficient approach")),(0,i.kt)("h3",{id:"discussion"},"Discussion"),(0,i.kt)("p",null,"bias tuning \uc740 \uc774\uc804 parameter-efficient method \uc640 \uc720\uc0ac"),(0,i.kt)("p",null,"BERT fine-tuning \uc744 \uc704\ud55c BitFit \ubc0f visual prompt tuning \uc744 \uc704\ud55c SSF \uac00 \uc788\uc74c"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"BitFit \ubc0f SSF \ub294 80M parameter scale \uc744 \uac00\uc9c4 comprehension task \ub97c \uc704\ud574 \uc124\uacc4"),(0,i.kt)("li",{parentName:"ul"},"\uc800\uc790\uc758 approach \ub294 70B - 650B parameter scale \uc758 LLM \uc5d0\uc11c \ud6a8\uc728\uc131 \ub098\ud0c0\ub0c4"),(0,i.kt)("li",{parentName:"ul"},"bias tuning \uc740 input \uc5d0 \ub3c5\ub9bd\uc801\uc774\uba70, row-rank \ub97c \uc0ac\uc6a9\ud558\uc5ec input \uc5d0 \uc758\uc874\uc801\uc778 bias \ub97c \ucd94\uac00\ud558\ub294 LoRA \uc640\ub294 \ub2ec\ub9ac, fine-tuning \ube44\uc6a9\uc744 \ub354 \uc904\uc784")),(0,i.kt)("h2",{id:"42-joint-training-with-disjoint-parameters"},"4.2 Joint Training with Disjoint Parameters"),(0,i.kt)("p",null,"\uc800\uc790\uc758 \ubaa9\ud45c\ub294 LLaMA-Adapter V2 \uc5d0\uac8c long language response \ub97c \uc0dd\uc131\ud558\ub294 \ub2a5\ub825\uacfc multi-modal understanding \uc744 \ub3d9\uc2dc\uc5d0 \ubd80\uc5ec\ud558\ub294 \uac83"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Figure 2",src:e(6446).Z,width:"774",height:"433"})),(0,i.kt)("p",null,"\uc800\uc790\ub294 LLaMA-Adapter V2 \ub97c \uc704\ud574 image-text captioning data \ubc0f language-only instruction examples \ub97c \ud65c\uc6a9\ud558\uae30 \uc704\ud55c ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("em",{parentName:"strong"},"joint training paradigm"))," \uc81c\uc548"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"500K image-text pairs \ubc0f 50K instruction data \uc0ac\uc774\uc758 \ub370\uc774\ud130\uc591 \ucc28\uc774\ub85c \uc778\ud574, instruction-following \ub2a5\ub825\uc5d0 \ud53c\ud574\uac00 \uac08 \uc218 \uc788\uc74c"),(0,i.kt)("li",{parentName:"ul"},"\ub530\ub77c\uc11c ",(0,i.kt)("strong",{parentName:"li"},(0,i.kt)("em",{parentName:"strong"},"\uc774\uc9c8\uc801\uc778 (disjoint) parameter groups"))," \ub97c \ucd5c\uc801\ud654",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"image-text captioning data : visual projection layer \ubc0f \ucd08\uae30 zero-initialized gating \uacfc \uad00\ub828\ub41c \ubd80\ubd84\ub9cc \ud559\uc2b5"),(0,i.kt)("li",{parentName:"ul"},"language instruction data : late adaptation prompts \uc640 zero gating, unfrozen norm \ubc0f newly added bias \ubc0f scale factor (\uc120\ud0dd\uc801\uc73c\ub85c low-rank adaptation) \uac00 \uc0ac\uc6a9")))),(0,i.kt)("p",null,"\uc774\ub97c \ud1b5\ud574 image-text understanding \uacfc instruction-following \uac04\uc758 \uac04\uc12d \ubb38\uc81c\ub97c \uc790\uc5f0\uc2a4\ub7fd\uac8c \ud574\uacb0"),(0,i.kt)("h3",{id:"discussion-1"},"Discussion"),(0,i.kt)("p",null,"joint training \uc804\ub7b5 \ub355\uc5d0 MiniGPT-4 \uc640 LLaVA \uac19\uc740 high-quality multi-modal instruction data \uac00 \ubd88\ud544\uc694"),(0,i.kt)("p",null,"\ub300\uc2e0 image-text pairs \ubc0f instruction-following \ub370\uc774\ud130\ub9cc \uc694\uad6c (Fig. 1 \uc5d0\uc11c \ube44\uad50)"),(0,i.kt)("p",null,"captioning data \ub294 Fig. 2 \uc5d0\uc11c \ucc98\ub7fc short answers \ub97c \ud3ec\ud568\ud558\uc5ec image understanding \uc5d0 \ub300\ud55c LLM \uc744 \ud655\uc7a5\ud558\ub294 \uc5ed\ud560\uc744 \ud574\uc900\ub2e4."),(0,i.kt)("p",null,"\ud55c\ud3b8 language-only instruction data \ub294 long detailed sentences \ub97c \uc0dd\uc131\ud560 \ub2a5\ub825\uc744 \ubcf4\uc874\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ud55c\ub2e4."),(0,i.kt)("p",null,"\uc704\uc640 \uac19\uc740 \uc0c1\ud638\ubcf4\uc644\uc801\uc73c\ub85c \uc870\ud569\ud558\uc5ec LLaMA-Adapter V2 \ub294 high-quality instruction data \uc5c6\uc774 \uc18c\uaddc\ubaa8\uc758 image-text \ubc0f instruction-following data \ub9cc\uc73c\ub85c \uc6b0\uc218\ud55c multi-modal reasoning \uc744 \ub2ec\uc131"),(0,i.kt)("h2",{id:"43-early-fusion-of-visual-knowledge"},"4.3 Early Fusion of Visual Knowledge"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Figure 3",src:e(866).Z,width:"377",height:"323"})),(0,i.kt)("p",null,"vision \ubc0f language fine-tuning \uac04\uc758 \uac04\uc12d\uc744 \ud53c\ud558\uae30 \uc704\ud574, visual prompts \uc640 adaptation prompts \uac04\uc758 \uc9c1\uc811\uc801\uc778 \uc0c1\ud638\uc791\uc6a9 \ubc29\uc9c0\ub97c \uc704\ud574 ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("em",{parentName:"strong"},"early fusion"))," \uc81c\uc548"),(0,i.kt)("p",null,"LLaMA-Adapter \uc5d0\uc120 visual prompt input \uc774 frozen visual encoder \uc5d0 \uc758\ud574 \uc21c\ucc28\uc801\uc73c\ub85c encoding \ub418\uace0 learnable visual projection layer \uc5d0 \uc758\ud574 \ucd94\uac00\ub418\uc5b4 \uac01 inserted layer \uc5d0 adaptation prompt \uacb0\ud569"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"LLaMA-Adapter V2 \ub294 encoded visual tokens \uc640 adaptation prompt \ub97c \uc11c\ub85c\ub2e4\ub978 Transformer layer \uc5d0 fusing \ud558\uc9c0 \uc54a\uace0 \uc0bd\uc785",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"dataset-shared adaptation prompts : LLaMA-Adapter \ub97c \ub530\ub77c, last ",(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"L")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"L")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6833em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"L")))))," layers (e.g. ",(0,i.kt)("span",{parentName:"li",className:"math math-inline"},(0,i.kt)("span",{parentName:"span",className:"katex"},(0,i.kt)("span",{parentName:"span",className:"katex-mathml"},(0,i.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,i.kt)("semantics",{parentName:"math"},(0,i.kt)("mrow",{parentName:"semantics"},(0,i.kt)("mi",{parentName:"mrow"},"L"),(0,i.kt)("mo",{parentName:"mrow"},"="),(0,i.kt)("mn",{parentName:"mrow"},"30")),(0,i.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"L=30")))),(0,i.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6833em"}}),(0,i.kt)("span",{parentName:"span",className:"mord mathnormal"},"L"),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.kt)("span",{parentName:"span",className:"mrel"},"="),(0,i.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,i.kt)("span",{parentName:"span",className:"base"},(0,i.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6444em"}}),(0,i.kt)("span",{parentName:"span",className:"mord"},"30"))))),") \uc5d0 \uc0bd\uc785"),(0,i.kt)("li",{parentName:"ul"},"input visual prompts : first Transformer layer with zero-initialized attention \uc5d0\uc11c\uc758 word token \uc5d0 \uc9c1\uc811 \uc5f0\uacb0")))),(0,i.kt)("p",null,"\uc774 early fusion \uc73c\ub85c \ub450 \uac00\uc9c0 fine-tuning target \uac04\uc758 \ucda9\ub3cc\uc744 \ud6a8\uacfc\uc801\uc73c\ub85c \ud574\uacb0\ud558\ub294\ub370 \ub3c4\uc6c0\ub418\uba70, proposed joint training \uacfc \ud568\uaed8 \uc0ac\uc6a9\ud558\uc5ec \uc6b0\uc218\ud55c multi-modal reasoning \ub2a5\ub825\uc744 \uac00\uc9d0"),(0,i.kt)("h2",{id:"44-integration-with-experts"},"4.4 Integration with Experts"),(0,i.kt)("p",null,"MiniGPT4 \ubc0f LLaVA \ubaa8\ub378\ub4e4\uc740 visual model \uacfc LLM \uc5f0\uacb0\uc744 \uc704\ud574 \ub300\uaddc\ubaa8 image-text \ud6c8\ub828\uc774 \ud544\uc694"),(0,i.kt)("p",null,"\uc774\uc5d0 \ubc18\ud574, LLaMA-Adapter V2 \ub294 \uc791\uc740 \uaddc\ubaa8\uc758 image captioning data \uc5d0 fine-tuning \ud558\uc5ec \ub192\uc740 data-efficient"),(0,i.kt)("p",null,"\ud558\uc9c0\ub9cc \uc774 \ubc29\ubc95\uc758 image understanding \ub2a5\ub825\uc774 \ube44\uad50\uc801 \uc57d\ud558\uc5ec \ub54c\ub85c \ubd80\uc815\ud655\ud558\uac70\ub098 \uad00\ub828 \uc5c6\uc740 \uc751\ub2f5\uc744 \uc720\ubc1c"),(0,i.kt)("p",null,"\ub354 \ub9ce\uc740 image-text data \uc218\uc9d1 \ubc0f \uac15\ub825\ud55c multi-modal module \ub3c4\uc785 \ub300\uc2e0, caption, OCR \ubc0f search engines \uac19\uc740 expert system \uc744 \ud1b5\ud569\ud558\uc5ec ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("em",{parentName:"strong"},"additional visual reasoning proficiency"))," \uc744 \ubd80\uc5ec\ud558\ub294 \uac83\uc744 \uc81c\uc548"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Figure 4",src:e(1628).Z,width:"723",height:"923"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"\uc800\uc790\ub294 caption, detection \ubc0f OCR \uac19\uc740 expert system \uc73c\ub85c visual instruction-following \ub2a5\ub825 \ud5a5\uc0c1"),(0,i.kt)("li",{parentName:"ul"},"input image \ub97c \uace0\ub824\ud558\uc5ec pre-trained visual encoder \ub97c \uc0ac\uc6a9\ud558\uc5ec visual context \ub97c encoding \ud558\uace0 expert system \uc5d0\uac8c textual context \uc758 caption \uc744 \uc0dd\uc131\ud558\ub3c4\ub85d \uc694\uccad"),(0,i.kt)("li",{parentName:"ul"},"COCO Caption \uc5d0 pre-trainig \ub41c LLaMA-Adapter \ub97c expert system \uc73c\ub85c \ucc44\ud0dd"),(0,i.kt)("li",{parentName:"ul"},"\uc5b4\ub5a0\ud55c image \ubc0f text model \ub610\ub294 search engine \uc744 \uc774 expert system \uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc74c\uc744 \uc8fc\ubaa9")),(0,i.kt)("p",null,"\uc704 approach \ub294 \ud2b9\uc815 downstream task \uc5d0 \ub530\ub77c \ub2e4\uc591\ud55c expert system \uac04\uc5d0 \uc27d\uac8c \uc804\ud658 \uac00\ub2a5"),(0,i.kt)("h1",{id:"5-experiments"},"5. Experiments"),(0,i.kt)("h2",{id:"51-experimental-setups"},"5.1 Experimental Setups"),(0,i.kt)("h3",{id:"training-data"},"Training Data"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Table 1",src:e(4107).Z,width:"1736",height:"445"})),(0,i.kt)("p",null,"LLaMA-Adapter V2 \ub294 52K single-turn instruction data from GPT-4-LLM \ubc0f 567K captioning data from COCO Caption \uc5d0 \ud6c8\ub828"),(0,i.kt)("p",null,"MiniGPT-4 \ubc0f LLaVA \uc640 \ub2ec\ub9ac \uc5b4\ub5a0\ud55c visual instruction data \ub3c4 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc74c"),(0,i.kt)("p",null,"\ub610\ud55c ShareGPT \uc758 80K conversation data \ub97c \uc0ac\uc6a9\ud558\uc5ec chatbot system \ud6c8\ub828"),(0,i.kt)("h3",{id:"implementation-details"},"Implementation Details"),(0,i.kt)("p",null,"32 Transformer layers \ub97c \uc0ac\uc6a9\ud55c LLaMA-7B model \uc758 \uacbd\uc6b0"),(0,i.kt)("p",null,"static adaptation prompts \ub97c last 31 layers \uc5d0 \uc0bd\uc785"),(0,i.kt)("p",null,"dynamic visual prompts \ub294 prompt length \ub97c 20\uc73c\ub85c \uc124\uc815\ud558\uace0, \uccab \ubc88\uc9f8 layer \uc5d0 \uc0bd\uc785"),(0,i.kt)("p",null,"normalization layers, linear layer bias \ubc0f scalie \uc758 \ubaa8\ub4e0 parameter \ub294 training \uc911 update \ub418\uba70, LLaMA \uc758 \ub098\uba38\uc9c0 parameter \ub294 freezing \uc720\uc9c0"),(0,i.kt)("h2",{id:"52-stronger-language-instruction-model"},"5.2 Stronger Language Instruction Model"),(0,i.kt)("p",null,"bias tuning \ubc0f high-quality instruction data \ub97c \uc0ac\uc6a9\ud55c LLaMA-Adapter V2 \ub294 LLaMA \uc758 instruction-following \ub2a5\ub825\uc744 \ud5a5\uc0c1"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Table 2",src:e(6045).Z,width:"1704",height:"1628"})),(0,i.kt)("p",null,"Table 1 \uc758 \uacb0\uacfc\uc5d0\uc11c \ucc98\ub7fc, LLaMA-Adapter V2 \ub294 \uc778\uac04\uc758 \uc9c0\uc2dc\uc5d0 \ud3ec\uad04\uc801\uc778 \ub2f5\ubcc0\uacfc \uc0c1\uc138\ud55c \uc124\uba85 \uc81c\uacf5"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Figure 11",src:e(9780).Z,width:"1726",height:"1029"})),(0,i.kt)("p",null,"knowledge updating \uc744 \uc704\ud574 bias tuning \uc5d0 \ub354 \ub9ce\uc740 learnable parameter \ub97c \uc218\ubc18\ud588\uc744 \ub54c, language context \uc5d0 \ub300\ud55c \uae4a\uc740 \uc774\ud574\uac00 \ud544\uc694\ud55c chatbot system \ub3c4 \uad6c\ucd95\uc774 \uac00\ub2a5\ud588\ub2e4."),(0,i.kt)("p",null,"80K conversation data \ub97c \ud6c8\ub828\uc2dc\ud0a4\uba74, \ub354 \uac15\ub825\ud55c chatbot model \uac1c\ubc1c"),(0,i.kt)("p",null,"Fig. 11 \uc740 7B \uc758 chatbot examples \uc774\uba70,"),(0,i.kt)("p",null,"\uc2dc\uc2a4\ud15c\uc740 \uc9c8\ubb38\uc5d0 \ub300\ub2f5\ud558\uc9c0\ub9cc \ubb38\ub9e5 \uc774\ud574\ub294 \uadf8\ub9ac \uc815\ud655\ud558\uc9c4 \uc54a\ub2e4."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Figure 10",src:e(7364).Z,width:"1731",height:"1543"})),(0,i.kt)("p",null,"\ubaa8\ub378\uc744 65B \uc73c\ub85c \ud655\uc7a5\ud558\uba74 (Fig. 10), chatbot \uc740 \ub354\uc6b1 \uac15\ub825\ud558\uace0 \ub300\ub2f5\ub3c4 \uc798 \ud55c\ub2e4."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Figure 5",src:e(9452).Z,width:"863",height:"1332"})),(0,i.kt)("p",null,"Fig. 5 \uc5d0\uc11c\ub294 GPT-4 \ub97c \uc0ac\uc6a9\ud558\uc5ec response quality \ud3c9\uac00."),(0,i.kt)("p",null,"LLaMA-Adapter V2 \ub294 total score \ubc0f 50/80 qustions \uc5d0 \ub300\ud574 ChatGPT \ub97c \uc774\uae30\ub294 \uc131\ub2a5 \ubcf4\uc784"),(0,i.kt)("h2",{id:"53-visual-instruction-model"},"5.3 Visual Instruction Model"),(0,i.kt)("p",null,"LLaMA-Adapter \ub294 \uc8fc\ub85c language instruction model / close-set vision-language model \uc778 \ubc18\uba74, LLaMA-Adapter V2 \ub294 caption \ubc0f language-only instruction data \uc5d0 joinly training \ud55c \uac15\ub825\ud55c vision instruction model."),(0,i.kt)("p",null,"\uc774\ubc88 \uc139\uc158\uc5d0\uc11c LLaMA-Adapter V2 \uc758 image captioning \ub2a5\ub825 \ubc0f \uc5b4\ub5bb\uac8c GPT-4 \uac19\uc740 \uc77c\ubc18\uc801\uc778 \ubaa9\uc801\uc758 multi-modal understanding \uc2dc\uc2a4\ud15c\uc73c\ub85c \ud655\uc7a5\ud558\ub294\uc9c0 \ubcf4\uc5ec\uc90c."),(0,i.kt)("p",null,"\ub610\ud55c expert system \uc744 \ud1b5\ud569\ud558\uc5ec instruction-following \ub2a5\ub825\uc744 \ub354\uc6b1 \ud5a5\uc0c1"),(0,i.kt)("h3",{id:"image-captioning"},"Image Captioning"),(0,i.kt)("p",null,"LLaMA-Adapter \ub294 \ub2e8\uc21c\ud788 adaptation prompts \uc5d0 visual feature \ub97c \ucd94\uac00\ud558\uc5ec multi-modal input \uc744 \uc9c0\uc6d0."),(0,i.kt)("p",null,"COCO Caption dataset \uc5d0 fine-tuning \ud6c4, \uac15\ub825\ud55c image captioning model \ub85c \ubcc0\ud588\ub2e4."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Table 3",src:e(847).Z,width:"872",height:"585"})),(0,i.kt)("p",null,"\uc704 \uacb0\uacfc\uc5d0\uc11c LLaMA-Adapter \uac00 \ub300\uaddc\ubaa8 image-text pretraining \uc5c6\uc774 BLIP \uacfc comparable \uacb0\uacfc \ub2ec\uc131\ud558\ub294 \uac83 \uad00\ucc30."),(0,i.kt)("p",null,"\ud558\uc9c0\ub9cc LLM \ub2a5\ub825\uc744 \uc7ac\uc0ac\uc6a9 \ubd88\uac00\ub2a5 \ubc0f \ud2b9\uc815 prompt (e.g. Generate caption for this image) \uc5d0\ub294 \ubbfc\uac10\ud558\uac8c \ub41c\ub2e4."),(0,i.kt)("hr",null),(0,i.kt)("p",null,"early fusion \ubc0f joint training \uc0ac\uc6a9\uc73c\ub85c, LLaMA-Adapter V2 \ub294 language instruction-following \ubc0f image captioning \uc774 \ub3d9\uc2dc\uc5d0 \uc218\ud589 \uac00\ub2a5\ud55c \uac15\ub825\ud55c visual instruction model \uc774 \ub410\ub2e4."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Figure 6",src:e(2759).Z,width:"1669",height:"744"})),(0,i.kt)("p",null,"\uc704\uc5d0\uc11c LLaMA-Adapter \ubc0f LLaMA-Adapter V2 \uc758 image captioning \uacb0\uacfc\ub97c \ube44\uad50\ud55c\ub2e4."),(0,i.kt)("p",null,"LLaMA-Adatper \ub294 \ub300\ub2f5\uc774 \uc9e7\uc740 \ubc18\uba74 LLaMA-Adatper V2 \ub294 natural \ud558\uace0 detail \ud55c \uc124\uba85\uc744 \uc0dd\uc131\ud55c\ub2e4."),(0,i.kt)("p",null,"Failure Case \ub97c \ubcf4\uba74, \uc758\ub3c4\uc801\uc73c\ub85c \ubd84\ud3ec \ubc16\uc758 \uc608\uc81c (\uce74\ud230\ud48d)\uc744 \uc120\ud0dd\ud588\uc744 \ub54c \ud56d\uc0c1 \uc815\ud655\ud55c \uc774\ubbf8\uc9c0 \uc124\uba85\uc744 \uc0dd\uc131\ud558\uc9c4 \uc54a\uc74c\uc744 \ubcfc \uc218 \uc788\ub2e4."),(0,i.kt)("p",null,"\uc774\ub294 image-text alignment stage \uac00 \ubd80\uc871\ud55c \uac83\uc77c \uc218 \uc788\ub2e4."),(0,i.kt)("h3",{id:"visual-understanding"},"Visual Understanding"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Figure 7",src:e(5267).Z,width:"1675",height:"836"})),(0,i.kt)("p",null,'Fig. 7 \uc5d0\uc11c \ubcf4\uc774\ub4ef, image content \uc5d0 \ub300\ud55c prompt \ub97c "why is ..." \ubc0f "what should ..." \uac19\uc740 \ud615\ud0dc\ub85c \uc9c8\ubb38\ud588\uc744 \uacbd\uc6b0, \ubaa8\ub378\uc740 visual information \uc744 language context \uc640 \ud1b5\ud569\ud558\uc5ec \ub354 \ubcf5\uc7a1\ud55c reasoning \ubc0f decision \uc744 \ud558\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\ub2e4.'),(0,i.kt)("p",null,"image \uc5d0\uc11c question \uc774 \ucc38\uc870\ud558\ub294 \uac1d\uccb4\ub098 \ud2b9\uc9d5\uc744 \uc2dd\ubcc4\ud558\uace0 \uc124\uba85\ud558\uba70, context \uae30\ubc18\uc73c\ub85c \uad00\ub828 \uc815\ubcf4\ub098 \uc81c\uc548\uc744 \ud574\uc900\ub2e4."),(0,i.kt)("p",null,"\uc774\ub294 image-text pairs \uc640 instruction data \uac04\uc758 \uac04\uc12d\uc744 \ud574\uacb0\ud558\ub294 \ud6a8\uacfc\ub97c \ubcf4\uc5ec\uc8fc\uba70, language \ubc0f vision understanding \uc774 \ubaa8\ub450 \ud544\uc694\ud55c \ud604\uc2e4 \uc138\uacc4 \uc751\uc6a9\uc5d0 \ub300\ud55c \uc7a0\uc7ac\ub825\uc744 \ubcf4\uc5ec\uc900\ub2e4."),(0,i.kt)("h3",{id:"integration-with-experts"},"Integration with Experts"),(0,i.kt)("p",null,"visual understanding \ud5a5\uc0c1\uc744 \uc704\ud574, inference \uc911 visual expert models \ub97c \ud1b5\ud569\ud558\uc5ec \ucd94\uac00\uc801\uc778 textual contexts \ub97c \uc81c\uacf5"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Figure 8",src:e(7823).Z,width:"1715",height:"921"})),(0,i.kt)("p",null,"Fig. 8 \uc5d0\uc11c caption expert \ub97c \ud3ec\ud568\ud55c LLaMA-Adapter V2 \ub97c \ubcf4\uc5ec\uc900\ub2e4."),(0,i.kt)("p",null,"image \uc758 visual contents \uc5d0 \ub300\ud55c \uc815\ud655\ud558\uace0 \uc0c1\uc138\ud55c \uc124\uba85\uc744 \uc0dd\uc131\ud55c\ub2e4."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Figure 9",src:e(8845).Z,width:"1669",height:"849"})),(0,i.kt)("p",null,"Fig. 9 \uc5d0\uc11c DocVQA \uc758 OCR expert \ub97c \uc0ac\uc6a9\ud55c \uc608\uc81c\ub97c \ubcfc \uc218 \uc788\ub2e4."),(0,i.kt)("p",null,"image \uc5d0\uc11c \uac10\uc9c0\ub41c text \ub97c \ud65c\uc6a9\ud558\uc5ec \uc548\uacbd\uc758 \uac00\uaca9 \uac19\uc740 \uad6c\uccb4\uc801\uc778 \ub2e8\uc11c\ub85c \uc9c8\ubb38\uc5d0 \ub300\ud55c \uc815\ud655\ud55c \ub2f5\ubcc0\uc744 \uc0dd\uc131."),(0,i.kt)("h1",{id:"6-conclusion"},"6. Conclusion"),(0,i.kt)("p",null,"\ubcf8 \uc5f0\uad6c\ub294 parameter-efficient visual instructions tuning system \uc778 ",(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("em",{parentName:"strong"},"LLaMA-Adapter V2"))," \uc81c\uc548"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"joint training on image-text pairs \ubc0f instruction-following data",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"\uc774\ub97c \ud1b5\ud574, pre-trained LLM \uc744 zero-shot visual instruction model \ub85c \ubcc0\ud658"),(0,i.kt)("li",{parentName:"ul"},"zero-shot visual instruction-following \uc740 image-text pairs \uc640 instruction-following data \uac04\uc758 \uac04\uc12d\uc744 \uc904\uc5ec \ub354\uc6b1 \ud5a5\uc0c1"))),(0,i.kt)("li",{parentName:"ul"},"chatbot \uacfc \uac19\uc774 \uac15\ub825\ud55c multi-turn dialog \ub2a5\ub825\uc744 \ubcf4\uc720"),(0,i.kt)("li",{parentName:"ul"},"\ubd80\uc815\ud655\ud55c \uc774\ubbf8\uc9c0 \uc124\uba85 \ubb38\uc81c \ud574\uacb0\uc744 \uc704\ud574 expert system \uacfc \ud1b5\ud569",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"expert \ud1b5\ud569\uc73c\ub85c zero-shot instruction-following \uc740 \uc218\ud589"),(0,i.kt)("li",{parentName:"ul"},"understanding \uc740 LLaVA \ubcf4\ub2e4 \ub4a4\uccd0\uc9c0\uba70, expert \ub85c\ubd80\ud130\uc758 \ubd80\uc815\ud655\ud55c \uc815\ubcf4 \uc601\ud5a5\uc744 \ubc1b\uc744 \uc218 \uc788\uc74c")))),(0,i.kt)("p",null,"\uc774\ud6c4 visual-following \ud5a5\uc0c1\uc744 \uc704\ud574 multi-modal instruction dataset \ub610\ub294 \ub2e4\ub978 PEFT \ubc29\ubc95\uc744 \ud1b5\ud55c fine-tuning \ubc29\ubc95 \ud0d0\uad6c"))}d.isMDXComponent=!0},602:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-29-229a019717e22853c28bc789c8d05fff.png"},6446:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-33-53a6717166734c2f6aa7056a16bcee9c.png"},866:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-34-efbb4d3eab485e179a1444576ca2cafe.png"},1628:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-35-7948b11188882a033992c9799c5a7900.png"},4107:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-36-a9f7acaf78eff000e051371f5d974d70.png"},6045:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-37-e405578f14a544b1492a929006d8e067.png"},9780:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-38-006ece47754f453b05f2920bc7cd9fee.png"},7364:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-39-8553d8b416783ee43574a219b98877c8.png"},9452:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-40-a2607087622f6c0b60a4c48ccd4e4112.png"},847:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-41-71617552745aedc7a82614501ee2cba7.png"},2759:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-42-e8417eaf728b9cd46b349975bca0423c.png"},5267:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-43-f90cfa28fdbbc6596240f81e6c4ed2e1.png"},7823:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-44-5af71143fd33239228c43ea5fe06593f.png"},8845:(a,t,e)=>{e.d(t,{Z:()=>n});const n=e.p+"assets/images/image-45-76090926954d4b0d31811a3c22c28a89.png"}}]);