"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[11967],{37465:i=>{i.exports=JSON.parse('{"label":"Prompt","permalink":"/docs/tags/prompt","allTagsPath":"/docs/tags","count":12,"items":[{"id":"Paper/NLP/Multi-Task/2022-01-CoT","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Multi-Task/Chain-of-Thought"},{"id":"Paper/Vision-Language/CLIP-Few-Shot/Prompting/Textual-Token/2022-03-CoCoOp","title":"Conditional Prompt Learning for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Textual-Token/CoCoOp"},{"id":"Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/2023-03-CVP","title":"Convolutional Visual Prompt for Robust Visual Perception","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/CVP"},{"id":"Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/2023-07-E2VPT","title":"E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/E2VPT"},{"id":"Paper/Vision-Language/CLIP-Few-Shot/Prompting/Textual-Token/2021-09-CoOp","title":"Learning to Prompt for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Textual-Token/CoOp"},{"id":"Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/2022-10-LPT","title":"LPT: Long-Tailed Prompt Tuning For Image Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/LPT"},{"id":"Paper/Vision-Language/CLIP-Few-Shot/Prompting/Textual-Token/2022-10-PLOT","title":"PLOT: Prompt Learning with Optimal Transport for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Textual-Token/PLOT"},{"id":"Paper/Vision-Language/CLIP-Few-Shot/Prompting/Textual-Token/2022-05-ProGrad","title":"Prompt-aligned Gradient for Prompt Tuning","description":"\ub17c\ubb38 \ubc0f image \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Textual-Token/ProGrad"},{"id":"Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/2023-12-SA2VP","title":"SA^2VPT: Spatially Aligned-and-Adapted Visual Prompt","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/SA2VP"},{"id":"Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/2022-03-VPT","title":"Visual Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/VPT"},{"id":"Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/2022-10-DePT","title":"Visual Prompt Tuning For Test-time Domain Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Visual-Token/DePT"},{"id":"Paper/Vision-Language/CLIP-Few-Shot/Prompting/Textual-Token/2023-03-KgCoOp","title":"Visual-Language Prompt Tuning with Knowledge-guided Context Optimization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/CLIP-Few-Shot/Prompting/Textual-Token/KgCoOp"}]}')}}]);