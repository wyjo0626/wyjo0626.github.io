"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[13258],{80258:i=>{i.exports=JSON.parse('{"label":"PEFT","permalink":"/docs/tags/peft","allTagsPath":"/docs/tags","count":93,"items":[{"id":"Paper/Vision-Language/PEFT/Module/2023-12-CLAP","title":"A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Module/CLAP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-11-A2XP","title":"A2XP: Towards Private Domain Generalization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/A2XP"},{"id":"Paper/NLP/PEFT/Composition/2024-02-IR-QLoRA","title":"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/IR-QLoRA"},{"id":"Paper/NLP/PEFT/Module/2020-05.AdapterFusion","title":"AdapterFusion: Non-Destructive Task Composition for Transfer Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Module/AdapterFusion"},{"id":"Paper/NLP/PEFT/Composition/2023-03-AdaLoRA","title":"Adaptive Budget Allocation For Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/AdaLoRA"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-03-ADAVIPRO","title":"AdaViPro: Region-Based Adaptive Visual Prompt For Large-Scale Models Adapting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/AdaViPro"},{"id":"Paper/NLP/PEFT/Composition/2024-03-ALoRA","title":"ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models","description":"\ub17c\ubb38 \ubc0f \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ALoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-02-ApiQ","title":"ApiQ: Finetuning of 2-Bit Quantized Large Language Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ApiQ"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-12-APrompt","title":"APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/APrompt"},{"id":"Paper/NLP/PEFT/Soft Prompt/2022-05-ATTEMPT","title":"ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/ATTEMPT"},{"id":"Paper/NLP/PEFT/Composition/2024-03-AutoLoRA","title":"AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/AutoLoRA"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-10-AutoVP","title":"AutoVP: An Automated Visual Prompting Framework and Benchmark","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/AutoVP"},{"id":"Paper/NLP/PEFT/Composition/2024-03-BiLoRA","title":"BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/BiLoRA"},{"id":"Paper/NLP/PEFT/Composition/2022-05-BitFit","title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/BitFit"},{"id":"Paper/NLP/PEFT/Quantization/2024-11-BitNet-a4.8","title":"BitNet a4.8: 4-bit Activations for 1-bit LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/BitNet-a4.8"},{"id":"Paper/NLP/PEFT/Quantization/2023-10-BitNet","title":"BitNet: Scaling 1-bit Transformers for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/BitNet"},{"id":"Paper/NLP/PEFT/Composition/2023-10-SaLoRA","title":"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/SaLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-01-COLA","title":"Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/COLA"},{"id":"Paper/Vision-Language/PEFT/Prompting/Textual-Token/2022-03-CoCoOp","title":"Conditional Prompt Learning for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Textual-Token/CoCoOp"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-03-CVP","title":"Convolutional Visual Prompt for Robust Visual Perception","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/CVP"},{"id":"Paper/NLP/PEFT/Composition/2023-09-Delta-LoRA","title":"Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/Delta-LoRA"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-09-DEPT","title":"DEPT: Decomposed Prompt Tuning For Parameter-Efficient Fine Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/DEPT"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-03-DAM-VP","title":"Diversity-Aware Meta Visual Prompting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/DAM-VP"},{"id":"Paper/NLP/PEFT/Composition/2024-05-DoRA","title":"DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank Distribution","description":"\ub17c\ubb38 \ubc0f \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/DoRA2"},{"id":"Paper/NLP/PEFT/Composition/2024-02-DoRA","title":"DoRA: Weight-Decomposed Low-Rank Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/DoRA"},{"id":"Paper/NLP/PEFT/Composition/2022-10-DyLoRA","title":"DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/DyLoRA"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-07-E2VPT","title":"E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/E2VPT"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-03-EVP-L","title":"Explicit Visual Prompting for Low-Level Structure Segmentations","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/EVP-L"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-03-VP","title":"Exploring Visual Prompts for Adapting Large-Scale Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/VP"},{"id":"Paper/NLP/PEFT/Composition/2021-08-LoHa","title":"FedPara: Low-Rank Hadamard Product For Communication-Efficient Federated Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoHa"},{"id":"Paper/NLP/PEFT/Composition/2022-05-IA\xb3","title":"Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/IA\xb3"},{"id":"Paper/NLP/PEFT/Composition/2024-02-FLoRA","title":"FLoRA: Low-Rank Adapters Are Secretly Gradient Compressors","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/FLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-08-GIFT-SW","title":"GIFT-SW: Gaussian noise Injected Fine-Tuning of Salient Weights for LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/GIFT-SW"},{"id":"Paper/NLP/PEFT/Soft Prompt/2021-03-P-tuning","title":"GPT Understands, Too","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/P-tuning"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-04-HintAug","title":"Hint-Aug: Drawing Hints from Foundation Vision Transformers towards Boosted Few-shot Parameter-Efficient Tuning","description":"\ub17c\ubb38 \ubc0f image \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/HinTAug"},{"id":"Paper/NLP/PEFT/Composition/2023-08-IncreLoRA","title":"IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/IncreLoRA"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-11-InMeMo","title":"Instruct Me More! Random Prompting for Visual In-Context Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/InMeMo"},{"id":"Paper/NLP/PEFT/Composition/2022-12-LoKr","title":"KronA: Parameter Efficient Tuning with Kronecker Adapter","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoKr"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2023-12-LaViP","title":"LaViP: Language-Grounded Visual Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/LaViP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Textual-Token/2021-09-CoOp","title":"Learning to Prompt for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Textual-Token/CoOp"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-04-LLaMA-Adapter V2","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/LLaMA-Adapter V2"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-03-LLaMA-Adapter","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/LLaMA-Adapter"},{"id":"Paper/NLP/PEFT/Quantization/2024-10-LLM-FP4","title":"LLM-FP4: 4-Bit Floating-Point Quantized Transformers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/LLM-FP4"},{"id":"Paper/NLP/PEFT/Composition/2023-10-LoftQ","title":"LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoftQ"},{"id":"Paper/NLP/PEFT/Composition/2023-09-LongLoRA","title":"LongLoRA: Efficient Fine-Tuning of LongContext Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LongLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-04-LoRA-Drop","title":"LoRA Dropout as a Sparsity Regularizer for Overfitting Control","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoRA-Dropout"},{"id":"Paper/NLP/PEFT/Composition/2024-03-HiddenKey","title":"LoRA Meets Dropout under a Unified Framework","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/HiddenKey"},{"id":"Paper/NLP/PEFT/Composition/2021-06-LoRA","title":"LoRA: Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-02-LoRA+","title":"LoRA+: Efficient Low Rank Adaptation of Large Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LoRA+"},{"id":"Paper/NLP/PEFT/Composition/2024-06-LR-QAT","title":"Low-Rank Quantization-Aware Training for LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/LR-QAT"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-10-LPT","title":"LPT: Long-Tailed Prompt Tuning For Image Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/LPT"},{"id":"Paper/NLP/PEFT/Composition/2024-02-MeLoRA","title":"MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/MeLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-06-MiLoRA","title":"MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/MiLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-06-MoSLoRA","title":"Mixture-of-Subspaces in Low-Rank Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/MoSLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-05-MoRA","title":"MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/MoRA"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-03-MPT","title":"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/Multitask Prompt Tuning"},{"id":"Paper/NLP/PEFT/Composition/2024-06-OLoRA","title":"OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/OLoRA"},{"id":"Paper/NLP/PEFT/Quantization/2025-01-fp4","title":"Optimizing Large Language Model Training Using FP4 Quantization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/fp4"},{"id":"Paper/NLP/PEFT/Soft Prompt/2021-10-P-tuning v2","title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/P-tuning v2"},{"id":"Paper/Survey/2024-02-PEFT Vision","title":"Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Survey/PEFT for PVMs"},{"id":"Paper/NLP/PEFT/Module/2019-02-Adapter","title":"Parameter-Efficient Transfer Learning for NLP","description":"\ub17c\ubb38 \uc774\ubbf8\uc9c0 \ubc0f \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Module/Adapter"},{"id":"Paper/NLP/PEFT/Mixture/2023-12-No-Gradients","title":"Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Mixture/PEFT without Its Gradients"},{"id":"Paper/NLP/PEFT/Composition/2024-04-PISSA","title":"PISSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/PISSA"},{"id":"Paper/Vision-Language/PEFT/Prompting/Textual-Token/2022-10-PLOT","title":"PLOT: Prompt Learning with Optimal Transport for Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Textual-Token/PLOT"},{"id":"Paper/NLP/PEFT/Soft Prompt/2021-01-Prefix-Tuning","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/Prefix-Tuning"},{"id":"Paper/Vision-Language/PEFT/Prompting/Textual-Token/2022-05-ProGrad","title":"Prompt-aligned Gradient for Prompt Tuning","description":"\ub17c\ubb38 \ubc0f image \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Textual-Token/ProGrad"},{"id":"Paper/NLP/Augmentation/2023-05-PromptDA","title":"PromptDA : Label-guided Data Augmentation for Prompt-based Few Shot Learners","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Augmentation/PromptDA"},{"id":"Paper/NLP/PEFT/Pruning/2023-07-SMP","title":"Pruning Pre-trained Language Models Without Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Pruning/SMP"},{"id":"Paper/NLP/Prompt Tuning/2022-11-PTR","title":"PTR: Prompt Tuning with Rules for Text Classification","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","permalink":"/docs/Paper/NLP/Prompt Tuning/PTR"},{"id":"Paper/NLP/PEFT/Composition/2023-09-QA-LoRA","title":"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/QA-LoRA"},{"id":"Paper/NLP/PEFT/Composition/2023-05-QLoRA","title":"QLORA: Efficient Finetuning of Quantized LLMs","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/QLoRA"},{"id":"Paper/NLP/PEFT/Composition/2023-07-ReLoRA","title":"ReLoRA: High-Rank Training Through Low-Rank Updates","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ReLoRA"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-05-Residual-Prompt-Tuning","title":"Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98:","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/Residual Prompt Tuning"},{"id":"Paper/NLP/PEFT/Composition/2024-02-ResLoRA","title":"ResLoRA: Identity Residual Mapping in Low-Rank Adaption","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/ResLoRA"},{"id":"Paper/NLP/PEFT/Composition/2024-06-RoseLoRA","title":"RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/RoseLoRA"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2023-12-SA2VP","title":"SA^2VPT: Spatially Aligned-and-Adapted Visual Prompt","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/SA2VP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2024-06-SMM","title":"Sample-specific Masks for Visual Reprogramming-based Prompting","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/SMM"},{"id":"Paper/NLP/PEFT/Composition/2024-02-SIBO","title":"SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/SIBO"},{"id":"Paper/NLP/PEFT/Composition/2024-06-SinkLoRA","title":"SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/SinkLoRA"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-12-SMoP","title":"SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/SMoP"},{"id":"Paper/NLP/PEFT/Composition/2023-11-SoRA","title":"Sparse Low-rank Adaptation of Pre-trained Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Composition/SoRA"},{"id":"Paper/NLP/PEFT/Soft Prompt/2022-05-SPoT","title":"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/SPoT"},{"id":"Paper/Vision-Language/PEFT/Module/2022-11-TaskRes","title":"Task Residual for Tuning Vision-Language Models","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Module/TaskRes"},{"id":"Paper/NLP/PEFT/Quantization/2024-02-BitNet-1.58b","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Quantization/BitNet-1.58B"},{"id":"Paper/NLP/PEFT/Soft Prompt/2021-04-Prompt-Tuning","title":"The Power of Scale for Parameter-Efficient Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/Prompt Tuning"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-11-ILM-VP","title":"Understanding and Improving Visual Prompting: A Label-Mapping Perspective","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/ILM-VP"},{"id":"Paper/NLP/PEFT/Mixture/2022-05-UniPELT","title":"UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Mixture/UniPELT"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-12-EVP","title":"Unleashing the Power of Visual Prompting At the Pixel Level","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/EVP"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-03-VPT","title":"Visual Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/VPT"},{"id":"Paper/Vision-Language/PEFT/Prompting/Visual-Token/2022-10-DePT","title":"Visual Prompt Tuning For Test-time Domain Adaptation","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Visual-Token/DePT"},{"id":"Paper/Vision-Language/PEFT/Prompting/Textual-Token/2023-03-KgCoOp","title":"Visual-Language Prompt Tuning with Knowledge-guided Context Optimization","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Textual-Token/KgCoOp"},{"id":"Paper/Vision-Language/PEFT/Prompting/Pixel-Level/2022-10-Watermarking","title":"Watermarking for Out-of-distribution Detection","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/PEFT/Prompting/Pixel-Level/Watermarking"},{"id":"Paper/NLP/PEFT/Soft Prompt/2022-12-XPrompt","title":"XPrompt: Exploring the Extreme of Prompt Tuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/XPrompt"}]}')}}]);