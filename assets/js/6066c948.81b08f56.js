"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[75116],{14265:e=>{e.exports=JSON.parse('{"label":"LLaMA","permalink":"/docs/tags/l-la-ma","allTagsPath":"/docs/tags","count":5,"items":[{"id":"Paper/NLP/Generalization/2023-12-SymNoise","title":"`SymNoise`: Advancing Language Model Fine-tuning with Symmetric Noise","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Generalization/SymNoise"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-04-LLaMA-Adapter V2","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/LLaMA-Adapter V2"},{"id":"Paper/NLP/PEFT/Soft Prompt/2023-03-LLaMA-Adapter","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/PEFT/Soft Prompt/LLaMA-Adapter"},{"id":"Paper/NLP/Generalization/2023-10-NEFTune","title":"NEFTune: Noisy Embeddings Improve Instruction FineTuning","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Generalization/NEFTune"},{"id":"Paper/NLP/Generalization/2022-02-NoisyTune","title":"NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Generalization/NoisyTune"}]}')}}]);