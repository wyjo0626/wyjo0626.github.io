"use strict";(self.webpackChunkWon_Yong_Jo=self.webpackChunkWon_Yong_Jo||[]).push([[68985],{34575:e=>{e.exports=JSON.parse('{"label":"transformer","permalink":"/docs/tags/transformer","allTagsPath":"/docs/tags","count":4,"items":[{"id":"Paper/NLP/Model/2017-06-Transformer","title":"Attention Is All You Need","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Model/Transformer"},{"id":"Paper/NLP/Model/2018-10-BERT","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","description":"\ub17c\ubb38 \uc774\ubbf8\uc9c0 \ubc0f \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Model/BERT"},{"id":"Paper/NLP/Model/2018-06-GPT","title":"Improving Language Understanding by Generative Pre-Training","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/NLP/Model/GPT-1"},{"id":"Paper/Vision-Language/Two-Stream/2019-08-LXMERT","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers","description":"\ub17c\ubb38 \ubc0f \uc774\ubbf8\uc9c0 \ucd9c\ucc98 :","permalink":"/docs/Paper/Vision-Language/Two-Stream/LXMERT"}]}')}}]);